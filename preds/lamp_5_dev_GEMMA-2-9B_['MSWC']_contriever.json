{"task": "LaMP_5", "golds": [{"id": "410", "output": "Skewed Stable Random Projections for Estimating Frequency Moments of Dynamic Data Streams", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe method of stable random projections(39, 41) is popular for data streaming computations, data mining, and machine learning. For example, in data streaming, stable random projections offer a unified, efficient, and elegant methodology for approximating the l\u03b1 norm of a single data stream, or the l\u03b1 distance between a pair of streams, for any 0 < \u03b1 \u2264 2. (18) and (20) applied stable random projections for approximating the Hamming norm and the max-dominance norm, respectively, using very small \u03b1. Another application is to approximate all pairwise l\u03b1 distances in a data matrix to speed up clustering, classifica tion, or kernel computations. Given that stable random projections have been successful in various applications, this paper will focus on three different aspects in improving the current practice of stable random projections. Firstly, we propose very sparse stable random projectionsto significantly reduce the processing and storage cost, by replacing the \u03b1-stable distribution with a mixture of a symmetric \u03b1-Pareto distribution (with probability \u03b2, 0 < \u03b2 \u2264 1) and a point mass at the origin (with a probability 1 \u2212 \u03b2). This leads to a significant 1 \u03b2 -fold speedup for small \u03b2. We analyze the rate of convergence as a function of \u03b2, \u03b1, and the data regularity conditions. For example, when \u03b1 = 1 and the data have bounded second moments, then if we choose \u03b2 = 1 D1/2 , the rate of convergence would be O D \u22121/2 \ufffd , which is fast even for moderate D.\nTitle:\nVery Sparse Stable Random Projections, Estimators and Tail Bounds for Stable Random Projections\n\nAbstract:\nCompressed Counting (CC) was recently proposed for approximating the \u03b1th frequency moments of data streams, for 0 < \u03b1 \u2264 2. Under the relaxed strict-Turnstile model, CC dramatically improves the standard algorithm based on symmetric stable random projections, especially as \u03b1 \u2192 1. A direct application of CC is to estimate the entropy, which is an important summary statistic in Web/network measure- ment and often serves a crucial \"feature\" for data mining. The Renyi entropy and the Tsallis entropy are functions of the \u03b1th frequency moments; and both approach the Shan- non entropy as \u03b1 \u2192 1. A recent theoretical work suggested using the \u03b1th frequency moment to approximate the Shan- non entropy with \u03b1 = 1+\u03b4 and very small |\u03b4| (e.g., < 10\u22124). In this study, we experiment using CC to estimate fre- quency moments, Renyi entropy, Tsallis entropy, and Shan- non entropy, on real Web crawl data. We demonstrate the variance-bias trade-off in estimating Shannon entropy and provide practical recommendations. In particular, our ex- periments enable us to draw some important conclusions: \u2022 As \u03b1 \u2192 1, CC dramatically improves symmetric stable random projections in estimating frequency moments, Renyi entropy, Tsallis entropy, and Shannon entropy. The improvements appear to approach \"infinity.\"\nTitle:\nA Very Efficient Scheme for Estimating Entropy of Data Streams Using Compressed Counting\n\nAbstract:\nThe method of stable random projections is popular in data stream computations, data mining, information retrieval, and machine learning, for efficiently computing the l\u03b1 (0 < \u03b1 \u2264 2) distances using a small (memory) space, in one pass of the data.\n\nWe propose algorithms based on (1) the geometric mean estimator, for all 0 <\u03b1 \u2264 2, and (2) the harmonic mean estimator, only for small \u03b1 (e.g., \u03b1 < 0.344). Compared with the previous classical work [27], our main contributions include:\n\n\u2022 The general sample complexity bound for \u03b1 \u2260 1,2.\n\nFor \u03b1 = 1, [27] provided a nice argument based on the inverse of Cauchy density about the median, leading to a sample complexity bound, although they did not provide the constants and their proof restricted \u03b5 to be \"small enough.\"\n\nFor general \u03b1 \u2260 1, 2, however, the task becomes much more difficult. [27] provided the \"conceptual promise\" that the sample complexity bound similar to that for \u03b1 = 1 should exist for general \u03b1, if a \"non-uniform algorithm based on t-quantile\" could be implemented. Such a conceptual algorithm was only for supporting the arguments in [27], not a real implementation. We consider this is one of the main problems left open in [27]. In this study, we propose a practical algorithm based on the geometric mean estimator and derive the sample complexity bound for all 0 < \u03b1 \u2264 2.\n\n\u2022 The practical and optimal algorithm for \u03b1 = 0+\n\nThe l0 norm is an important case. Stable random projections can provide an approximation to the l0 norm using \u03b1 \u2192 0+. We provide an algorithm based on the harmonic mean estimator, which is simple and statistically optimal. Its tail bounds are sharper than the bounds derived based on the geometric mean. We also discover a (possibly surprising) fact: in boolean data, stable random projections using \u03b1 = 0+ with the harmonic mean estimator will be about twice as accurate as (l2) normal random projections. Because high-dimensional boolean data are common, we expect this fact will be practically quite useful.\n\n\u2022 The precise theoretical analysis and practical implications We provide the precise constants in the tail bounds for both the geometric mean and harmonic mean estimators. We also provide the variances (either exact or asymptotic) for the proposed estimators. These results can assist practitioners to choose sample sizes accurately.\n\n\nTitle:\nEstimators and tail bounds for dimension reduction in l\u03b1 (0 < \u03b1 \u2264 2) using stable random projections.\n\nAbstract:\nEstimating the p-th frequency moment of data stream is a very heavily studied\nproblem. The problem is actually trivial when p = 1, assuming the strict\nTurnstile model. The sample complexity of our proposed algorithm is essentially\nO(1) near p=1. This is a very large improvement over the previously believed\nO(1/eps^2) bound. The proposed algorithm makes the long-standing problem of\nentropy estimation an easy task, as verified by the experiments included in the\nappendix.\nTitle:\nOn Practical Algorithms for Entropy Estimation and the Improved Sample Complexity of Compressed Counting\n\nAbstract:\nThe method of stable random projections is a useful tool for efficiently computing the l(alpha) (0 < alpha <= 2) norms and distances in lmassive data in one pass. Consider a data matrix A is an element of R-nxD. If we multiply A with a projection matrix R is an element of R-DXk (k << D), whose entries are i.i.d. samples of an a-stable distribution, then the projected matrix B = A x R is an element of R-nXk contains enough information to approximately recover the l(alpha) properties in A. We propose very sparse stable random projections, by replacing the a-stable distribution with a (much simpler) mixture of a symmetric alpha-Pareto distribution (with probability beta, 0 < beta <= 1) and a point mass at the origin (with probability 1 - beta). This leads to a significant 1/beta-fold speedup for small beta when computing B = A x R and a 1/beta-fold cost reduction in storing R. By analyzing the convergence, we show that in \"reasonable\" datasets beta often can be very small (e.g., D-1/2) without hurting the estimation accuracy. Some numerical evaluations are conducted, on synthetic data, Web crawl data, and gene expression microarray data.\nTitle:\nVery sparse stable random projections for dimension reduction in lalpha (0 &lt;alpha<=2) norm\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.547, mean value for all the writers is 8.648, which makes it 0.086 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose skewed stable random projectionsfor approximating the \u03b1th frequency moments of dynamic data streams (0 < \u03b1 \ufffd 2). We show the sample complexity (number of projections) k = G 1 \u01eb2 log ` 2 \u03b4 \u00b4 , where G ! \u01eb 2 log(1+\u01eb) = O (\u01eb) as \u03b1 ! 1, i.e., \u03b1 = 1 \u00b1 \ufffd with \ufffd ! 0. Previous results based on symmetric stable random projections(12, 16) required G = non-zero constant + O(\u01eb), even when \ufffd = 0. The case \ufffd ! 0 is practically important. For example, \ufffd might be the \"decay rate\" or \"interest rate,\" which is usuall y small; and hence one might view skewed stable random projectionsas a \"generalized counter\" for estimating the total value in the future, taking in account of the effect of decaying or i nterest accruement. We consider the popular Turnstile data stream model. The input data stream at = (i, It) arriving sequentially describes the underlying signal A, meaning At(i) = At 1(i) + It, i 2 (1, D). We allow the increment It to be either positive (i.e., insertion) or negative (i.e., del etion). By definition, the \u03b1th frequency moment F(\u03b1) = PD i=1 |At(i)| \u03b1. Our method only requires that, at the time t for the evaluation, A\nTitle:", "model_inf_time": 1.99}, {"id": "411", "output": "The Eyes: Sensor and Browser of the Mind", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn neuropsychology it is commonly recognized that 70% of all the sensory receptors in the brain are inputted from the eyes. However, an important internal cognitive function of the eyes as the perceptual browser of the memory and the mind has not yet been recognized. This paper contrasts the cognitive mechanisms of the eyes as both the sensor of the brain externally and the browser of the mind internally. The key assertion is that the eyes function as a bi-directional organ: a visual sensor of the brain, more important, a perceptual browser of the mind. The sensory of the brain can be categorized into external and internal senses. The former encompass vision, auditory, smell, tactility, and tastes. The latter refers to perceptivity that forms the sixth sense of the brain. The perceptual sense encompasses consciousness, memory searching, motivation, willingness, goal setting, emotion, sense of spatiality, and sense of motion. The reveal of the internal perceptual mechanisms of the eyes is not only theoretically significant to identify the physiological organ of the thinking engine of the brain, but also practically useful to explain a wide range of cognitive mechanisms of he brain and mind.\nTitle:\nOn cognitive mechanism of the eyes: the sensor vs. the browser of the brain\n\nAbstract:\nThe human brain is the most complicated organ in the universe and a new frontier yet to be explored in an interdisciplinary approach. Investigation of the brain is a unique problem that requires recursive mental power to explore the brain using the brain. This paper attempts to develop functional and cognitive models of the brain by using cognitive informatics and formal methodologies. This paper adopts a memory-based approach to explore the brain, and to demonstrate that memory is the foundation for any natural intelligence. Structures of memories are explored and cognitive models of the natural intelligence are proposed. Cognitive mechanisms of the brain, including hypotheses and theories on the thinking engine of the brain, long-term memory establishment, and roles of sleep in long-term memory development, are investigated. The models and theories are applied to explain a number of fundamental physiological and psychological phenomena.\nTitle:\nCognitive models of the brain\n\nAbstract:\nThis paper presents the cognitive foundations and processes of consciousness and attention. It explores one of the profound myths in cognitive informatics, brain science, and intelligence science, and explains how abstract consciousness is generated by physical and physiological organs. Consciousness as the sense of self and the sign of life is systematically studied. The hierarchical levels of consciousness and its generation are analyzed. Then, the mathematical models and cognitive process of consciousness are developed that elaborate the nature of consciousness with a computational intelligence treatment. As an important part of consciousness, attention is formally modeled in mathematics and the cognitive process of attention is elaborated.\nTitle:\nThe Cognitive Processes Of Consciousness And Attention\n\nAbstract:\nThis paper presents a cognitive informatics theory of visual information and knowledge processing in the brain and natural intelligence. A set of cognitive principles of visual perception is reviewed, such as the gestalt principles, the cognitive informatics principles, and the hypercolumn theory. A visual frame theory is developed to explain the visual information processing mechanisms of human vision, where the size of a unit visual frame is tested and calibrated based on vision experiments. Then, the framework of human visual information processing is established. Based on it, the mechanisms of visual information processing and the compatibility of internal representations between visual and abstract information and knowledge are elaborated.\nTitle:\nA Cognitive Informatics Theory For Visual Information Processing\n\nAbstract:\nThe human brain is a superbly marvelous and extremely complicated neurophysiological structure for generating natural intelligence that transforms cognitive information into colorful behaviors. The brain is the most complex and interesting objects in nature that requires rigorous scientific investigations by multidisciplinary methodologies and via transdisciplinary approaches where only low-level studies could not explain it. A fundamental problem and difficulty in contemporary brain science is the indistinguishable confusion of the cognitive mechanisms and neurophysiological structures of the kernel brain and its memories. This paper presents a set of formal neuroinformatics models of memory and a rigorous mapping between the cognitive functions of memory and their neurophysiological structures. The neurophysiological foundations of memory are rigorously described based on comprehensive cognitive models of memory. The cognitive architecture of human memory and its relationship to the intelligence power of the brain are logically analyzed. The cognitive roles of memory allocated in both cerebrum and cerebellum are revealed by mapping the functional models of memory onto corresponding neurophysiological structures of the brain. As a result, fundamental properties of memory and knowledge as well as their neurophysiological forms in the brain are systematically explained.\nTitle:\nNeuroinformatics Models of Human Memory: Mapping the Cognitive Functions of Memory onto Neurophysiological Structures of the Brain\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.372, mean value for all the writers is 8.648, which makes it 0.618 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nEyes as the unique organ possess intensively direct connections to the brain and dynamically perceptual accessibility to the mind. This paper analyzes the cognitive mechanisms of eyes not only as the sensory of vision, but also the browser of internal memory in thinking and perception. The browse function of eyes is created by abstract conditioning of the eye's tracking pathway for accessing internal memories, which enables eye movements to function as the driver of the perceptive thinking engine of the brain. The dual mechanisms of the eyes as both the external sensor of the brain and the internal browser of the mind are explained based on evidences and cognitive experiences in cognitive informatics, neuropsychology, cognitive science, and brain science. The finding on the experiment's internal browsing mechanism of eyes reveals a crucial role of eyes interacting with the brain for accessing internal memory and the cognitive knowledge base in thinking, perception, attention, consciousness, learning, memorization, and inference.\nTitle:", "model_inf_time": 1.32}, {"id": "412", "output": "Joint Code-Decoder-Encoder Design for High-Performance ( )-Regular LDPC Coding Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents a design approach for low-density parity-check (LDPC) coding system hardware implementation by jointly conceiving irregular LDPC code construction and VLSI implementations of encoder and decoder. The key idea is to construct good irregular LDPC codes subject to two constraints that ensure the effective LDPC encoder and decoder hardware implementations. We propose a heuristic algorithm to construct such implementation-aware irregular LDPC codes that can achieve very good error correction performance. The encoder and decoder hardware architectures are correspondingly presented.\nTitle:\nJoint code-encoder-decoder design for LDPC coding system VLSI implementation\n\nAbstract:\nThis paper presents a joint low-density parity-check (LDPC) code-encoder-decoder design approach, called Block-LDPC, for practical LDPC coding system implementations. The key idea is to construct LDPC codes subject to certain hardware-oriented constraints that ensure the effective encoder and decoder hardware implementations. We develop a set of hardware-oriented constraints, subject to which a se...\nTitle:\nBlock-LDPC: a practical LDPC coding system design approach.\n\nAbstract:\nIn this paper, we investigate an efficient encoding approach for generalized low-density (GLD) parity check codes, a generalization of Gallager's (1962, 1963) low-density parity check (LDPC) codes. We propose a systematic approach to construct an approximate upper triangular GLD parity check matrix which defines a class of efficient-encoding GLD codes. It is shown that such GLD codes have equally good performance. By effectively exploiting structure sharing in the encoding process, we also present a hardware/software codesign for practical encoder implementation of these efficient-encoding GLD codes\nTitle:\nA class of efficient-encoding generalized low-density parity-check codes\n\nAbstract:\nBy implementing an FPGA-based simulator, we investigate the performance of high-rate quasi-cyclic (QC) LDPC codes for the magnetic recording channel at very low sector error rates. Results show that error-floor-free performance can be realized by randomly constructed high-rate regular QC-LDPC codes with column weight 4 for sector error rates as low as 10(-9). We also conjecture several rules for designing randomly constructed high-rate regular QC-LDPC codes with low error floor. We also present a decoder architecture that is well suited to achieving high decoding throughput for these high-rate QC-LDPC codes with low error floor.\nTitle:\nHigh-Rate Quasi-Cyclic Ldpc Codes For Magnetic Recording Channel With Low Error Floor\n\nAbstract:\n  This paper investigates the application of low-density parity-check (LDPC) codes to Flash memories. Multiple cell reads with distinct word-line voltages provide limited-precision soft information for the LDPC decoder. The values of the word-line voltages (also called reference voltages) are optimized by maximizing the mutual information (MI) between the input and output of the multiple-read channel. Constraining the maximum mutual-information (MMI) quantization to enforce a constant-ratio constraint provides a significant simplification with no noticeable loss in performance.   Our simulation results suggest that for a well-designed LDPC code, the quantization that maximizes the mutual information will also minimize the frame error rate. However, care must be taken to design the code to perform well in the quantized channel. An LDPC code designed for a full-precision Gaussian channel may perform poorly in the quantized setting. Our LDPC code designs provide an example where quantization increases the importance of absorbing sets thus changing how the LDPC code should be optimized.   Simulation results show that small increases in precision enable the LDPC code to significantly outperform a BCH code with comparable rate and block length (but without the benefit of the soft information) over a range of frame error rates. \nTitle:\nLDPC Decoding with Limited-Precision Soft Information in Flash Memories\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.691, mean value for all the writers is 8.648, which makes it 2.596 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the past few years, Gallager's Low-Density Parity-Check (LDPC) codes have received a lot of atten- tion and tremendous efforts have been devoted to analyze and improve their error-correcting performance. However, little consideration has been given to the practical LDPC codec hardware implementations. The straightforward fully parallel LDPC decoder architecture usually incurs too high complexity for many prac- tical purposes, thus effective partly parallel decoder design approaches are highly desirable. Due to the randomness of LDPC code, it is extremely difficult, if not impossible, to develop a direct transformation from fully parallel architecture to partly parallel ones for a given LDPC code. Meanwhile, also because of its randomness, the direct LDPC encoding scheme has quadratic complexity in the block length, which makes the efficient encoder design not trivial. We believe that jointly conceiving code construction and decoder/encoder design is a promising direction to develop high performance LDPC coding systems. In this paper, we propose a joint ( )-regular LDPC code and decoder/encoder design approach to construct a class of ( )-regular LDPC codes that not only have very good performance but also exactly fit to a high- speed partly parallel decoder and low-complexity encoder. Moreover, we propose a modified joint design approach in order to further reduce the decoder hardware complexity for those high-rate ( )-regular LDPC codes applied to silicon area critical applications.\nTitle:", "model_inf_time": 1.79}, {"id": "413", "output": "Efficient Algorithms for Optimizing the Sum of Linear Fractional Functions in 2-D", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents an improved algorithm for solving the sum of linear fractional functions (SOLF) problem in 1-D and 2-D. A key subproblem to our solution is the off-line ratio query (OLRQ) problem, which asks to find the optimal values of a sequence of m linear fractional functions (called ratios), each ratio subject to a feasible domain defined by O(n) linear constraints. Based on some geometric properties and the parametric linear programming technique, we develop an algorithm that solves the OLRQ problem in O((m+n)log (m+n)) time. The OLRQ algorithm can be used to speed up every iteration of a known iterative SOLF algorithm, from O(m(m+n)) time to O((m+n)log (m+n)), in 1-D and 2-D. Implementation results of our improved 1-D and 2-D SOLF algorithm have shown that in most cases it outperforms the commonly-used approaches for the SOLF problem. We also apply our techniques to some problems in computational geometry and other areas, improving the previous results.\nTitle:\nEfficient Algorithms and Implementations for Optimizing the Sum of Linear Fractional Functions, with Applications\n\nAbstract:\nIn this paper, we present techniques and algorithms for processing an offline sequence of insertion and query operations and\n for related problems. Most problems we consider are solved optimally in linear time by our algorithms, which are based on\n a new geometric modeling and interesting techniques. We also discuss some applications to which our algorithms and techniques\n can be applied to yield efficient solutions.\n \nTitle:\nProcessing an Offline Insertion-Query Sequence with Applications\n\nAbstract:\nWe study the problems of processing single-source and all-pairs shortest path queries among weighted polygonal obstacles in the rectilinear plane. For the single-source case, we construct a data structure in O(n log3/2 n) time and O(n log n) space, where n is the number of obstacle vertices; this data structure enables us to report the length of a shortest path between the source and an arbitrary query point in O (log n) time, and an actual shortest path in O(log n. + k) time, where k is the number of line segments on the output path. For the all-pairs case, we construct a data structure in 0(n2 log2 n) time and space; this data structure enables us to report the length of a shortest path between two arbitrary query points in O (log2 n) time, and an actual shortest path in 0(log2 n + k) time. Our work improves and generalizes the previously best known results on computing rectilinear shortest paths among weighted polygonal obstacles. We also apply our techniques to solve other rectilinear shortest path query problems.\nTitle:\nShortest path queries among weighted obstacles in the rectilinear plane\n\nAbstract:\nWe present $O(\\min\\{n\\log^{1.5} n, n\\log n+k^2\\log^2\\frac{n}{k}\\log^2 n\\})$ time algorithms for the weighted k-problem on a real line. Previously, the best known algorithms for this problem take O(nlog2n) time, or O(knlogn) time, or a time linear in n but exponential in k. Our techniques involve developing efficient data structures for processing queries that find a lowest point in the common intersection of a certain subset of half-planes. This subproblem is interesting in its own right.\nTitle:\nEfficient algorithms for the weighted k-center problem on a real line\n\nAbstract:\nWe consider the problem of finding k centers for n weighted points on a real line. This (weighted) k-center problem was solved in O ( n log \u00bf n ) time previously by using Cole's parametric search and other complicated approaches. In this paper, we present an easier O ( n log \u00bf n ) time algorithm that avoids the parametric search, and in certain special cases our algorithm solves the problem in O ( n ) time. In addition, our techniques involve developing interesting data structures for processing queries that find a lowest point in the common intersection of a certain subset of half-planes. This subproblem is interesting in its own right and our solution for it may find other applications as well.\nTitle:\nEfficient algorithms for the one-dimensional k-center problem\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.628, mean value for all the writers is 8.648, which makes it 0.017 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe problem of optimizing the sum of m linear fractional functions (SOLF) in a fixed dimension d, subject to n linear constraints, arises in a number of theoretical and applied areas. This paper presents an improved algorithm for solving the SOLF problem in 2-D. A key subproblem to our solution is the off-line ratio query (OLRQ) problem, which computes the optimal values of a sequence of m linear fractional functions(called ratios), with the ratios subject to a dynamically changing feasible domain defined by O(n) linear constraints. Based on useful geometric properties and the parametric linear programming technique, we develop an algorithm that solves the 2-D OLRQ problem in O((m + n)log(m. + n)) time. Our OLRQ algorithm can be easily implemented and is robust. More importantly, it enables us to speed up every iteration of a known iterative SOLF algorithm in 2-D, from O(m(m + n)) time to O((m + n) log(m + n)). Implementation results of our improved SOLF algorithm have shown that in most cases our algorithm outperforms the commonly-used approaches for the SOLF problem. We also show that several geometric optimization problems can be formulated as 2-D SOLF problems, and hence are solvable by our algorithm.\nTitle:", "model_inf_time": 1.65}, {"id": "414", "output": "Performance Evaluation of Topology Control Algorithms in Wireless Sensor Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nPower efficient operation is very critical in energy constrained multi-hop wireless networks. One important technique is to intelligently assign transmission powers to nodes while maintaining network connectivity. Previous work has focused on assigning a single transmission power to each node. This often leads to \"power imbalance\", where some nodes use much more power than other nodes. This can reduce network lifetime. In this paper, we investigate the problem of two power assignments where nodes alternate the use of these assigned powers. We rigorously formulate the problem of two power assignment under the constraint that the network connectivity is maintained. The objective here is to minimize the maximum average power used by the nodes. We show that, in general, the problem is not just NP-hard but also hard to approximate. We then propose a distributed localized heuristic to compute the two power assignments. We perform extensive simulations to show that the algorithm can reduce the average power significantly when compared with algorithms that assign a single power. By assuming some properties on radio propagation, we also present a centralized algorithm with bounded worst case guarantees for the two power assignment problem.\nTitle:\nThe Power Balancing Problem In Energy Constrained Multi-Hop Wireless Networks\n\nAbstract:\nWe propose a protocol that, given a communication network, computes a\nsubnetwork such that, for every pair $(u,v)$ of nodes connected in the original\nnetwork, there is a minimum-energy path between $u$ and $v$ in the subnetwork\n(where a minimum-energy path is one that allows messages to be transmitted with\na minimum use of energy). The network computed by our protocol is in general a\nsubnetwork of the one computed by the protocol given in [13]. Moreover, our\nprotocol is computationally simpler. We demonstrate the performance\nimprovements obtained by using the subnetwork computed by our protocol through\nsimulation.\nTitle:\nMinimum-Energy Mobile Wireless Networks Revisited\n\nAbstract:\nWe present a general model of interdomain route selection to study interdomain traffic engineering. In this model, the routing of multiple destinations can be coordinated. Thus the model can capture general traffic engineering behaviors such as load balancing and link capacity constraints. We first identify potential routing instability and inefficiency of interdomain traffic engineering. We then derive a sufficient condition to guarantee convergence. We also show that the constraints on local policies imposed by business considerations in the Internet can guarantee stability without global coordination. Using realistic Internet topology, we evaluate the extent to which routing instability of interdomain traffic engineering can happen when the constraints are violated.\nTitle:\nStable Egress Route Selection for Interdomain Traffic Engineering: Model and Analysis\n\nAbstract:\nMultiple input multiple output (MIMO) antennas use sophisticated physical layer techniques to provide significant benefits over conventional antenna technology. Multiple independent data streams can be sent over the MIMO antenna elements. MIMO link can also suppress interference from neighboring links as long as the total useful streams and interfering streams are no greater than the number of receiving antenna elements. For these reasons MIMO antennas are increasingly being considered for use in interference limited wireless mesh networks and have been adopted by WLAN and WIMAX standards. However, the benefits of the MIMO technology in improving network performance are limited unless the higher layer protocols also exploit these capabilities. In this paper we are interested in characterizing the benefits of cross-layer optimizations in interference limited wireless mesh networks with MIMO links. We formulate a framework where data routing at the protocol layer, link scheduling at the MAC layer and stream control at the physical layer can be jointly optimized for throughput maximization in the presence of interference. We then develop an efficient algorithm to solve the resulting throughput optimization problem subject to fairness constraints.\nTitle:\nThroughput Optimization of Wireless Mesh Networks with MIMO Links\n\nAbstract:\nInformation on network host connectivity patterns are important for network monitoring and traffic engineering. In this paper, an efficient streaming algorithm is proposed to estimate cardinality distributions including connectivity distributions, e.g. percent of hosts with any given number of distinct communicating peers or flows.\nTitle:\nEstimating cardinality distributions in network traffic: extended abstract\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.514, mean value for all the writers is 8.648, which makes it 0.114 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the context of multi-hop wireless networks, various topology control algorithms have been proposed to adapt the transmission range of nodes based on local information while maintaining a connected topology. These algorithms are particularly suited for deployment in sensor networks which typically consist of energy constrained sensors. Sensor nodes should support power adaptation in order to use the benefits of topology control for energy conservation. In this paper, we design a framework for evaluating the performance of topology control algorithms using overall network throughput, and total energy consumption per packet delivered, as the metrics. Our goal is to identify the scenarios in which topology control improves the network performance. We supplement our analysis with ns2 simulations using the cone-based topology control algorithm [10, 19].Based on our analysis and simulations, we find that link layer retransmissions are essential with topology control to avoid throughput degradation due to increase in number of hops in lightly loaded networks. In heavily loaded networks, the throughput can be improved by a factor up to k2, where k is the average factor of reduction in transmission range using topology control. Studies of energy consumption reveal that improvements of up to $k^4$ can be obtained using topology control. However, these improvements decrease as the traffic pattern shifts from local (few hop connections) to non-local (hop lengths of the order of the diameter of the network). These results can be used to guide the deployment of topology control algorithms in sensor networks.\nTitle:", "model_inf_time": 1.36}, {"id": "415", "output": "The 3-D Radon Transform Approach to Fan Filtering of Image Sequences", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA new approach to designing fan filters for moving objects that is based on mapping from velocity sets to slowness sets is presented. All of the complexity in the application of fan filters to image sequences results from the velocity-slowness mapping not being one-to-one, so the filter response cannot be independently specified at all velocities. Despite this limitation, the fan filter promises to be a powerful tool in image sequence analysis. Both the power and the limitations of the fan filter in this new application are elucidated\nTitle:\nThe role of the velocity-slowness mapping in fan filtering of image sequences\n\nAbstract:\nBroad-band (e.g. white) noise is sometimes the limiting factor in the detection of moving targets in a sequence of images. If the target velocity is known (and if the target intensity distribution is known) then the 3-D matched filter is optimal with respect to signal to noise ratio (SNR), however it suffers from velocity mismatch if the velocity is unknown. A bank of 3-D matched filters, each tuned to a different velocity, has been proposed for dealing with this problem. Alternatively one can use a new type of 3-D fan filter which passes undistorted all targets over a specified velocity set while providing maximum noise attenuation, thereby avoiding velocity mismatch. This paper obtains analytical expressions for the SNR improvement of these filters which facilitate the design of banks of these filters as well as performance trade-off studies.\nTitle:\nUniformly optimal 3-D fan filters for optical moving target detection\n\nAbstract:\nThe optimal algorithm for detecting a moving object in an image sequence that is corrupted by additive, white, Gaussian noise depends on the available prior knowledge of the object's velocity and its intensity distribution. This paper addresses the optimal detection problem where the object intensity distribution in known, but the object velocity is assumed only to be contained in some set, V. We formulate an optimal 3-D detection filter as the solution of a convex mini-max optimization problem, such that for all velocities contained in V, the minimum signal-to-noise ratio (SNR) improvement of the filter is maximized. The optimal filter is expressed in terms of a 2-D Lagrange multiplier function, that in turn satisfies a 2-D integral equation. For the case where V is a circular disk one can obtain a closed-form analytical solution for the optimal filter that gives a greater than 6.7 dB improvement in mini-max SNR compared with an optimized 3-D matched filter that is designed for the center velocity of the disk. Equivalently, for the same guaranteed SNR the optimal filter can cover a region of the velocity plane having an area 22 times greater than that covered by the 3-D matched filter. Put still another way, one optimal filter gives the same performance as a bank of 22 3-D matched filters\nTitle:\nOptimal detection of known moving objects in a noisy image sequence with velocity uncertainty\n\nAbstract:\nLarge-Scale Antenna Systems (LSAS) is a form of multi-user MIMO technology in which unprecedented numbers of antennas serve a significantly smaller number of autonomous terminals. We compare the two most prominent linear pre-coders, conjugate beamforming and zero-forcing, with respect to net spectral-efficiency and radiated energy-efficiency in a simplified single-cell scenario where propagation is governed by independent Rayleigh fading, and where channel-state information (CSI) acquisition and data transmission are both performed during a short coherence interval. An effective-noise analysis of the pre-coded forward channel yields explicit lower bounds on net capacity which account for CSI acquisition overhead and errors as well as the sub-optimality of the pre-coders. In turn the bounds generate trade-off curves between radiated energy-efficiency and net spectral-efficiency. For high spectral-efficiency and low energy-efficiency zero-forcing outperforms conjugate beamforming, while at low spectral-efficiency and high energy-efficiency the opposite holds. Surprisingly, in an optimized system, the total LSAS-critical computational burden of conjugate beamforming may be greater than that of zero-forcing. Conjugate beamforming may still be preferable to zero-forcing because of its greater robustness, and because conjugate beamforming lends itself to a de-centralized architecture and de-centralized signal processing.\nTitle:\nPerformance of Conjugate and Zero-Forcing Beamforming in Large-Scale Antenna Systems\n\nAbstract:\nIn a frequency division duplex system, a transmitter adaptive antenna array can potentially improve the performance of a wireless downlink, but because the uplink and downlink channels have different wavelengths, and, therefore, different responses, direct downlink adaptation based on channel estimates of the uplink is generally not feasible. Instead, there has been some interest in adaptations that require only the second-order statistics of the uplink and downlink to be similar. These algorithms derive adaptive weights from the covariance of the received signal to apply to a downlink transmitter array. We make two contributions to this area. First, we introduce an array configuration employing M+1 elements with log-periodic spacing that comprises two overlapping subarrays, each with M elements, that are scaled versions of each other, with the scale factor equal to the ratio of the uplink wavelength to the downlink wavelength. This array has identical beampatterns at the two wavelengths, thus helping to fulfill the requirement that the uplink and downlink second-order statistics be the same. Second, we demonstrate that obtaining a good estimate of the uplink covariance matrix is not essential for the successful operation of the adaptive scheme. Even when the mobile is at rest and the uplink information comprises only a single snapshot from the receiver array, an adaptive scheme can improve the SNR\nTitle:\nAdapting a downlink array from uplink measurements\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.408, mean value for all the writers is 8.648, which makes it 0.648 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper develops a theory for the application of fan filters to moving objects. In contrast to previous treatments of the subject based on the 3-D Fourier transform, simplicity and insight are achieved by using the 3-D Radon transform. With this point of view, the Radon transform decomposes the image sequence into a set of plane waves that are parameterized by a two-component slowness vector. Fan filtering is equivalent to a multiplication in the Radon transform domain by a slowness response function, followed by an inverse Radon transform. The plane wave representation of a moving object involves only a restricted set of slownesses such that the inner product of the plane wave slowness vector and the moving object velocity vector is equal to one. All of the complexity in the application of fan filters to image sequences results from the velocity-slowness mapping not being one-to-one; therefore, the filter response cannot be independently specified at all velocities. A key contribution of this paper is to elucidate both the power and the limitations of fan filtering in this new application. A potential application of 3-D fan filters is in the detection of moving targets in clutter and noise. For example, an appropriately designed fan filter can reject perfectly all moving objects whose speed, irrespective of heading, is less than a specified cut-off speed, with only minor attenuation of significantly faster objects. A simple geometric construction determines the response of the filter for speeds greater than the cut-off speed.\nTitle:", "model_inf_time": 1.71}, {"id": "416", "output": "Beamsteering for Energy-Efficient Mobile Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCurrent and emerging mobile devices are omni directional in wireless\ncommunication. Such omni directionality not only limits device energy\nefficiency but also poses a significant challenge toward the capacity of\nwireless networks through inter-link interference. In this work, we seek to\nmake mobile clients directional with beamsteering. We first demonstrate that\nbeamsteering is already feasible to mobile devices such as Netbooks and eBook\nreaders in terms of form factor, power efficiency, and device mobility. We\nfurther reveal that beamsteering mobile clients face a unique challenge to\nbalance client efficiency and network capacity. There is an optimal operating\npoint for a beamsteering mobile client in terms of the number of antennas and\ntransmit power that achieve the required capacity with lowest power. Finally,\nwe provide a distributed algorithm called BeamAdapt that allows each client to\nclosely approach its optimal point iteratively without central coordination. We\nalso offer a cellular system realization of BeamAdapt. Using Qualnet-based\nsimulation, we show that BeamAdapt with four antennas can reduce client power\nconsumption by 55% while maintaining a required network throughput for a\nlarge-scale network, compared to the same network with omni directional mobile\nclients.\nTitle:\nBeamsteering on Mobile Devices: Network Capacity and Client Efficiency\n\nAbstract:\nIn this work, we report the first study of an important realization of directional communication, beamforming, on mobile devices. We first demonstrate that beamforming is already feasible on mobile devices in terms of form factor, device mobility and power efficiency. Surprisingly, we show that by making an increasingly profitable tradeoff between transmit and circuit power, beamforming with state-of-the-art integrated CMOS implementations can be more power-efficient than its single antenna counterpart. We then investigate the optimal way of using beamforming in terms of device power efficiency, by allowing a dynamic number of active antennas. We propose a simple yet effective solution, BeamAdapt, which allows each mobile client in a network to individually identify the optimal number of active antennas with guaranteed convergence and close-to-optimal performance. We finally report a WARP-based prototype of BeamAdapt and experimentally demonstrate its effectiveness in realistic environments, and then complement the prototype-based experiments with Qualnet-based simulation of a large-scale network. Our results show that BeamAdapt with four antennas can reduce the power consumption of mobile clients by more than half compared to a single antenna, while maintaining a required network throughput.\nTitle:\nBeamforming on mobile devices: a first study\n\nAbstract:\nWireless access is known to be power-hungry for mobile devices. A key reason is that devices radiate power in all directions and much of this power will not reach the destination. To address this waste, we present BeamSwitch, a multi-antenna system designed to realize directional communication efficiently. Unlike power-hungry and expensive beamforming, BeamSwitch requires only one transceiver. We provide an 802.11-compliant design and prototype of BeamSwitch. Our measurements show that with three passive directional antennas, BeamSwitch reduces the power consumption of a commercial 802.11 adapter by up to 20% and provide better quality under diverse propagation environments and extreme rotation.\nTitle:\nPower-efficient directional wireless communication on small form-factor mobile devices\n\nAbstract:\nHigh-speed wireless network interfaces are among the most power-hungry components on mobile systems. This is particularly true for multiple-input-multiple-output (MIMO) network interfaces which use multiple RF chains simultaneously. In this paper, we present a novel power management solution for MIMO network interfaces on mobile systems, called antenna management. The key idea is to adaptively disable a subset of antennas and their RF chains to reduce circuit power consumption, when the capacity improvement of using a large number of antennas is small. Antenna management judiciously determines the number of active antennas to minimize energy per bit while satisfying the data rate requirement. This work provides both theoretical framework and system design of antenna management. We first present an algorithm that efficiently solves the problem of minimizing energy per bit and, then offer its 802.11n-compliant system designs. We employ both MATLAB-based simulation and prototype-based experiment to validate the energy efficiency benefit of antenna management. The results showthat antenna management can achieve 21% one-end energy per bit reduction to the front end of the MIMO network interface, compared to a static MIMO configuration that keeps all antennas active.\nTitle:\nPower management of MIMO network interfaces on mobile systems\n\nAbstract:\nThe extensive coverage of wireless networks brings tremendous opportunities for messaging services to satisfy the demands of accessing multimedia and time-critical information from mobile users. This trend makes energy efficiency an increasing challenge on mobile devices. In this paper, we propose a novel solution toward meeting this challenge. We explore communications utilities along both \"cross-network\" and \"cross-device\" dimensions for managing multimedia messaging services. Along the cross-network dimension, we consider the multitude of networking capabilities at a mobile terminal and select the most energy-efficient communication medium to maintain the connectivity or accomplish data transfer. Along the cross-device dimension, we consider the multitude of user interface devices by which mobile users can further improve overall energy efficiency in consuming multimedia messages. In this writing, we discuss our method as a position paper. Related experiments were performed and results are presented to justify our approach.\nTitle:\nEnergy aware multimedia messaging services across networks and across devices for mobile users\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.065, mean value for all the writers is 8.648, which makes it 0.356 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAll current mobile devices are omni directional in transmission, which not only limits device energy efficiency but also poses a significant challenge toward network capacity through inter-link interference. In this work, we make mobile devices directional with beamsteering. By allowing a dynamic number of antennas in beamsteering, we reveal an important tradeoff between device energy efficiency and network capacity. We then provide a distributed algorithm called BeamAdapt that allows each mobile device in the network to closely approach the optimal tradeoff, with minimum aggregated device power to achieve certain network capacity. We also offer a cellular system realization of BeamAdapt as well as a Qualnet-based evaluation. The results show the effectiveness of BeamAdapt in device power reduction without sacrificing network throughput.\nTitle:", "model_inf_time": 1.37}, {"id": "417", "output": "Hybrid Similarity Measures for Time Series Classification", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nTime-series classification is a widely examined data mining task with various scientific and industrial applications. Recent research in this domain has shown that the simple nearest-neighbor classifier using Dynamic Time Warping (DTW) as distance measure performs exceptionally well, in most cases outperforming more advanced classification algorithms. Instance selection is a commonly applied approach for improving efficiency of nearest-neighbor classifier with respect to classification time. This approach reduces the size of the training set by selecting the best representative instances and use only them during classification of new instances. In this paper, we introduce a novel instance selection method that exploits the hubness phenomenon in time-series data, which states that some few instances tend to be much more frequently nearest neighbors compared to the remaining instances. Based on hubness, we propose a framework for score-based instance selection, which is combined with a principled approach of selecting instances that optimize the coverage of training data. We discuss the theoretical considerations of casting the instance selection problem as a graph-coverage problem and analyze the resulting complexity. We experimentally compare the proposed method, denoted as INSIGHT, against FastAWARD, a state-of-the-art instance selection method for time series. Our results indicate substantial improvements in terms of classification accuracy and drastic reduction (orders of magnitude) in execution times.\nTitle:\nINSIGHT: efficient and effective instance selection for time-series classification\n\nAbstract:\nTime-series classification is an active research topic in machine learning, as it finds applications in numerous domains. The k-NN classifier, based on the discrete time warping (DTW) distance, had been shown to be competitive to many state-of-the art time-series classification methods. Nevertheless, due to the complexity of time-series data sets, our investigation demonstrates that a single, global choice for k (= 1) can become sub optimal, because each individual region of a data set may require a different k value. In this paper, we proposed a novel individualized error prediction (IEP) mechanism that considers a range of k-NN classifiers (for different k values) and uses secondary regression models that predict the error of each such classifier. This permits to perform k-NN time-series classification in a more fine grained fashion that adapts to the varying characteristics among different regions by avoiding the restriction of a single value of k. Our experimental evaluation, using a large collection of real time-series data, indicates that the proposed method is more robust and compares favorably against two examined baselines by resulting in significant reduction in the classification error.\nTitle:\nTime-Series Classification Based on Individualised Error Prediction\n\nAbstract:\nDue to its various applications, time-series classification is a prominent research topic in data mining and computational intelligence. The simple k-NN classifier using dynamic time warping (DTW) distance had been shown to be competitive to other state-of-the art time-series classifiers. In our research, however, we observed that a single fixed choice for the number of nearest neighbors k may lead to suboptimal performance. This is due to the complexity of time-series data, especially because the characteristic of the data may vary from region to region. Therefore, local adaptations of the classification algorithm is required. In order to address this problem in a principled way by, in this paper we introduce individual quality (IQ) estimation. This refers to estimating the expected classification accuracy for each time series and each k individually. Based on the IQ estimations we combine the classification results of several k-NN classifiers as final prediction. In our framework of IQ, we develop two time-series classification algorithms, IQ-MAX and IQ-WV. In our experiments on 35 commonly used benchmark data sets, we show that both IQ-MAX and IQ-WV outperform two baselines.\nTitle:\nIQ estimation for accurate time-series classification\n\nAbstract:\nOne of the key challenges in large information systems such as online shops and digital libraries is to discover the relevant knowledge from the enormous volume of information. Recommender systems can be viewed as a way of reducing large information spaces and to personalize information access by providing recommendations for information items based on prior usage. Collaborative Filtering, the most commonly-used technique for this task, which applies the nearest-neighbor algorithm, does not make use of object attributes. Several so-called content-based and hybrid recommender systems have been proposed, that aim at improving the recommendation quality by incorporating attributes in a collaborative filtering model. In this paper, we will present an adapted as well as two novel hybrid techniques for recommending items. To evaluate the performances of our approaches, we have conducted empirical evaluations using a movie dataset. These algorithms have been compared with several collaborative filtering and non-hybrid approaches that do not consider attributes. Our experimental evaluations show that our novel hybrid algorithms outperform state-of-the-art algorithms.\nTitle:\nAttribute-aware Collaborative Filtering\n\nAbstract:\nTime-series analysis is an important domain of machine learning and a plethora of methods have been developed for the task. This paper proposes a new representation of time series, which in contrast to existing approaches, decomposes a time-series dataset into latent patterns and membership weights of local segments to those patterns. The process is formalized as a constrained objective function and a tailored stochastic coordinate descent optimization is applied. The time-series are projected to a new feature representation consisting of the sums of the membership weights, which captures frequencies of local patterns. Features from various sliding window sizes are concatenated in order to encapsulate the interaction of patterns from different sizes. The derived representation offers a set of features that boosts classification accuracy. Finally, a large-scale experimental comparison against 11 baselines over 43 real life datasets, indicates that the proposed method achieves state-of-the-art prediction accuracy results.\nTitle:\nInvariant Factorization Of Time-Series.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.733, mean value for all the writers is 8.648, which makes it 0.781 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTime series classification, due to its applications in various domains, is one of the most important data-driven decision tasks of artificial intelligence. Recent results show that the simple nearest neighbor method with an appropriate distance measure performs surprisingly well, outperforming many state-of-the art methods. This suggests that the choice of distance measure is crucial for time series classification. In this paper we shortly review the most important distance measures of the literature, and, as major contribution, we propose a framework that allows fusion of these different similarity measures in a principled way. Within this framework, we develop a hybrid similarity measure. We evaluate it in context of time series classification on a large, publicly available collection of 35 real-world datasets and we show that our method achieves significant improvements in terms of classification accuracy.\nTitle:", "model_inf_time": 1.33}, {"id": "418", "output": "Magic Closet: Occasion-Oriented Clothing Recommendation and Pairing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we address the problem of automatically parsing the fashion images with weak supervision from the user-generated color-category tags such as \u201cred jeans\u201d and \u201cwhite T-shirt\u201d. This problem is very challenging due to the large diversity of fashion items and the absence of pixel-level tags, which make the traditional fully supervised algorithms inapplicable. To solve the problem, we propose to combine the human pose estimation module, the MRF-based color and category inference module and the (super)pixel-level category classifier learning module to generate multiple well-performing category classifiers, which can be directly applied to parse the fashion items in the images. Besides, all the training images are parsed with color-category labels and the human poses of the images are estimated during the model learning phase in this work. We also construct a new fashion image dataset called Colorful-Fashion, in which all 2,682 images are labeled with pixel-level color-category labels. Extensive experiments on this dataset clearly show the effectiveness of the proposed method for the weakly supervised fashion parsing task.\nTitle:\nFashion Parsing With Weak Color-Category Labels\n\nAbstract:\nWe address the problem of cross-domain image retrieval, considering the following practical application: given a user photo depicting a clothing image, our goal is to retrieve the same or attribute-similar clothing items from online shopping stores. This is a challenging problem due to the large discrepancy between online shopping images, usually taken in ideal lighting/pose/background conditions, and user photos captured in uncontrolled conditions. To address this problem, we propose a Dual Attribute-aware Ranking Network (DARN) for retrieval feature learning. More specifically, DARN consists of two sub-networks, one for each domain, whose retrieval feature representations are driven by semantic attribute learning. We show that this attribute-guided learning is a key factor for retrieval accuracy improvement. In addition, to further align with the nature of the retrieval problem, we impose a triplet visual similarity constraint for learning to rank across the two subnetworks. Another contribution of our work is a large-scale dataset which makes the network learning feasible. We exploit customer review websites to crawl a large set of online shopping images and corresponding offline user photos with fine-grained clothing attributes, i.e., around 450,000 online shopping images and about 90,000 exact offline counterpart images of those online ones. All these images are collected from real-world consumer websites reflecting the diversity of the data modality, which makes this dataset unique and rare in the academic community. We extensively evaluate the retrieval performance of networks in different configurations. The top-20 retrieval accuracy is doubled when using the proposed DARN other than the current popular solution using pre-trained CNN features only (0.570 vs. 0.268).\nTitle:\nCross-domain Image Retrieval with a Dual Attribute-aware Ranking Network\n\nAbstract:\nBeauty e-Experts, a fully automatic system for makeover recommendation and synthesis, is developed in this work. The makeover recommendation and synthesis system simultaneously considers many kinds of makeover items on hairstyle and makeup. Given a user-provided frontal face image with short/bound hair and no/light makeup, the Beauty e-Experts system not only recommends the most suitable hairdo and makeup, but also synthesizes the virtual hairdo and makeup effects. To acquire enough knowledge for beauty modeling, we built the Beauty e-Experts Database, which contains 1,505 female photos with a variety of attributes annotated with different discrete values. We organize these attributes into two different categories, beauty attributes and beauty-related attributes. Beauty attributes refer to those values that are changeable during the makeover process and thus need to be recommended by the system. Beauty-related attributes are those values that cannot be changed during the makeup process but can help the system to perform recommendation. Based on this Beauty e-Experts Dataset, two problems are addressed for the Beauty e-Experts system: what to recommend and how to wear it, which describes a similar process of selecting hairstyle and cosmetics in daily life. For the what-to-recommend problem, we propose a multiple tree-structured supergraph model to explore the complex relationships among high-level beauty attributes, mid-level beauty-related attributes, and low-level image features. Based on this model, the most compatible beauty attributes for a given facial image can be efficiently inferred. For the how-to-wear-it problem, an effective and efficient facial image synthesis module is designed to seamlessly synthesize the recommended makeovers into the user facial image. We have conducted extensive experiments on testing images of various conditions to evaluate and analyze the proposed system. The experimental results well demonstrate the effectiveness and efficiency of the proposed system.\nTitle:\n\u201cWow! You Are So Beautiful Today!\u201d\n\nAbstract:\nThis paper aims at developing an integrated system for clothing co-parsing (CCP), in order to jointly parse a set of clothing images (unsegmented but annotated with tags) into semantic configurations. A novel data-driven system consisting of two phases of inference is proposed. The first phase, referred as image cosegmentation, iterates to extract consistent regions on images and jointly refines the regions over all images by employing the exemplar-SVM technique [1]. In the second phase (i.e., region colabeling), we construct a multiimage graphical model by taking the segmented regions as vertices, and incorporating several contexts of clothing configuration (e.g., item locations and mutual interactions). The joint label assignment can be solved using the efficient Graph Cuts algorithm. In addition to evaluate our framework on the Fashionista dataset [2], we construct a dataset called the SYSU-Clothes dataset consisting of 2098 high-resolution street fashion photos to demonstrate the performance of our system. We achieve 90.29%/88.23% segmentation accuracy and 65.52%/63.89% recognition rate on the Fashionista and the SYSU-Clothes datasets, respectively, which are superior compared with the previous methods. Furthermore, we apply our method on a challenging task, i.e., cross-domain clothing retrieval: given user photo depicting a clothing image, retrieving the same clothing items from online shopping stores based on the fine-grained parsing results.\nTitle:\nClothes Co-Parsing Via Joint Image Segmentation and Labeling With Application to Clothing Retrieval.\n\nAbstract:\nThis paper mainly introduces the techniques required for a future system, called Magic Mirror. Imagine when you wake up in the morning and prepare for the coming day, the Magic Mirror will automatically recommend to you the most appropriate styles of hair, makeup, and dressing, according to the events and activities on your calendar, with which it is linked, so that you can present yourself on these occasions with elegant and suitable appearance. The work shall focus on the mathematical models for these tasks, particularly on how to model the relations between low-level human body features, middle-level facial/body attributes, and high-level recommendations. Being automatic and intelligent are the two main characteristics of the system, and this work shall show two prototype sub-systems related with the whole Magic Mirror system.\nTitle:\nMagic Mirror: An Intelligent Fashion Recommendation System\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.642, mean value for all the writers is 8.648, which makes it 0.858 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we aim at a practical system, magic closet, for automatic occasion-oriented clothing recommendation. Given a user-input occasion, e.g., wedding, shopping or dating, magic closet intelligently suggests the most suitable clothing from the user's own clothing photo album, or automatically pairs the user-specified reference clothing (upper-body or lower-body) with the most suitable one from online shops. Two key criteria are explicitly considered for the magic closet system. One criterion is to wear properly, e.g., compared to suit pants, it is more decent to wear a cocktail dress for a banquet occasion. The other criterion is to wear aesthetically, e.g., a red T-shirt matches better white pants than green pants. To narrow the semantic gap between the low-level features of clothing and the high-level occasion categories, we adopt middle-level clothing attributes (e.g., clothing category, color, pattern) as a bridge. More specifically, the clothing attributes are treated as latent variables in our proposed latent Support Vector Machine (SVM) based recommendation model. The wearing properly criterion is described in the model through a feature-occasion potential and an attribute-occasion potential, while the wearing aesthetically criterion is expressed by an attribute-attribute potential. To learn a generalize-well model and comprehensively evaluate it, we collect a large clothing What-to-Wear (WoW) dataset, and thoroughly annotate the whole dataset with 7 multi-value clothing attributes and 10 occasion categories via Amazon Mechanic Turk. Extensive experiments on the WoW dataset demonstrate the effectiveness of the magic closet system for both occasion-oriented clothing recommendation and pairing.\nTitle:", "model_inf_time": 1.68}, {"id": "419", "output": "Efficient Range Reporting for Intersecting Objects", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present a data structure for ray-shooting queries in a set of convex fat polyhedra of total complexity n in R3. The data structure uses O(n2+\u03b5) storage and preprocessing time, and queries can be answered in O(log2 n) time. A trade-off between storage and query time is also possible: for any m with n 2, we can construct a structure that uses O(m1+\u03b5) storage and preprocessing time such that queries take O((n/\u221am)log2 n) time.We also describe a data structure for simplex intersection queries in a set of n convex fat constant-complexity polyhedra in R3. For any m with n 3, we can construct a structure that uses O(m1+\u03b5) storage and preprocessing time such that all polyhedra intersecting a query simplex can be reported in O((n/m1/3)log n+k) time, where k is the number of answers.\nTitle:\nRay shooting and intersection searching amidst fat convex polyhedra in 3-space\n\nAbstract:\nWe develop a cache-oblivious data structure for storing a set S of N axis-aligned rectangles in the plane, such that all rectangles in S intersecting a query rectangle or point can be found efficiently. Our structure is an axis-aligned bounding-box hierarchy and as such it is the first cache-oblivious R-tree with provable performance guarantees. If no point in the plane is contained in more than a constant number of rectangles in S, we can construct, for any constant \u03b5, a structure that answers a rectangle query using $O(\\sqrt{N/B}+T/B)$memory transfers and a point query using O((N/B)\u03b5 ) memory transfers, where T is the number of reported rectangles and B is the block size of memory transfers between any two levels of a multilevel memory hierarchy. We also develop a variant of our structure that achieves the same performance on input sets with arbitrary overlap among the rectangles. The rectangle query bound matches the bound of the best known linear-space cache-aware structure.\nTitle:\nCache-Oblivious R-Trees\n\nAbstract:\nInspired by video analysis of team sports, we study the following problem. Let P be a polygonal path in the plane with n vertices. We want to preprocess P into a data structure that can quickly count the number of inclusion-minimal subpaths of P whose Fr\u00e9chet Distance to a given query segment Q is at most some threshold value \u03b5. We present a data structure that solves an approximate version of this problem: it counts all subpaths whose Fr\u00e9chet Distance is at most \u03b5, but this count may also include subpaths whose Fr\u00e9chet Distance is up to $(2+3\\sqrt{2})\\varepsilon $. For any parameter n\u2264s\u2264n2, our data structure can be tuned such that it uses O(s polylog n) storage and has $O((n/\\sqrt{s}){\\rm polylog} n)$ query time. For the special case where we wish to count all subpaths whose Fr\u00e9chet Distance to Q is at most \u03b5\u00b7length(Q), we present a structure with O(n polylog n) storage and O(polylog n) query time.\nTitle:\nFast fr\u00e9chet queries\n\nAbstract:\nIn this paper we study the ray-shooting problem for three special classes of polyhedral objects in space: axis-parallel polyhedra, curtains (unbounded polygons with three edges, two of which are parallel to thez-axis and extend downward to minus infinity), and fat horizontal triangles (triangles parallel to thexy-plane whose angles are greater than some given constant). For all three problems structures are presented usingO(n2+?) preprocessing, for any fixed? > 0, withO(logn) query time. We also study the general ray-shooting problem in an arbitrary set of triangles. Here we present a structure that usesOn4+?) preprocessing and has a query time ofO(logn).\nTitle:\nEfficient Ray Shooting and Hidden Surface Removal\n\nAbstract:\nWe present improved and simplified I/O-efficient algorithms for two problems on planar low-density subdivisions, namely map overlay and point location. More precisely, we show how to preprocess a low-density subdivision with n edges in O(sort (n)) I/O's into a compressed linear quadtree such that one can: (i) compute the overlay of two preprocessed subdivisions in O(scan(n)) I/O's, where n is the total number of edges in the two subdivisions, (ii) answer a single point location query in O(logB n) I/O's and k batched point location queries in O(scan(n) + sort(k)) I/O's. For the special case where the subdivision is a fat triangulation, we show how to obtain the same bounds with an ordinary (uncompressed) quadtree, and we show how to make the structure fully dynamic using O(logB n) I/O's per update. Our algorithms and data structures improve on the previous best known bounds for general subdivisions both in the number of I/O's and storage usage, they are significantly simpler, and several of our algorithms are cache-oblivious.\nTitle:\nI/O-efficient map overlay and point location in low-density subdivisions\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.699, mean value for all the writers is 8.648, which makes it 0.81 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe study the following problem: preprocess a set \\(\\mathcal {O}\\) of objects into a data structure that allows us to efficiently report all pairs of objects from \\(\\mathcal {O}\\) that intersect inside an axis-aligned query range \\({Q}\\). We present data structures of size \\(O(n\\cdot {{\\mathrm{polylog\\,}}}n)\\) and with query time \\(O((k+1)\\cdot {{\\mathrm{polylog\\,}}}n)\\) time, where k is the number of reported pairs, for two classes of objects in \\({\\mathbb R}^2\\): axis-aligned rectangles and objects with small union complexity. For the 3-dimensional case where the objects and the query range are axis-aligned boxes in\u00a0\\({\\mathbb R}^3\\), we present a data structure of size \\(O(n\\sqrt{n}\\cdot {{\\mathrm{polylog\\,}}}n)\\) and query time \\(O((\\sqrt{n}+k)\\cdot {{\\mathrm{polylog\\,}}}n)\\). When the objects and query are fat, we obtain \\(O((k+1)\\cdot {{\\mathrm{polylog\\,}}}n)\\) query time using \\(O(n\\cdot {{\\mathrm{polylog\\,}}}n)\\) storage.\nTitle:", "model_inf_time": 1.41}, {"id": "4110", "output": "Two-Stage Graph-Based Multi-Camera Tracking-by-Detection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present an approach for tackling the problem of automatically detecting and tracking a varying number of people in complex scenes. We follow a robust and fast framework to handle unreliable detections from each camera by extensively making use of multi-camera systems to handle occlusions and ambiguities. Instead of using the detections of each frame directly for tracking, we associate and combine the detections to form so called tracklets. From the triangulation relationship between two views, the 3D trajectory is estimated and back-projected to provide valuable cues for particle filter tracking. Most importantly, a novel motion model considering different velocity cues is proposed for particle filter tracking. Experiments are done on the challenging dataset PETS'09 to show the benefits of our approach and the integrated multi-camera extensions.\nTitle:\nMulti-person tracking-by-detection based on calibrated multi-camera systems\n\nAbstract:\nAnalyzing large amounts of video data is one of the key challenges in the trend towards big data. In the field of medical research, for example, to analyze infected cardiac movements, stereo X-ray sequences of beating animal hearts implanted with radiopaque markers are recorded. As manual annotation of exact marker positions in large-scale recordings is time-consuming and infeasible, research on automatic tracking of multiple markers is a crucial task. We propose an efficient two-stage graph-based data association approach to tackle this problem. Difficulties of the sequences like 2D occlusions, low contrast, inhomogeneous movement, and inaccurate detections, are considered in the framework. Reconstructed 3D observations are modeled and connected using a weighted directed acyclic graph to obtain tracklets with high confidence via shortest path extraction. Afterwards, tracklets are linked into longer tracks by a tracklet graph in a similar manner while global features are adopted. The approach is validated on eight X-ray cardiac datasets of beating sheep hearts with various diseases. Outperforming standard tracking approaches, e.g.\u00a0particle filter, the experimental results show a high accuracy comparable to human experts and efficiency in the meantime. The proposed approach is generic and can be directly applied to other video data as well.\nTitle:\nMulti-marker tracking for large-scale X-ray stereo video data.\n\nAbstract:\nGuided Kanade-Lucas-Tomasi (GKLT) tracking is a suitable way to incorporate knowledge about camera parameters into the standard KLT tracking approach for feature tracking in rigid scenes. By this means, feature tracking can benefit from additional knowledge about camera parameters as given by a controlled environment within a next-best-view (NBV) planning approach for three-dimensional (3D) reconstruction. We extend the GKLT tracking procedure for controlled environments by establishing a method for combined 2D tracking and robust 3D reconstruction. Thus we explicitly use the knowledge about the current 3D estimation of the tracked point within the tracking process. We incorporate robust 3D estimation, initialization of lost features, and an efficient detection of tracking steps not fitting the 3D model. Our experimental evaluation on real data provides a comparison of our extended GKLT tracking method, the former GKLT, and standard KLT tracking. We perform 3D reconstruction from predefined image sequences as well as within an information-theoretic approach for NBV planning. The results show that the reconstruction error using our extended GKLT tracking method can be reduced up to 71% compared to standard KLT and up to 39% compared to the former GKLT tracker.\nTitle:\nCombined GKLT Feature Tracking and Reconstruction for Next Best View Planning\n\nAbstract:\nMany new applications are enabled by combining a multi-camera system with a Time-of-Flight (ToF) camera, which is able to simultaneously record intensity and depth images. Classical approaches for self-calibration of a multi-camera system fail to calibrate such a system due to the very different image modalities. In addition, the typical environments of multi-camera systems are man-made and consist primary of only low textured objects. However, at the same time they satisfy the Manhattan-world assumption. We formulate the multi-modal sensor network calibration as a Maximum a Posteriori (MAP) problem and solve it by minimizing the corresponding energy function. First we estimate two separate 3D reconstructions of the environment: one using the pan-tilt unit mounted ToF camera and one using the multi-camera system. We exploit the Manhattan-world assumption and estimate multiple initial calibration hypotheses by registering the three dominant orientations of planes. These hypotheses are used as prior knowledge of a subsequent MAP estimation aiming to align edges that are parallel to these dominant directions. To our knowledge, this is the first self-calibration approach that is able to calibrate a ToF camera with a multi-camera system. Quantitative experiments on real data demonstrate the high accuracy of our approach.\nTitle:\nExploiting the Manhattan-world assumption for extrinsic self-calibration of multi-modal sensor networks\n\nAbstract:\nNext-best-view (NBV) planning is an important aspect for three-dimensional (3D) reconstruction within controlled environments, such as a camera mounted on a robotic arm. NBV methods aim at a purposive 3D reconstruction sustaining predefined goals and limitations. Up to now, literature mainly presents NBV methods for range sensors, model-based approaches or algorithms that address the reconstruction of a finite set of primitives. For this work, we use an intensity camera without active illumination. We present a novel combined online approach comprising feature tracking, 3D reconstruction, and NBV planning that addresses arbitrary unknown objects. In particular we focus on accuracy optimization based on the reconstruction uncertainty. To this end we introduce an extension of the statistical E-criterion to model directional uncertainty, and we present a closed-form, optimal solution to this NBV planning problem. Our experimental evaluation demonstrates the effectivity of our approach using an absolute error measure.\nTitle:\nOnline Next-Best-View Planning for Accuracy Optimization Using an Extended E-Criterion\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.098, mean value for all the writers is 8.648, which makes it 1.237 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAccurate multi-person tracking under complex conditions is an important topic in computer vision with various application scenarios such as visual surveillance. Taking into account the difficulties caused by 2D occlusions, missing detections, and false positives, we propose a two-stage graph-based object tracking-by-detection approach using multiple calibrated cameras. Firstly, data association is formulated into a maximum a posteriori (MAP) problem. After transformation, we show that this single MAP problem is equivalent of finding min-cost paths in a two-stage directed acyclic graph. The first graph aims to extract an optimal set of tracklets based on the hypotheses on the ground plane by using both 2D appearance feature and 3D spatial distances. Subsequently, the tracklets are linked into complete tracks in the second graph utilizing spatial and temporal distances. This results in a global optimization over all the 2D detections obtained from multiple cameras. Finally, the experimental results on three difficult sequences of the PETS'09 dataset with comparison to the state-of-the-art methods show the precision and consistency of our approach.\nTitle:", "model_inf_time": 1.71}, {"id": "4111", "output": "Evaluating the Cost-Effectiveness of JPF 2 for Deadlock Detection in TestCon", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe Java programming language supports concurrency. Concurrent programs are harder to verify than their sequential counterparts due to their inherent non-determinism and a number of specific concurrency problems, such as interference and deadlock. In previous work, we have developed the ConAn testing tool for the testing of concurrent Java components. ConAn has been found to be effective at testing a large number of components, but there are certain classes of failures that are hard to detect using ConAn. Although a variety of other verification tools and techniques have been proposed for the verification of concurrent software, they each have their strengths and weaknesses. In this paper, we propose a method for verifying concurrent Java components that includes ConAn and complements it with other static and dynamic verification tools and techniques. The proposal is based on an analysis of common concurrency problems and concurrency failures in Java components. As a starting point for determining the concurrency failures in Java components, a Petri-net model of Java concurrency is used. By systematically analysing the model, we come up with a complete classification of concurrency failures. The classification and analysis are then used to determine suitable tools and techniques for detecting each of the failures. Finally, we propose to combine these tools and techniques into a method for verifying concurrent Java components. Copyright (c) 2006 John Wiley & Sons, Ltd.\nTitle:\nA Method For Verifying Concurrent Java Components Based On An Analysis Of Concurrency Failures\n\nAbstract:\nConcurrent programs are hard to test due to the inherent nondeterminism. This paper presents a method and tool support for testing concurrent Java components. Tool support is offered through ConAn (Concurrency Analyser), a tool for generating drivers for unit testing Java classes that are used in a multithreaded context. To obtain adequate controllability over the interactions between Java threads, the generated driver contains threads that are synchronized by a clock. The driver automatically executes the calls in the test sequence in the prescribed order and compares the outputs against the expected outputs specified in the test sequence. The method and tool are illustrated in detail on an asymmetric producer-consumer monitor. Their application to testing over 20 concurrent components, a number of which are sourced from industry and were found to contain faults, is presented and discussed.\nTitle:\nTool support for testing concurrent Java components\n\nAbstract:\nThe Java programming language supports concurrency. Concurrent programs are hard to test due to their inherent non-determinism. This paper presents a classification of concurrency failures that is based on a model of Java concurrency. The model and failure classification is used to justify coverage of synchronization primitives of concurrent components. This is achieved by constructing concurrency flow graphs for each method call. A producer-consumer monitor is used to demonstrate how the approach can be used to measure coverage of concurrency primitives and thereby assist in determining test sequencesfor deterministic execution.\nTitle:\nA Classification of Concurrency Failures in Java Components\n\nAbstract:\nThe results of empirical studies in Software Engineering are limited to particular contexts, difficult to generalise and the studies themselves are expensive to perform. Despite these problems, empirical studies can be made effective and they are important to both researchers and practitioners. The key to their effectiveness lies in the maximisation of the information that can be gained by examining and replicating existing studies and using power analyses for an accurate minimum sample size. This approach was applied in a controlled experiment examining the combination of automated static analysis tools and code inspection in the context of the verification and validation (V&V) of concurrent Java components. The paper presents the results of this controlled experiment and shows that the combination of automated static analysis and code inspection is cost-effective. Throughout the experiment a strategy to maximise the information gained from the experiment was used. As a result, despite the size of the study, conclusive results were obtained, contributing to the research on V&V technology evaluation.\nTitle:\nMaximising the information gained from a study of static analysis technologies for concurrent software\n\nAbstract:\nTesting concurrent software is difficult due to problems with inherent non-determinism. In previous work, we have presented a method and tool support for the testing of concurrent Java components. In this paper, we extend that work by presenting and discussing techniques for testing Java thread interrupts and timed waits. Testing thread interrupts is important because every Java component that calls wait must have code dealing with these interrupts. For a component that uses interrupts and timed waits to provide its basic functionality, the ability to test these features is clearly even more important. We discuss the application of the techniques and tool support to one such component, which is a non-trivial implemenation of the readers-writers problem.\nTitle:\nTesting Java interrupts and timed waits\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.744, mean value for all the writers is 8.648, which makes it 0.771 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nVerifying concurrent Java programs is difficult due to the many possible interleavings of threads and a number of specific concurrency defects such as interference and deadlock. To verify concurrent Java components, the TestCon method combines code inspection, static analysis and dynamic analysis. The deadlock detection steps of TestCon include static analysis (using Jlint) that may result in false positives or false negatives; therefore code inspection is combined with Jlint, but inspection can be time-consuming and depends on the inspector's skills. In this paper, we evaluate the cost-effectiveness of the Java PathFinder 2 (JPF 2) model checker for the detection of deadlocks in the context of the TestCon method. The results of the study show that using JPF 2 can improve TestCon's effectiveness but a trade-off has to be made in terms of cost in the development of the driver and analysis of its output. General conclusions cannot be drawn since the study was exploratory and small-scale; however the observations highlight some of the strengths and weaknesses of using JPF 2 compared to static analysis and code inspection.\nTitle:", "model_inf_time": 1.71}, {"id": "4112", "output": "The Hardness of Optimal Two-Handed Assembly Sequencing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis article considers the computational complexity of automat ically determining assembly sequences for mechanical products. Specifically, we address the partitioning problem: given an assembly of rigid parts, identify a proper subassembly that can be removed as a rigid object without disturbing the rest of the assembly. We examine the complexity of the partition ing problem under various types of relative motions allowed for the subassemblies. We show that when arbitrary motions are allowed to separate the two subassemblies, partitioning is NP-complete.We then describe a general framework for reasoning about assembly motions called the interference diagram. In its most general form the interference diagram yields an exponential- time algorithm to partition an assembly. However, two special cases of the interference diagram studied in this article yield polynomial-time sequencing algorithms. The first case occurs when assembly motions are restricted to single translations. The second case considers infin...\nTitle:\nTwo-handed assembly sequencing\n\nAbstract:\n. \u00a0\u00a0 Assembly planning is the problem of finding a sequence of motions to assemble a product from its parts. We present a general\n framework for finding assembly motions based on the concept of motion space . Assembly motions are parameterized such that each point in motion space represents a mating motion that is independent\n of the moving part set. For each motion we derive blocking relations that explicitly state which parts collide with other\n parts; each subassembly (rigid subset of parts) that does not collide with the rest of the assembly can easily be derived\n from the blocking relations. Motion space is partitioned into an arrangement of cells such that the blocking relations are\n fixed within each cell. We apply the approach to assembly motions of several useful types, including one-step translations,\n multistep translations, and infinitesimal rigid motions. Several efficiency improvements are described, as well as methods\n to include additional assembly constraints into the framework. The resulting algorithms have been implemented and tested extensively\n on complex assemblies. We conclude by describing some remaining open problems.\nTitle:\nA general framework for assembly planning: the motion space approach\n\nAbstract:\nThe goal of assembly sequencing is to plan a feasible series of operations to construct a product from its individual parts. Previous research has investigated assembly sequencing under the assumption that parts have nominal geometry. This paper considers the case where parts have toleranced geometry. Its main contribution is an efficient procedure that decides if a product admits an assembly sequence with infinite translations (i.e. translations that can be extended arbitrarily far along a fixed direction), that is feasible for all possible instances of the components within the specified tolerances. If the product admits one such sequence, the procedure can also generate it. For the cases where there exists no such assembly sequence, another procedure is proposed which generates assembly sequences that are feasible only for some values of the toleranced dimensions. If this procedure produces no such sequence, then no instance of the product is assemblable. These two procedures are described for 2D assemblies made of polygonial parts and for 3D assemblies made of polyhedral parts. So far, only the first has been implemented (for the planar case). This work assumes a simple, but non-trivial tolerance language that falls short of capturing all imperfections of a manufacturing process. In particular, it assumes that faces and edges have perfect relative orientations. Thus, it is only one step towards dealing with tolerances in assembly sequencing.\nTitle:\nAssembly sequencing with toleranced parts\n\nAbstract:\nWe introduce the notion of expansiveness to characterize a family of robot configuration spaces whose connectivity can be effectively captured by a roadmap of randomly-sampled milestones. The analysis of expansive configuration spaces has inspired us to develop a new randomized planning algorithm. This algorithm tries to sample only the portion of the configuration space that is relevant to the current query, avoiding the cost of precomputing a roadmap for the entire configuration space. Thus, it is well-suited for problems where a single query is submitted for a given environment. The algorithm has been implemented and successfully applied to complex assembly maintainability problems from the automotive industry\nTitle:\nPath planning in expansive configuration spaces\n\nAbstract:\nWe study the following problem: Given a collection A of polyhedral parts in 3D, determine whether there exists a subset S of the parts that can be moved as a rigid body by an infinitesimal translation and rotation, without colliding with the rest of the parts, AS. A negative result implies that the object whose constituent parts are the collection A cannot be taken apart with two hands. A positive result, together with the list of movable parts in S and a direction of motion for S, can be used by an assembly sequence planner. This problem has attracted considerable attention within and outside the robotics community. We devise an efficient algorithm to solve this problem. Our solution is based on the ability to focus on selected portions of the tangent space of rigid motions and efficiently access these portions. The algorithm is complete (in the sense that it is guaranteed to find a solution if one exists), simple, and improves significantly over the best previously known solutions. We report experimental results with an implementation of our algorithm\nTitle:\nA simple and efficient procedure for polyhedral assembly partitioning under infinitesimal motions\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.674, mean value for all the writers is 8.648, which makes it 0.022 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOur work focuses on various complexity measures for two-handed assembly sequences. For many products, there exist an exponentially large set of valid sequences, and a nat- ural goal is to use automated systems to select wisely from the choices. Although there has been a great deal of algorithmic success for finding feasible assembly sequences, there has been very little success towards optimizing the costs of sequences. We attempt to explain this lack of progress, by proving the inherent difficulty in finding optimal, or even near-optimal, assembly sequences. To begin, we define, \"virtual assembly se- quencing,\" a graph-theoretic problem that is a generalization of assembly sequencing, focusing on the combinatorial aspect of the family of feasible assembly sequences while temporarily separating out the specific geometric assumptions inherent to assembly se- quencing. We formally prove the hardness of finding even near-optimal sequences for most cost measures in our generalized framework. As a special case, we prove equally strong inapproximability results for the problem of scheduling with AND/OR precedence constraints. Finally, we re-introduce the geometry, and continue by realizing several of these hardness results in rather simple geometric settings. We are able to show strong inapproximability results, for example using an assembly consisting solely of unit disks in the plane.\nTitle:", "model_inf_time": 1.41}, {"id": "4113", "output": "Scaling Federated Simulations for High-Performance Network Simulation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n The DoD High Level architecture (HLA) has recentlybecome the required method for the interconnection ofall DoD computer simulations. The HLA addressesthe rules by which simulations are designed to facilitateinteroperability, the method by which informationexchanged between simulations is described, anda standard set of software services provided by a commonRuntime Infrastructure (RTI). The RTI is responsiblefor the coordination of collections of cooperatingsimulations. The... \nTitle:\nDesign and implementation of HLA time management in the RTI version F.0\n\nAbstract:\n The simulation of high--speed telecommunication systems such as ATM (AsynchronousTransfer Mode) networks has generally required excessively long run times.This paper reviews alternative approaches using parallelism to speed up simulations ofdiscrete event systems, and telecommunication networks in particular. Subsequently,a new simulation method is introduced for the fast parallel simulation of a commonnetwork element, namely, a work--conserving finite capacity statistical multiplexer of... \nTitle:\nParallel Simulation of Statistical Multiplexers\n\nAbstract:\nWe describe an approach and our experiences in applying federated simulation techniques to create large-scale parallel simulations of computer networks. Using the federated approach, the topology and the protocol stack of the simulated network is partitioned into a number of submodels, and a simulation process is instantiated for each one. Runtime infrastructure software provides services for interprocess communication and synchronization (time management). We first describe issues that arise in homogeneous federations where a sequential simulator is federated with itself to realize a parallel implementation. We then describe additional issues that must be addressed in heterogeneous federations composed of different network simulation packages, and describe a dynamic simulation backplane mechanism that facilitates interoperability among different network simulators. Specifically, the dynamic simulation backplane provides a means of addressing key issues that arise in federating different network simulators: differing packet representations, incomplete implementations of network protocol models, and differing levels of detail among the simulation processes. We discuss two different methods for using the backplane for interactions between heterogeneous simulators: the cross-protocol stack method and the split-protocol stack method. Finally, results from an experimental study are presented for both the homogeneous and heterogeneous cases that provide evidence of the scalability of our federated approach on two moderately sized computing clusters. Two different homogeneous implementations are described: Parallel/Distributed ns (pdns) and the Georgia Tech Network Simulator (GTNetS). Results of a heterogeneous implementation federating ns with GloMoSim are described. This research demonstrates that federated simulations are a viable approach to realizing efficient parallel network simulation tools.\nTitle:\nA federated approach to distributed network simulation\n\nAbstract:\nParallel and distributed simulation tools are emerging that offer the ability to perform detailed, packet-level simulations of large-scale computer networks on an unprecedented scale. The state-of-the-art in large-scale network simulation is characterized quantitatively. For this purpose, a metric based on the number of packet transmissions that can be processed by a simulator per second of wallclock time (PTS) is used as a means to quantitatively assess packet-level network simulator performance. An approach to realizing scalable network simulations that leverages existing sequential simulation models and software is described. Results from a recent performance study are presented concerning large-scale network simulation on a variety of platforms ranging from workstations to cluster computers to supercomputers. These experiments include runs utilizing as many as 1536 processors yielding performance as high as 106 million PTS. The performance of packet-level simulations of web and ftp traffic, and denial of service attacks on networks containing millions of network nodes are briefly described, including a run demonstrating the ability to simulate a million web traffic flows in near real-time. New opportunities and research challenges to fully exploit this capability are discussed.\nTitle:\nLarge-scale network simulation: how big? how fast?\n\nAbstract:\nWe present a case study in which we apply parallel simulation methods and interoperability techniques to network simulations for simulation-based on-line control of military communication networks. The on-line simulations model actual military networks, including wired shipboard sub-networks connected via satellite links, and wireless mobile devices. The modeled scenario depicts the communication requirements of an amphibious landing where a complex network connects troops ashore and naval vessels. The simulations use a heterogeneous set of tools, including ns2 models for shipboard wired networks, and GloMoSim models for the wireless devices. In this paper, we document the challenges we encountered in applying parallel and interoperable simulation methods, and describe our solutions. We describe our experiences in addressing the interoperability problems that naturally arose due to the heterogeneity of scenario models. We also present a preliminary study on the scalability of real-time performance of parallel network simulations, which is crucial for on-line simulations. Salient system characteristics of the subject military network scenarios are described for the benefit of exposure to the modeling and simulation research community. Our exercise not only highlights the relevance of parallel and distributed simulation techniques to an important real-life problem, but also demonstrates the feasibility of applying those techniques in a practical setting.\nTitle:\nExperiences applying parallel and interoperable network simulation techniques in on-line simulations of military networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.377, mean value for all the writers is 8.648, which makes it 0.231 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nFederated simulation interfaces such as the High LevelArchitecture (HLA) were designed for interoperability,and as such are not traditionally associated with highperformancecomputing. In this paper, we present resultsof a case study examining the use of federated simulationsusing runtime infrastructure (RTI) software to realizelarge-scale parallel network simulators. We examine theperformance of two different federated networksimulators, and describe RTI performance optimizationsthat were used to achieve efficient execution. We showthat RTI-based parallel simulations can scale extremelywell and achieve very high speedup. Our experimentsyielded more than 80-fold scaled speedup in simulatinglarge TCP/IP networks, demonstrating performance of upto 6 million simulated packet transmissions per second ona Linux cluster. Networks containing up to two millionnetwork nodes (routers and end systems) were simulated.\nTitle:", "model_inf_time": 1.32}, {"id": "4114", "output": "Color Ellipsoids for Single Image Defogging", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSeveral methods exist that try to remove haze from a single image but they lack in the support for why a particular method was chosen. The idea of using color ellipsoids is presented in this paper for the purpose of developing a new framework for analyzing single image dehazing methods. Synthetic and real world images are used to support important properties of the color ellipsoids that give a queue to the amount of haze in an image. As an example of its usefulness, a single dehazing method is analyzed within the color ellipsoid framework.\nTitle:\nHazy Image Modeling Using Color Ellipsoids\n\nAbstract:\nWe present in this paper a fast single image defogging method that uses a novel approach to refining the estimate of amount of fog in an image with the Locally Adaptive Wiener Filter. We provide a solution for estimating noise parameters for the filter when the observation and noise are correlated by decorrelating with a naively estimated defogged image. We demonstrate our method is 50 to 100 times faster than existing fast single image defogging methods and that our proposed method subjectively performs as well as the Spectral Matting smoothed Dark Channel Prior method.\nTitle:\nFast Single Image Fog Removal Using The Adaptive Wiener Filter\n\nAbstract:\nThere is an increasing number of methods for removing haze and fog from a single image. One of such methods is Dark Channel Prior (DCP). The goal of this paper is to develop a mathematical explanation on why DCP works well by using principal component analysis, and minimum volume ellipsoid approximations.\nTitle:\nOn the effectiveness of the Dark Channel Prior for single image dehazing by approximating with minimum volume ellipsoids\n\nAbstract:\nThis paper presents a new approach to estimate fog-free images from stereo foggy images. We investigate a new way to estimate transmission by computing the scattering coefficient and depth information of a scene. However, most existing visibility restoration algorithms estimate transmission independently on scattering coefficient and object distance. In the proposed method, the natural color of a foggy image is recovered using depth information from a stereo image pair even though prior knowledge or multiple images taken at different times are not required. Furthermore, we explore a new way to measure the scattering coefficient by using a stereo image pair from an image processing perspective. Experimental results verify that the proposed method outperforms the conventional defogging methods.\nTitle:\nStereo image defogging\n\nAbstract:\nA multirate filter bank model is considered for reconstruction of periodically sampled signals. In contrast to many previous methods which considered perfect reconstruction of deterministic signals, this paper's approach uses a known discrete-time cyclostationary signal model to find a minimum mean-squared error reconstruction solution. A primary advantage of this approach is that it does not require a minimum sampling density, allowing optimal solutions to be determined in cases of undersampling. This allows for consideration of a wide range of generalized sampling problems under a single framework. An example with simulation results is presented.\nTitle:\nOptimal Filter Bank Reconstruction Of Periodically Undersampled Signals\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.546, mean value for all the writers is 8.648, which makes it 0.766 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe goal of this article is to explain how several single image defogging methods work using a color ellipsoid framework. The foundation of the framework is the atmospheric dichromatic model which is analogous to the reflectance dichromatic model. A key step in single image defogging is the ability to estimate relative depth. Therefore, properties of the color ellipsoids are tied to depth cues within an image. This framework is then extended using a Gaussian mixture model to account for multiple mixtures which gives intuition in more complex observation windows, such as observations at depth discontinuities which is a common problem in single image defogging. A few single image defogging methods are analyzed within this framework and surprisingly tied together with a common approach in using a dark prior. A new single image defogging method based on the color ellipsoid framework is introduced and compared to existing methods.\nTitle:", "model_inf_time": 1.19}, {"id": "4115", "output": "Distributed Fairness in Heterogeneous Storage Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe paper examines the problem of fair bandwidth allocation in heterogeneous storage systems in the framework of multi-resource allocation. We first extend the Bottleneck Aware Allocation model recently proposed by the authors to directly compute the maximum allocation satisfyinglocal fairness, envy freedom and sharing incentive. Next, we broaden the solution space to all allocations that satisfy envy freedom and sharing incentive even if they do not satisfy local fairness. We present an efficient algorithm to maximize the system utilization in the more general model.\nTitle:\nBrief announcement: fairness-efficiency tradeoffs in tiered storage allocation\n\nAbstract:\nThe growing popularity of hosted storage services and shared storage infrastructure in data centers is driving the recent interest in performance isolation and QoS in storage systems. Due to the bursty nature of storage workloads, meeting the traditional response-time Service Level Agreements requires significant over provisioning of the server capacity. We present a graduated, distribution-based QoS specification for storage servers that provides cost benefits over traditional QoS models. Our method RTT partitions the workload to minimize the capacity required to meet response time requirements of any specified fraction of the requests.\nTitle:\nWorkload decomposition for QoS in hosted storage services\n\nAbstract:\nHigh-end shared storage systems serving multiple independent workloads must assure that concurrently executing clients will receive a fair or agreed-upon share of system I/O resources. In a parallel I/O system an application makes requests for specific disks at different steps of its computation depending on the data layout and its computational state. Different applications contend for disk access making the problem of maintaining fair allocation challenging.We propose a model for differentiated disk bandwidth allocation based on lexicographic minimization, and provide new efficient scheduling algorithms to allocate the I/O bandwidth fairly among contending applications. A major contribution of our model is its ability to handle multiple parallel disks and contention for disks among the concurrent applications. Analysis and simulation-based evaluation shows that our algorithms provide performance isolation, weighted allocation of resources, and are work conserving. The solutions are also applicable to other shared resource environments dealing with non-uniform heterogeneous servers.\nTitle:\nLexicographic QoS scheduling for parallel I/O\n\nAbstract:\nThe increasing popularity of storage and server consolidation introduces new challenges for resource management. In this paper we propose a Nested QoS service model that offers multiple response time guarantees for a workload based on its burstiness. The client workload is filtered into classes based on the Service Level Objective (SLO) and scheduled to provide requests in each class a stipulated response time guarantee. The Nested QoS model provides an intuitive, enforceable, and verifiable SLO between provider and client. The server capacity in the nested model is reduced significantly over a traditional SLO while the performance is only marginally affected.\nTitle:\nNested QoS: providing flexible performance in shared IO environment\n\nAbstract:\nThe growing popularity of hosted storage services and shared storage infrastructure in data centers is driving the recent interest in resource management and QoS in storage systems. The bursty nature of storage workloads raises significant performance and provisioning challenges, leading to increased resource requirements, management costs, and energy consumption. We present a novel workload shaping framework to handle bursty workloads, where the arrival stream is dynamically decomposed to isolate its bursts, and then rescheduled to exploit available slack. We show how decomposition reduces the server capacity requirements and power consumption significantly, while affecting QoS guarantees minimally. We present an optimal decomposition algorithm RTT and a recombination algorithm Miser, and show the benefits of the approach by evaluating the performance of several storage workloads using both simulation and Linux implementation.\nTitle:\nDecomposing Workload Bursts for Efficient Storage Resource Management\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.867, mean value for all the writers is 8.648, which makes it 0.666 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe examine the problem of providing fair bandwidth allocation in distributed storage systems similar to the research prototypes HP FAB and IBM Intelligent Bricks. The problem poses significant new challenges beyond those encountered in network and storage QoS scheduling. Specifically, resources are heterogeneous i.e. an IO request can only be serviced by a particular server, and service is both requested and provided in a distributed manner with no centralized controller. We provide a distributed algorithm, d-Clock, that runs locally on each of the servers and provides global fairness guarantees without causing resource-specific starvation, with minimal synchronization overhead.\nTitle:", "model_inf_time": 1.06}, {"id": "4116", "output": "Collaborative Detection of Inter-domain Port Scans Using Complex Event Processing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nLarge enterprises are nowadays complex interconnected software systems spanning over several domains. This new dimension makes difficult for enterprises the task of enabling efficient security defenses. This paper addresses the problem of detecting inter-domain stealthy port scans and proposes an architecture of an Intrusion Detection System which uses, for such purpose, an open source Complex Event Processing engine named Esper. Esper provides low cost of ownership and high flexibility. The architecture consists of software sensors deployed at different enterprise domains. Each sensor sends events to the Esper event processor for correlation. We implemented an algorithm for the detection of interdomain SYN port scans named Rank-based SYN (R-SYN) port scan detection algorithm. It combines and adapts three detection techniques in order to obtain a unique global statement about the malicious behavior of host activities. An evaluation of the accuracy of our approach has been carried out using several traces, some of which including original traffic dumps, some others altered by injecting packets that simulate port scan activities. Accuracy results show that our algorithm is able to produce a list of scanners characterized by high detection and low false positive rates.\nTitle:\nInter-domain stealthy port scan detection through complex event processing\n\nAbstract:\nOrganizations must protect their information systems from a variety of threats. Usually they employ isolated defenses such as firewalls, intrusion detection and fraud monitoring systems, without cooperating with the external world. Organizations belonging to the same markets (e.g., financial organizations, telco providers) typically suffer from the same cyber crimes. Sharing and correlating information could help them in early detecting those crimes and mitigating the damages. The paper discusses the Semantic Room (SR) abstraction which enables the development of collaborative event-based platforms, on the top of Internet, where data from different information systems are shared, in a controlled manner, and correlated to detect and timely react to coordinated Internet-based security threats (e.g., port scans, botnets) and frauds. In order to show the flexibility of the abstraction, the paper proposes the design, implementation and validation of two SRs: an SR that detects inter-domain port scan attacks and an SR that enables an online fraud monitoring over the Italian territory. In both cases, the SRs use real data traces for demonstrating the effectiveness of the proposed approach. In the first SR, high detection accuracy and small detection delays are achieved whereas in the second, new fraud evidence and investigation instruments are provided to law enforcement agencies.\nTitle:\nAn event-based platform for collaborative threats detection and monitoring\n\nAbstract:\nCooperative information systems are characterized by distribution, high heterogeneity and scale. Therefore they require interoperable, dependable services on top of which the development of cooperative application can take place. This paper studies, in the context of the Unitary Network of the Italian Public Administration, the problem of increasing the availability of the services exported by a cooperating entity through interoperable middleware. In particular we show how a Fault Tolerant CORBA compliant system, namely the Interoperable Replication Logic (IRL), can be used to increase such availability by building a replicated cooperative gateway that wraps enterprise cooperative applications.\nTitle:\nEnhancing Availability of Cooperative Applications Through Interoperable Middleware\n\nAbstract:\nIntegrating autonomous enterprise systems allows the cooperation among applications belonging to distinct systems. As an example, this problem shows up when integrating software services of large departments and organizations in e-Government initiatives. This paper studies, in the context of the Unitary Network of the Italian Public Administration, the problem of increasing the availability of services exported by an autonomous enterprise system towards others. In particular we show how a fault tolerant CORBA system, namely the Interoperable Replication Logic (IRL), can be used to increase such an availability by building a replicated cooperative gateway that wraps enterprise applications.\nTitle:\nIntegrating Autonomous Enterprise Systems through Dependable CORBA Objects\n\nAbstract:\nRecent evidence of successful Internet-based attacks and frauds involving financial institutions highlights the inadequacy of the existing protection mechanisms, in which each instutition implements its own isolated monitoring and reaction strategy. Analyzing on-line activity and detecting attacks on a large scale is an open issue due to the huge amounts of events that should be collected and processed. In this paper, we propose a large-scale distributed event processing system, called intelligence cloud, allowing the financial entities to participate in a widely distributed monitoring and detection effort through the exchange and processing of information locally available at each participating site. We expect this approach to be able to handle large amounts of events arriving at high rates from multiple domains of the financial scenario. We describe a framework based on the intelligence cloud where each participant can receive early alerts enabling them to deploy proactive countermeasures and mitigation strategies.\nTitle:\nDefending financial infrastructures through early warning systems: the intelligence cloud approach\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.128, mean value for all the writers is 8.648, which makes it 0.41 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe describe an Internet-based collaborative environment that protects geographically dispersed organizations of a critical infrastructure (e.g., financial institutions, telco providers) from coordinated cyber attacks. A specific instance of a collaborative environment for detecting malicious inter-domain port scans is introduced. This instance uses the open source Complex Event Processing (CEP) engine ESPER to correlate massive amounts of network traffic data exhibiting the evidence of those scans. The paper presents two inter-domain SYN port scan detection algorithms we designed, implemented in ESPER, and deployed on the collaborative environment; namely, Rank-based SYN (R-SYN) and Line Fitting. The paper shows the usefulness of the collaboration in terms of detection accuracy. Finally, it shows how Line Fitting can both achieve a higher detection accuracy with a smaller number of participants than R-SYN, and exhibit better detection latencies than R-SYN in the presence of low link bandwidths (i.e., less than 3Mbit/s) connecting the organizations to Esper.\nTitle:", "model_inf_time": 1.52}, {"id": "4117", "output": "Unequal Error Protection for DVB-H Media Streaming", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nBi-predictive (B) slices are not supported in the Baseline profile of the Advanced Video Coding (H.264/AVC) standard, which results in a decreased coding efficiency compared with other profiles supporting B slices. However, many application standards, such as the mobile multimedia services specified by the Third Generation Partnership Project (3GPP), use only the Baseline profile for H.264/AVC. Therefore, it is worth investigating H.264/AVC coding when only intra (I) and inter (P) slices are supported. In this paper, a content-adaptive Quantization Parameter (QP) cascading scheme for the hierarchical P coding method compatible with Baseline profile of H.264/AVC is proposed. The proposed method is based on a picture-level QP optimization. The proposed method has a significantly better rate-distortion performance than the traditional IPPP coding structure and outperforms hierarchical P coding methods using fixed delta QP settings between temporal levels noticeably with up to 0.53 dB gain in average luminance Peak Signal-to-Noise Ratio (PSNR).\nTitle:\nEfficient hierarchical inter picture coding for H.264/AVC baseline profile.\n\nAbstract:\nIn this paper a real-time simulcast multi-view video coding based on three-dimensional discrete wavelet transform (3D DWT) is considered. An efficient rate-distortion criterion of skipping spatial subbands is proposed. A processing of subbands is done in a group of frames from low frequency to high-frequency temporal subbands and from low frequency to high-frequency spatial subbands. If for the processed subband in the current view it appears to be more efficient not to include the highest significant bit-plane into the output bit stream, then all the corresponding temporal and spatial child subbands are skipped without any calculations of 2-D wavelet transforms and entropy encoding. Moreover, all corresponding spatial subbands in sequel views (with its child subbands) are skipped as well. Simulations results have demonstrated that the 3-D DWT codec with the proposed skipping rule has much lower computational complexity (from 2 up to 8 times) for the same quality level compared to the H.264/AVC standard in the low complexity mode.\nTitle:\nA real-time simulcast multi-view wavelet video coding based on skipping of spatial subbands\n\nAbstract:\nIn this paper we propose a low-complexity fuzzy video rate control algorithm with buffer constraint designed for real- time streaming applications. While in low delay video communications bit streams with constant bitrate are required, in streaming application more delay and variation in bitrate is acceptable. The described video rate control algorithm (RCA) provides a variable bitrate video by control of the quantization scale (QS) on picture basis. The QS is mainly controlled by a fuzzy controller such that it minimizes the variation of QS to provide encoded video with high visual quality so as to utilize the variable bitrate benefits as much as possible. The proposed rate control algorithm (RCA) has been implemented in the MPEG-4, H.263 and H.264/AVC standard video codecs and the experimental results show that it provides high level average quality for encoded video while it strictly obeys streaming constraints.\nTitle:\nLow-Complexity Fuzzy Video Rate Controller for Streaming\n\nAbstract:\nRandom access is a desirable feature in many video communication systems. Intra pictures have been conventionally used as random access points, but correct picture content can also be recovered gradually within a range of pictures starting from a non-intra random access point. This paper proposes the use of the isolated regions technique for gradual decoder refresh and presents how the proposed method can be used in the upcoming ITU-T Recommendation H.264, also known as MPEG-4 Part 10 or Advanced Video Coding. The presented simulations reveal that the proposed method outperforms intra- picture-based random access points in error-prone network conditions. It is also shown that the proposed method is more flexible and suits packet-based transmission better compared to progressively located intra-coded slices.\nTitle:\nRandom access using isolated regions\n\nAbstract:\nSignaling of scene information in coded bitstreams was proposed by the authors and adopted into the emerging video coding standard H.264 (also known as MPEG-4 part 10 or AVC) as a supplemental enhancement information (SEI) message. This paper proposes some improved error concealment methods for intra coded pictures and scene transition pictures using the signaled scene information. Simulation results show that the proposed methods outperform conventional techniques significantly.\nTitle:\nImproved Error Concealment Using Scene Information\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.197, mean value for all the writers is 8.648, which makes it 0.468 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper introduces a method for unequal error protection (UEP) of media data in a time-sliced DVB-H channel. Media datagrams are assigned priorities using some a-priori knowledge. Datagrams covering a certain period of playback time are first grouped based on the priority assignment. Each group is then protected using Reed-Solomon forward error correction (FEC) codes and packed into multi-protocol encapsulation (MPE) FEC frames as defined by the DVB-H standard. All MPE-FEC frames for a certain period of playback time are then sent back to back without any delay between these MPE-FEC frames. This method of UEP is generic and can be tuned according to the priority assignment algorithm. Simulations using H.264/AVC video were conducted to evaluate the performance of the proposed method. It used a simple priority assignment algorithm. The resulting rate distortion graphs show good performance and an average luma peak signal-to-noise ratio (PSNR) improvement of up to 0.8 dB was achieved.\nTitle:", "model_inf_time": 1.37}, {"id": "4118", "output": "Order-Optimal Caching and Coded Multicasting with Zipfian Demands", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nFor a network with one sender, n receivers (users) and m possible messages (files), caching side information at the users allows to satisfy arbitrary simultaneous demands by sending a common (multicast) coded message. In the worst-case demand setting, explicit deterministic and random caching strategies and explicit linear coding schemes have been shown to be order optimal. In this work, we consider the same scenario where the user demands are random i.i.d., according to a Zipf popularity distribution. In this case, we pose the problem in terms of the minimum average number of equivalent message transmissions. We present a novel decentralized random caching placement and a coded delivery scheme which are shown to achieve order-optimal performance. As a matter of fact, this is the first order-optimal result for the caching and coded multicasting problem in the case of random demands.\nTitle:\nOn the average performance of caching and coded multicasting with random demands\n\nAbstract:\nThe capacity of caching networks has received considerable attention in the past few years. A particularly studied setting is the shared link caching network, in which a single source with access to a file library communicates with multiple users, each having the capability to store segments (packets) of the library files, over a shared multicast link. Each user requests one file from the library according to a common demand distribution and the server sends a coded multicast message to satisfy all users at once. The problem consists of finding the smallest possible average codeword length to satisfy such requests. In this paper, we consider the generalization to the case where each user places L \u2265 1 independent requests according to the same common demand distribution. We propose an achievable scheme based on random vector (packetized) caching placement and multiple groupcast index coding, shown to be order-optimal in the asymptotic regime in which the number of packets per file B goes to infinity. We then show that the scalar (B = 1) version of the proposed scheme can still preserve order-optimality when the number of per-user requests L is large enough. Our results provide the first order-optimal characterization of the shared link caching network with multiple random requests, revealing the key effects of L on the performance of caching-aided coded multicast schemes.\nTitle:\nCaching-aided coded multicasting with multiple random requests\n\nAbstract:\nThe capacity of caching networks has received considerable attention in the past few years. A particularly studied setting is the case of a single server (e.g., a base station) and multiple users, each of which caches segments of files in a finite library. Each user requests one (whole) file in the library and the server sends a common coded multicast message to satisfy all users at once. The problem consists of finding the smallest possible codeword length to satisfy such requests. In this paper we consider the generalization to the case where each user places L \u2265 1 requests. The obvious naive scheme consists of applying L times the order-optimal scheme for a single request, obtaining a linear in L scaling of the multicast codeword length. We propose a new achievable scheme based on multiple groupcast index coding that achieves a significant gain over the naive scheme. Furthermore, through an information theoretic converse we find that the proposed scheme is approximately optimal within a constant factor of (at most) 18.\nTitle:\nCaching and coded multicasting: Multiple groupcast index coding\n\nAbstract:\nWe study the throughput of a network formed by one server, k \"helpers\" and n users. The users may request any file from a fixed library of m files, where each file can be regarded as the realization of an independent random variable with entropy F bits. The users can locally cache up to MF information bits. Each user can connect simultaneously to r helpers. All links in the network (from the server to the helpers, and from the helpers to the users) have normalized capacity of F bits per unit time. We study the achievable download time, expressed in multiples of the time necessary to transmit F bits over a link. In particular, we are interested in minimizing the worst-case download time over all possible demand configurations and realizations of the user-helper connectivity. We present a simple scheme that combines network-coded multicasting and MDS coding and achieves a speed-up factor of 1/r in download time with respect to the case where the server is connected directly to the users through a shared multicast link. We also show that the achieved performance is order-optimal (up to at most a logarithmic factor) in the regime where the total system cache memory is large with respect to the file library size.\nTitle:\nCaching In Combination Networks\n\nAbstract:\nWe consider a wireless device-to-device (D2D) network where communication is restricted to be single-hop. Users make arbitrary requests from a finite library of files and have pre-cached information on their devices, subject to a per-node storage capacity constraint. A similar problem has already been considered in an infrastructure setting, where all users receive a common multicast (coded) message from a single omniscient server (e.g., a base station having all the files in the library) through a shared bottleneck link. In this paper, we consider a D2D infrastructureless version of the problem. We propose a caching strategy based on deterministic assignment of subpackets of the library files, and a coded delivery strategy where the users send linearly coded messages to each other in order to collectively satisfy their demands. We also consider a random caching strategy, which is more suitable to a fully decentralized implementation. Under certain conditions, both approaches can achieve the information theoretic outer bound within a constant multiplicative factor. In our previous work, we showed that a caching D2D wireless network with one-hop communication, random caching, and uncoded delivery (direct file transmissions) achieves the same throughput scaling law of the infrastructure-based coded multicasting scheme, in the regime of large number of users and files in the library. This shows that the spatial reuse gain of the D2D network is order-equivalent to the coded multicasting gain of single base station transmission. It is, therefore, natural to ask whether these two gains are cumulative, i.e., if a D2D network with both local communication (spatial reuse) and coded multicasting can provide an improved scaling law. Somewhat counterintuitively, we show that these gains do not cumulate (in terms of throughput scaling law). This fact can be explained by noticing that the coded delivery scheme creates messages that are useful to multiple nodes, such that it bene- its from broadcasting to as many nodes as possible, while spatial reuse capitalizes on the fact that the communication is local, such that the same time slot can be reused in space across the network. Unfortunately, these two issues are in contrast with each other.\nTitle:\nFundamental Limits of Caching in Wireless D2D Networks.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.598, mean value for all the writers is 8.648, which makes it 0.811 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe consider the canonical shared link caching network formed by a source node, hosting a library of $m$ information messages (files), connected via a noiseless multicast link to $n$ user nodes, each equipped with a cache of size $M$ files. Users request files independently at random according to an a-priori known demand distribution q. A coding scheme for this network consists of two phases: cache placement and delivery. The cache placement is a mapping of the library files onto the user caches that can be optimized as a function of the demand statistics, but is agnostic of the actual demand realization. After the user demands are revealed, during the delivery phase the source sends a codeword (function of the library files, cache placement, and demands) to the users, such that each user retrieves its requested file with arbitrarily high probability. The goal is to minimize the average transmission length of the delivery phase, referred to as rate (expressed in channel symbols per file). In the case of deterministic demands, the optimal min-max rate has been characterized within a constant multiplicative factor, independent of the network parameters. The case of random demands was previously addressed by applying the order-optimal min-max scheme separately within groups of files requested with similar probability. However, no complete characterization of order-optimality was previously provided for random demands under the average rate performance criterion. In this paper, we consider the random demand setting and, for the special yet relevant case of a Zipf demand distribution, we provide a comprehensive characterization of the order-optimal rate for all regimes of the system parameters, as well as an explicit placement and delivery scheme achieving order-optimal rates. We present also numerical results that confirm the superiority of our scheme with respect to previously proposed schemes for the same setting.\nTitle:", "model_inf_time": 1.92}, {"id": "4119", "output": "Distributed Cooperative Multi-Antenna Transmission with Local CSI", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper considers downlink multiantenna communication with base stations that perform cooperative precoding in a distributed fashion. Most previous work in the area has assumed that transmitters have common knowledge of both data symbols of all users and full or partial channel state information (CSI). Herein, we assume that each base station only has local CSI, either instantaneous or statistical. For the case of instantaneous CSI, a parametrization of the beamforming vectors used to achieve the outer boundary of the achievable rate region is obtained for two multi-antenna transmitters and two single-antenna receivers. Distributed generalizations of classical beamforming approaches that satisfy this parametrization are provided, and it is shown how the distributed precoding design can be improved using the so-called virtual SINR framework [1]. Conceptually analog results for both the parametrization and the beamforming design are derived in the case of local statistical CSI. Heuristics on the distributed power allocation are provided in both cases, and the performance is illustrated numerically.\nTitle:\nDistributed Multicell and Multiantenna Precoding: Characterization and Performance Evaluation\n\nAbstract:\nBase station cooperation is an attractive technique to increase the spectral efficiency of multi-cell systems. One of the challenges in multi-cell systems is obtaining accurate channel state information (CSI) at the base station. One source of CSI inaccuracy is the delay incurred in obtaining CSI and exchanging it among base stations. The presence of delay is inevitable when CSI is exchanged over the back-haul. In addition, CSI is commonly obtained using limited feedback techniques which further contribute to its inaccuracy. In this paper, we re-visit the comparison between competitive (egoistic) and cooperative (altruistic) beamforming strategies in the presence of imperfect CSI. The impact of CSI inaccuracy due to delay and finite codebook size on the achieved sum rate is analyzed. Closed-form expressions for a lower bound and a first-order approximation of the average sum rates achieved by these beamforming strategies are derived. Using the closed-form expressions, a mode switching criterion is proposed to switch between competitive and cooperative beamforming based on the signal-to-noise ratio (SNR), delay, Doppler frequency and codebook size. It is shown that competitive beamforming is preferred in the limit of low SNR irrespective of the quality of CSI, while cooperative beamforming is preferred only at high SNR and low Doppler frequencies, confirming that base station cooperation is not always advantageous.\nTitle:\nEgoistic vs. altruistic beamforming in multi-cell systems with feedback and back-haul delays.\n\nAbstract:\nIn this letter, we address the problem of distributed multi-antenna cooperative transmission in a cellular system. Most research in this area has so far assumed that base stations not only have the data dedicated to all the users but also share the full channel state information (CSI). In what follows, we assume that each base station (BS) only has local CSI knowledge. We propose a suboptimal, yet efficient, way in which the multicell MISO precoders may be designed at each BS in a distributed manner, as a superposition of so-called virtual SINR maximizations: a virtual SINR maximizing transmission scheme yields Pareto optimal rates for the MISO Interference Channel (IC); its extension to the multicell MISO channel is shown to provide a distributed precoding scheme achieving a certain fairness optimality for the two link case. We illustrate the performance of our algorithm through Monte Carlo simulations.\nTitle:\nDistributed Multicell-MISO Precoding Using the Layered Virtual SINR Framework\n\nAbstract:\nIn this paper, the problem of the coexistence of two multiple-antenna wireless links is addressed in a cognitive radio scenario. The novelty brought by our setup is three-fold: First we consider a more realistic rate target constraint at the primary receiver instead of the less meaningful maximum interference temperature, second we propose a limited channel state information (CSI) structure whereby transmitters only have access to partly instantaneous feedback (i.e., about the direct channels) and partly statistical feedback (i.e., about the interference channels). Third, we formulate a distributed decision making scenario, by which channel information is not shared among primary and secondary transmitters. Instead, a transmitter must make a precoding decision based on local CSI only. The problem is recast as a team decision theoretic problem and the optimal precoders are obtained by solving semidefinite programs (SDPs). A distributed algorithm is derived and compared with classical precoding solutions and gains are illustrated over a range of scenarios.\nTitle:\nA Team Decisional Beamforming Approach For Underlay Cognitive Radio Networks\n\nAbstract:\nDense interference-limited wireless networks can rely coordinated multipoint transmission (such as Network MIMO) as a way to improve on spectral efficiency. Unfortunately, Network MIMO requires global channel state information (CSI) at all transmitters, hence places stringent requirements on backhaul rate and even more on latency. As a solution, this paper investigates an emerging design philosophy for CSI that exploits the broadcast nature of wireless which is well suited to dense networks. In our design, feedback is broadcast from each terminal and decoded opportunistically by any overhearing base station which in turn must design opportunistic interferencecancelling precoders. The corresponding precoder design is shown to be equivalent to a decentralized decision problem whose general solution is challenging, yet for which heuristic schemes can be derived. The obtained algorithms are able to capitalize on the opportunistic feedback without the need for global CSI sharing.\nTitle:\nOpportunistic Feedback Mechanisms For Decentralized Network Mimo Systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.689, mean value for all the writers is 8.648, which makes it 0.888 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nBase station cooperation is an attractive way of increasing the spectral efficiency in multiantenna communication. By serving each terminal through several base stations in a given area, intercell interference can be coordinated and higher performance achieved, especially for terminals at cell edges. Most previous work in the area has assumed that base stations have common knowledge of both data dedicated to all terminals and full or partial channel state information (CSI) of all links. Herein, we analyze the case of distributed cooperation where each base station has only local CSI, either instantaneous or statistical. In the case of instantaneous CSI, the beamforming vectors that can attain the outer boundary of the achievable rate region are characterized for an arbitrary number of multi antenna transmitters and single-antenna receivers. This characterization only requires local CSI and justifies distributed precoding design based on a novel virtual signal-to-interference noise ratio (SINR) framework, which can handle an arbitrary SNR and achieves the optimal multiplexing gain. The local power allocation between terminals is solved heuristically. Conceptually, analogous results for the achievable rate region characterization and precoding design are derived in the case of local statistical CSI. The benefits of distributed cooperative transmission are illustrated numerically, and it is shown that most of the performance with centralized cooperation can be obtained using only local CSI.\nTitle:", "model_inf_time": 1.46}, {"id": "4120", "output": "Decidability and Topological Properties of Reachability for Piecewise Affine Maps", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nReachability for piecewise affine systems is known to be undecidable, starting from dimension 2. In this paper we investigate the exact complexity of several decidable variants of reachability and control questions for piecewise affine systems. We show in particular that the region-to-region bounded time versions leads to NP-complete or co-NP-complete problems, starting from dimension 2. We also prove that a bounded precision version leads to PSPACE-complete problems.\nTitle:\nOn the complexity of bounded time and precision reachability for piecewise affine systems.\n\nAbstract:\n  Reachability for piecewise affine systems is known to be undecidable, starting from dimension $2$. In this paper we investigate the exact complexity of several decidable variants of reachability and control questions for piecewise affine systems. We show in particular that the region to region bounded time versions leads to $NP$-complete or co-$NP$-complete problems, starting from dimension $2$. We also prove that a bounded precision version leads to $PSPACE$-complete problems. \nTitle:\nOn The Complexity of Bounded Time Reachability for Piecewise Affine Systems.\n\nAbstract:\nIn this paper we discuss the computational power of Lipschitz dynamical systems which are robust to infinitesimal perturbations. Whereas the study in [1] was done only for not-so-natural systems from a classical mathematical point of view (discontinuous differential equation systems, discontinuous piecewise affine maps, or perturbed Turing machines), we prove that the results presented there can be generalized to Lipschitz and computable dynamical systems. In other words, we prove that the perturbed reachability problem (i.e. the reachability problem for systems which are subjected to infinitesimal perturbations) is co-recursively enumerable for this kind of systems. Using this result we show that if robustness to infinitesimal perturbations is also required, the reachability problem becomes decidable. This result can be interpreted in the following manner: undecidability of verification doesn't hold for Lipschitz, computable and robust systems. We also show that the perturbed reachability problem is co-r.e. complete even for C\u221e-systems.\nTitle:\nRobust computations with dynamical systems\n\nAbstract:\nIn this paper, we characterize the computational power of dynamical systems with piecewise constant derivatives (PCD) considered as computational machines working on a continuous real space with a continuous real time: we prove that piecewise constant derivative systems recognize precisely the languages of the \u03c9 k th (respectively ( \u03c9 k + 1)th) level of the hyper-arithmetical hierarchy in dimension d = 2 k + 3 (respectively d = 2 k + 4), k \u2a7e 0. Hence we prove that the reachability problem for PCD systems of dimension d = 2 k + 3 (resp. d = 2 k + 4), k \u2a7e 1, is hyper-arithmetical and is \u2211 \u03c9 k -complete (resp. \u2211 \u03c9 k -complete).\nTitle:\nAchilles and the Tortoise climbing up the hyper-arithmetical hierarchy\n\nAbstract:\n. \u00a0\u00a0 We study the computational power of Piecewise Constant Derivative (PCD) systems. PCD systems are dynamical systems defined\n by a piecewise constant differential equation and can be considered as computational machines working on a continuous space\n with a continuous time. We show that the computation time of these machines can be measured either as a discrete value, called\n discrete time, or as a continuous value, called continuous time. We relate the two notions of time for general PCD systems.\n We prove that general PCD systems are equivalent to Turing machines and linear machines in finite discrete time. We prove\n that the languages recognized by purely rational PCD systems in dimension d in finite continuous time are precisely the languages of the (d-2) th level of the arithmetical hierarchy. Hence the reachability problem of purely rational PCD systems of dimension d in finite continuous time is \u03a3\n \n \n \n d-2\n \n  -complete.\nTitle:\nSome Bounds on the Computational Power of Piecewise Constant Derivative Systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.456, mean value for all the writers is 8.648, which makes it 0.164 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nPiecewise affine maps (PAMs) are frequently used as a reference model to discuss the frontier between known and open questions about the decidability for reachability questions. In particular, the reachability problem for one-dimensional PAM is still an open problem, even if restricted to only two intervals. As the main contribution of this paper we introduce new techniques for solving reachability problems based on p-adic norms and weights as well as showing decidability for two classes of maps. Then we show the connections between topological properties for PAM's orbits, reachability problems and representation of numbers in a rational base system. Finally we construct an example where the distribution properties of well studied sequences can be significantly disrupted by taking fractional parts after regular shifts. The study of such sequences could help with understanding similar sequences generated in PAMs or in well known Mahler's 3/2 problem.\nTitle:", "model_inf_time": 1.64}, {"id": "4121", "output": "Monotone Connected Visible Search", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn the graph searching game the opponents are a set of searchers and a fugitive in a graph. The searchers try to capture the fugitive by applying some sequence of moves that include placement, removal, or sliding of a searcher along an edge. The fugitive tries to avoid capture by moving along unguarded paths. The search number of a graph is the minimum number of searchers required to guarantee the capture of the fugitive. In this paper, we initiate the study of this game under the natural restriction of connectivity where we demand that in each step of the search the locations of the graph that are clean (i.e. non-accessible to the fugitive) remain connected. We give evidence that many of the standard mathematical tools used so far in classic graph searching fail under the connectivity requirement. We also settle the question on ''the price of connectivity'', that is, how many searchers more are required for searching a graph when the connectivity demand is imposed. We make estimations of the price of connectivity on general graphs and we provide tight bounds for the case of trees. In particular, for an n-vertex graph the ratio between the connected searching number and the non-connected one is O(logn) while for trees this ratio is always at most 2. We also conjecture that this constant-ratio upper bound for trees holds also for all graphs. Our combinatorial results imply a complete characterization of connected graph searching on trees. It is based on a forbidden-graph characterization of the connected search number. We prove that the connected search game is monotone for trees, i.e. restricting search strategies to only those where the clean territories increase monotonically does not require more searchers. A consequence of our results is that the connected search number can be computed in polynomial time on trees, moreover, we show how to make this algorithm distributed. Finally, we reveal connections of this parameter to other invariants on trees such as the Horton-Strahler number.\nTitle:\nConnected graph searching\n\nAbstract:\nWe give a constructive proof of the equality between treewidth and connected treewidth. More precisely, we describe an O(nk3)-time algorithm that, given any n-node width-k tree-decomposition of a connected graph G, returns a connected tree-decomposition of G of width \u2264 k. The equality between treewidth and connected treewidth finds applications in graph searching problems. First, using equality between treewidth and connected treewidth, we prove that the connected search number cs(G) of a connected graph G is at most logn+1 times larger than its search number. Second, using our constructive proof of equality between treewidth and connected treewidth, we design an $O(log n\\sqrt{log OPT}$)-approximation algorithm for connected search, running in time O(t(n)+nk3log3/2k+mlog n) for n-node m-edge connected graphs of treewidth at most k, where t(n) is the time-complexity of the fastest algorithm for approximating the treewidth, up to a factor $O(\\sqrt{log OPT}$).\nTitle:\nConnected treewidth and connected graph searching\n\nAbstract:\nGraph searching is one of the most popular tools for analyzing the chase for a powerful and hostile software agent (called the ''intruder''), by a set of software agents (called the ''searchers'') in a network. The existing solutions for the graph searching problem suffer however from a serious drawback: they are mostly centralized and assume a global synchronization mechanism for the searchers. In particular: (1) the search strategy for every network is computed based on the knowledge of the entire topology of the network, and (2) the moves of the searchers are controlled by a centralized mechanism that decides at every step which searcher has to move, and what movement it has to perform. This paper addresses the graph searching problem in a distributed setting. We describe a distributed protocol that enables searchers with logarithmic size memory to clear any network, in a fully decentralized manner. The search strategy for the network in which the searchers are launched is computed online by the searchers themselves without knowing the topology of the network in advance. It performs in an asynchronous environment, i.e., it implements the necessary synchronization mechanism in a decentralized manner. In every network, our protocol performs a connected strategy using at most k+1 searchers, where k is the minimum number of searchers required to clear the network in a monotone connected way using a strategy computed in the centralized and synchronous setting.\nTitle:\nDistributed chasing of network intruders\n\nAbstract:\nWe study the size of memory of mobile agents that permits to solve deterministically the rendezvous problem, i.e., the task of meeting at some node, for two identical agents moving from node to node along the edges of an unknown anonymous connected graph. The rendezvous problem is unsolvable in the class of arbitrary connected graphs, as witnessed by the example of the cycle. Hence we restrict attention to rendezvous in trees, where rendezvous is feasible if and only if the initial positions of the agents are not symmetric. We prove that the minimum memory size guaranteeing rendezvous in all trees of size at most nis \u8302\u622e\u9a74(logn) bits. The upper bound is provided by an algorithm for abstract state machines accomplishing rendezvous in all trees, and using O(logn) bits of memory in trees of size at most n. The lower bound is a consequence of the need to distinguish between up to n\u8302\u622e\u9a74 1 links incident to a node. Thus, in the second part of the paper, we focus on the potential existence of pairs of finiteagents (i.e., finite automata) capable of accomplishing rendezvous in all bounded degreetrees. We show that, as opposed to what has been proved for the graph exploration problem, there are no finite agents capable of accomplishing rendezvous in all bounded degree trees.\nTitle:\nDeterministic Rendezvous in Trees with Little Memory\n\nAbstract:\nIt is known that graphs of doubling dimension O(loglogn) can be augmented to become navigable. We show that for doubling dimension \u226bloglogn, an infinite family of graphs cannot be augmented to become navigable. Our proof uses a counting argument which enable us to consider any kind of augmentations. In particular we do not restrict our analysis to the case of symmetric distributions, nor to distributions for which the choice of the long range link at a node must be independent from the choices of long range links at other nodes.\nTitle:\nBrief announcement: on augmented graph navigability\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.194, mean value for all the writers is 8.648, which makes it 1.241 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSearch games are attractive for their correspondence with classical width parameters. For instance, the invisible search number (a.k.a. node search number) of a graph is equal to its pathwidth plus 1, and the visible search number of a graph is equal to its treewidth plus 1. The connected variants of these games ask for search strategies that are connected, i.e., at every step of the strategy, the searched part of the graph induces a connected subgraph. We focus on monotone search strategies, i.e., strategies for which every node is searched exactly once. It is known that the monotone connected visible search number of an n-node graph is at most O(logn) times its visible search number. First, we prove that this logarithmic bound is tight. Precisely, we prove that there is an infinite family of graphs for which the ratio monotone connected visible search number over visible search number is \u03a9(logn). Second, we prove that, as opposed to the non-connected variant of visible graph searching, \u201crecontamination helps\u201d for connected visible search. Precisely, we describe an infinite family of graphs for which any monotone connected visible search strategy for any graph in this family requires strictly more searchers than the connected visible search number of the graph.\nTitle:", "model_inf_time": 1.31}, {"id": "4122", "output": "Dynamic Bound Checking for Software Fault Detection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMany embedded processing applications, such as those found in the automotive or medical field, require hardware designs that are at the same time low cost and reliable. Traditionally, reliable memory systems have been implemented using coded storage techniques, such as ECC. While these designs can effectively detect and correct memory faults such as transient errors and single-bit defects, their use bears a significant cost overhead. In this article, we propose a novel partial memory protection scheme that provides high-coverage fault protection for program code and data, but with much lower cost than traditional approaches. Our approach profiles program code and data usage to assess which program elements are most critical to maintaining program correctness. Critical code and variables are then placed into a limited protected storage resources. To ensure high coverage of program elements, our placement technique considers all program components simultaneously, including code, global variables, stack frames, and heap variables. The fault coverage of our approach is gauged using Monte Carlo fault-injection experiments, which confirm that our technique provides high levels of fault protection (99&percnt; coverage) with limited memory protection resources (36&percnt; protected area).\nTitle:\nExploiting selective placement for low-cost memory protection\n\nAbstract:\nSoftware bugs comprise the greatest threat to computer security today. Though enormous effort has been expended on eliminating security exploits, contemporary testing techniques are insufficient to deliver software free of security vulnerabilities. In this paper we propose a novel approach to security vulnerability analysis: dynamic control frontier profiling. Security exploits are often buried in rarely executed code paths hidden just beyond the path space explored by end-users. Therefore, we develop Schnauzer, a distributed sampling technology to discover the dynamic control frontier, which forms the line of demarcation between dynamically executed and unseen paths. This frontier may then be used to direct tools (such as white-box fuzz testers) to attain a level of testing coverage currently unachievable. We further demonstrate that the dynamic control frontier paths are a rich source of security bugs, sensitizing many known security exploits.\nTitle:\nSchnauzer: scalable profiling for likely security bug sites\n\nAbstract:\nThe verification of modern computing systems has grown to dominate the cost of system design, often with limited success as designs continue to be released with latent bugs. This trend is accelerated with the advent of highly integrated system-on-a-chip (SoC) designs, which feature multiple complex subcomponents connected by simultaneously active interfaces.In this paper, we introduce a closed-loop feedback technique targeting the verification of multiple components connected by parallel interfaces. We utilize an environment with hierarchical Markov models, where top-level submodels specify overarching simulation goals of the system, while lower-level submodels specify the detailed component-level input generation. Test accuracy is improved through the use of depth-driven random test generation. The approach allows users to specify correctness properties and key activity nodes in the design to be exercises. We examine three non-trivial designs, two microprocessors and a chip multiprocessor router switch, and we demonstrate that our technique finds many more bugs than constrained-random test generation technique and reduces the simulation effort in half, compared to previous Markov-model based solutions.\nTitle:\nDepth-driven verification of simultaneous interfaces\n\nAbstract:\nLow cost protection of embedded systems against soft errors has recently become a major concern. This issue is even more critical in memory elements that are inherently more prone to transient faults. In this paper, we propose a reliability aware data placement technique in order to partially protect embedded memory systems. We show that by adopting this method instead of traditional placement schemes with complete memory protection, an acceptable level of fault tolerance can be achieved while incurring less area and power overhead. In this approach, each variable in the program is placed in either protected or non-protected memory area according to the profile-driven liveness analysis of all memory variables. In order to measure the level of fault coverage, we inject faults into the memory during the course of program execution in a Monte Carlo simulation framework. Subsequently, we calculate the coverage of partial protection scheme based on the number of protected, failed and crashed runs during the fault injection experiment.\nTitle:\nReliability-aware data placement for partial memory protection in embedded processors\n\nAbstract:\nNumerous tools have been proposed to help developers fix software errors and inefficiencies. Widely-used techniques such as memory checking suffer from overheads that limit their use to pre-deployment testing, while more advanced systems have such severe performance impacts that they may require special-purpose hardware. Previous works have described hardware that can accelerate individual analyses, but such specialization stymies adoption; generalized mechanisms are more likely to be added to commercial processors. This paper demonstrates that the ability to set an unlimited number of fine-grain data watchpoints can reduce the runtime overheads of numerous dynamic software analysis techniques. We detail the watchpoint capabilities required to accelerate these analyses while remaining general enough to be useful in the future. We describe a hardware design that stores watchpoints in main memory and utilizes two different on-chip caches to accelerate performance. The first is a bitmap lookaside buffer that stores fine-grained watchpoints, while the second is a range cache that can efficiently hold large contiguous regions of watchpoints. As an example of the power of such a system, it is possible to use watchpoints to accelerate read/write set checks in a software data race detector by nearly 9x.\nTitle:\nA case for unlimited watchpoints\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.8, mean value for all the writers is 8.648, which makes it 0.723 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nImproperly bounded program inputs present a major class of program defects. In secure applications, these bugs can be exploited by malicious users, allowing them to overwrite buffers and execute harmful code. In this paper, we present a high coverage dynamic technique for detecting software faults caused by improperly bounded program inputs. Our approach is novel in that it retains the advantages of dynamic bug detection, scope and precision; while at the same time, relaxing the requirement that the user specify the input that exposes the bug. To implement our approach, inputs are shadowed by additional state that characterize the allowed bounds of input-derived variables. Program operations and decision points may alter the shadowed state associated with input variables. Potentially hazardous program sites, such as an array references and string functions, are checked against the entire range of values that the user might specify. The approach found several bugs including two high-risk security bugs in a recent version of OpenSSH.\nTitle:", "model_inf_time": 1.28}, {"id": "4123", "output": "Small-Time Scaling Behavior of Internet Backbone Traffic", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe perform an extensive wavelet analysis of Internet backbone traffic traces to observe and understand the causes of small-time scaling phenomena present in them. We observe that for a majority of the traces, the second-order scaling exponents at small time scales (1-100 ms) are fairly close to 0.5, indicating that traffic fluctuations at these time scales are nearly uncorrelated. Some traces, however, do exhibit moderately large scaling exponents (\u22480.7) at small time scales. In addition, the traces manifest mostly monofractal behaviors at small time scales. To identify the network causes of the observed scaling behavior, we analyze the flow composition of the traffic along two dimensions--flow byte contribution and flow density. Our study points to the dense flows (i.e., flows with densely clustered packets) as the correlation-causing factor in small time scales, and reveals that the traffic composition in terms of proportions of dense vs. sparse flows plays a major role in influencing the small-time scalings of aggregate traffic. Since queuing inside routers is influenced by traffic fluctuations at small time-scales, our observations and results have important implications for networking modeling, service provisioning and traffic engineering.\nTitle:\nSmall-time scaling behavior of Internet backbone traffic\n\nAbstract:\nIn this paper, we perform a detailed analysis of point-to-point packet delay in an operational tier-1 network. The point-to-point delay is the time experienced by a packet from an ingress to an egress point in an ISP, and it provides the most basic information regarding the delay performance of the ISP's network. Using packet traces captured in the operational network, we obtain precise point-to-point packet delay measurements and analyze the various factors affecting them. Through a simple, step-by-step, systematic methodology and careful data analysis, we identify the major network factors that contribute to point-to-point packet delay and characterize their effect on the network delay performance. Our findings are: (1) delay distributions vary greatly in shape, depending on the path and link utilization; (2) after constant factors dependent only on the path and packet size are removed, the 99th percentile variable delay remains under 1ms over several hops and under link utilization below 90% on a bottleneck; (3) a very small number of packets experience very large delay in short bursts.\nTitle:\nAnalysis of point-to-point packet delay in an operational network\n\nAbstract:\nThis paper presents the design and implementation of a real-time behavior profiling system for high-speed Internet links. The profiling system uses flow-level information from continuous packet or flow monitoring systems, and uses data mining and information-theoretic techniques to automatically discover significant events based on the communication patterns of end-hosts. We demonstrate the operational feasibility of the system by implementing it and performing extensive benchmarking of CPU and memory costs using a variety of packet traces from OC-48 links in an Internet backbone network. To improve the robustness of this system against sudden traffic surges such as those caused by denial of service attacks or worm outbreaks, we propose a simple yet effective filtering algorithm. The proposed algorithm successfully reduces the CPU and memory cost while maintaining high profiling accuracy.\nTitle:\nA Real-Time Network Traffic Profiling System\n\nAbstract:\nThis paper presents the design and implementation of a real-time behaviour profiling system for internet links. The system uses flow-level information, and applies data mining and information-theoretic techniques to automatically discover significant events based on communication patterns. We demonstrate the operational feasibility of the system by implementing it and performing benchmarking of CPU and memory costs using packet traces from backbone links. To improve the robustness of this system against sudden traffic surges, we propose a novel filtering algorithm. The proposed algorithm successfully reduces the CPU and memory cost while maintaining high profiling accuracy. Finally, we devise and evaluate simple yet effective blocking strategies to reduce prevalent exploit traffic, and build a simple event analysis engine to generate ACL rules for filtering unwanted traffic.\nTitle:\nReal-time behaviour profiling for network monitoring\n\nAbstract:\nTimely detection of changes in traffic load is critical for initiating appropriate traffic engineering mechanisms. Accurate measurement of traffic is essential since the efficacy of change detection depends on the accuracy of traffic estimation. However, precise traffic measurement involves inspecting every packet traversing a link, resulting in significant overhead, particularly on high speed links. Sampling techniques for traffic load estimation are proposed as a way to limit the measurement overhead. In this paper, we address the problem of bounding sampling error within a pre-specified tolerance level and propose an adaptive random sampling technique that determines the minimum sampling probability adaptively according to traffic dynamics. Using real network traffic traces, we show that the proposed adaptive random sampling technique indeed produces the desired accuracy, while also yielding significant reduction in the amount of traffic samples. We also investigate the impact of sampling errors on the performance of load change detection.\nTitle:\nAdaptive random sampling for load change detection\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.225, mean value for all the writers is 8.648, which makes it 0.492 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe small-time (sub-seconds) scaling behaviors of Internet backbone traffic, based on traces collected from OC3/12/48 links in a tier-1 ISP is studied. We observe that for a majority of these traces, the (second-order) scaling exponents at small time scales (1 ms - 100 ms) are fairly close to 0.5, indicating that traffic fluctuations at these time scales are (nearly) uncorrelated. In addition, the traces manifest mostly monofractal behaviors at small time scales. The objective of the paper is to understand the potential causes or factors that influence the small-time scalings of Internet backbone traffic via empirical data analysis. We analyze the traffic composition of the traces along two dimensions - flow size and flow density. Our study uncovers dense flows (i.e., flows with bursts of densely clustered packets) as the correlation-causing factor in small time scales, and reveals that the traffic composition in terms of proportions of dense vs. sparse flows plays a major role in influencing the small-time scalings of aggregate traffic.\nTitle:", "model_inf_time": 1.32}, {"id": "4124", "output": "Inferring Congestion Sharing for Cooperative Control", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe explore processor-cache affinity scheduling of parallel network protocol processing in a setting in which protocol processing executes on a shared-memory multiprocessor concurrently with a general workload of non-protocol activity. We find that affinity scheduling can significantly reduce the communication delay associated with protocol processing, enabling the host to support a greater number of concurrent streams and to provide a higher maximum throughput to individual streams. In addition, we compare implementations of two parallelization approaches (Locking and Independent Protocol Stacks) with very different caching behaviors.\nTitle:\nScheduling for cache affinity in parallelized communication protocols\n\nAbstract:\nParallel discrete-event simulation offers the promise of harnessing the computational power of multiple processors in order to reduce the time needed for simulation-based performance studies. In this paper, we investigate the use of optimistic parallel simulation techniques in simulating reliable multicast communication network protocols. Through empirical studies (using the TeD simulation programming language, the Georgia Tech time warp simulator, and a 12-processor SGI Challenge), we find that these parallelized simulations can run noticeably faster than a uniprocessor simulation and, in a number of cases, can make effective use of parallel resources. These results are somewhat surprising because reliable multicast protocols require considerable communication (and hence synchronization) among different network entities.\nTitle:\nOptimistic parallel simulation of reliable multicast protocols\n\nAbstract:\nWe explore processor-cache affinity scheduling of parallel network protocol processing, in a setting in which protocol processing executes on a shared-memory multiprocessor concurrently with a general workload of non-protocol activity. We find that affinity-based scheduling can significantly reduce the communication delay associated with protocol processing, enabling the host to support a greater number of concurrent streams and to provide higher maximum throughput to individual streams. In addition, we compare the performance of two parallelization alternatives, locking and independent protocol stacks (IPS), with very different caching behaviors. We find that IPS (which maximizes cache affinity) delivers much lower message latency and significantly higher message throughput capacity, yet exhibits less robust response to infra-stream burstiness and limited intra-stream scalability.\nTitle:\nThe performance impact of scheduling for cache affinity in parallel network processing\n\nAbstract:\nEstimating per-flow performance characteristics such as latency, loss, and jitter from a location other than the connection end-points can help locate performance problems affecting end-to-end flows. However, doing this accurately in real-time is challenging and requires tracking extensive amounts of TCP state and is thus infeasible on nodes that process large volumes of traffic. In this paper, we propose an approximate and scalable method to estimate TCP flow latency in the network. Our method scales with the number of flows by keeping approximate TCP state in a compressed, probabilistic data structure that requires less memory and compute, but sacrifices a small amount of accuracy. We validate our method using backbone link traces and compare it against an exact, baseline approach. In our approximate method, 99% of the reported latencies are within 10.3 ms of the baseline reported value, while taking an order of magnitude less memory.\nTitle:\nEstimating TCP latency approximately with passive measurements\n\nAbstract:\nIn this paper, we develop a rigorous, unified framework based on ordinary differential equations (ODEs) to study epidemic routing and its variations. These ODEs can be derived as limits of Markovian models under a natural scaling as the number of nodes increases. While an analytical study of Markovian models is quite complex and numerical solution impractical for large networks, the corresponding ODE models yield closed-form expressions for several performance metrics of interest, and a numerical solution complexity that does not increase with the number of nodes. Using this ODE approach, we investigate how resources such as buffer space and the number of copies made for a packet can be traded for faster delivery, illustrating the differences among various forwarding and recovery schemes considered. We perform model validations through simulation studies. Finally we consider the effect of buffer management by complementing the forwarding models with Markovian and fluid buffer models.\nTitle:\nPerformance modeling of epidemic routing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.425, mean value for all the writers is 8.648, which makes it 0.663 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCurrent Internet congestion control protocols operate independently on a per-flow basis. Recent work has demonstrated that cooperative congestion control strategies between flows can improve performance for a variety of applications, ranging from aggregated TCP transmissions to multiple-sender multicast applications. However, in order for this cooperation to be effective, one must first identify the flows that are congested at the same set of resources. In this paper, we present techniques based on loss or delay observations at end-hosts to infer whether or not two flows experiencing congestion are congested at the same network resources. We validate these techniques via queueing analysis, simulation, and experimentation within the Internet.\nTitle:", "model_inf_time": 1.15}, {"id": "4125", "output": "MSPlayer: Multi-Source, Multi-Path Video Streaming for Mobile Devices", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nOnline video streaming through mobile devices has become extremely popular nowadays. YouTube, for example, reported that the percentage of its traffic streaming to mobile devices has soared from 6% to more than 40% over the past two years. Moreover, people are constantly seeking to stream high quality video for better experience while often suffering from limited bandwidth. Thanks to the rapid deployment of content delivery networks (CDNs), popular videos are now replicated at different sites, and users can stream videos from close-by locations with low latencies. As mobile devices nowadays are equipped with multiple wireless interfaces (e.g., WiFi and 3G/4G), aggregating bandwidth for high definition video streaming has become possible.\n\nWe propose a client-based video streaming solution, MSPlayer, that takes advantage of multiple video sources as well as multiple network paths through different interfaces. MSPlayer reduces start-up latency and provides high quality video streaming and robust data transport in mobile scenarios. We experimentally demonstrate our solution on a testbed and through the YouTube video service.\n\n\nTitle:\nMSPlayer: Multi-Source and multi-Path LeverAged YoutubER.\n\nAbstract:\nWe present a proxy-assisted video delivery architecture that can simultaneously reduce the resources requirements at the central server and the service latency experienced by clients (i.e., end users). Under the proposed video delivery architecture, we develop and analyze two novel proxy-assisted video streaming techniques for on-demand delivery of video objects to a large number of clients. By taking advantage of the resources available at the proxy servers, these techniques not only significantly reduce the central server and network resource requirements, but are also capable of providing near-instantaneous service to a large number of clients. We optimize the performance of our video streaming architecture by carefully selecting video delivery techniques for videos of various popularity and intelligently allocating resources between proxy servers and the central server. Through empirical studies, we demonstrate the efficacy of the proposed proxy-assisted video streaming techniques.\nTitle:\nProxy-assisted techniques for delivering continuous multimedia streams\n\nAbstract:\nThe high-bandwidth requirements and long-lived characteristics of digital video make transmission bandwidth usage a key limiting factor in the widespread streaming of such content over the Internet. A challenging problem is to develop bandwidth-efficient techniques for delivering popular videos to a large, asynchronous client population with time-varying demand characteristics. In this paper, we propose smooth workload adaptive broadcast to address the above issues. A key component of our scheme is Flexible Periodic Broadcast (FPB). By introducing a feedback control loop into FPB, and enhancing FPB using techniques such as parsimonious transmission, smooth workload adaptive broadcast provides instantaneous or near-instantaneous playback services and can smoothly adapt to workload changes. Furthermore, FPB, as proposed in this paper, is bandwidth efficient and exhibits the periodic smooth channel transition property.\nTitle:\nSmooth workload adaptive broadcast\n\nAbstract:\nMotivated by the wide use of TCP for multimedia streaming in practice and the increasing availability of multipath between end hosts, we study multipath live streaming via TCP in this article. We first design a simple and practical TCP-based multipath streaming scheme, named Dynamic MPath-streaming (DMP-streaming), which dynamically distributes packets over multiple paths by implicitly inferring the available bandwidths on these paths. To allow systematic performance study, we develop an analytical model for DMP-streaming and validate the model using extensive ns simulation and Internet experiments. We explore the parameter space of this model and find that DMP-streaming generally provides satisfactory performance when the aggregate achievable TCP throughput is 1.6 times the video bitrate, when allowing a few seconds of startup delay. Last, we comment on the benefits of using multipath versus single path for TCP-based streaming.\nTitle:\nMultipath live streaming via TCP: Scheme, performance and benefits\n\nAbstract:\nWe present a novel video streaming technique called catching for on-demand delivery of \u201chot\u201d (i.e., frequently accessed) video objects to a large number of clients. This technique not only significantly reduces the server and network resource requirements but also is capable of providing near-instantaneous service to a large number of clients. By combining this technique for delivery of \u201chot\u201d video objects with controlled multicast [4] for delivery of \u201ccold\u201d video objects, we design an efficient video delivery scheme referred to as selective catching. Through empirical studies, we demonstrate the efficacy of the proposed video delivery schemes.\nTitle:\nCatching and selective catching: efficient latency reduction techniques for delivering continuous multimedia streams\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.952, mean value for all the writers is 8.648, which makes it 0.259 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOnline video streaming through mobile devices has become extremely popular nowadays. YouTube, for example, reported that the percentage of its traffic streaming to mobile devices has soared from 6% to more than 40% over the past two years. Moreover, people are constantly seeking to stream high-quality videos for better experience while often suffering from limited bandwidth. With the rapid deployment of content delivery networks, popular videos are now replicated at different sites, and users can stream over-the-top videos from close-by sources with low latency. Aggregating bandwidth for high definition video streaming has become possible as mobile devices, nowadays, are equipped with multiple wireless interfaces (e.g., WiFi and 3G/4G). We propose a client-based video streaming solution, MSPlayer, that takes advantage of multiple video sources and leverages multiple network paths through different interfaces. MSPlayer reduces start-up latency and provides robust data transport with high video quality in mobile scenarios. We experimentally demonstrate our solution on a test bed and through the YouTube video service.\nTitle:", "model_inf_time": 1.61}, {"id": "4126", "output": "Social Sciences Shaping the Future of the Internet", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nRethinking the role of journals in computer science.\nTitle:\nJournals for certification, conferences for rapid dissemination\n\nAbstract:\nThis paper introduces computation compatibility and communication compatibility as requirements for a distributed mechanism implementation. Just as payments are used to create incentive compatible mechanisms, some technique must be used to create computation/communication compatible mechanisms. This paper explores computation redundancy and communication redundancy as two such techniques. This paper uses interdomain routing as an example domain, and considers where redundancy can succeed and fail in addressing cheating with respect to computation and communication.\nTitle:\nUsing redundancy to improve robustness of distributed mechanism implementations\n\nAbstract:\n As traditional commerce moves on-line more business transactions will be mediated by software agents, and the ability of agentmediated electronic marketplaces to efficiently allocate resources will be highly dependent on the complexity of the decision problems that agents face; determined in part by the structure of the marketplace, resource characteristics, and the nature of agents\" local problems. We compare auction performance for agents that have hard local problems, and uncertain values... \nTitle:\nOptimal Auction Design for Agents with Hard Valuation Problems\n\nAbstract:\nWe consider the problem of designing mechanisms for online problems in which agents arrive over time and truthfully announce their arrival. These problems are becoming extremely common in a wide variety of problems involving wireless networking and webserving. We show how the standard results of mechanism design can be modified to apply to this setting, provide conditions under which efficient and incentive compatible mechanisms exist and analyze several important online models including wireless networks and web serving.\nTitle:\nPricing WiFi at Starbucks: issues in online mechanism design\n\nAbstract:\nThe next decade will see an abundance of new intelligent systems, many of which will be market-based. Soon, users will indirectly interact with many markets without knowing it: when driving their car, when listening to a song, when talking on the phone, when backing up their files, or even when surfing the web. I argue that these new systems can only be successful if a new approach is chosen towards designing them. In particular, the complexities of the market must be hidden and the interaction for the user must be seamless. In this paper I introduce the general problem of \"Hidden Market Design.\" An important goal of this research agenda is to understand the trade-off between increasing market efficiency on the one side, and decreasing interaction complexity for the user on the other side. To illustrate the main paradigm, I give a series of examples where hidden markets could be applied. I hope that the problem of hidden market design will inspire other researchers and lead to new research in this direction, paving the way for more successful market-based systems in the future.\nTitle:\nHidden Market Design\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.201, mean value for all the writers is 8.648, which makes it 1.235 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nEconomic and social sciences will drive Internet protocols and services into the future.\nTitle:", "model_inf_time": 0.97}, {"id": "4127", "output": "Performance Evaluation of Domain Aggregation Schemes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn high-speed networks it is desirable to interleave routing and resource (such as bandwidth) reservation. The PNNI standard for private ATM networks is a recent example for an algorithm that does this using a sequential crank-back mechanism. In this work, we suggest to do resource reservation along several routes in parallel. We present an analytical model that demonstrates that when there are several routes to the destination it pays to attempt reservation along more than a single route. Following this analytic observation, we present a family of algorithms that route and reserve resources along parallel subroutes. The algorithms of the family represent different trade-offs between the speed and the quality of the established route. The presented algorithms are simulated against several legacy algorithm, including PNNI crank-back, and exhibit higher network utilization and faster connection set-up time.\nTitle:\nMulti-path routing combined with resource reservation\n\nAbstract:\nThis paper addresses the problem of aggregating the topology of a sub-network in a compact way with minimum distortion. The problem arises from networks that have a hierarchical structure, where each sub-network must advertise the cost of routing between each pair of its border nodes. The straight-forward solution of advertising the exact cost for each pair has a quadratic cost which is not practical. We look at the realistic scenario of networks where all links are bidirectional, but their cost (or distance) in the opposite directions might differ significantly. The paper presents a solution with distortion that is bounded by the logarithm of the number of border nodes and the square-root of the asymmetry in the cost of a link. This is the first time that a theoretical bound is given to an undirected graph. We show how to apply our solution to PNNI, and suggest some other heuristics that are tested to perform better than the provenly bounded solution\nTitle:\nTopology aggregation for directed graphs\n\nAbstract:\nWe study a map of the Internet (at the autonomous systems level), by introducing and using the method of k-shell decomposition and the methods of percolation theory and f ractal geometry, to find a model for the structure of the Internet. In particular, our analysis uses information on the connectivity of the network shells to separate, in a unique (no parameters) way, the Internet into three subcomponents: (i) a nucleus that is a small (approximate to 100 nodes), very well connected globally distributed subgraph; (ii) a fractal subcomponent that is able to connect the bulk of the Internet without congesting the nucleus, with self-similar properties and critical exponents predicted from percolation theory; and (iii) dendrite-like structures, usually isolated nodes that are connected to the rest of the network through the nucleus only. We show that our method of decomposition is robust and provides insight into the underlying structure of the Internet and its functional consequences. Our approach of decomposing the network is general and also useful when studying other complex networks.\nTitle:\nA Model Of Internet Topology Using K-Shell Decomposition\n\nAbstract:\nIn this paper we investigate the problem of finding minimum-delay application-layer multicast trees, such as the trees constructed in overlay networks. It is accepted that shortest path trees are not a good solution for the problem since such trees can have nodes with very large degree, termed high-load nodes. The load on these nodes makes them a bottleneck in the distribution tree, due to computation load and access link bandwidth constraints. Many previous solutions limited the maximum degree of the nodes by introducing arbitrary constraints. In this work, we show how to directly map the node load to the delay penalty at the application host, and create a new model that captures the trade offs between the desire to select shortest path trees and the need to constrain the load on the hosts. In this model the problem is shown to be NP-hard. We therefore present an approximation algorithm and an alternative heuristic algorithm. Our heuristic algorithm is shown by simulations to be scalable for large group sizes, and produces results that are very close to optimal.\nTitle:\nApproximation and heuristic algorithms for minimum-delay application-layer multicast trees\n\nAbstract:\nWe present an anti-pirate revocation scheme for broadcastencryption systems (e.g., pay TV), in which the data isencrypted to ensure payment by users. In the systems weconsider, decryption of keys is done on smartcards, and keymanagement is done in-band. Our starting point is a recentscheme of Naor and Pinkas. The basic scheme uses secretsharing to remove up to t parties, is information theoreticsecure against coalitions of size t, and is capable of creatinga new group key. However, with current smartcard technology,this scheme is only feasible for small system parameters,allowing up to about 100 pirates to be revoked beforeall the smartcards need to be replaced.We first present a novel implementation method of theirbasic scheme that distributes the work in novel ways amongthe smartcard, set-top terminal, and center. Based on this,we construct several improved schemes for many statefulrevocation rounds that scale to realistic system sizes. Weallow up to about 10000 pirates to be revoked using currentsmartcard technology before re-carding is needed. Thetransmission lengths of our constructions are on par withthose of the best tree-based schemes. However, our constructionshave much lower smartcard CPU complexity:only O (1) smartcard operations per revocation round, asopposed to a poly-logarithmic complexity of the best tree-basedschemes.We evaluate the system behavior via an exhaustive simulationstudy. Our simulations show that with mild assumptionson the piracy discovery rate, our constructions canperform effective pirate revocation for realistic broadcastencryption scenarios.\nTitle:\nA Practical Revocation Scheme for Broadcast Encryption Using Smart Cards\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.91, mean value for all the writers is 8.648, which makes it 0.63 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the future, global networks will consist of a hierarchy ofsubnetworks called domains. For reasons of both scalability andsecurity, domains will not reveal details of their internalstructure to outside nodes. Instead, these domains will advertiseonly a summary, or aggregated view, of their internal structure,e.g., as proposed by the ATM PNNI standard.This work compares, by simulation, the performance of severaldifferent aggregation schemes in terms of network throughput (thefraction of attempted connections that are realized), and networkcontrol load (the average number of crankbacks per realizedconnection). The simulation emulate a connection oriented networkwith a PNNI-like hierarchical source routing algorithm.Our main results are: (1) minimum spanning tree is a goodaggregation scheme; (2) exponential link cost functions performbetter than min-hop routing; (3) our suggested logarithmic updatescheme that determine when re-aggregation should be computed cansignificantly reduce the computational overhead due tore-aggregation with a negligible decrease in performance.\nTitle:", "model_inf_time": 1.32}, {"id": "4128", "output": "A Wearable Textile Platform for Cloud-Based Pervasive Healthcare Monitoring", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nPervasive healthcare applications utilizing body sensor networks generate a vast amount of data that need to be managed and stored for processing and future usage. Cloud computing among with the Internet of Things (IoT) concept is a new trend for efficient managing and processing of sensor data online. This paper presents a platform based on Cloud Computing for management of mobile and wearable healthcare sensors, demonstrating this way the IoT paradigm applied on pervasive healthcare.\nTitle:\nBringing IoT and Cloud Computing towards Pervasive Healthcare\n\nAbstract:\nThe paper presents the concept and an initial implementation of a patient status awareness system that may be used for patient activity interpretation and emergency recognition in cases like elder falls and distress speech expressions. The awareness is performed through collecting, analyzing and classifying motion and sound data. The latter are collected through sensors equipped with accelerometers and microphones that are attached on the body of the patients and transmit patient movement and sound data wirelessly to the monitoring unit. Applying Short Time Fourier Transform (STFT) and spectrogram analysis on sounds detection of fall incidents is possible. The classification of the sound and movement data is performed using Support Vector Machines. Evaluation results indicate the high accuracy and the effectiveness of the proposed implementation. The system architecture is open and can be easily enhanced to include patient awareness based on additional context (e.g., physiological data).\nTitle:\nEnabling human status awareness in assistive environments based on advanced sound and motion data classification\n\nAbstract:\nAcquisition of pervasive sensor data can be often unsuccessful due to power outage at nodes, time synchronization issues, interference, network transmission failures or sensor hardware issues. Such failures can lead to inadequate data delivery to the monitoring applications resulting in erroneous conclusions. This paper presents a missing values substitution framework that addresses the aforementioned issue. The presented framework has been evaluated within a pervasive sensor monitoring environment that collects and transmits patient health related data and results have been presented.\nTitle:\nOptimizing pervasive sensor data acquisition utilizing missing values substitution\n\nAbstract:\nMobile devices have entered our daily life in several forms, such as tablets, smartphones, smartwatches and wearable devices, in general. The majority of those devices have built-in several motion sensors, such as accelerometers, gyroscopes, orientation and rotation sensors. The activity recognition or emergency event detection in cases of falls or abnormal activity conduce a challenging task, especially for elder people living independently in their homes. In this work, we present a methodology capable of performing real time fall detect, using data from a mobile accelerometer sensor. To this end, data taken from the 3-axis accelerometer is transformed using the Incremental Principal Components Analysis methodology. Next, we utilize the cumulative sum algorithm, which is capable of detecting changes using devices having limited CPU power and memory resources. Our experimental results are promising and indicate that using the proposed methodology, real time fall detection is feasible.\nTitle:\nOn-Line Fall Detection Via Mobile Accelerometer Data\n\nAbstract:\nThis paper presents a novel implementation of a patient fall detection system that may be used for patient activity recognition and emergency treatment. Sensors equipped with accelerometers are attached on the body of the patients and transmit patient movement data wirelessly to the monitoring unit. The methodology of support Vector Machines is used for precise classification of the acquired data and determination of a fall emergency event. Then a context-aware server transmits video from patient site properly coded according to both patient and network status. Evaluation results indicate the high accuracy of the classification method and the effectiveness of the proposed implementation.\nTitle:\nPatient Fall Detection using Support Vector Machines\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.433, mean value for all the writers is 8.648, which makes it 2.376 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMobile pervasive healthcare technologies can support a wide range of applications and services including patient monitoring and emergency response. At the same time they introduce several challenges, like data storage and management, interoperability and availability of heterogeneous resources, unified and ubiquitous access issues. One potential solution for addressing all aforementioned issues is the introduction of the Cloud Computing concept. Within this context, in this work we have developed and present a wearable -- textile platform based on open hardware and software that collects motion and heartbeat data and stores them wirelessly on an open Cloud infrastructure for monitoring and further processing. The proposed system may be used to promote the independent living of patient and elderly requiring constant surveillance.\nTitle:", "model_inf_time": 1.39}, {"id": "4129", "output": "Self-Aware Data Compression on FPGA for Data Warehousing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe performance of large-scale graph processing suffers from challenges including poor locality, lack of scalability, random access pattern, and heavy data conflicts. Some characteristics of FPGA make it a promising solution to accelerate various applications. For example, on-chip block RAMs can provide high throughput for random data access. However, large-scale processing on a single FPGA chip is constrained by limited on-chip memory resources and off-chip bandwidth. Using a multi-FPGA architecture may alleviate these problems to some extent, while the data partitioning and communication schemes should be considered to ensure the locality and reduce data conflicts. In this paper, we propose ForeGraph, a large-scale graph processing framework based on the multi-FPGA architecture. In ForeGraph, each FPGA board only stores a partition of the entire graph in off-chip memory. Communication over partitions is reduced. Vertices and edges are sequentially loaded onto the FPGA chip and processed. Under our scheduling scheme, each FPGA chip performs graph processing in parallel without conflicts. We also analyze the impact of system parameters on the performance of ForeGraph. Our experimental results on Xilinx Virtex UltraScale XCVU190 chip show ForeGraph outperforms state-of-the-art FPGA-based large-scale graph processing systems by 4.54x when executing PageRank on the Twitter graph (1.4 billion edges). The average throughput is over 900 MTEPS in our design and 2.03x larger than previous work.\nTitle:\nForeGraph: Exploring Large-scale Graph Processing on Multi-FPGA Architecture.\n\nAbstract:\nThis paper describes a FPGA-based hardware acceleration system for LambdaRank algorithm. LambdaRank Algorithm is a Neural Network (NN)-based learning to rank algorithm. It is intensively used by web search engine companies to increase the search relevance. Since i) the cost function for the ranking problem is much more complex than that of traditional Back-Propagation(BP) NNs, and ii) no coarse-grained parallelism exists, LambdaRank is hard to be efficiently accelerated by GPU or computer clusters. We presents a FPGA-based accelerator solution to provide high computing performance. A compact deep pipeline is proposed to handle the complex computing in the batch updating. The area scales linearly with the number of hidden nodes in the NN model. We also carefully design a data format to enable streaming consumption of the training data from host computer. The accelerator shows up to 24.6 speedup compared with the pure software implementation on datasets from a commercial search engine.\nTitle:\nLambdaRank acceleration for relevance ranking in web search engines (abstract only)\n\nAbstract:\nSparse matrix-vector multiplication (SpMV) is a fundamental operation for many applications. Many studies have been done to implement the SpMV on different platforms, while few work focused on the very large scale datasets with millions of dimensions. This paper addresses the challenges of implementing large scale SpMV with FPGA and GPU in the application of web link graph analysis. In the FPGA implementation, we designed the task partition and memory hierarchy according to the analysis of datasets scale and their access pattern. In the GPU implementation, we designed a fast and scalable SpMV routine with three passes, using a modified Compressed Sparse Row format. Results show that FPGA and GPU implementation achieves about 29x and 30x speedup on a StratixII EP2S180 FPGA and Radeon 5870 Graphic Card respectively compared with a Phenom 9550 CPU.\nTitle:\nFPGA and GPU implementation of large scale SpMV\n\nAbstract:\nLarge-scale graph processing is gaining increasing attentions in many domains. Meanwhile, FPGA provides a power-efficient and highly parallel platform for many applications, and has been applied to custom computing in many domains. In this paper, we describe FPGP (FPGA Graph Processing), a streamlined vertex-centric graph processing framework on FPGA, based on the interval-shard structure. FPGP is adaptable to different graph algorithms and users do not need to change the whole implementation on the FPGA. In our implementation, an on-chip parallel graph processor is proposed to both maximize the off-chip bandwidth of graph data and fully utilize the parallelism of graph processing. Meanwhile, we analyze the performance of FPGP and show the scalability of FPGP when the bandwidth of data path increases. FPGP is more power-efficient than single machine systems and scalable to larger graphs compared with other FPGA-based graph systems.\n\n\nTitle:\nFPGP: Graph Processing Framework on FPGA A Case Study of Breadth-First Search.\n\nAbstract:\nFrequent Itemset Mining (FIM) is designed to find frequently occurring itemsets among a series of transactions. It is extremely memory and time expensive. Frequent Itemset Mining from a Data Stream (FIM-DS) is even more challenging since storing the infinite data to memory is infeasible. In recent years, researchers have proposed various approximation algorithms for FIM-DS. However, the computation complexity is still high, and these methods are difficult to be accelerated using hardware accelerators. In this paper, we propose a Space-Saving based approximate algorithm for FIM-DS. It avoids exponential candidates generation and comparisons. We realize a hardware accelerator design and implement it on an FPGA platform. Experimental results show that our algorithm in software implementation achieves up to 8.4\u00d7 speedup for transactions with small item database, and our hardware accelerator achieves up to 50,000\u00d7 speedup for transactions with small number of items, and 5.3\u00d7 speedup for transactions with extremely large number of items.\nTitle:\nApproximate Frequent Itemset Mining for streaming data on FPGA\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.993, mean value for all the writers is 8.648, which makes it 1.148 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWith the exponential growth of data size, data storage and analysis have been exposed to more challenges due to the lack of disk capacity and the limited network bandwidth. Data compression technique provides a good solution to mitigate these effects. In this paper, we propose a self-aware data compression system on FPGA for typical data warehousing, such as Hive, with column stored data and multi-threading requirements. The hardware accelerators can change the degree and hierarchy of parallelism depending on the data to be compressed (during the runtime). We test the system performance on a Xilinx VC707 FPGA board and the experimental results show that, up to 16 3-parallelism accelerators can be implemented and the throughput could be improved up to 432 MB/s. It is 6.25X speedup compared with the software solution under the same number of threads.\nTitle:", "model_inf_time": 1.52}, {"id": "4130", "output": "An aggressive reduction scheme for the simple plant location problem.", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n\u2022The Simple Plant Location Problem is a classic problem in OR.\u2022It is a strongly NP-hard combinatorial optimisation problem.\u2022We derive a new and very large family of cutting planes for it.\u2022We show that these cutting planes define facets under certain conditions.\nTitle:\nNew valid inequalities and facets for the Simple Plant Location Problem.\n\nAbstract:\n\u2022We present several new formulations for the classical \u201cSequential Ordering Problem\u201d.\u2022Two interesting families of equations are useful to strengthen the linear programming relaxations.\u2022We prove that the linear programming relaxations of the new formulations give good bounds.\u2022We analyse computational results to compare the known and new formulations.\u2022The paper also addresses the capacitated variant of the problem.\nTitle:\nStronger multi-commodity flow formulations of the (capacitated) sequential ordering problem.\n\nAbstract:\n\u2022The Steiner TSP is a variant of the TSP suitable for road networks.\u2022We consider an extension with stochastic and correlated costs.\u2022This is modelled as a mean-variance optimisation problem.\u2022We show how to approximate the efficient frontier via integer programming.\u2022Instances with up to 100 nodes can be solved in reasonable times.\nTitle:\nThe Steiner travelling salesman problem with correlated costs.\n\nAbstract:\n\u2022We present several new formulations for the classical \u201cCapacitated Vehicle Routing Problem\u201d.\u2022We survey other previous flow-based formulations.\u2022We show that our new formulations have desired properties.\u2022We prove that the linear programming relaxations of the new formulations give better bounds.\u2022We analyse computational results to compare the known and new formulations.\nTitle:\nStronger multi-commodity flow formulations of the Capacitated Vehicle Routing Problem.\n\nAbstract:\n\u2022New mathematical models for the Capacitated Vehicle Routing Problem.\u2022Lower bounds computable in pseudo-polynomial time.\u2022Column-generation technique to solve vehicle routing problems with capacity constraints.\nTitle:\nThe Capacitated Vehicle Routing Problem: Stronger bounds in pseudo-polynomial time.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.614, mean value for all the writers is 8.648, which makes it 0.029 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\n\u2022The simple plant location problem is a classical problem in OR.\u2022We present an aggressive reduction scheme for it.\u2022The reduction procedures are based on optimality arguments and linear programming.\u2022Typically over 98% of the variables are eliminated.\u2022This enables larger instances than ever before to be solved to proven optimality.\nTitle:", "model_inf_time": 1.07}, {"id": "4131", "output": "Optimal Algorithms for Graph Generation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe summarize the currently best known theoretical results for the single-source shortest paths problem for directed graphs with non-negative edge weights. We also point out that a recent result due to Cherkassky, Goldberg and Silverstein (1996) leads to even better time bounds for this problem than claimed by the authors.\nTitle:\nRecent results on the single-source shortest paths problem\n\nAbstract:\nWe study uncertainty models in sequential pattern mining. We discuss some kinds of uncertainties that could exist in data, and show how these uncertainties can be modelled using probabilistic databases. We then obtain possible world semantics for them and show how frequent sequences could be mined using the probabilistic frequentness measure.\nTitle:\nUncertainty in sequential pattern mining\n\nAbstract:\nRandom sampling is an important tool in the design of parallel algorithms. Using random sampling it is possible to obtain simple parallel algorithms which are efficient in practice. We will focus on the use of random sampling in fundamental problems such as sorting, selection, list ranking and graph connectivity.\nTitle:\nRandom Sampling Techniques in Parallel Computation\n\nAbstract:\nWe present a deterministic algorithm for selecting the element of rank k among N=n2 elements, 1kN, on an n\u00d7n mesh-connected processor array in (1.44+ parallel computation steps, for any constant >0, using constant sized queues. This is a considerable improvement over the best previous deterministic algorithm, which was based upon sorting and required 3n steps. Our algorithm can be generalized to solve the problem of selection on higher dimensional meshes, achieving time bounds better than the known results in each case.\nTitle:\nFast Deterministic Selection on Mesh-Connected Processor Arrays\n\nAbstract:\nThis paper considers the problem of sequential pattern mining (SPM) in probabilistic databases. Specifically, we consider SPM in situations where there is uncertainty in associating an event with a source, model this kind of uncertainty in the probabilistic database framework and consider the problem of enumerating all sequences whose expected support is sufficiently large. We give an algorithm based on dynamic programming to compute the expected support of a sequential pattern. Next, we propose three algorithms for mining sequential patterns from probabilistic databases. The first two algorithms are based on the candidate generation framework\u2014one each based on a breadth-first (similar to GSP) and a depth-first (similar to SPAM) exploration of the search space. The third one is based on the pattern-growth framework (similar to PrefixSpan). We propose optimizations that mitigate the effects of the expensive dynamic programming computation step. We give an empirical evaluation of the probabilistic SPM algorithms and the optimizations and demonstrate the scalability of the algorithms in terms of CPU time and the memory usage. We also demonstrate the effectiveness of the probabilistic SPM framework in extracting meaningful sequences in the presence of noise.\nTitle:\nMining sequential patterns from probabilistic databases.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.186, mean value for all the writers is 8.648, which makes it 1.247 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe consider the algorithmic complexity of generating labeled (directed and undirected) graphs under various distributions. We describe three natural optimality criteria for graph generating algorithms, and show algorithms that are optimal for many distributions.\nTitle:", "model_inf_time": 0.86}, {"id": "4132", "output": "Eigen-mode Power Allocation for MU-MIMO Downlink Transmission with Statistical CSI", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMulti-user multiple antenna systems have attracted much attention because of large spectral efficiency. However, receiver and transmitter channel state information (CSI) is generally required, especially for the downlink channel. In this study, the authors investigate the ergodic sum rate and present low complexity adaptive transmission scheme for multiple-input single-output downlink multi-user systems with statistical CSI at the base station (BS). The authors derive an approximation of the ergodic sum rate, and the constraint of each user's statistical CSI under which the approximation of the ergodic sum rate is maximised. Then, statistical beamforming space division multiple access transmission is derived through maximising the approximation of the ergodic sum rate, and new adaptive transmission schemes are proposed. The proposed algorithms are shown to perform well and require the BS only to know the statistical CSI of each user.\nTitle:\nMulti-user multiple-input single-output downlink transmission systems exploiting statistical channel state information\n\nAbstract:\nIn this paper, we consider the power allocation problem for single-user multiple-input multiple-output (MIMO) systems. We propose new sub-optimal power allocation algorithms for MIMO systems with channel mean or covariance information at the transmitter, through the maximization of a lower bound of the ergodic channel capacity. Assume that the receiver has perfect channel knowledge. The algorithms proposed here need only the channel statistical information, and are very useful for systems in which only the channel statistical information is accessible to the transmitter. Simulation results confirm the performance.\nTitle:\nPower Allocation for MIMO Systems with Channel Mean or Covariance Information Feedback\n\nAbstract:\nThis paper investigates the downlink transmission for cooperative cellular networks with multi-base stations and single mobile station using statistical channel state information at the base stations. We consider a general jointly correlated MIMO channel model. We first propose an optimal transmission scheme to maximize the ergodic sum capacity, which reveals the transmit signals of all base stations are mutual independent and the optimal signaling directions are the eigenvectors of the transmit correlation matrix of the MIMO channel. Then we employ the matrix permanents to derive a closed-form tight upper bound for the ergodic sum capacity. Based on these results, we develop a low-complexity power allocation solution using convex optimization techniques and a simple iterative water-filling algorithm. Simulations demonstrate the upper bound of ergodic sum capacity is tight and the cooperative base stations transmission schemes increase the downlink system sum capacity.\nTitle:\nMulti-base station cooperation downlink statistical eigenmode transmission\n\nAbstract:\nWe investigate power allocation strategies for beam division multiple access transmission in multicell massive multiple-input-multiple-output (MIMO) communications. Focusing on massive MIMO downlink with only statistical channel state information at serving base stations and multiantenna terminals, the eigenmatrices of channel transmit covariance matrices become identical and independent of terminals. Utilizing these eigenmatrices as precoding matrices, we consider power allocation of beam domain transmission. By treating cochannel user interferences as noises, the objective function of achievable ergodic sum-rate of the multicell massive MIMO downlink reduces to a difference of concave functions. We identify the orthogonality conditions for optimal power allocation. The results show that the transmit power allocated to the users should be nonoverlapping across beams defined by the channel transmit covariance matrices. We present two algorithms to optimize power allocation in the beam space. We also prove the convergence of the algorithms and show that the solution obtained by the algorithms satisfies the orthogonality conditions. Numerical results confirm the efficiency and the improved performance of the proposed algorithms.\nTitle:\nBDMA in Multicell Massive MIMO Communications: Power Allocation Algorithms.\n\nAbstract:\nMultiple-input multiple-output (MIMO) technique plays a key role in improving the spectrum and power efficiency in future\n mobile communication systems. Exploiting a unified MIMO transmission scheme that can adapt with various channel conditions\n is well motivated both in theory and practical applications. An eigen-mode based closed-loop MIMO transmission over frequency\n selective fading MIMO channels, which considers receive correlation, transmit correlation and line of sight (LOS) components,\n is investigated by maximizing the upper bound of channel capacity under the assumption that the channel is partially known\n at the transmitter and perfectly known at the receiver. Based on the eigen-mode transmission, several key techniques including\n linear precoding, stream selection, virtual spatial hopping and online capacity estimation are proposed, and a unified MIMO\n transmission scheme is established. Both computer simulation and field test results show that the proposed scheme can significantly\n improve the spectral efficiency and link reliability under various channel conditions.\nTitle:\nUnifying eigen-mode MIMO transmission.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.668, mean value for all the writers is 8.648, which makes it 1.723 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe study of multi-user multiple-input multiple-output (MU-MIMO) systems has emerged recently as an important research topic because such systems have the potential to combine the high capacity that can be achieved through MIMO processing with the benefits of spatial-division multiple-access.However,receiver and transmitter channel state information (CSI) is generally required,especially for the downlink channel.In this paper,MU-MIMO downlink transmission systems with statistical CSI at the transmitter are studied for jointly correlated MIMO channels.Eigen-mode space division multiple access transmission is derived based on the maximization of the ergodic sum rate,and two eigen-mode power allocation algorithms are proposed using matrix permanent and convex optimization theory.The new algorithms can overcome the limitations of the existing multi-user downlink transmission algorithm in practical applications.\nTitle:", "model_inf_time": 1.65}, {"id": "4133", "output": "Performance Bottlenecks of Double Precision Floating-Point Arithmetic on a Hybrid Reconfigurable CPU", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nApplications requiring double precision (DP) arithmetic executed on embedded CPUs without native DP support suffer from prohibitively low performance and power efficiency. Hybrid reconfigurable CPUs, allowing for reconfiguration of the instruction set at runtime, appear as a viable computing platform for applications requiring instructions not supported by existing fixed architectures. Our experiments on a Stretch S6 as prototypical platform show that limited reconfigurable resources on such architectures are sufficient for providing native support of DP arithmetic. Our design using a DP fused multiply-accumulate (FMA) extension instruction achieves a peak performance of 200~MFlop/s and a sustained performance of 22.7~MFlop/s at a clock frequency of 100~MHz. It outperforms LINPACK using software-emulated DP floating-point arithmetic on the S6 by a factor of 5.7 while achieving slightly higher numerical accuracy. In single precision, multiple floating-point operators can be implemented in parallel on the S6.\nTitle:\nNative Double Precision LINPACK Implementation on a Hybrid Reconfigurable CPU\n\nAbstract:\nHigh Performance Fortran (HPF) was created to simplify high-level programming on parallel computers. The inventors of HPF strove for an easy-to-use language which was intended to enable portability and efficiency. However, until now the desired efficiency has not been reached. On the contrary, HPF programs are notorious for their poor performance. This paper provides a rehabilitation of HPF. It is demonstrated how currently available HPF constructs can be utilized to solve sizeable numerical problems efficiently. The method suggested utilizes HPF's EXTRINSIC mechanism to integrate existing numerical single processor software for computationally expensive kernels into HPF programs. By using the technique described in this paper, the empirical efficiency, i.e., the ratio of the empirical floating-point performance to the theoretical peak performance, can be raised to 50% and more. Even on message-passing machines with slow communication networks, such as PC clusters (Beowulf clusters) using a 100 Mbit/s Ethernet interconnection, highly satisfactory empirical efficiency results. The performance achieved is even competitive with that of well-established numerical libraries based on MPI. In contrast to earlier approaches for utilizing existing numerical software in HPF programs, the method presented here uses only HPF features and is therefore portable.\nTitle:\nOptimizing local performance in HPF\n\nAbstract:\nFPGAs have the native feature that reduced resource usage of single operators can be directly translated in additional parallelism. For floating-point (FP) operators, such reduced resource usage can be achieved by reducing the mantissa bit width. The work presented here pursues two objectives: First, the maximum number of operands of a parallel dot product architecture is explored experimentally on an FPGA for different custom precision FP number formats. Given the resources of this FPGA, it is shown that based on non-pipelined basic FP operators, a dot product for input vector size 21, 57 and 123 can be implemented for double-, single- and half-precision, respectively. This corresponds to a respective peak performance of 1, 3.2 and 9.9 GFlop/s. Second, it is shown that the maximum dot product peak performance as a function of used precision can be modeled by a function of the form P(p) = c1 + c2 \u010b pc3, given a certain type of FPGA, library and synthesis settings. Fitting experimental data to this model reveals similarities as well as differences among generations of devices.\nTitle:\nPeak performance model for a custom precision floating-point dot product on FPGAs\n\nAbstract:\nThis paper discusses and compares several parallelization strategies for tree-structured computations. In particular, we focus on the parallelization of the eigenvector accumulation process in divide-and-conquer eigensolvers, such as the recently developed block divide-and-conquer (BD&C) eigensolver. We describe a model algorithm for evaluating the performance of several parallel variants of this accumulation process, and we develop a block parallel approach which is shown to achieve good speedup in experiments on PC clusters.\nTitle:\nParallelization of divide-and-conquer eigenvector accumulation\n\nAbstract:\nIn future computing systems, handling faults efficiently at the algorithmic level is expected to become more and more important. In this paper, we illustrate that in practice classical algorithm-based fault tolerance (ABFT) cannot protect all exponent bits of a floating-point number. Consequently, we extend the method to recover from bit-flips in all positions without additional overhead. We also derive fault detection conditions suitable for multiple checksum encoding vectors. Moreover, we show how to efficiently employ ABFT to protect communication-optimal parallel 2.5D matrix multiplication against bit-flips occurring silently during the computation. Furthermore, we show that for very low fault rates the overhead of fault tolerance in the context of the 2.5D matrix multiplication algorithms can be reduced even further. Numerical experiments on a high-performance cluster illustrate the high scalability and low overhead of our algorithms. We demonstrate the fault tolerance of our approach with randomly and asynchronously injected bit-flips and illustrate that our method can also handle bit-flips occurring at high frequencies. Like in classical ABFT, the overhead per correctable bit-flip of our approach decreases with increasing error rate. dABFT: a novel ABFT method which can handle bit-flips at any bit of a FP number.Fault detection conditions are derived for multiple checksum encoding vectors.Experimental evaluation shows the increased resilience and low overhead of dABFT.2.5D FTMM: combining dABFT with high performance 2.5D matrix multiplications.Experimental evaluation shows the scalability and low overhead of 2.5D FTMM.\nTitle:\nFault tolerant communication-optimal 2.5D matrix multiplication.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.684, mean value for all the writers is 8.648, which makes it 0.031 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nEmbedded CPUs typically use much less power than desktop or server CPUs but provide limited or no support for floating-point arithmetic. Hybrid reconfigurable CPUs combine fixed and reconfigurable computing fabrics to balance better execution performance and power consumption. We show how a Stretch S6 hybrid reconfigurable CPU (S6) can be extended to natively support double precision floating-point arithmetic. For lower precision number formats, multiple parallel arithmetic units can be implemented. We evaluate if the superlinear performance improvement of floating-point multiplication on reconfigurable fabrics can be exploited in the framework of a hybrid reconfigurable CPU. We provide an in-depth investigation of data paths to and from the S6 reconfigurable fabric and present peak and sustained throughput as a function of wide registers used and total operand size. We demonstrate the effect of the given interface when using a floating-point fused multiply-accumulate (FMA) SIMD unit to accelerate the LINPACK benchmark. We identify a mismatch between the size of the S6s reconfigurable fabric and the available interface bandwidth as the major bottleneck limiting performance which makes it a poor choice for scientific workloads relying on native support for floating-point arithmetic.\nTitle:", "model_inf_time": 1.9}, {"id": "4134", "output": "Integrated K-means and Normalized Cut Clustering", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAbstract The recent years have witnessed a surge of interests of semi-supervised clustering methods, which aim to clus- ter the data set under the guidance of some supervi- sory information. Usually those supervisory informa- tion takes the form of pairwise constraints that indi- cate the similarity/dissimilarity between the two points. In this paper, we propose a novel matrix factorization based approach for semi-supervised clustering. In addi- tion, we extend our algorithm to co-cluster the data sets of difierent types with constraints. Finally the experi- ments on UCI data sets and real world Bulletin Board Systems (BBS) data sets show the superiority of our proposed method.\nTitle:\nSemi-Supervised Clustering via Matrix Factorization\n\nAbstract:\nCo-clustering on heterogeneous data has attracted more and more attention in web mining and information retrieval. The clustering approaches for two type heterogeneous data (bi-type co-clustering) have been well studied in the lit- erature. However, the work on data with more than two types (high-order co-clustering or multi-type co-clustering) is still limited. In this paper, we present a multi-type co- clustering algorithm, which clusters the data from differ- ent types simultaneously. We use a higher-order tensor to model the high-order relationships, each element of which describes the relation (similarity) among a given set com- posed by data objects from every types. Based on the high- order relationships, we embed the multi-type data objects into the low dimensional spaces by the algorithm based on Clique Expansion which can be viewed as a high-order extension of the normalized cut approach. At last, the k- means method is used to cluster the lower dimensional data. Experiment results show the effectiveness of the proposed method on both toy problem and real data.\nTitle:\nSimultaneous Heterogeneous Data Clustering Based on Higher Order Relationships\n\nAbstract:\nIn recent years, semi-supervised clustering (SSC) has aroused considerable interests from the machine learning and data mining communities. In this paper we propose a novel SSC approach with enhanced spectral embedding (ESE), which not only considers the geometric structure information contained in data sets, but also can make use of the given side information such as pairwise constraints. Specially, we first construct a symmetry-favored k-NN graph, which is highly robust to noise and outliers, and can reflect the underlying manifold structures of data sets. Then we learn the enhanced spectral embedding towards an ideal data representation as consistent with the given pairwise constraints as possible. Finally, by using the regularization of spectral embedding we formulate learning the new data representation as a semidefinite-quadratic-linear programming (SQLP) problem, which can be efficiently solved. Experimental results on a variety of synthetic and real-world data sets show that our ESE approach outperforms the state-of-the-art SSC algorithms in terms of speed and quality on both vector-based and graph-based clustering.\nTitle:\nFast semi-supervised clustering with enhanced spectral embedding\n\nAbstract:\nIn recent years, semi-supervised clustering (SSC) has aroused considerable interests from the machine learning and data mining communities. In this paper, we propose a novel semi-supervised clustering approach with enhanced spectral embedding (ESE) which not only considers structure information contained in data sets but also makes use of prior side information such as pair wise constraints. Specially, we first construct a symmetry-favored k-NN graph which is highly robust to noisy objects and can reflect the underlying manifold structure of data. Then we learn the enhanced spectral embedding towards an ideal representation as consistent with the pair wise constraints as possible. Finally, through taking advantage of Laplacian regularization, we formulate learning spectral representation as semi definite-quadratic-linear programs (SQLPs) under the squared loss function or small semi definitive programs (SDPs) under the hinge loss function, which both can be efficiently solved. Experimental results on a variety of synthetic and real-world data sets show that our approach outperforms the state-of-the-art SSC algorithms on both vector-based and graph-based clustering.\nTitle:\nLearning Spectral Embedding for Semi-supervised Clustering\n\nAbstract:\nClustering aggregation has emerged as an important extension of the classical clustering problem. It refers to the situation in which a number of different (input) clusterings have been obtained for a particular data set and it is desired to aggregate those clustering results to get a better clustering solution. In this paper, we propose a unified framework to solve the clustering aggregation problem, where the aggregated clustering result is obtained by minimizing the (weighted) sum of the Bregman divergence between it and all the input clusterings. Moreover, under our algorithm framework, we also propose a novel cluster aggregation problem where some must-link and cannot-link constraints are given in addition to the input clusterings. Finally the experimental results on some real world data sets are presented to show the effectiveness of our method.\nTitle:\nGeneralized Cluster Aggregation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.561, mean value for all the writers is 8.648, which makes it 0.074 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMost datasets in real applications come in from multiple sources. As a result, we often have attributes informa- tion about data objects and various pairwise relations (similarity) between data objects. Traditional cluster- ing algorithms use either data attributes only or pair- wise similarity only. We propose to combine K-means clustering on data attributes and normalized cut spec- tral clustering on pairwise relations. We show that these two methods can be coherently integrated together to make use of difierent data sources to obtain good clus- tering results. We also show that our integrated KL (K-means - Laplacian) clustering method can be natu- rally extended to semi-supervised clustering, data em- bedding and metric learning. Finally the experimental results on benchmark data sets are presented to show the efiectiveness of our method.\nTitle:", "model_inf_time": 1.33}, {"id": "4135", "output": "Semi-Supervised Orthogonal Discriminant Analysis via Label Propagation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nUsing the labeled and unlabeled data to enhance the performance of classification is the core idea of transductive learning, It has recently attracted much interest of researchers on this topic. In this paper, we extend the harmonic energy minimization algorithm and propose a novel transductive learning algorithm on graph with soft label and soft constraint. Relaxing the label to real value makes the transductive problem easy to solve, while softening the hard constraint for the labeled data makes it tolerable to the noise in labeling. We discuss two cases for our algorithm and derive exactly the same form of solution. More importantly, such form of solution can be interpreted from the view of label propagation and a special random walks on graph, which make the algorithm intuitively reasonable. We also discuss several related issues of the proposed algorithm. Experiments on toy examples and real world classification problems demonstrate the effectiveness of our algorithm.\nTitle:\nSoft constraint harmonic energy minimization for transductive learning and its two interpretations\n\nAbstract:\nIn this paper, we propose a general graph-based semi-supervised learning algorithm. The core idea of our algorithm is to not only achieve the goal of semi-supervised learning, but also to discover the latent novel class in the data, which may be unlabeled by the user. Based on the normalized weights evaluated on data graph, our algorithm is able to output the probabilities of data points belonging to the labeled classes or the novel class. We also give the theoretical interpretations for the algorithm from three viewpoints on graph, i.e., regularization framework, label propagation, and Markov random walks. Experiments on toy examples and several benchmark datasets illustrate the effectiveness of our algorithm.\nTitle:\nA general graph-based semi-supervised learning with novel class discovery\n\nAbstract:\nDimensionality reduction is one of the widely used techniques for data analysis. However, it is often hard to get a demanded\n low-dimensional representation with only the unlabeled data, especially for the discriminative task. In this paper, we put\n forward a novel problem of Transferred Dimensionality Reduction, which is to do unsupervised discriminative dimensionality\n reduction with the help of related prior knowledge from other classes in the same type of concept. We propose an algorithm\n named Transferred Discriminative Analysis to tackle this problem. It uses clustering to generate class labels for the target\n unlabeled data, and use dimensionality reduction for them joint with prior labeled data to do subspace selection. This two\n steps run adaptively to find a better discriminative subspace, and get better clustering results simultaneously. The experimental\n results on both constrained and unconstrained face recognition demonstrate significant improvements of our algorithm over\n the state-of-the-art methods.\n \nTitle:\nTransferred Dimensionality Reduction\n\nAbstract:\nIn machine learning problems, labeled data are often in short supply. One of the feasible solution for this problem is transfer learning. It can make use of the labeled data from other domain to discriminate those unlabeled data in the target domain. In this paper, we propose a transfer learning framework based on similarity matrix approximation to tackle such problems. Two practical algorithms are proposed, which are the label propagation and the similarity propagation. In these methods, we build a hybrid graph based on all available data. Then the information is transferred cross domains through alternatively constructing the similarity matrix for different part of the graph. Among all related methods, similarity propagation approach can make maximum use of all available similarity information across domains. This leads to more efficient transfer and better learning result. The experiment on real world text mining applications demonstrates the promise and effectiveness of our algorithms.\nTitle:\nKnowledge transfer on hybrid graph\n\nAbstract:\nSupervised dimensionality reduction with tensor representation has attracted great interest in recent years. It has been successfully applied to problems with tensor data, such as image and video recognition tasks. However, in the tensor-based methods, how to select the suitable dimensions is a very important problem. Since the number of possible dimension combinations exponentially increases with respect to the order of tensor, manually selecting the suitable dimensions becomes an impossible task in the case of high-order tensor. In this paper, we aim at solving this important problem and propose an algorithm to extract the optimal dimensionality for local tensor discriminant analysis. Experimental results on a toy example and real-world data validate the effectiveness of the proposed method.\nTitle:\nExtracting the optimal dimensionality for local tensor discriminant analysis\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.957, mean value for all the writers is 8.648, which makes it 0.59 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTrace ratio is a natural criterion in discriminant analysis as it directly connects to the Euclidean distances between training data points. This criterion is re-analyzed in this paper and a fast algorithm is developed to find the global optimum for the orthogonal constrained trace ratio problem. Based on this problem, we propose a novel semi-supervised orthogonal discriminant analysis via label propagation. Differing from the existing semi-supervised dimensionality reduction algorithms, our algorithm propagates the label information from the labeled data to the unlabeled data through a specially designed label propagation, and thus the distribution of the unlabeled data can be explored more effectively to learn a better subspace. Extensive experiments on toy examples and real-world applications verify the effectiveness of our algorithm, and demonstrate much improvement over the state-of-the-art algorithms.\nTitle:", "model_inf_time": 1.46}, {"id": "4136", "output": "Accelerating Multi-Threaded Architecture Design Evaluation with Synthetic Workload Synthesis", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nModeling and analysis of program behavior are at the foundation of computer system design and optimization. As computer systems become more adaptive, their efficiency increasingly depends on program dynamic characteristics. Previous studies have revealed that program runtime execution manifests phase behavior. Recently, methods and tools to analyze and classify program phases have also been developed. However, very few studies have been proposed so far to understand and evaluate program phases from their dynamics and complexity perspectives. In this work, we propose new methods, metrics and frameworks which aim to analyze, quantify, and classify the dynamics and complexity of program phases. Our methods use wavelet techniques to represent program phases at multiresolution scales. The cross-correlation coefficients between phase dynamics observed at different scales are then computed as metrics to quantify phase complexity. We propose to apply wavelet-based multiresolution analysis and data clustering to classify program execution into phases that exhibit similar degree of complexity. Experimental results on SPEC CPU 2000 benchmarks show that the proposed schemes classify complexity-based program phases better than currently used approaches.\nTitle:\nComplexity-based program phase analysis and classification\n\nAbstract:\nProgram runtime behavior exhibits significant variations across multiple scales. The increasing design complexity and technology scaling make microprocessor performance and efficiency increasingly depend on runtime workload dynamics. Therefore understanding the effect of design parameters on workload dynamics at early, microarchitecture exploration stage is crucial for high-performance and complexity-efficient designs. In this study, we apply wavelet-based analysis to decompose workload dynamics into a series of wavelet coefficients, which represent program behavior ranging from low-resolution approximation to high-resolution detail. We then construct error-bounded linear regression models that relate microarchitecture design parameters to various wavelet coefficients that capture workload dynamics at multiresolution levels. The most significant factors affecting program dynamics at different scales are obtained. To our knowledge, this paper presents the first work on microarchitecture design space exploration focusing on workload dynamics.\nTitle:\nCharacterizing the Effect of Microarchitecture Design Parameters on Workload Dynamic Behavior\n\nAbstract:\nProgramming to exploit the resources in a multicore system remains a major obstacle for both computer and software engineers. Transactional memory offers an attractive alternative to traditional concurrent programming but implementations emerged before the programming model, leaving a gap in the design process. In previous research, transactional microbenchmarks have been used to evaluate designs or lock-based multithreaded workloads have been manually converted into their transactional equivalents; others have even created dedicated transactional benchmarks. Yet, throughout all of the investigations, transactional memory researchers have not settled on a way to describe the runtime characteristics that these programs exhibit; nor has there been any attempt to unify the way transactional memory implementations are evaluated. In addition, the similarity (or redundancy) of these workloads is largely unknown. Evaluating transactional memory designs using workloads that exhibit similar characteristics will unnecessarily increase the number of simulations without contributing new insight. On the other hand, arbitrarily choosing a subset of transactional memory workloads for evaluation can miss important features and lead to biased or incorrect conclusions. In this work, we propose a set of architecture-independent transaction-oriented workload characteristics that can accurately capture the behavior of transactional code. We apply principle component analysis and clustering algorithms to analyze the proposed workload characteristics collected from a set of SPLASH-2, STAMP, and PARSEC transactional memory programs. Our results show that using transactional characteristics to cluster the chosen benchmarks can reduce the number of required simulations by almost half. We also show that the methods presented in this paper can be used to identify specific feature subsets. With the increasing number of TM workloads in the future, we believe that the proposed transactional memory workload characterization techniques will help TM architects select a small, diverse, set of TM workloads for their design evaluation.\nTitle:\nOn the (dis)similarity of transactional memory workloads\n\nAbstract:\nThe GPUs are emerging as a general-purpose high-performance computing device. Growing GPGPU research has made numerous GPGPU workloads available. However, a systematic approach to characterize these benchmarks and analyze their implication on GPU microarchitecture design evaluation is still lacking. In this research, we propose a set of microarchitecture agnostic GPGPU workload characteristics to represent them in a microarchitecture independent space. Correlated dimensionality reduction process and clustering analysis are used to understand these workloads. In addition, we propose a set of evaluation metrics to accurately evaluate the GPGPU design space. With growing number of GPGPU workloads, this approach of analysis provides meaningful, accurate and thorough simulation for a proposed GPU architecture design choice. Architects also benefit by choosing a set of workloads to stress their intended functional block of the GPU microarchitecture. We present a diversity analysis of GPU benchmark suites such as Nvidia CUDA SDK, Parboil and Rodinia. Our results show that with a large number of diverse kernels, workloads such as Similarity Score, Parallel Reduction, and Scan of Large Arrays show diverse characteristics in different workload spaces. We have also explored diversity in different workload subspaces (e.g. memory coalescing and branch divergence). Similarity Score, Scan of Large Arrays, MUMmerGPU, Hybrid Sort, and Nearest Neighbor workloads exhibit relatively large variation in branch divergence characteristics compared to others. Memory coalescing behavior is diverse in Scan of Large Arrays, K-Means, Similarity Score and Parallel Reduction.\nTitle:\nExploring GPGPU workloads: Characterization methodology, analysis and microarchitecture evaluation implications\n\nAbstract:\nTransactional memory (TM) has emerged as a parallel programming paradigm for multi-core processors yet there is no standardized set of metrics with which to describe their behavior. In this work, we propose a set of transaction-oriented workload characteristics that can accurately capture the behavior of transactional memory programs. We apply principle component analysis and clustering algorithms to analyze the proposed transactional workload characteristics and show that these characteristics are architecturally independent\nTitle:\nTransMetric: architecture independent workload characterization for transactional memory benchmarks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.666, mean value for all the writers is 8.648, which makes it 0.015 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe design and evaluation of microprocessor architectures is a difficult and time-consuming task. Although small, hand-coded microbenchmarks can be used to accelerate performance evaluation, these programs lack the complexity to stress increasingly complex architecture designs. Larger and more complex real-world workloads should be employed to measure the performance of a given design or to evaluate the efficiency of various design alternatives. These applications can take days or weeks if run to completion on a detailed architecture simulator. In the past, researchers have applied machine learning and statistical sampling methods to reduce the average number of instructions required for detailed simulation. Others have proposed statistical simulation and workload synthesis techniques, which can produce programs that emulate the execution characteristics of the application from which they are derived but have a much shorter execution period than the original. However, these existing methods are difficult to apply to multi-threaded programs and can result in simplifications that miss the complex interactions between multiple, concurrently running threads.This study focuses on developing new techniques for accurate and effective multi-threaded workload synthesis, which can significantly accelerate architecture design evaluation of multi-core processors. We propose to construct synchronized statistical flow graphs that incorporate inter-thread synchronization and sharing behavior to capture the complex characteristics and interactions of multiple threads. Moreover, we develop thread-aware data reference models and wavelet-based branching models to generate accurate memory access and dynamic branch statistics. Experimental results show that a framework integrated with the aforementioned models can automatically generate synthetic programs that maintain characteristics of original workloads but have significantly reduced runtime.\nTitle:", "model_inf_time": 1.72}, {"id": "4137", "output": "A Bandwidth-Efficient Cooperative Coding Scheme for Diversity Improvement", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA cooperative coding scheme using M-ary modulation for high spectral efficiency and repetitive information transmission for diversity improvement is proposed. The encoder is a parallel concatenation of two bit-interleaved coded M-ary modulators (BICM) with scalable repetition of information symbols. As a result, the fraction of repeated information and the spectral efficiency of M-ary modulation can be used for bandwidth-performance tradeoff. Simulation results for different M-PSK and M-QAM systems in various block fading scenarios show that for a given bandwidth efficiency, a proper selection of the fraction of repeated information and the spectral efficiency of M-ary modulation can provide a large performance improvement.\nTitle:\nA Bandwidth-Efficient Coded Cooperative Communications System\n\nAbstract:\nThis paper studies bit-interleaved coded modulation with iterative decoding (BICM-ID) systems that employ multi-dimensional mappings of M-ary constellations to improve the error performance over Rayleigh fading channels. Based on the analytical evaluations of the asymptotic bit error probability (BEP), the distance criteria for the mapping designs can be obtained. A binary switching algorithm (BSA) is then applied to find the optimal mappings with respect to the asymptotic performance. Simulation and analytical results show that the use of multi-dimensional mappings of M-ary constellations can significantly improve the error performance.\nTitle:\nMulti-Dimensional Mappings of M-ary Constellations for BICM-ID Systems\n\nAbstract:\nThe transmission of vector quantization over a frequency-selective Rayleigh fading code-division multiple access (CDMA) channel is considered. Suboptimal decoding approaches are presented for systems that employ M-ary amplitude shift keying (M-ASK) to improve the spectral efficiency. Analysis and simulation results are provided to quantify the performance loss in order to obtain a better spectral efficiency of the systems\nTitle:\nSuboptimal Decoding of Vector Quantization Over a CDMA Channel with M-Ask\n\nAbstract:\nIn this paper, the asymptotic performance of a precoded bit-interleaved coded modulation with multiple-input multiple-output (BICM-MIMO) system in a block-fading environment is first evaluated. Specifically, the asymptotic pairwise error probability is derived for imperfect channel state information (CSI) and of the minimum mean squared-error (MMSE) detector. The analysis is then used to evaluate the impact of two different training schemes. It is shown that inserting training symbols before precoder is a better option than multiplexing training symbols with data symbols after the precoder. The theoretical result is verified by simulation.\nTitle:\nAsymptotic performance analysis of precoded BICM-MIMO under channel estimation errors.\n\nAbstract:\nThis paper proposes a combined cooperative coding and hybrid-ARQ scheme suitable for power-efficient transmission in wireless sensor networks. By exploiting cooperative coding in multiple-node wireless sensor networks to provide spatial diversity in addition to time diversity offered by Automatic Retransmission Request (ARQ), the proposed scheme can support high-performance transmission at low power consumption. Analytical derivations of both the throughput and bit error probability of the proposed transmission scheme, using a tree representation, are obtained. Simulation and analytical results are in good agreement and show that the proposed scheme can offer a significant saving in the transmitted power for a target link performance while maintaining the same throughput as compared to the traditional non cooperative hybrid-ARQ scheme. In particular, the power saving increases at higher link performance requirements.\nTitle:\nPower-efficient cooperative coding with hybrid-ARQ soft combining for wireless sensor networks in block-fading environment\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.821, mean value for all the writers is 8.648, which makes it 1.001 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA cooperative coding scheme using M-ary modulation for high spectral efficiency and repetitive information transmission for diversity improvement is proposed. The encoder is a parallel concatenation of two bit-interleaved coded M-ary modulators (BICM) with scalable repetition of information symbols. As a result, the fraction of repeated information and the spectral efficiency of M-ary modulation can be used for bandwidth-performance tradeoff. Performance analysis is carried out to verify the performance advantage of the proposed system. In particular, analytical and simulation results in various block fading scenarios show that for a given bandwidth efficiency, a proper selection of the fraction of repeated information and the spectral efficiency of M-ary modulation can provide a large performance improvement.\nTitle:", "model_inf_time": 1.24}, {"id": "4138", "output": "Nonsmooth Optimization for Beamforming in Cognitive Multicast Transmission", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIt is well-known that the optimal beamforming problems for cognitive multicast transmission are indefinite quadratic (nonconvex) optimization programs. The conventional approach is to reformulate them as convex semi-definite programs (SDPs) with additional rank-one (nonconvex and discontinuous) constraints. The rank-one constraints are then dropped for relaxed solutions, and randomization techniques are employed for solution search. In many practical cases, this approach fails to deliver satisfactory solutions, i.e., its found solutions are very far from the optimal ones. In contrast, in this paper we cast the optimal beamforming problems as SDPs with the additional reverse convex (but continuous) constraints. An efficient algorithm of nonsmooth optimization is then proposed for seeking the optimal solution. Our simulation results show that the proposed approach yields almost global optimal solutions with much less computational load than the mentioned conventional one.\nTitle:\nNonsmooth Optimization for Beamforming in Cognitive Multicast Transmission\n\nAbstract:\nThe optimal beamforming for cognitive multicast transmission is nonconvex rank-one constrained optimization problem. For a solution, a popular method is the combination of relaxed convex semi-definite programming, where the rank-one constraint is dropped, and randomization. We show that in many cases, this method cannot give satisfactory solutions. As an initial step, we develop a simple alternative method, which gives much better solutions. Our simulation confirms this fact.\nTitle:\nNew Optimized Solution Method for Beamforming in Cognitive Multicast Transmission\n\nAbstract:\nOptimization problems of beamforming in multi-user amplify-and-forward (AF) wireless relay networks are indefinite (nonconvex) quadratic programs, which require effective computational solutions. Solutions to these problems have often been obtained by relaxing the original problems to semi-definite programs (SDPs) of convex optimization. Most existing works have claimed that these relaxed SDPs act...\nTitle:\nBeamforming Optimization in Multi-User Amplify-and-Forward Wireless Relay Networks.\n\nAbstract:\nOptimizations of precoding matrices in precode-and-forward (PF) MIMO relaying are nonconvex programs in precoding matrix variables. The semidefinite relaxation (SDR) technique, which relaxes the concerned nonconvex quadratic constraints by (convex) semi-definite ones, can locate the optimal solutions, provided that the numbers of relaying antennas and users are very small. The computational complexity of the SDR grows explosively even with a very moderate increase in the numbers of relaying antennas and/or users, making the existing semidefinite programming (SDP) solvers incapable. In this paper, much more efficient problem formulations of precoding matrix design that exploit the spectral matrix optimization are developed. Such formulations have a low dimensionality and are computationally-tractable nonconvex matrix programs. Furthermore, by exploiting their partial convex structures in the d.c. (difference of two convex functions) framework, new effective iterative solutions are obtained. Extensive simulation results are presented to support the computational advantage of the proposed approach and show that the proposed approach can effectively handle all three considered optimization problems of precoding matrices in MIMO PF relaying, while the SDR approach either is computationally impractical or fails.\nTitle:\nIterative D.C. Optimization of Precoding in Wireless MIMO Relaying\n\nAbstract:\nConsidering a dense small-cell network with simultaneous wireless information and power transfer (SWIPT), this work jointly designs transmit beamformers at the base stations (BSs) and receive power splitting ratios at the users (UEs). Our objectives is to maximize the minimum UE signal-to-interference-plus-noise-ratio (SINR) under BS transmit power and UE minimum harvested energy constraints. This problem is highly nonconvex, for which semidefinite programming (SDP) relaxation may even fail to locate a feasible solution. We propose an efficient spectral optimization method by expressing the rank-one constraints as a single reverse convex nonsmooth constraint and incorporating it in the optimization objective. The proposed algorithm practically achieves the theoretical bound given by SDP relaxation with almost similar complexity.\nTitle:\nIterative Optimization For Max-Min Sinr In Dense Small-Cell Multiuser Miso Swipt System\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.755, mean value for all the writers is 8.648, which makes it 0.944 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIt is known that the design of optimal transmit beamforming vectors for cognitive radio multicast transmission can be formulated as indefinite quadratic optimization programs. Given the challenges of such nonconvex problems, the conventional approach in literature is to recast them as convex semidefinite programs (SDPs) together with rank-one constraints. Then, these nonconvex and discontinuous constraints are dropped allowing for the realization of a pool of relaxed candidate solutions, from which various randomization techniques are utilized with the hope to recover the optimal solutions. However, it has been shown that such approach fails to deliver satisfactory outcomes in many practical settings, wherein the determined solutions are found to be unacceptably far from the actual optimality. On the contrary, we in this contribution tackle the aforementioned optimal beamforming problems differently by representing them as SDPs with additional reverse convex (but continuous) constraints. Nonsmooth optimization algorithms are then proposed to locate the optimal solutions of such design problems in an efficient manner. Our thorough numerical examples verify that the proposed algorithms offer almost global optimality whilst requiring relatively low computational load.\nTitle:", "model_inf_time": 1.51}, {"id": "4139", "output": "A Novel Error Rate Metric for Text Entry Research Based on Levenshtein Distance", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe describe and identify shortcomings in two statistics recently introduced to measure accuracy in text entry evaluations: the minimum string distance (MSD) error rate and keystrokes per character (KSPC). To overcome the weaknesses, a new framework for error analysis is developed and demonstrated. It combines the analysis of the presented text, input stream (keystrokes), and transcribed text. New statistics include a unified total error rate, combining two constituent error rates: the corrected error rate (errors committed but corrected) and the not corrected error rate (errors left in the transcribed text). The framework includes other measures including error correction efficiency, participant conscientiousness, utilised bandwidth, and wasted bandwidth. A text entry study demonstrating the new methodology is described.\nTitle:\nMetrics for text entry research: an evaluation of MSD and KSPC, and a new unified error metric\n\nAbstract:\nPreviously, we defined robust and easy-to-calculate error metrics for text entry research. Herein, we announce a software implementation of this error analysis technique. We build on previous work, by introducing two new metrics, and we extend error rate analyses to high key-stroke-per-character entry techniques, such as Multi-Tap.\nTitle:\nRecent developments in text-entry error rate measurement\n\nAbstract:\nKSPC is the number of keystrokes, on average, to generate each character of text in a given language using a given text entry technique. We systematically describe the calculation of KSPC and provide examples across a variety of text entry techniques. Values for English range from about 10 for methods using only cursor keys and a SELECT key to about 0.5 for word prediction techniques. It is demonstrated that KSPC is useful for a priori analyses, thereby supporting the characterisation and comparison of text entry methods before labour-intensive implementations and evaluations.\nTitle:\nKSPC (Keystrokes per Character) as a Characteristic of Text Entry Techniques\n\nAbstract:\nWe describe a technique to analyse character-level errors in evaluations of text entry methods. Using an algorithm for sequence comparisons, we generate the set of optimal alignments between the presented and transcribed text. Percharacter errors, categorized as insertions, substitutions, or deletions, are obtained by analysing the alignments and applying a weighting factor. A detailed example using a real data set is given.\nTitle:\nA character-level error analysis technique for evaluating text entry methods\n\nAbstract:\nWe developed an application to gather text entry speed and accuracy metrics on Android devices. This paper details the features of the application and describes a pilot study to demonstrate its utility. We evaluated and compared three mobile text entry methods: QWERTY typing, handwriting recognition, and shape writing recognition. Handwriting was the slowest and least accurate technique. QWERTY was faster than shape writing, but we found no significant difference in accuracy between the two techniques.\nTitle:\nGathering text entry metrics on android devices\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.022, mean value for all the writers is 8.648, which makes it 0.319 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose a new technique based on the Levenshtein minimum string distance statistic for measuring error rates in text entry research. The technique obviates the need to artificially constrain subjects to maintain synchronization with the presented text, thus affording a more natural interaction style in the evaluation. Methodological implications are discussed, including the additional need to use keystrokes per characters (KSPC) as a dependent measure to capture the overhead in correcting errors.\nTitle:", "model_inf_time": 1.34}, {"id": "4140", "output": "Depth Deficiencies and 3D Pointing in Peri-Personal Space", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe investigated mouse-based 3D selection using one-eyed cursors, evaluating stereo and head-tracking. Stereo cursors significantly reduced performance for targets at different depths, but the one-eyed cursor yielded some discomfort.\nTitle:\nDepth cues and mouse-based 3D target selection\n\nAbstract:\nWe present a study of cursors for selecting 2D-projected 3D targets. We compared a stereo- and mono-rendered (one-eyed) cursor using two mouse-based and two remote pointing techniques in a 3D Fitts' law pointing experiment. The first experiment used targets at fixed depths. Results indicate that one-eyed cursors only improve screen-plane pointing techniques, and that constant target depth does not influence pointing throughput. A second experiment included pointing between targets at varying depths and used only \"screen-plane\" pointing techniques. Our results suggest that in the absence of stereo cue conflicts, screen-space projections of Fitts' law parameters (target size and distance) yield constant throughput despite target depth differences and produce better models of performance.\nTitle:\nPointing at 3d target projections with one-eyed and stereo cursors\n\nAbstract:\nWe present two experiments on mouse-based point selection in a desktop virtual reality system using stereo display and head-tracking. To address potential issues of using a mouse cursor with stereo display, we also evaluate the impact of using a one-eyed (mono) cursor. While a one-eyed cursor visualization eliminates depth conflicts, recent work suggests it offers worse performance than stereo cursors, possibly due to eye discomfort. Our results indicate that presenting the cursor in stereo significantly reduces performance for targets at different depths. The one-eyed cursor eliminates this effect, offering better performance than both screen-plane and geometry-sliding cursors visualized in stereo. However, it also performed slightly worse than stereo cursors in situations without depth conflicts. Our study suggests that this difference is not due exclusively to the relative transparency of such a cursor, hence eye fatigue or similar may be responsible.\nTitle:\nFactors Affecting Mouse-Based 3D Selection in Desktop VR Systems\n\nAbstract:\nWe present an experiment that examines 3D pointing in fish tank VR using the ISO 9241-9 standard. The experiment used three pointing techniques: mouse, ray, and touch using a stylus. It evaluated user pointing performance with stereoscopically displayed varying height targets above an upward-facing display. Results show differences in upwards and downwards motions for the 3D touch technique.\nTitle:\nUp- and downwards motions in 3D pointing\n\nAbstract:\nWe present a study investigating the influence of visual aids on 3D point selection tasks. In a Fitts' law pointing experiment, we compared the effects of texturing, highlighting targets upon being touched, and the presence of support cylinders intended to eliminate floating targets. Results of the study indicate that texturing and support cylinders did not significantly influence performance. Enabling target highlighting increased movement speed, while decreasing error rate. Pointing throughput was unaffected by this speed-accuracy tradeoff. Highlighting also eliminated significant differences between selection coordinate depth deviation and the deviation in the two orthogonal axes.\nTitle:\nVisual aids in 3D point selection experiments\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.75, mean value for all the writers is 8.648, which makes it 0.087 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nPrevious work has documented that limitations of current stereo display systems affect depth perception. We performed an experiment to understand if such stereo display deficiencies affect 3D pointing for targets in front of a screen and close to the user, i.e., in peri-personal space. Our experiment compares isolated movements with and without a change in visual depth for virtual targets. Results indicate that selecting targets along the depth axis is slower and has less throughput than laterally positioned targets.\nTitle:", "model_inf_time": 1.38}, {"id": "4141", "output": "W-Boost for Self-Similar and Non-linear Internet Traffic Prediction", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nInternet traffic prediction plays a fundamental role in network design, management, control, and optimization. The self-similar and non-linear nature of network traffic makes highly accurate prediction difficult. In this paper, a boosting-based framework is proposed for self-similar and non-linear traffic prediction by considering it as a classical regression problem. The framework is based on Ada-Boost on the whole. It adopts Principle Component Analysis as an optional step to take advantage of self-similar nature of traffic while avoiding the disadvantage of self-similarity. Feed-forward neural network is used as the basic regressor to capture the non-linear relationship within the traffic. Experimental results on real network traffic validate the effectiveness of the proposed framework.\nTitle:\nA Boosting-Based Framework for Self-Similar and Non-linear Internet Traffic Prediction\n\nAbstract:\nWhen training data is not sufficient, boosting algorithms tend to overfit as more weak learners are combined to form a strong classifier. In this paper, we propose a new variant of RealBoost, called W-Boost, which is based on a novel weight update scheme and uses changeable bin number to estimate marginal distributions in weak learner design. This new boosting procedure results in both fast convergence rate and small generalization error. Experimental results on synthetic data and Web image classification demonstrate the effectiveness of our approach.\nTitle:\nW-Boost and its application to Web image classification\n\nAbstract:\nRoad traffic prediction is a critical component in modern smart transportation systems. It provides the basis for traffic management agencies to generate proactive traffic operation strategies for alleviating congestion. Existing work on near-term traffic prediction (forecasting horizons in the range of 5 minutes to 1 hour) relies on the past and current traffic conditions. However, once the forecasting horizon is beyond 1 hour, i.e., in longer-term traffic prediction, these techniques do not work well since additional factors other than the past and current traffic conditions start to play important roles. To address this problem, in this paper, for the first time, we examine whether it is possible to use the rich information in online social media to improve longer-term traffic prediction. To this end, we first analyze the correlation between traffic volume and tweet counts with various granularities. Then we propose an optimization framework to extract traffic indicators based on tweet semantics using a transformation matrix, and incorporate them into traffic prediction via linear regression. Experimental results using traffic and Twitter data originated from the San Francisco Bay area of California demonstrate the effectiveness of our proposed framework.\nTitle:\nImproving traffic prediction with tweet semantics\n\nAbstract:\nAs a crucial issue in computer network security, anomaly detection is receiving more and more attention from both application and theoretical point of view. In this paper, a novel anomaly detection scheme is proposed. It can detect anomaly network traffic which has extreme large value on some original feature by the major component, or does not follow the correlation structure of normal traffic by the minor component. By introducing kernel trick, the non-linearity of network traffic can be well addressed. To save the processing time, a simplified version is also proposed, where only major component is adopted. Experimental results validate the effectiveness of the proposed scheme.\nTitle:\nAnomaly internet network traffic detection by kernel principle component classifier\n\nAbstract:\n\u2022We propose a novel non-negative matrix tri-factorization model based on cosparsity regularization to enable the co-feature-selection for co-clustering. It aims to learn the inter-correlation among the multi-way features while co-shrinking the irrelevant ones by encouraging the co-sparsity of the model parameters.\u2022We propose an efficient algorithm to solve the non-smooth optimization problem. It works in an iteratively update fashion, and is guaranteed to converge.\u2022Experimental results on various data sets show the effectiveness of the proposed approach.\nTitle:\nFeature co-shrinking for co-clustering.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.308, mean value for all the writers is 8.648, which makes it 1.143 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nInternet traffic prediction plays a fundamental role in network design, management, control, and optimization. The self-similar and non-linear nature of network traffic makes highly accurate prediction difficult. In this paper, we proposed a new boosting scheme, namely W-Boost, for traffic prediction from two perspectives: classification and regression. To capture the non-linearity of the traffic while introducing low complexity into the algorithm, \u2018stump' and piece-wise-constant function are adopted as weak learners for classification and regression, respectively. Furthermore, a new weight update scheme is proposed to take the advantage of the correlation information within the traffic for both models. Experimental results on real network traffic which exhibits both self-similarity and non-linearity demonstrate the effectiveness of the proposed W-Boost.\nTitle:", "model_inf_time": 1.44}, {"id": "4142", "output": "Clustering in a Network of Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nNetworks are prevalent and have posed many fascinating research questions. How can we spot similar users, e.g., virtual identical twins, in Cleveland for a New Yorker? Given a query disease, how can we prioritize its candidate genes by incorporating the tissue-specific protein interaction networks of those similar diseases? In most, if not all, of the existing network ranking methods, the nodes are the ranking objects with the finest granularity. In this paper, we propose a new network data model, a Network of Networks (NoN), where each node of the main network itself can be further represented as another (domain-specific) network. This new data model enables to compare the nodes in a broader context and rank them at a finer granularity. Moreover, such an NoN model enables much more efficient search when the ranking targets reside in a certain domain-specific network. We formulate ranking on NoN as a regularized optimization problem; propose efficient algorithms and provide theoretical analysis, such as optimality, convergence, complexity and equivalence. Extensive experimental evaluations demonstrate the effectiveness and the efficiency of our methods.\nTitle:\nInside the atoms: ranking on a network of networks\n\nAbstract:\nOverlapping clustering allows a data point to be a member of multiple clusters, which is more appropriate for modeling many real data semantics. However, much of the existing work on overlapping clustering simply assume that a data point can be assigned to any number of clusters without any constraint. This assumption is not supported by many real contexts. In an attempt to reveal true data cluster structure, we propose sparsity constrained overlapping clustering by incorporating sparseness constraints into an overlapping clustering process. To solve the derived sparsity constrained overlapping clustering problems, efficient and effective algorithms are proposed. Experiments demonstrate the advantages of our overlapping clustering model.\nTitle:\nOverlapping Clustering with Sparseness Constraints\n\nAbstract:\nMultiple networks naturally appear in numerous high-impact applications. Network alignment (i.e., finding the node correspondence across different networks) is often the very first step for many data mining tasks. Most, if not all, of the existing alignment methods are solely based on the topology of the underlying networks. Nonetheless, many real networks often have rich attribute information on nodes and/or edges. In this paper, we propose a family of algorithms FINAL to align attributed networks. The key idea is to leverage the node/edge attribute information to guide (topology-based) alignment process. We formulate this problem from an optimization perspective based on the alignment consistency principle, and develop effective and scalable algorithms to solve it. Our experiments on real networks show that (1) by leveraging the attribute information, our algorithms can significantly improve the alignment accuracy (i.e., up to a 30% improvement over the existing methods); (2) compared with the exact solution, our proposed fast alignment algorithm leads to a more than 10 times speed-up, while preserving a 95% accuracy; and (3) our on-query alignment method scales linearly, with an around 90% ranking accuracy compared with our exact full alignment method and a near real-time response time.\nTitle:\nFINAL: Fast Attributed Network Alignment\n\nAbstract:\nMulti-layered networks have recently emerged as a new network model, which naturally finds itself in many high-impact application domains, ranging from critical inter-dependent infrastructure networks, biological systems, organization-level collaborations, to cross-platform e-commerce, etc. Cross-layer dependency, which describes the dependencies or the associations between nodes across different layers/networks, often plays a central role in many data mining tasks on such multi-layered networks. Yet, it remains a daunting task to accurately know the cross-layer dependency a prior. In this paper, we address the problem of inferring the missing cross-layer dependencies on multi-layered networks. The key idea behind our method is to view it as a collective collaborative filtering problem. By formulating the problem into a regularized optimization model, we propose an effective algorithm to find the local optima with linear complexity. Furthermore, we derive an online algorithm to accommodate newly arrived nodes, whose complexity is just linear wrt the size of the neighborhood of the new node. We perform extensive empirical evaluations to demonstrate the effectiveness and the efficiency of the proposed methods.\nTitle:\nFASCINATE: Fast Cross-Layer Dependency Inference on Multi-layered Networks\n\nAbstract:\nNetwork clustering is an important problem thathas recently drawn a lot of attentions. Most existing workfocuses on clustering nodes within a single network. In manyapplications, however, there exist multiple related networks, inwhich each network may be constructed from a different domainand instances in one domain may be related to instances in otherdomains. In this paper, we propose a robust algorithm, MCA, formulti-network clustering that takes into account cross-domain relationshipsbetween instances. MCA has several advantages overthe existing single network clustering methods. First, it is ableto detect associations between clusters from different domains, which, however, is not addressed by any existing methods. Second, it achieves more consistent clustering results on multiple networksby leveraging the duality between clustering individual networksand inferring cross-network cluster alignment. Finally, it providesa multi-network clustering solution that is more robust to noiseand errors. We perform extensive experiments on a variety ofreal and synthetic networks to demonstrate the effectiveness andefficiency of MCA.\nTitle:\nRobust Multi-Network Clustering via Joint Cross-Domain Cluster Alignment\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.884, mean value for all the writers is 8.648, which makes it 0.652 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIntegrating multiple graphs (or networks) has been shown to be a promising approach to improve the graph clustering accuracy. Various multi-view and multi-domain graph clustering methods have recently been developed to integrate multiple networks. In these methods, a network is treated as a view or domain.The key assumption is that there is a common clustering structure shared across all views (domains), and different views (domains) provide compatible and complementary information on this underlying clustering structure. However, in many emerging real-life applications, different networks have different data distributions, where the assumption that all networks share a single common clustering structure does not hold. In this paper, we propose a flexible and robust framework that allows multiple underlying clustering structures across different networks. Our method models the domain similarity as a network, which can be utilized to regularize the clustering structures in different networks. We refer to such a data model as a network of networks (NoN). We develop NoNClus, a novel method based on non-negative matrix factorization (NMF), to cluster an NoN. We provide rigorous theoretical analysis of NoNClus in terms of its correctness, convergence and complexity. Extensive experimental results on synthetic and real-life datasets show the effectiveness of our method.\nTitle:", "model_inf_time": 1.28}, {"id": "4143", "output": "Automated Defect Correction with Relational Concept Analysis", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDesign defects are poor design choices resulting in a hard-to-maintain software, hence their detection and correction are key steps of a disciplined software process aimed at yielding high-quality software artifacts. While modern structure- and metric-based techniques enable precise detection of design defects, the correction of the discovered defects, e.g., by means of refactorings, remains a manual, hence error-prone, activity. As many of the refactorings amount to re-distributing class members over a (possibly extended) set of classes, formal concept analysis (FCA) has been successfully applied in the past as a formal framework for refactoring exploration. Here we propose a novel approach for defect removal in object-oriented programs that combines the effectiveness of metrics with the theoretical strength of FCA. A case study of a specific defect, the Blob, drawn from the Azureus project illustrates our approach.\nTitle:\nUsing FCA to suggest refactorings to correct design defects\n\nAbstract:\nSoftware functionalities and behavior are accomplished by the cooperation of code artifacts. The understanding of this type of source code collaboration provides an important aid to the maintenance and evolution of legacy systems. However, the original collaboration design information is dispersed at the implementation level. The extraction of code artifacts' collaborations and the roles is therefore an important support in legacy software comprehension and design recovery. In this paper, we present a novel approach to automatically detect and analyze code collaborations and roles based on dynamic program analysis technique. We also demonstrate the tools that we have developed to support our approach and illustrate the viability of our approach in a case study.\nTitle:\nAutomatic Detecting Code Cooperation\n\nAbstract:\nIt is now widely accepted that in order to optimize both their usage and their design and maintenance ontologies should comply to design quality criteria, e.g., absence of redundancies and appropriate level of abstraction. Yet given the variety and scope of activities comprised in the life-cycle of an ontological model (OM), such as adapting, splitting, populating, this quality is easily compromised, especially with ontologies of larger size and/or resulting from the merge of smaller ones. Conversely, restoring it through refactoring, i.e., restructuring of the ontology to improve defects, is knowingly a challenging task as relocating an ontology element can adversely affect its neighbors. We investigate here a holistic refactoring approach that, given an ontology, amounts to presenting its designer with a list of the most plausible abstract entities missing in it. The core of the approach is a recently devised concept analysis method, called 'relational', that allows deeper refactoring by feeding into the process various ontological relations, e.g., concept-to-property incidences. The focus here is put on the NLP-aspects of the refactoring, while we also provide some preliminary results from a series of validating experiments.\nTitle:\nRefactoring of Ontologies: Improving the Design of Ontological Models with Concept Analysis\n\nAbstract:\nOntologies are designed to evolve and this is typically done through sequences of a local modifications in the ontological structure, a.k.a. refactorings. Yet the more complex the structure the less obvious the full impact of such a refactoring. Thus, after a protracted period of maintenance, the overall quality of an ontology may substantially deteriorate. As a remedy, an ontology restructuring task would be performed that cleans its structure and enhances the ontology with new and previously missing entities. We investigate an approach for ontology restructuring based on relational concept analysis (RCA) that allows for a thorough reshuffling of the ontology. Here we present a platform for ontology maintenance, INUKHUK, and illustrate its main workflow dedicated to restructuring. We also report on a preliminary validating study involving several small-to-medium size ontologies.\nTitle:\nSupporting ontology design through large-scale FCA-based ontology restructuring\n\nAbstract:\nMaintenance is undoubtedly the most effort-consuming activity in software production whereby the entropy of legacy systems is a major challenge. Migration of legacy systems to object-oriented technology is considered by many organizations as a suitable way out, however, the cost and the complexity of the task may dissuade the decision-makers. As a contribution to the automation, complete or partial, of the migration process, this paper presents two algorithms for identifying objects in procedural code, a task which is crucial within the entire process. The suggested algorithms are experimentally evaluated, using the examples of three existing systems.\nTitle:\nObject identification in legacy code as a grouping problem\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.556, mean value for all the writers is 8.648, which makes it 0.078 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSoftware engineers often need to identify and correct design defects, i.e., recurring design problems that hinder development and maintenance by making programs harder to comprehend and/or evolve. While detection of design defects is an actively researched area, their correction- mainly a manual and time-consuming activity- is yet to be extensively investigated for automation. In this paper, we propose an automated approach for suggesting defect-correcting refactorings using relational concept analysis (RCA). The added value of rca consists in exploiting the links between formal objects which abound in a software re-engineering context. We validated our approach on instances of the Blob design defect taken from four different open-source programs.\nTitle:", "model_inf_time": 1.21}, {"id": "4144", "output": "Restoring Information Flow After Node Disappearance in Social Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSocial networks are dynamic structures that contain a set of entities and links. In such a dynamic environment, a specific node or a group of nodes can play an important role in the information flow transmission within the network and therefore, its disappearance may lead to a disconnected network or a breakdown in the information flow. The objective of this paper is to extend our previous work on managing a node disappearance to handling the disappearance of a group of nodes. The proposed approach relies on the role played by a group of nodes to conduct network changes and maintain the network connected while restoring the information flow with a similar quality as before the group disappearance. We consider two situations (only one versus many communities) and categorize groups of nodes into three classes (scattered, contiguous and hybrid). Hence, we manage a group disappearance with respect to its class and the network topology by adding new links in a parsimonious way and finding a substitute for a leaving group. Our approach differs from existing link prediction solutions by the fact that it uses the information flow quality as a key performance indicator to identify the potential links to add and/or the possible substitute to a disappearing group. We implement a prototype by using an open source social network analysis library (NetworkX) and we validate our solution through experiments. The results show the benefits of our solution in terms of response time and the number of added links.\nTitle:\nDealing with Disappearance of an Actor Set in Social Networks\n\nAbstract:\nSocial networks are dynamic structures in which entities and links appear and disappear for different reasons. Starting from the observation that each entity has a more or less important role within the network, the objective of this article is to propose a method which exploits the role played by nodes to predict the new structure of a social network once one entity disappears. The role of a node in the network is expressed in terms of the number of interactions it has with the rest of the network. Two roles are considered: the leader and the mediator with their corresponding measure: the degree centrality and the betweenness centrality.\nTitle:\nPredicting a Social Network Structure Once a Node Is Deleted\n\nAbstract:\nCentral nodes i.e., prominent actors in a social network are those that are linked to other nodes in an extensive or critical manner. Therefore, their removal may lead to points of failure. The objective of the present work is to exploit network topology to devise an approach towards: 1 finding a substitute to a deleted node if the latter is a central one; 2 adding appropriate links to maintain the network connected. The approach exploits the role played by nodes to predict the new structure of a social network once one entity disappears. The role of a node in the network is expressed in terms of its centrality in the network. Three important roles are considered: the leader, the mediator and the witness. An entity acts as a leader, a mediator or a witness if it has a high degree centrality, betweenness centrality and closeness centrality, respectively.\nTitle:\nSocial network restructuring after a node removal\n\nAbstract:\nThe purpose of this paper is to handle the disappearance of a group of nodes in a social network. The quality of the information flow is used as a key performance indicator to conduct network changes after group disappearance. Nodes as well as node sets are first classified into categories (critical and non-critical nodes, and scattered, contiguous and hybrid groups) and then analyzed according to two distinct perspectives: the network as a whole or its identified communities. Finally, algorithms are devised to manage group disappearance according to different cases. New links are added in a parsimonious way and a possible substitute for a leaving group is found based on the adage \u201cbirds of a feather flock together\u201d and the homophily principle. This means that new links (e.g., relationships) and a potential substitute are found only between individuals that share common characteristics such as beliefs, values, and education, i.e., individuals that are more likely neighbors of the leaving node or group. To validate our approach, an empirical study is conducted using various kinds of data sets and a set of criteria. The results show the benefits of our solution in terms of response time, number of added links and metrics of the overall network topology.\nTitle:\nGroup disappearance in social networks with communities\n\nAbstract:\nIn this paper, we propose a graph theoretic approach to deal with the implication problem for inclusion dependencies. By analogy with functional dependencies, we define and present algorithms for computing the following concepts: the closure of a relation scheme R for X according to a set of inclusion dependencies and the minimal cover for inclusion dependencies.\nTitle:\nThe implication problem for inclusion dependencies: a graph approach\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.717, mean value for all the writers is 8.648, which makes it 0.059 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSocial networks are dynamic structures in which entities and links appear and disappear for different reasons. Starting from the observation that each entity plays a more or less important role in transmitting the information inside a network, the objective of this article is to propose a method which exploits the role played by a given node to both estimate the impact of its disappearance on the information flow, and conduct network changes to restore the information flow with a similar quality as before the node disappearance. To this end, we propose a network restructuring approach that categorizes nodes into critical and non-critical classes based on their role, and hence, manages their disappearance appropriately by adding new links in a parsimonious way and selecting a substitute for a deleted critical node. As opposed to a previously defined solution, our approach adds links that are just enough to maintain the quality of the information flow within the network as before a node deletion. A prototype is designed and implemented using an open source social network analysis library (NetworkX). Its validation is conducted using network data sets with various sizes. The empirical study shows a low network update, a quite constant quality of the information flow and reasonable execution times after a node deletion.\nTitle:", "model_inf_time": 1.48}, {"id": "4145", "output": "An efficient dynamic programming approach for the 0-1 multi-objective knapsack problem", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present an approach, based on dynamic programming, for solving 0-1 multi-objective knapsack problems. The main idea of the approach relies on the use of several complementary dominance relations to discard partial solutions that cannot lead to new nondominated criterion vectors. This way, we obtain an efficient method that outperforms the existing methods both in terms of CPU time and size of solved instances. Extensive numerical experiments on various types of instances are reported. A comparison with other exact methods is also performed. In addition, for the first time to our knowledge, we present experiments in the three-objective case.\nTitle:\nAn efficient implementation for the 0-1 multi-objective Knapsack problem\n\nAbstract:\nIn the present work, we are interested in the practical behavior of a new fptas to solve the approximation version of the 0-1 multiobjective knapsack problem. Nevertheless, our methodology focuses on very general techniques (such as dominance relations in dynamic programming) and thus may be applicable in the implementation of fptas for other problems as well. Extensive numerical experiments on various types of instances establish that our method performs very well both in terms of CPU time and size of solved instances. We point out some reasons for the good practical performance of our algorithm. A comparison with an exact method is also performed.\nTitle:\nA practical efficient fptas for the 0-1 multi-objective knapsack problem\n\nAbstract:\nIn the present work, we are interested in the practical behavior of a new fully polynomial time approximation schemes (fptas) to solve the approximation version of the 0\u20131 multi-objective knapsack problem. The proposed methodology makes use of very general techniques (such as dominance relations in dynamic programming) and thus may be applicable in the implementation of fptas for other problems as well.\nTitle:\nImplementing an efficient fptas for the 0-1 multi-objective knapsack problem\n\nAbstract:\nThis paper investigates, for the first time in the literature, the approximation of min-max (regret) versions of classical problems like shortest path, minimum spanning tree, and knapsack. For a bounded number of scenarios, we establish fully polynomial-time approximation schemes for the min-max versions of these problems, using relationships between multi-objective and min-max optimization. Using dynamic programming and classical trimming techniques, we construct a fully polynomial-time approximation scheme for min-max regret shortest path. We also establish a fully polynomial-time approximation scheme for min-max regret spanning tree and prove that min-max regret knapsack is not at all approximable. We also investigate the case of an unbounded number of scenarios, for which min-max and min-max regret versions of polynomial-time solvable problems usually become strongly NP-hard. In this setting, non-approximability results are provided for min-max (regret) versions of shortest path and spanning tree.\nTitle:\nApproximation complexity of min-max (regret) versions of shortest path, spanning tree, and knapsack\n\nAbstract:\nThis paper investigates, for the first time in the literature, the approximation of min\u2013max (regret) versions of classical problems like shortest path, minimum spanning tree, and knapsack. For a constant number of scenarios, we establish fully polynomial-time approximation schemes for the min\u2013max versions of these problems, using relationships between multi-objective and min\u2013max optimization. Using dynamic programming and classical trimming techniques, we construct a fully polynomial-time approximation scheme for min\u2013max regret shortest path. We also establish a fully polynomial-time approximation scheme for min\u2013max regret spanning tree and prove that min\u2013max regret knapsack is not at all approximable. For a non-constant number of scenarios, in which case min\u2013max and min\u2013max regret versions of polynomial-time solvable problems usually become strongly NP-hard, non-approximability results are provided for min\u2013max (regret) versions of shortest path and spanning tree.\nTitle:\nApproximation of min-max and min-max regret versions of some combinatorial optimization problems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.065, mean value for all the writers is 8.648, which makes it 0.356 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we present an approach, based on dynamic programming, for solving the 0-1 multi-objective knapsack problem. The main idea of the approach relies on the use of several complementary dominance relations to discard partial solutions that cannot lead to new non-dominated criterion vectors. This way, we obtain an efficient method that outperforms the existing methods both in terms of CPU time and size of solved instances. Extensive numerical experiments on various types of instances are reported. A comparison with other exact methods is also performed. In addition, for the first time to our knowledge, we present experiments in the three-objective case.\nTitle:", "model_inf_time": 1.68}, {"id": "4146", "output": "Hamiltonian paths and cycles in Toeplitz graphs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe cycles of length k in a complete graph on n vertices are colored in such a way that edge-disjoint cycles get distinct colors. The minimum number of colors is asymptotically determined.\nTitle:\nColor the cycles.\n\nAbstract:\n\n It is a simple fact that cubic Hamiltonian graphs have at least two Hamiltonian cycles. Finding such a cycle is NP-hard in\n general, and no polynomial time algorithm is known for the problem of fording a second Hamiltonian cycle when one such cycle\n is given as part of the input. We investigate the complexity of approximating this problem where by a feasible solution we\n mean a(nother) cycle in the graph. First we prove a negative result showing that the LONGEST PATH problem is not constant\n approximable in cubic Hamiltonian graphs unless P = NP. No such negative result was previously known for this problem in Hamiltonian\n graphs. In strong opposition with this result we show that there is a polynomial time approximation scheme for fording another\n cycle in cubic Hamiltonian graphs if a Hamiltonian cycle is given in the input.\n \n \nTitle:\nOn the approximation of finding a(nother) hamiltonian cycle in cubic hamiltonian graphs\n\nAbstract:\nIt is known that the Mycielski graph can be generalized to obtain an infinite family of 4-chromatic graphs with no short odd cycles. The first proof of this result, due to Stiebitz, applied the topological method of Lov\u00e1sz. The proof presented here is elementary combinatorial.\nTitle:\n4-chromatic graphs with large odd girth\n\nAbstract:\nA method is given for solving extremal problems of the following type: determine the maximal number of vertices in a class of hypergraphs. Results are applied for \u03c4-critical and \u03bd-critical hypergraphs.\nTitle:\nCritical hypergraphs and intersecting set-pair systems\n\nAbstract:\nWe raise the following general problem: Which structural properties of dominating subgraphs in finite graphs remain valid for infinite graphs? Positive and negative results are presented.\nTitle:\nInfinite versus finite graph domination\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.537, mean value for all the writers is 8.648, which makes it 0.948 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nConditions are given for the existence of hamiltonian paths and cycles in the so-called Toeplitz graphs, i.e. simple graphs with a symmetric Toeplitz adjacency matrix.\nTitle:", "model_inf_time": 1.02}, {"id": "4147", "output": "Approximation Algorithms for Min-Max (Regret) Versions of Classical Problems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper investigates, for the first time in the literature, the approximation of min-max (regret) versions of classical problems like shortest path, minimum spanning tree, and knapsack. For a bounded number of scenarios, we establish fully polynomial-time approximation schemes for the min-max versions of these problems, using relationships between multi-objective and min-max optimization. Using dynamic programming and classical trimming techniques, we construct a fully polynomial-time approximation scheme for min-max regret shortest path. We also establish a fully polynomial-time approximation scheme for min-max regret spanning tree and prove that min-max regret knapsack is not at all approximable. We also investigate the case of an unbounded number of scenarios, for which min-max and min-max regret versions of polynomial-time solvable problems usually become strongly NP-hard. In this setting, non-approximability results are provided for min-max (regret) versions of shortest path and spanning tree.\nTitle:\nApproximation complexity of min-max (regret) versions of shortest path, spanning tree, and knapsack\n\nAbstract:\nThis paper investigates the complexity of the min-max and min-max regret versions of the s\u2013t min cut and min cut problems. Even if the underlying problems are closely related and both polynomial, we show that the complexity of their min-max and min-max regret versions, for a constant number of scenarios, are quite contrasted since they are respectively strongly NP-hard and polynomial. Thus, we exhibit the first polynomial problem, s\u2013t min cut, whose min-max (regret) versions are strongly NP-hard. Also, min cut is one of the few polynomial problems whose min-max (regret) versions remain polynomial. However, these versions become strongly NP-hard for a non constant number of scenarios. In the interval data case, min-max versions are trivially polynomial. Moreover, for min-max regret versions, we obtain the same contrasted result as for a constant number of scenarios: min-max regret s\u2013t cut is strongly NP-hard whereas min-max regret cut is polynomial.\nTitle:\nComplexity of the min-max (regret) versions of cut problems\n\nAbstract:\nThis paper investigates the complexity of the min-max and min-max regret versions of the min s-t cut and min cut problems. Even if the underlying problems are closely related and both polynomial, the complexities of their min-max and min-max regret versions, for a constant number of scenarios, are quite contrasted since they are respectively strongly NP-hard and polynomial. However, for a non-constant number of scenarios, these versions become strongly NP-hard for both problems. In the interval scenario case, min-max versions are trivially polynomial. Moreover, for min-max regret versions, we obtain the same contrasted results as for a constant number of scenarios: min-max regret min s-t cut is strongly NP-hard whereas min-max regret min cut is polynomial.\nTitle:\nComplexity of the min-max (regret) versions of min cut problems\n\nAbstract:\nWhile the complexity of min-max and min-max regret versions of most classical combinatorial optimization problems has been thoroughly investigated, there are very few studies about their approximation. For a bounded number of scenarios, we establish a general approximation scheme which can be used for min-max and min-max regret versions of some polynomial problems. Applying this scheme to shortest path and minimum spanning tree, we obtain fully polynomial-time approximation schemes with much better running times than the ones previously presented in the literature.\nTitle:\nApproximating min-max (regret) versions of some polynomial problems\n\nAbstract:\nMin\u2013max and min\u2013max regret criteria are commonly used to define robust solutions. After motivating the use of these criteria, we present general results. Then, we survey complexity results for the min\u2013max and min\u2013max regret versions of some combinatorial optimization problems: shortest path, spanning tree, assignment, min cut, min s\u2013t cut, knapsack. Since most of these problems are NP-hard, we also investigate the approximability of these problems. Furthermore, we present algorithms to solve these problems to optimality.\nTitle:\nMin-max and min-max regret versions of combinatorial optimization problems: A survey\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.24, mean value for all the writers is 8.648, which makes it 0.505 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper investigates, for the first time in the literature, the approximation of min\u2013max (regret) versions of classical problems like shortest path, minimum spanning tree, and knapsack. For a constant number of scenarios, we establish fully polynomial-time approximation schemes for the min\u2013max versions of these problems, using relationships between multi-objective and min\u2013max optimization. Using dynamic programming and classical trimming techniques, we construct a fully polynomial-time approximation scheme for min\u2013max regret shortest path. We also establish a fully polynomial-time approximation scheme for min\u2013max regret spanning tree and prove that min\u2013max regret knapsack is not at all approximable. For a non-constant number of scenarios, in which case min\u2013max and min\u2013max regret versions of polynomial-time solvable problems usually become strongly NP-hard, non-approximability results are provided for min\u2013max (regret) versions of shortest path and spanning tree.\nTitle:", "model_inf_time": 1.55}, {"id": "4148", "output": "Sharpening the LYM Inequality for Sperner Families", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nLet F \u2282 2[n] be a 3-wise 2-intersecting Sperner family. It is proved that |F| \u2264 {(n-2 (n-2)/2) if n even, (n-2 (n-1)/2)+2 if n odd holds for n \u2265 n0. The unique extremal configuration is determined as well.\nTitle:\nRandom walks and multiply intersecting families\n\nAbstract:\nLet n and r be positive integers. Suppose that a family F \u2286 2'n' satisfies |F1 \u2229 F2 \u2229 F3|\u22652 for all F1, F1, F2, F3, \u2208 F. We prove that if wF\u2208F W|F(1 - w)n-F| \u2264 w2.\nTitle:\nWeighted 3-wise 2-intersecting families\n\nAbstract:\nLet n \u00bf k \u00bf l \u00bf 2 be integers, and let F be a family of k -element subsets of an n -element set. Suppose that l divides the size of the intersection of any two (not necessarily distinct) members in F . We prove that the size of F is at most ( \u00bf n / l \u00bf k / l ) provided n is sufficiently large for fixed k and l .\nTitle:\nUniform eventown problems\n\nAbstract:\nFor positive integers n,q,t we determine the maximum number of integer sequences (a1,...,an) which satisfy 1 ai q for 1 i n, and any two sequences agree in at least t positions. The result gives an armative answer to a conjecture of Frankl and Furedi.\nTitle:\nThe Erdos-Ko-Rado Theorem for Integer Sequences\n\nAbstract:\nSuppose that F[n]3 contains no three sets whose intersection is empty and their union has size at most 6. We prove a structure theorem for such families which easily implies the best possible bound, Fn12.\nTitle:\nA structural result for 3-graphs.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.145, mean value for all the writers is 8.648, which makes it 1.282 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe level sequence of a Sperner familyF is the sequencef(F)={fi(F)}, wherefi(F) is the number ofi element sets ofF . TheLYM inequality gives a necessary condition for an integer sequence to be the level sequence of a Sperner family on ann element set. Here we present an indexed family of inequalities that sharpen theLYM inequality.\nTitle:", "model_inf_time": 1.05}, {"id": "4149", "output": "Constructions of External Difference Families", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAlmost difference families (ADFs) are a useful generalization of almost difference sets (ADSs). In this paper, we present some constructive techniques to obtain ADFs and establish a number of infinite classes of ADFs. Our results can be regarded as a generalization of the known difference families. It is clear that ADFs give partially balance incomplete block designs which arise in a natural way in many combinatorial and statistical problems.\nTitle:\nConstructions of almost difference families\n\nAbstract:\nDifference systems of sets (DSS) were introduced in 1971 by Levenstein for the construction of codes for synchronization, and are closely related to cyclic difference families. In this paper, algebraic constructions of difference systems of sets using functions with optimum nonlinearity are presented. All the difference systems of sets constructed in this paper are perfect and optimal. One conjecture on difference systems of sets is also presented.\nTitle:\nOptimal and perfect difference systems of sets\n\nAbstract:\nlmost difference sets are an interesting subject of combinatorics, and have applications in many areas of engineering such as CDMA communications, error correcting codes and cryptography. The objective of this paper is to present some new constructions of almost difference sets, together with several results on the equivalence relation.\nTitle:\nConstructions of almost difference sets from finite fields\n\nAbstract:\nAlmost difference sets have interesting applications in cryptography and coding theory. We give a well-rounded treatment of known families of almost difference sets, establish relations between some difference sets and some almost difference sets, and determine the numerical multiplier group of some families of almost difference sets. We also construct six new classes of almost difference sets, and four classes of binary sequences of period n\u22610 (mod 4) with optimal autocorrelation. We have also obtained two classes of relative difference sets and four classes of divisible difference sets (DDSs). We also point out that a result due to Jungnickel (1982) can be used to construct almost difference sets and sequences of period 4l with optimal autocorrelation\nTitle:\nAlmost difference sets and their sequences with optimal autocorrelation\n\nAbstract:\nIn this paper a number of binary cyclotomic generators based on cyclotomy are described. A number of cryptographic properties of the generators are controlled. A general approach to control the linear complexity and its stability for periodic sequences over any field is shown. Two bridges between number theory and stream ciphers have been established, and the relations between the design and analysis of some stream ciphers and some number-theoretic problems are shown. A number of cryptographic ideas are pointed out.\nTitle:\nBinary Cyclotomic Generators\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.093, mean value for all the writers is 8.648, which makes it 0.474 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nExternal difference families (EDFs) are a type of new combinatorial designs originated from cryptography. In this paper, some earlier ideas of recursive and cyclotomic constructions of combinatorial designs are extended, and a number of classes of EDFs and disjoint difference families are presented. A link between a subclass of EDFs and a special type of (almost) difference sets is set up.\nTitle:", "model_inf_time": 0.9}, {"id": "4150", "output": "Adaptive Loop Filtering for Video Coding via Nonlocal Image Priors", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nImages coded at low bit rates in real-world applications usually suffer from significant compression noise, which significantly degrades the visual quality. Traditional denoising methods are not suitable for the content-dependent compression noise, which usually assume that noise is independent and with identical distribution. In this paper, we propose a unified framework of content-adaptive estim...\nTitle:\nLow-Rank Decomposition-Based Restoration of Compressed Images via Adaptive Noise Estimation.\n\nAbstract:\nIn image restoration tasks, image priors generally utilize correlation within image contents to predict the latent image signal. In this paper, we propose to jointly exploit both intra- and inter-patch correlation of the input image, so as to further reduce the uncertainty of the unknown signal, and thus improve the prediction of the latent image. The proposed scheme evolves from the low-rank regu...\nTitle:\nImage Denoising via Low Rank Regularization Exploiting Intra and Inter Patch Correlation.\n\nAbstract:\nBlock transform coding using discrete cosine transform is the most popular approach for image compression. However, many annoying blocking artifacts are generated due to coarse quantization on transform coefficients independently. This paper proposes an effective blocking artifacts reduction method by estimating the transform coefficients from their quantized version. In the proposed scheme, we estimate the transform coefficients based on an image statistic model and non-local similarity among blocks in transform domain. The parameters used in our proposed scheme are discussed and adaptively selected. Extensive experimental results show that our proposed method significantly reduces blocking artifacts and improves the subjective and the objective quality of block transform coded images.\nTitle:\nReducing Blocking Artifacts in Compressed Images via Transform-Domain Non-local Coefficients Estimation\n\nAbstract:\nThis paper proposes a deep learning method for intra prediction. Different from traditional methods utilizing some fixed rules, we propose using a fully connected network to learn an end-to-end mapping from neighboring reconstructed pixels to the current block. In the proposed method, the network is fed by multiple reference lines. Compared with traditional single line-based methods, more contextu...\nTitle:\nFully Connected Network-Based Intra Prediction for Image Coding.\n\nAbstract:\nThe compressive sensing (CS) theory indicates that robust reconstruction of signals can be obtained from far fewer measurements than those required by the Nyquist-Shannon theorem. Thus, CS has great potential in video acquisition and processing, considering that it makes the subsequent complex data compression unnecessary. In this paper, we propose a novel algorithm for effectively reconstructing ...\nTitle:\nVideo Compressive Sensing Reconstruction via Reweighted Residual Sparsity\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.57, mean value for all the writers is 8.648, which makes it 0.787 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn video coding, the in-loop filtering has emerged as a key module due to its significant improvement on compression performance since H.264/Advanced Video Coding. Existing incorporated in-loop filters in video coding standards mainly take advantage of the local smoothness prior model used for images. In this paper, we propose a novel adaptive loop filter utilizing image nonlocal prior knowledge b...\nTitle:", "model_inf_time": 1.19}, {"id": "4151", "output": "Anisotropic Dual-Tree Discrete Wavelet Transform for Image Coding", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose a novel coding and transmission scheme, called LineCast, for broadcasting satellite images to a large number of receivers. The proposed LineCast matches perfectly with the line scanning cameras that are widely adopted in orbit satellites to capture high-resolution images. On the sender side, each captured line is immediately compressed by a transform-domain scalar modulo quantization. Without syndrome coding, the transmission power is directly allocated to quantized coefficients by scaling the coefficients according to their distributions. Finally, the scaled coefficients are transmitted over a dense constellation. This line-based distributed scheme features low delay, low memory cost, and low complexity. On the receiver side, our proposed line-based prediction is used to generate side information from previously decoded lines, which fully utilizes the correlation among lines. The quantized coefficients are decoded by the linear least square estimator from the received data. The image line is then reconstructed by the scalar modulo dequantization using the generated side information. Since there is neither syndrome coding nor channel coding, the proposed LineCast can make a large number of receivers reach the qualities matching their channel conditions. Our theoretical analysis shows that the proposed LineCast can achieve Shannon's optimum performance by using a high-dimensional modulo-lattice quantization. Experiments on satellite images demonstrate that it achieves up to 1.9-dB gain over the state-of-the-art 2D broadcasting scheme and a gain of more than 5 dB over JPEG 2000 with forward error correction.\nTitle:\nLineCast: Line-Based Distributed Coding and Transmission for Broadcasting Satellite Images\n\nAbstract:\nIn lifting-based directional wavelet transforms, different subsampling patterns may show significant difference for directional signals in image coding. This paper investigates the influence of subsampling in directional wavelet transform. We show that the best subsampling depends on the direction and the directionality strength of the signal. To improve the coding performance, we further propose a subsampling-adaptive directional wavelet transform, which can use different subsampling patterns adaptively and according to the local characteristics of the image. To handle the boundary transition when subsampling changes, a phase completion process is applied to ensure that wavelet transform with various subsampling can be performed without introducing boundary effects and performance loss. Experimental results show that the proposed transform can achieve significant coding gain in image coding compared to other existing directional wavelet transforms.\nTitle:\nSubsampling-Adaptive Directional Wavelet Transform for Image Coding\n\nAbstract:\nIn this paper, we present an object-based coding scheme using three-dimensional shape-adaptive discrete wavelet transforms (SA-DWT). Rather than straightforward extension of 2D SA-DWT, a novel way to handle the temporal wavelet transform using a motion model is proposed to achieve higher coding efficiency. Corresponding to this transform scheme, we use a 3D entropy coding algorithm called Motion-based Embedded Subband Coding with Optimized Truncation (ESCOT) to code the wavelet coefficients. Results show that ESCOT can achieve comparable coding performance with the state-of-the-art MPEG-4 verification model (VM) 13.0 while having the scalability and flexibility of the bitstream in low bit-rate object-based video coding. And in relative higher bit-rate, our coding approach outperforms MPEG-4 VM 13.0 by about 2.5dB.\nTitle:\nThree-Dimensional Shape-Adaptive Discrete Wavelet Transforms For Efficient Object-Based Video Coding\n\nAbstract:\nThis paper introduces a lossy to lossless coding technique for compression of multitarget fluorescence in situ hybridization (M-FISH) images using 3-D embedded subband coding with optimal truncation (3-D ESCOT) (Xu et al.). With a lifting-based integer wavelet decomposition, 3-D ESCOT achieves about twice as much compression as Lempel-Ziv (WinZip) coding-the current method for archiving M-FISH images. The lossy coding performance of 3-D ESCOT is significantly better than that of 2-D based JPEG-2000\nTitle:\nCompression of M-FISH images using 3-D ESCOT\n\nAbstract:\nIn subband image and video compression, the issue of dyadic spatial scalability with a downsizing ratio of 2:1 has already been widely investigated. In some application scenarios, however, other downsizing ratios can be desirable. This paper proposes a wavelet coding scheme to support non-dyadic spatial scalability. By extending lifting-based transform structure, we show how wavelet transform can directly support non-dyadic spatial scalability. Specially, based on the extended lifting framework, a wavelet transform is designed to decompose an image with size 3W\u00d73H to a low-pass subband with size 2W\u00d72H and three high-pass subbands to support 3:2-ratio spatial scalability. The characteristic of proposed wavelet low-pass filter is investigated and experimental results show that the low resolution image produced by our scheme has good quality while the coding performance just has marginal loss compared to Daubechies 9/7 filter.\nTitle:\nA Lifting-Based Wavelet Transform Supporting Non-Dyadic Spatial Scalability\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.544, mean value for all the writers is 8.648, which makes it 0.089 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose an image coding scheme using 2-D anisotropic dual-tree discrete wavelet transform (DDWT). First, we extend 2-D DDWT to anisotropic decomposition, and obtain more directional subbands. Second, an iterative projection-based noise shaping algorithm is employed to further sparsify anisotropic DDWT coefficients. At last, the resulting coefficients are rearranged to preserve zero-tree relationship so that they can be efficiently coded with SPIHT. Experimental results show that our proposed scheme outperforms JPEG2000 and SPIHT at low bit rates despite the redundancy of DDWT.\nTitle:", "model_inf_time": 1.53}, {"id": "4152", "output": "Joint Optimization of File Delivery Delay and Power Consumption in Dense Small Cell Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis research addresses the problem of robust linear video transmission over the Rayleigh fading channel, where only statistical channel state information (CSI) is available to the sender. We observe that discarding low-priority (LP) data and saving the channel uses for high-priority (HP) data can significantly improve the quality of the received video. We formulate an optimization problem that aims to minimize the total squared error of a multi-variant Gaussian random vector under the given bandwidth and power resources. To tame the complexity of this NP-hard problem, we analyze two sub-problems, namely power allocation and bandwidth allocation, and propose an iterative algorithm to approximate the solution. Subsequently, we propose a one-pass two-step fast algorithm that further reduces both algorithmic and computational complexity. A linear video transmission system is implemented based on the proposed algorithm. Simulations show that our system significantly outperforms Soft-Cast, and the PSNR gain at 5th percentile of 1000 test runs is between 4.0 dB and 7.5 dB under varying noise levels.\nTitle:\nRobust Linear Video Transmission Over Rayleigh Fading Channel\n\nAbstract:\nEnergy supply is always a bottleneck in wireless video applications. To prolong the battery life, a typical wireless video encoding and streaming system should have the capability of dynamically adjusting its encoding parameters based on the remaining battery supply. In this paper, we consider two types of application scenarios as well as the associated optimization problems. The first one is, given a video quality level, how to find out the optimized encoding parameters so that the total power consumption is minimized. The second one is, given a fixed power consumption level, how to calculate the optimized encoding parameters so that the video quality is best. Accordingly, we jointly consider the power consumption and video quality and pose the above two problems into a uniform optimization framework. Under this framework, we give an efficient solution for the MPEG-4 AVC/H.264 codec and Wi-Fi transmission devices based on a detailed complexity analysis of the H.264 encoder. The analysis method can be easily extended to other video encoder and transmission configuration.\nTitle:\nJoint Power-Distortion Optimization on Devices with MPEG-4 AVC/H.264 Codec\n\nAbstract:\nWe propose a progressive pseudo-analog video transmission scheme that simultaneously handles SNR and bandwidth variations with graceful quality degradation for mobile video streaming. With the inherited SNR-adaptability from pseudo-analog transmission, the proposed progressive solution acquires bandwidth adaptability through an innovative scheduling algorithm with optimal power allocation. The basic idea is to aggressively transmit or retransmit important coefficients so that distortion is minimized at the receiver after each received packet. We derive the closed-form expression of reduced distortion for each packet under given transmission power and known channel conditions, and show that the optimal solution can be obtained with a water-filling algorithm. We also illustrate through analyses and simulations that a near-optimal solution can be found through approximation when only statistical channel information is available. Simulations show that our solution approaches the performance upper bound of pseudo-analog transmission in an additive white Gaussian noise channel and significantly outperforms existing pseudo-analog solutions in a fast Rayleigh fading channel. Trace-driven emulations are also carried out to demonstrate the advantage of the proposed solution over the state-of-the-art digital and pseudo-analog solutions under a real dramatically varying wireless environment.\nTitle:\nProgressive Pseudo-analog Transmission for Mobile Video Streaming.\n\nAbstract:\nThis research studies robust uncoded video transmission over wireless fast fading channel, where only statistical channel state information (CSI) is available at the transmitter. We observe that increasing channel diversity for high priority (HP) data is essential to improving the robustness of video transmission in fading channels. By utilizing the noise and loss resilient nature of video, we find it possible to design a more robust system by re-allocating the power and channel uses among HP and LP (low priority) data. With total power and channel use constraints, we derive an optimal resource allocation scheme under the squared error distortion criterion. In particular, we first propose a new power allocation algorithm at given channel allocation. Second, based on the proposed power allocation algorithm, we design a channel allocation algorithm to strike the tradeoff between the diversity increase of HP data and the information loss of LP data. Third, under known noise power distribution, we derive the optimal resource allocation for uncoded video multicast. Simulations show that the proposed system achieves 2dB and 5dB gain in average and outage PSNR over Softcast in video unicast, and around 1.4dB and 4dB gain in multicast.\nTitle:\nRobust uncoded video transmission over wireless fast fading channel\n\nAbstract:\nEfficient and robust wireless stereo video delivery is an enabling technology for various mobile 3D applications. Existing digital solutions have high source coding efficiency but are not robust to channel variations, while analog solutions have the opposite characteristics. In this paper, we design a novel hybrid digital-analog (HDA) solution to embrace the advantages of both solutions and avoid their drawbacks. Basically, in each pair of stereo frames, one frame is digitally encoded to ensure basic quality and the other is analogly processed to opportunistically utilize good channels for better quality. To improve the system efficiency, we design a zigzag coding structure such that both intra-view and inter-view correlations can be explored through prediction in the frames to be analogly coded. A reference selection mechanism is proposed to further improve the coding efficiency. In addition, we address the problem of optimal power and bandwidth allocation between digital and analog streams. We implement a system, named Swift, and perform extensive trace-driven evaluations based on a software-defined radio platform. We show that Swift outperforms an omniscient digital scheme under the same bandwidth and power constraints, or can have around 2x power saving in order to achieve comparable performance. Subjective quality assessment evidences that Swift provides significantly better visual quality than a straightforward HDA extension of SoftCast.\nTitle:\nSwift: A Hybrid Digital-Analog Scheme for Low-Delay Transmission of Mobile Stereo Video\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.611, mean value for all the writers is 8.648, which makes it 0.032 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nEnabling caching capabilities in dense small cell networks (DSCNs) has a direct impact on file delivery delay and power consumption. Most existing work studied these two performance metrics separately in cache-enabled DSCNs. However, file delivery delay and power consumption are coupled with each other and cannot be minimized simultaneously. In this paper, we investigate the optimal tradoff between these two performance metrics. Firstly, we formulate the joint file delivery delay and power consumption optimization (JDPO) problem where power control, user association and file placement are jointly considered. Then we convert it to a form that can be handled by Generalized Benders Decomposition (GDB). with GDB, we decompose the converted JDPO problem into two smaller problems, i.e., primal problem related to power control and master problem related to user association and file placement. An iterative algorithm is proposed and proved to be $epsilon$-optimal, in which the primal problem and master problem are solved iteratively to approach the optimal solution. To further reduce the complexity of the master problem, an accelerated algorithm based on semi-definite relaxation is proposed. Finally, the simulation results demonstrate that the proposed algorithm can approach the optimal tradeoff between file delivery delay and power consumption.\nTitle:", "model_inf_time": 1.71}, {"id": "4153", "output": "Adaptive QoS Control for Robust Real-Time Video Transmission over Wireless Channels", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper addresses the important issues of resource allocation and rate adaptation for multiple media, such as audio, video, email, and Web traffic, transmitted over W-CDMA (wideband code division multiple access) channel with adaptive QoS (quality of service) support. In order to have QoS support for different types of media, we develop an architecture combining the link layer with application layer controls. It consists of the following contributions: (1) an appropriate model to estimate the varying fading channel is proposed; (2) a hybrid delay-constrained ARQ (automatic repeat request) and UEP (unequal error protection) mechanism that dynamically adapt to the time-varying channel is presented to meet the QoS requirements for different applications; (3) a new resource allocation scheme that considers varying media characteristics is described to be adapted to changing bit error rate (BER) conditions. Simulation results demonstrate the effectiveness of our proposed scheme\nTitle:\nResource allocation with adaptive QoS for multimedia transmission over W-CDMA channels\n\nAbstract:\nScalable video delivery over wireless link is a very challenging task due to the time-varying characteristics of wireless channels. This paper proposes a channel-adaptive error control scheme for efficiently video delivery, which consists of dynamically channel estimation and channel-adaptive Unequal Error Protection (UEP). In our proposed channel-adaptive UEP scheme, a bit allocation algorithm is presented to periodically allocate the available bits among different video layers based on varying channel conditions so as to minimize the end-to-end distortion. Simulation results show that our proposed scheme is efficient under various channel conditions.\nTitle:\nChannel-Adaptive Unequal Error Protection For Scalable Video Transmission Over Wireless Channel\n\nAbstract:\nThe paper addresses the important issues of resource allocation for scalable video transmission over third generation (3G) wireless networks. By taking the time-varying wireless channel/network condition and scalable video codec characteristic into account, we allocate resources between source and channel coders based on the minimum-distortion or minimum-power consumption criterion. Specifically, we first present how to estimate the time-varying wireless channel/network condition through measurements of throughput and error rate in a 3G wireless network. Then, we propose a new distortion-minimized bit allocation scheme with hybrid unequal error protection (UEP) and delay-constrained automatic repeat request (ARQ), which dynamically adapts to the estimated time-varying network conditions. Furthermore, a novel power-minimized bit allocation scheme with channel-adaptive hybrid UEP and delay-constrained ARQ is proposed for mobile devices. In our proposed distortion/power-minimized bit-allocation scheme, bits are optimally distributed among source coding, forward error correction, and ARQ according to the varying channel/network condition. Simulation and analysis are performed using a progressive fine granularity scalability video codec. The simulation results show that our proposed schemes can significantly improve the reconstructed video quality under the same network conditions.\nTitle:\nChannel-adaptive resource allocation for scalable video transmission over 3G wireless network\n\nAbstract:\nScalable video streaming over wireless link with Quality of Service (QoS) is a very challenging task due to the time-varying characteristics of wireless channel and limited battery resource in the handheld devices. This paper proposes an end-to-end network-adaptive architecture for video streaming over 3G wireless network. The proposed architecture not only dynamically estimates the varying network status on the fly, but also simultaneously performs application-level error control and transmission-level power control to protect the random and fading error occurred across the 3G network. Distortion/power-minimized rate allocation is presented to achieve the minimum end-to-end distortion or minimum total power consumption based on user's requirement. Simulation, results demonstrate effectiveness of our proposed scheme.\nTitle:\nNetwork-adaptive scalable video streaming over 3G wireless network\n\nAbstract:\nWireless video communication is particularly challenging because it combines the already difficult problem of efficient compression with the additional and usually contradictory need to make the compressed bit stream robust to channel errors. We describe design and implementation strategies for error-robust video communications with an emphasis on techniques compatible with the coding approaches u...\nTitle:\nRobust video coding algorithms and systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.247, mean value for all the writers is 8.648, which makes it 0.511 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nRobust transmission of real-time video over wireless channels is a challenging problem. This is because the bit error rate (BER) in wireless channels is very high and the bit errors could have devastating effects on the video presentation quality. This paper addresses this problem by introducing adaptive quality-of-service (QoS) control for real-time video communication over wireless channels. The adaptive QoS control consists of optimal mode selection and delay-constrained hybrid automatic repeat request (ARQ). By combining the best features of error-resilient source encoding, forward error correction (FEC) and delay-constrained retransmission, our proposed adaptive QoS control system is capable of achieving bounded delay, high reliability, and efficiency.\nTitle:", "model_inf_time": 1.5}, {"id": "4154", "output": "All-Positive Pinched Hysteresis Loop Circuits Based on NMOS Transistors", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA novel single-input second-order multifunction filter realized by employing current mirrors is introduced in this manuscript. The proposed topology offers simultaneously the lowpass, highpass, bandpass, and bandstop frequency responses. Other attractive characteristics are its potential for low-voltage operation and the electronic tuning. In addition, the only passive elements used for realizing the multifunction filters are grounded capacitors. The filter topology could become universal by employing additional circuitry. Simulation results confirm the correct operation of the proposed structure.\nTitle:\nSingle input multiple output universal biquad using current mirrors\n\nAbstract:\nNovel building blocks for designing high-order current-mode filters by employing current mirrors as the elementary active elements are introduced in this paper. As a design example, a fifth-order elliptic current-mode filter is designed using only grounded capacitors. Simulation results using transistor parameters of a typical CMOS 0.35- technology are in a good agreement with the theoretically ex...\nTitle:\nCurrent-Mode Linear Transformation Filters Using Current Mirrors\n\nAbstract:\nA new voltage-mode fractional-order oscillator, employing in total 12 Metal-Oxide-Semiconductor (MOS) transistors, is introduced in this paper. The proposed circuit is composed of two operational transconductance amplifiers, two inverting voltage buffers, one resistor, and two fractional-order capacitors. Compared with the corresponding already introduced fractional-order oscillators, it offers the benefit of low transistor count and, therefore, simplicity of its structure. In addition, it offers the well-known advantages of fractional-order oscillators about the capability for achieving very low and high oscillation frequencies with reasonable component values. The behavior of the proposed oscillator has been numerically studied using the MATLAB program, while its performance has been evaluated by SPICE simulations, using TSMC 0.18 \u03bcm Level-7 CMOS process parameters with \u00b1I V supply voltages.\nTitle:\nCompact MOS-RC voltage-mode fractional-order oscillator design\n\nAbstract:\nA novel bias scheme for realizing low-voltage second-order translinear loops is introduced in this paper. The provided design examples include current geometric-mean, squarer/divider, and multiplier/divider cells. The performed comparison shows that the derived analog signal processing blocks offer reduced circuit complexity and improved performance, compared with the corresponding already published counterparts.\nTitle:\nLow-voltage reduced complexity cells for MOS translinear loops\n\nAbstract:\nIn this study, a new voltage-mode fractional-order oscillator using two unity-gain voltage buffers, two operational transconductance amplifiers, one resistor, and two capacitors is presented. The design procedure of integer-order as well as fractional-order oscillator employing in total 20 MOS transistors is discussed. Effects of fractional-order capacitors on amplitude, phase, condition of oscillation, and frequency of oscillation are shown. Various case examples are given while SPICE simulations using TSMC 0.35 mu m level-3 CMOS process parameters with +/- 1.65 V supply voltages verify their operation and compare with theoretical ones.\nTitle:\nFractional-order oscillator design using unity-gain voltage buffers and OTAs\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.255, mean value for all the writers is 8.648, which makes it 0.335 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTwo novel nonlinear circuits that exhibit an all-positive pinched hysteresis loop are proposed. These circuits employ two NMOS transistors, one of which operates in its triode region, in addition to two first-order filter sections. We show the equivalency to a charge-controlled resistance (memristance) in a decremental state via detailed analysis. Simulation and experimental results verify the proposed theory.\nTitle:", "model_inf_time": 1.43}, {"id": "4155", "output": "Building a Bug-Finding Business: Coverity's Approach", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAbstract: This paper describes experiences with software model checking after several years of using static analysis to find errors. We initially thought that the trade-off between the two was clear: static analysis was easy but would mainly find shallow bugs, while model checking would require more work but would be strictly better - it would find more errors, the errors would be deeper, and the approach would be more powerful. These expectations were often wrong.\nTitle:\nStatic Analysis versus Software Model Checking for Bug Finding\n\nAbstract:\nAutomatic tools for finding software errors require knowledge of the rules a program must obey, or \"specifications,\" before they can identify bugs. We present a method that combines factor graphs and static program analysis to automatically infer specifications directly from programs. We illustrate the approach on inferring functions in C programs that allocate and release resources, and evaluate the approach on three codebases: SDL, OpenSSH, and the OS kernel for Mac OS X (XNU). The inferred specifications are highly accurate and with them we have discovered numerous bugs.\nTitle:\nA factor graph model for software bug finding\n\nAbstract:\nSoftware bugs are a well-known source of security vulnerabilities. One technique for finding bugs, symbolic execution, considers all possible inputs to a program but suffers from scalability limitations. This paper uses a variant, under-constrained symbolic execution, that improves scalability by directly checking individual functions, rather than whole programs. We present UC-KLEE, a novel, scalable framework for checking C/C++ systems code, along with two use cases. First, we use UC-KLEE to check whether patches introduce crashes. We check over 800 patches from BIND and OpenSSL and find 12 bugs, including two OpenSSL denial-of-service vulnerabilities. We also verify (with caveats) that 115 patches do not introduce crashes. Second, we use UC-KLEE as a generalized checking framework and implement checkers to find memory leaks, uninitialized data, and unsafe user input. We evaluate the checkers on over 20,000 functions from BIND, OpenSSL, and the Linux kernel, find 67 bugs, and verify that hundreds of functions are leak free and that thousands of functions do not access uninitialized data.\nTitle:\nUnder-Constrained Symbolic Execution: Correctness Checking for Real Code\n\nAbstract:\nWe present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage -- on average over 90% per tool (median: over 94%) -- and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100% coverage on 31 of them. We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.\nTitle:\nKLEE: unassisted and automatic generation of high-coverage tests for complex systems programs\n\nAbstract:\nThis talk will draw on our efforts in using static analysis, model checking, and symbolic execution to find bugs in real code, both in academic and commercial settings. The unifying religion driving all these efforts has been: results matter more than anything. That which works is good, that which does not is not. While this worldview is simple, reality is not. I will discuss some what we learned in struggling with this mismatch.\nTitle:\nLessons in the Weird and Unexpected: Some Experiences from Checking Large Real Systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.615, mean value for all the writers is 8.648, which makes it 0.028 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nHow Coverity built a bug-finding tool, and a business, around the unlimited supply of bugs in software systems.\nTitle:", "model_inf_time": 1.34}, {"id": "4156", "output": "A rule-based system for contextualized information delivery to police officers", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nXQuery 1.0, the XML query language which is about to become a W3C Recommendation, lacks the ability to make persistent changes to instances of its data model. A number of proposals to extend XQuery with update facilities have been made lately, including a W3C Working Draft. In order to investigate some of the different constructs that are introduced in these proposals, we define an XQuery-based update language that combines them. By doing so, we show that it is possible to give a concise, complete and formal definition of such a language. We define subsets of this language to examine the relative expressive power of the different constructs, and we establish the relationships between these subsets in terms of queries and updates that can be expressed. Finally, we discuss the relationships between these subsets and existing XQuery-based update languages.\nTitle:\nOn the expressive power of XQuery-based update languages\n\nAbstract:\nRecently proposed form-based web information systems liberate the capture and reuse of data in organizations by substituting the development of technical implementations of electronic forms for the conceptual modelling of forms' tree-structured schemas and their data access rules. Significantly, these instance-dependent rules also imply a workflow process associated to a form, eliminating the need for a costly workflow design phase. Instead, the workflows thus created in an ad hoc manner by unsophisticated end-users can be automatically analyzed, and incorrect forms rejected.This paper examines fundamental correctness properties of workflows that are implied by instance-dependent access rules. Specifically, we study the decidability of the form completability property and the semi-soundness of a form's workflow. These problems are affected by a choice of constraints on the path language used to express access rules and completion formulas, and on the depth of the form's schema tree. Hence, we study these problems by examining them in the context of several different fragments determined by such constraints.\nTitle:\nAnalyzing workflows implied by instance-dependent access rules\n\nAbstract:\nThe hierarchical and semistructured nature of XML data may cause complicated update-behavior. Updates should not be limited to entire document trees, but should ideally involve subtrees and even individual elements. Providing a suitable scheduling algorithm for semistructured data can significantly improve collaboration systems that store their data -- e.g. word processing documents or vector graphics-- as XML documents. In this paper we improve upon earlier work (see [5]) which presented two equivalent concurrency control mechanisms based on Path Locks. In contrast to the earlier work, we now provide details regarding the workings of a commit scheduler for XML databases which uses the path lock conflict rules. We also give a comprehensive proof of serializability which enhances and clarifies the ideas in our previous work.\nTitle:\nA commit scheduler for XML databases\n\nAbstract:\nMany Web applications provide personalized and adapted services and contents to their users. As these Web applications are becoming increasingly connected, a new interesting challenge in their engineering is to allow the Web applications to exchange, reuse, integrate, interlink, and enrich their data and user models, hence, to allow for user modeling and personalization across application boundaries. In this paper, we present the Grapple User Modeling Framework (GUMF) that facilitates the brokerage of user profile information and user model representations. We show how the existing GUMF is extended with a new method that is based on configurable derivation rules that guide a new knowledge deduction process. Using our method, it is possible not only to integrate data from GUMF dataspaces, but also to incorporate and reuse RDF data published as Linked Data on the Web. Therefore, we introduce the so-called Grapple Derivation Rule (GDR) language as well as the corresponding GDR Engine. Further, we showcase the extended GUMF in the context of a concrete project in the e-learning domain.\nTitle:\nA flexible rule-based method for interlinking, integrating, and enriching user data\n\nAbstract:\nXQuery is considered to become the standard query language for XML documents. However, the complete XQuery syntax and semantics seem too complicated for research and educational purposes. By defining a concise backwards compatible subset of XQuery with a complete formal description, we provide a practical foundation for XQuery research. We pay special attention to usability by supporting the most typical XQuery expressions.\nTitle:\nLiXQuery: a formal foundation for XQuery research\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.091, mean value for all the writers is 8.648, which makes it 0.475 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn order to support police officers in their daily activities, we have designed a rule-based system which can deliver contextualized information to police officers, thus supporting decision making. In particular, we present a framework that has been designed on the basis of requirements elicited in a previous study, focusing on the rule language and the engine that essentially defines and allows to configure the behaviour of the system. The rules consist of a body which specifies conditions that need to be fulfilled in a certain context. The head of the rules specifies how the relevance ratings of certain information items for specific users need to be updated given that the conditions in the body are met. On the basis of cumulated ratings, the system generates a user- and context specific ranking of information items. Quantitative evaluations in terms of precision and recall with respect to a gold standard determined in cooperation with police officers show that the system can cater for the requirements of our end users and yields reasonable precision and recall values.\nTitle:", "model_inf_time": 1.51}, {"id": "4157", "output": "Scheduling Policies for Processor Co-allocation in Multicluster Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn systems consisting of multiple clusters of processors interconnected by relatively slow network connections such as our Distributed ASCI Supercomputer (DAS), applications may benefit from the availability of processors in multiple clusters. However, the performance of single-application multicluster execution may be degraded due to the slow wide-area links. In addition, scheduling policies for such systems have to deal with more restrictions than schedulers for single clusters in that every component of a job has to fit in separate clusters. In this paper we present a measurement study of the total runtime of two applications, and of the communication time of one of them, both on single clusters and on multicluster systems. In addition, we perform simulations of several multicluster scheduling policies based on our measurement results. Our results show that in many cases, restricted forms of co-allocation in multiclusters have better performance than not allowing co-allocation at all.\nTitle:\nA Measurement-Based Simulation Study of Processor Co-allocation in Multicluster Systems\n\nAbstract:\nIn multicluster grid systems, parallel applications may benefit from processor co-allocation, that is, the simulta- neous allocation of processors in multiple clusters. Although co-allocation allows the allocation of more processors than available in a single cluster, it may severely increase the ex- ecution time of applications due to the relatively slow wide- area communication. The aim of this paper is to investigate the benefit of co-allocation in multicluster grid systems, despite this drawback. To this end, we have conducted experiments in a real multicluster grid environment, as well as in a simulated environment, and we evaluate the performance of co-allocation for various applications that range from computation-intensive to communication-intensive and for various system load settings. In addition, we compare the performance of scheduling policies that are specifically designed for co-allocation. We demonstrate that considering latency in the resource selection phase improves the performance of co-allocation, especially for communication- intensive parallel applications.\nTitle:\nOn the Benefit of Processor Coallocation in Multicluster Grid Systems\n\nAbstract:\nIn multicluster systems, and more generally, in grids, parallel applications may require co-allocation, i.e., the simultaneous allocation of resources such as processors in multiple clusters. Although co-allocation enables the allocation of more processors than available on a single cluster, depending on the applications\u00c2\u00bf communication characteristics, it has the potential disadvantage of increased execution times due to relatively slow wide-area communication. In this paper, we present two job placement policies, the Cluster Minimization and the Flexible Cluster Minimization policies which take into account the wide-area communication overhead when co-allocating applications across the clusters. We have implemented these policies in our grid scheduler called KOALA in order to serve different job request types. To assess the performance of the policies, we perform experiments in a real multicluster testbed using communication-intensive parallel applications.\nTitle:\nCommunication-Aware Job Placement Policies for the KOALA Grid Scheduler\n\nAbstract:\nScientists increasingly rely on the execution of workflows in grids to obtain results from complex mixtures of applications. However, the inherently dynamic nature of grid workflow scheduling, stemming from the unavailability of scheduling information and from resource contention among the (multiple) workflows and the non-workflow system load, may lead to poor or unpredictable performance. In this paper we present a comprehensive and realistic investigation of the performance of a wide range of dynamic workflow scheduling policies in multicluster grids. We first introduce a taxonomy of grid workflow scheduling policies that is based on the amount of dynamic information used in the scheduling process, and map to this taxonomy seven such policies across the full spectrum of information use. Then, we analyze the performance of these scheduling policies through simulations and experiments in a real multicluster grid. We find that there is no single grid workflow scheduling policy with good performance across all the investigated scenarios. We also find from our real system experiments that with demanding workloads, the limitations of the head-nodes of the grid clusters may lead to performance loss not expected from the simulation results. We show that task throttling, that is, limiting the per-workflow number of tasks dispatched to the system, prevents the head-nodes from becoming overloaded while largely preserving performance, at least for communication-intensive workflows.\nTitle:\nPerformance analysis of dynamic workflow scheduling in multicluster grids\n\nAbstract:\nIn systems consistingof multiple clusters of processors interconnected by relatively slow connections such as our Distributed ASCI1 Supercomputer (DAS), jobs may request co-allocation, i.e., the simultaneous allocation of processors in different clusters. The performance of co-allocation may be severely impacted by the slowintercluster connections, and by the types of job requests. We distinguish different job request types ranging from ordered requests that specify the numbers of processors needed in each of the clusters, to flexible requests that only specify a total. We simulate multicluster systems with the FCFS policy-- and with two policies for placinga flexible request, one tries to balance cluster loads and one tries to fill clusters completely--to determine the response times under workloads consistingof a single or of different request types for different communication speeds across the intercluster connections. In addition to a synthetic workload, we also consider a workload derived from measurements of a real application on the DAS. We find that the communication speed difference has a severe impact on response times, that a relatively small amount of capacity is lost due to communication, and that for a mix of request types, the performance is determined not only by the separate behaviours of the different types of requests, but also by the way in which they interact.\nTitle:\nThe Influence of Communication on the Performance of Co-allocation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.577, mean value for all the writers is 8.648, which makes it 0.061 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nBuilding multicluster systems out of multiple, geographically distributed clusters interconnected by high-speed wide-area networks can provide access to a larger computational power and to a wider range of resources. Jobs running on multiclusters and, more generally, in grids, may require (processor) coallocation, i.e., the simultaneous allocation of resources (processors) in different clusters or subsystems of a grid. In this paper, we propose four scheduling policies for processor coallocation in multiclusters, and we assess with simulations their performance under a wide variety of parameter settings. In particular, in our simulations we use synthetic workloads and workloads derived from the logs of actual systems and from runtime measurements. We conclude that although coallocation makes scheduling more difficult and the wide-area communication critically impacts the performance, there is a wide range of realistic applications that may benefit from coallocation. However, unrestricted coallocation is not recommended: Limiting the total job size or the number or the sizes of their components improves performance.\nTitle:", "model_inf_time": 1.52}, {"id": "4158", "output": "A Framework for Evaluating Grid Performance", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nGrid computing is becoming the natural way to aggregate and share large sets of heterogeneous resources. With the infrastructure becoming ready for the challenge, current grid development and acceptance hinge on proving that grids reliably support real applications, and on creating adequate benchmarks to quantify this support. However, grid applications are just beginning to emerge, and traditional benchmarks have yet to prove representative in grid environments. To address this chicken-and-egg problem, we propose a middle-way approach: create and run synthetic grid workloads comprising applications representative for today's grids. For this purpose, we have designed and implemented GRENCHMARK, a framework for synthetic workload generation and submission. The framework greatly facilitates synthetic workload modeling, comes with over 35 synthetic and real applications, and is extensible and flexible. We show how the framework can be used for grid system analysis, functionality testing in grid environments, and for comparing different grid settings, and present the results obtained with GRENCHMARK in our multi-cluster grid, the DAS\nTitle:\nGRENCHMARK: A Framework for Analyzing, Testing, and Comparing Grids\n\nAbstract:\nMany advances in grid resource management are still required to realize the grid computing vision of the integration of a world-wide computing infrastructure for scientific use. The pressure for advances is increased by the fast evolution of single, large clusters, which are the primary technological alternative to grids. However, advances in grid resource management cannot be achieved without an appropriate toolbox, of which simulation environments form an essential part. The current grid simulation environments still lack important workload and system modeling features, and research productivity features such as automated experiment setup and management. In this paper we address these issues through the design and a reference implementation of DGSim, a framework for simulating grid resource management architectures. DGSimintroduces the concepts of grid evolution and of job selection policy, and extends towards realism the current body of knowledge on grid inter-operation, on grid dynamics, and on workload modeling. We also show through two real use cases how DGSimcan be used to compare grid resource management architectures.\nTitle:\nDGSim: Comparing Grid Resource Management Architectures through Trace-Based Simulation\n\nAbstract:\nGrid computing is becoming the natural way to aggregate and share large and heterogeneous sets of resources. However, grid development and acceptance hinge on proving that grids reliably support large communities of users, and their real applications. In this paper we assess the ability of existing grid infrastructures to provision resources for a class of applications with numerous potential users, namely the class of world-wide data-sharing services. For this purpose, we first analyze the requirements of this class of applications, and match them against the existing spare capacity in three existing large-scale grid environments, namely OSG/Grid3, NorduGrid, and CERN LCG. Having shown that the existing capacity is insufficient, we devise and assess through trace-based simulation five domainspecific scheduling policies. Our findings give evidence that grid technology could be successfully leveraged for worldwide data-sharing services, without impacting the level of service for the currently existing load.\nTitle:\nProvisioning and Scheduling Resources for World-Wide Data-Sharing Services\n\nAbstract:\nThe Grid computing vision promises to provide the needed platform for a new and more demanding range of applications. For this promise to become true, a number of hurdles, including the design and deployment of adequate resource management and information services, need to be overcome. In this context, understanding the characteristics of real Grid workloads is a crucial step for improving the quality of existing Grid services, and in guiding the design of new solutions. Towards this goal, in this work we present the characteristics of traces of four real Grid environments, namely LCG, Grid3, and TeraGrid, which are among the largest production Grids currently deployed, and the DAS, which is a research Grid. We focus our analysis on virtual organizations, on users, and on individual jobs characteristics. We further attempt to quantify the evolution and the performance of the Grid systems from which our traces originate. Finally, given the scarcity of the information available for analysis purposes, we discuss the requirements of a new format for Grid traces, and we propose the establishment of a virtual center for workload-based Grid benchmarking data: The Grid Workloads Archive.\nTitle:\nHow are Real Grids Used? The Analysis of Four Grid Traces and Its Implications\n\nAbstract:\nLarge-scale distributed computing systems such as grids are serving a growing number of scientists. These environments bring about not only the advantages of an economy of scale, but also the challenges of resource and workload heterogeneity. A consequence of these two forms of heterogeneity is that job runtimes and queue wait times are highly variable, which generally reduces system performance and makes grids difficult to use by the common scientist. Predicting job runtimes and queue wait times have been widely studied for parallel environments. However, there is no detailed investigation on how the proposed prediction methods perform in grids, whose resource structure and workload characteristics are very different from those in parallel systems. In this paper, we assess the performance and benefit of predicting job runtimes and queue wait times in grids based on traces gathered from various research and production grid environments. First, we evaluate the performance of simple yet widely used time series prediction methods and the effect of applying them to different types of job classes (e.g., all jobs submitted by single users or to single sites). Then, we investigate the performance of two kinds of queue wait time prediction methods for grids. Last, we investigate whether prediction-based grid-level scheduling policies can have better performance than policies that do not use predictions.\nTitle:\nTrace-based evaluation of job runtime and queue wait time predictions in grids\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.415, mean value for all the writers is 8.648, which makes it 0.654 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nGrid computing is becoming a common platform for solving large scale computing tasks. However, a number of major technical issues, including the lack of adequate performance evaluation approaches, hinder the grid computing's further development. The requirements herefore are manifold; adequate approaches must combine appropriate performance metrics, realistic workload models, and flexible tools for workload generation, submission, and analysis. In this paper we present an approach to tackle this complex problem. First, we introduce a set of grid performance objectives based on traditional and grid-specific performance metrics. Second, we synthesize the requirements for realistic grid workload modeling, e.g. co-allocation, data and network management, and failure modeling. Third, we show how GRENCHMARK, an existing framework for generating, running, and analyzing grid workloads, can be extended to implement the proposed modeling techniques. Our approach aims to be an initial and necessary step towards a common performance evaluation framework for grid environments.\nTitle:", "model_inf_time": 1.28}, {"id": "4159", "output": "Hierarchical Bayesian Pair-wise Ranking for One-Class Collaborative Filtering", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nRecommender systems have been successfully dealing with the problem of information overload. A considerable amount of research has been conducted on recommender systems, but most existing approaches only focus on user and item dimensions and neglect any additional contextual information, such as time and location. In this paper, we propose a Multi-Layer Context Graph (MLCG) model which incorporates a variety of contextual information into a recommendation process and models the interactions between users and items for better recommendation. Moreover, we provide a new ranking algorithm based on Personalized PageRank for recommendation in MLCG, which captures users' preferences and current situations. The experiments on two real-world datasets demonstrate the effectiveness of our approach.\nTitle:\nPersonalized Recommendation On Multi-Layer Context Graph\n\nAbstract:\nRecommender systems have been successfully dealing with the problem of information overload. However, most recommendation methods suit to the scenarios where explicit feedback, e.g. ratings, are available, but might not be suitable for the most common scenarios with only implicit feedback. In addition, most existing methods only focus on user and item dimensions and neglect any additional contextual information, such as time and location. In this paper, we propose a graph-based generic recommendation framework, which constructs a Multi-Layer Context Graph (MLCG) from implicit feedback data, and then performs ranking algorithms in MLCG for context-aware recommendation. Specifically, MLCG incorporates a variety of contextual information into a recommendation process and models the interactions between users and items. Moreover, based on MLCG, two novel ranking methods are developed: Context-aware Personalized Random Walk (CPRW) captures user preferences and current situations, and Semantic Path-based Random Walk (SPRW) incorporates semantics of paths in MLCG into random walk model for recommendation. The experiments on two real-world datasets demonstrate the effectiveness of our approach.\nTitle:\nA Graph-based model for context-aware recommendation using implicit feedback data\n\nAbstract:\nWeb service recommendation has become a hot yet fundamental research topic in service computing. The most popular technique is the Collaborative Filtering (CF) based on a user-item matrix. However, it cannot well capture the relationship between Web services and providers. To address this issue, we first design a cube model to explicitly describe the relationship among providers, consumers and Web services. And then, we present a Standard Deviation based Hybrid Collaborative Filtering (SD-HCF) for Web Service Recommendation (WSRec) and an Inverse consumer Frequency based User Collaborative Filtering (IF-UCF) for Potential Consumers Recommendation (PCRec). Finally, the decision-making process of bidirectional recommendation is provided for both providers and consumers. Sets of experiments are conducted on real-world data provided by Planet-Lab. In the experiment phase, we show how the parameters of SD-HCF impact on the prediction quality as well as demonstrate that the SD-HCF is much better than extant methods on recommendation quality, including the CF based on user, the CF based on item and general HCF. Experimental comparison between IF-UCF and UCF indicates the effectiveness of adding inverse consumer frequency to UCF. \u00a9 2012 Springer-Verlag London.\nTitle:\nHybrid Collaborative Filtering algorithm for bidirectional Web service recommendation.\n\nAbstract:\nCollaborative filtering (CF) technique is capable of generating personalized recommendations. However, the recommender systems utilizing CF as their key algorithms are vulnerable to shilling attacks which insert malicious user profiles into the systems to push or nuke the reputations of targeted items. There are only a small number of labeled users in most of the practical recommender systems, while a large number of users are unlabeled because it is expensive to obtain their identities. In this paper, Semi-SAD, a new semi-supervised learning based shilling attack detection algorithm is proposed to take advantage of both types of data. It first trains a na\u00efve Bayes classifier on a small set of labeled users, and then incorporates unlabeled users with EM-\u9a74 to improve the initial na\u00efve Bayes classifier. Experiments on MovieLens datasets are implemented to compare the efficiency of Semi-SAD with supervised learning based detector and unsupervised learning based detector. The results indicate that Semi-SAD can better detect various kinds of shilling attacks than others, especially against obfuscated and hybrid shilling attacks.\nTitle:\nShilling attack detection utilizing semi-supervised learning method for collaborative recommender system\n\nAbstract:\nWe propose an enhanced grid-density based approach for clustering high dimensional data. Our technique takes objects (or points) as atomic units in which the size requirement to cells is waived without losing clustering accuracy. For efficiency, a new partitioning is developed to make the number of cells smoothly adjustable; a concept of the ith-order neighbors is defined for avoiding considering the exponential number of neighboring cells; and a novel density compensation is proposed for improving the clustering accuracy and quality. We experimentally evaluate our approach and demonstrate that our algorithm significantly improves the clustering accuracy and quality.\nTitle:\nEnhancing grid-density based clustering for high dimensional data\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.889, mean value for all the writers is 8.648, which makes it 1.059 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nPair-wise ranking methods have been widely used in recommender systems to deal with implicit feedback. They attempt to discriminate between a handful of observed items and the large set of unobserved items. In these approaches, however, user preferences and item characteristics cannot be estimated reliably due to overfitting given highly sparse data. To alleviate this problem, in this paper, we propose a novel hierarchical Bayesian framework which incorporates \"bag-of-words\" type meta-data on items into pair-wise ranking models for one-class collaborative filtering. The main idea of our method lies in extending the pair-wise ranking with a probabilistic topic modeling. Instead of regularizing item factors through a zero-mean Gaussian prior, our method introduces item-specific topic proportions as priors for item factors. As a by-product, interpretable latent factors for users and items may help explain recommendations in some applications. We conduct an experimental study on a real and publicly available dataset, and the results show that our algorithm is effective in providing accurate recommendation and interpreting user factors and item factors.\nTitle:", "model_inf_time": 1.46}, {"id": "4160", "output": "Implementation and Evaluation of OpenSPlySER: Integrating DySER into OpenSPARC", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSpecialization and accelerators are being proposed as an effective way to address the slowdown of Dennard scaling. DySER is one such accelerator, which dynamically synthesizes large compound functional units to match program regions, using a co-designed compiler and microarchitecture. We have completed a full prototype implementation of DySER integrated into the OpenSPARC processor (called SPARC-DySER), a co-designed compiler in LLVM, and a detailed performance evaluation on an FPGA system, which runs an Ubuntu Linux distribution and full applications. Through the prototype, this paper evaluates the fundamental principles of DySER acceleration. Our two key findings are: i) the DySER execution model and microarchitecture provides energy efficient speedups and the integration of DySER does not introduce overheads ??? overall, DySER???s performance improvement to OpenSPARC is 6X, consuming only 200mW ; ii) on the compiler side, the DySER compiler is effective at extracting computationally intensive regular and irregular code. For non-computationally intense irregular code, two control flow shapes curtail the compiler???s effectiveness, and we identify potential adaptive mechanisms. Finally, our experience of bringing up an end-to-end prototype of an ISA-exposed accelerator has made clear that two particular artifacts are greatly needed to perform this type of design more quickly and effectively: 1) Open-source implementations of high-performance baseline processors, and 2) Declarative tools for quickly specifying combinations of known compiler transforms.\nTitle:\nPerformance evaluation of a DySER FPGA prototype system spanning the compiler, microarchitecture, and hardware implementation\n\nAbstract:\nDue to limits in technology scaling, energy efficiency of logic devices is decreasing in successive generations. To provide continued performance improvements without increasing power, regardless of the sequential or parallel nature of the application, microarchitectural energy efficiency must improve. We propose Dynamically Specialized Datapaths to improve the energy efficiency of general purpose programmable processors. The key insights of this work are the following. First, applications execute in phases and these phases can be determined by creating a path-tree of basic-blocks rooted at the inner-most loop. Second, specialized datapaths corresponding to these path-trees, which we refer to as DySER blocks, can be constructed by interconnecting a set of heterogeneous computation units with a circuit-switched network. These blocks can be easily integrated with a processor pipeline. A synthesized RTL implementation using an industry 55nm technology library shows a 64-functional-unit DySER block occupies approximately the same area as a 64 KB single-ported SRAM and can execute at 2 GHz. We extend the GCC compiler to identify path-trees and code-mapping to DySER and evaluate the PAR-SEC, SPEC and Parboil benchmarks suites. Our results show that in most cases two DySER blocks can achieve the same performance (within 5%) as having a specialized hardware module for each path-tree. A 64-FU DySER block can cover 12% to 100% of the dynamically executed instruction stream. When integrated with a dual-issue out-of-order processor, two DySER blocks provide geometric mean speedup of 2.1X (1.15X to 10X), and geometric mean energy reduction of 40% (up to 70%), and 60% energy reduction if no performance improvement is required.\nTitle:\nDynamically Specialized Datapaths for energy efficient computing\n\nAbstract:\nModern microprocessors exploit data level parallelism through in-core data-parallel accelerators in the form of short vector ISA extentions such as SSE/AVX and NEON. Although these ISA extentions have existed for decades, compilers do not generate good quality, high-performance vectorized code without significant programmer intervention and manual optimization. The fundamental problem is that the architecture is too rigid, which overly complicates the compiler's role and simultaneously restricts the types of codes that the compiler can profitably map to these data-parallel accelerators. We take a fundamentally new approach that first makes the architecture more flexible and exposes this flexibility to the compiler. Counter-intuitively, increasing the complexity of the accelerator's interface to the compiler enables a more robust and efficient system that supports many types of codes. This system also enables the performance of auto-acceleration to be comparable to that of manually-optimized implementations. To address the challenges of compiling for flexible accelerators, we propose a variant of Program Dependence Graph called the Access Execute Program Dependence Graph to capture spatio-temporal aspects of memory accesses and computations. We implement a compiler that uses this representation and evaluate it by considering both a suite of kernels developed and tuned for SSE, and \u201cchallenge\u201d data-parallel applications, the Parboil benchmarks. We show that our compiler, which targets the DySER accelerator, provides high-quality code for the kernels and full applications, commonly reaching within 30% of manually-optimized and out-performs compiler-produced SSE code by 1.8\u00d7.\nTitle:\nBreaking SIMD shackles with an exposed flexible microarchitecture and the access execute PDG\n\nAbstract:\nThis paper identifies a new opportunity for improving the efficiency of a processor core: memory access phases of programs. These are dynamic regions of programs where most of the instructions are devoted to memory access or address computation. These occur naturally in programs because of workload properties, or when employing an in-core accelerator, we get induced phases where the code execution on the core is access code. We observe such code requires an OOO core's dataflow and dynamism to run fast and does not execute well on an in-order processor. However, an OOO core consumes much power, effectively increasing energy consumption and reducing the energy efficiency of in-core accelerators. We develop an execution model called memory access dataflow (MAD) that encodes dataflow computation, event-condition-action rules, and explicit actions. Using it we build a specialized engine that provides an OOO core's performance but at a fraction of the power. Such an engine can serve as a general way for any accelerator to execute its respective induced phase, thus providing a common interface and implementation for current and future accelerators. We have designed and implemented MAD in RTL, and we demonstrate its generality and flexibility by integration with four diverse accelerators (SSE, DySER, NPU, and C-Cores). Our quantitative results show, relative to in-order, 2-wide OOO, and 4-wide OOO, MAD provides 2.4\u00d7, 1.4\u00d7 and equivalent performance respectively. It provides 0.8\u00d7, 0.6\u00d7 and 0.4\u00d7 lower energy.\nTitle:\nEfficient execution of memory access phases using dataflow specialization\n\nAbstract:\nData-parallel programs are both growing in importanceand increasing in diversity, resulting in specialized processorstargeted at specific classes of these programs. This paperpresents a classification scheme for data-parallelprogram attributes, and proposes micro-architecturalmechanisms to support applications with diverse behaviorusing a single reconfigurable architecture. We focuson the following four broad kinds of data-parallel programs- DSP/multimedia, scientific, networking, andreal-time graphics workloads. While all of these programsexhibit high computational intensity, coarse-grainregular control behavior, and some regular memory accessbehavior, they show wide variance in the computationrequirements, fine grain control behavior, and the frequencyof other types of memory accesses. Based onthis study of application attributes, this paper proposesa set of general micro-architectural mechanismsthat enable a baseline architecture to be dynamically tailoredto the demands of a particular application. Thesemechanisms provide efficient execution across a spectrumof data-parallel applications and can be applied todiverse architectures ranging from vector cores to conventionalsuperscalar cores. Our results using a baselineTRIPS processor show that the configurability of the architectureto the application demands provides harmonicmean performance improvement of 5%-55% over scalableyet less flexible architectures, and performs competitivelyagainst specialized architectures.\nTitle:\nUniversal Mechanisms for Data-Parallel Architectures\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.926, mean value for all the writers is 8.648, which makes it 0.237 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAccelerators and specialization in various forms are emerging as a way to increase processor performance. Examples include Navigo, Conservation-Cores, BERET, and DySER. While each of these employ different primitives and principles to achieve specialization, they share some common concerns with regards to implementation. Two of these concerns are: how to integrate them with a commercial processor and how to develop their compiler toolchain. This paper undertakes an implementation study of one design point: integration of DySER into OpenSPARC, a design we call OpenSPlySER. We report on our implementation exercise and quantitative results, and conclude with a set of our lessons learned. We demonstrate that DySER delivers on its goal of providing a non-intrusive accelerator design. OpenSPlySERruns on an Virtex-5 FPGA, boots unmodified Linux, and runs most of the SPECINT benchmarks with our compiler. Due to physical design constraints, speedups on full benchmarks are modest for the FPGA prototype. On targeted microbenchmarks, OpenSPlySER delivers up to a 31-fold speedup over the baseline OpenSPARC. We conclude with some lessons learned from this somewhat unique exercise of significantly modifying a commercial processor. To the best of our knowledge, this work is one of the most ambitious extensions of OpenSPARC.\nTitle:", "model_inf_time": 2.02}, {"id": "4161", "output": "A Multi-Task Embedding Model for Cross-Domain Recommendation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCyber-physical systems (CPS) are often characterized as smart systems, which intelligently interact with other systems across information and physical interfaces. An increased dependence on CPS led to the collection of a vast amount of human-centric data, which brings the information overload problem across multiple domains. Recommender systems in CPS, which always provide information recommendations for users based on historical ratings collected from a single domain only, suffer from the data sparsity problem. Recently, several recommendation models have been proposed to transfer knowledge across multiple domains to alleviate the sparsity problem, which typically assumes that multiple domains share a latent common rating pattern. However, real-world related domains do not necessarily share such a rating pattern, and diversity across domains might outweigh the advantages of such common pattern, which results in performance degradations. In this paper, we propose a novel cross-domain recommendation model, which not only learn the common rating pattern across domains with the flexibility in controlling the optimal level of sharing, but also learn the domain-specific rating patterns in each domain involving discriminative information propitious to performance improvement. Extensive experiments on real world data sets suggest that our proposed model outperforms the state-of-the-art methods for the cross-domain recommendation task in CPS.\nTitle:\nA Cross-Domain Recommendation Model for Cyber-Physical Systems\n\nAbstract:\nIn this paper we address the problem of modelling relational data, which has appeared in many applications such as social network analysis, recommender systems and bioinformatics. Previous studies either consider latent feature based models to do link prediction in the relational data but disregarding local structure in the network, or focus exclusively on capturing network structure of objects based on latent blockmodels without coupling with latent characteristics of objects to avoid redundant information. To combine the benefits of the previous work, we model the relational data as a function of both latent feature factors and latent cluster memberships of objects via our proposed Latent Factor BlockModel (LFBM) to collectively discover globally predictive intrinsic properties of objects and capture the latent block structure. We also develop an optimization transfer algorithm to learn the latent factors. Extensive experiments on the synthetic data and several real world datasets suggest that our proposed LFBM model outperforms the state-of-the-art approaches for modelling the relational data.\nTitle:\nLatent factor blockmodel for modelling relational data\n\nAbstract:\nKnowledge graph (KG) embedding aims at learning the latent semantic representations for entities and relations. However, most existing approaches can only be applied to KG completion, so cannot identify relations including unseen entities (or Out-of-KG entities). In this paper, motivated by the zero-shot learning, we propose a novel model, namely JointE, jointly learning KG and entity descriptions embedding, to extend KG by adding new relations with Out-of-KG entities. The JointE model is evaluated on entity prediction for zero-shot embedding. Empirical comparisons on benchmark datasets show that the proposed JointE model outperforms state-of-the-art approaches. The source code of JointE is available at https://github.com/yzur/JointE.\nTitle:\nZero-Shot Embedding For Unseen Entities In Knowledge Graph\n\nAbstract:\nA knowledge base of triples like  is a very important resource for knowledge management. It is very useful for human-like reasoning, query expansion, question answering (Siri) and other related AI tasks. However, such a knowledge base often suffers from incompleteness due to a large volume of increasing knowledge in the real world and a lack of reasoning capability. In this paper, we propose a Pairwise-interaction Differentiated Embeddings model to embed entities and relations in the knowledge base to low dimensional vector representations and then predict the possible truth of additional facts to extend the knowledge base. In addition, we present a probability-based objective function to improve the model optimization. Finally, we evaluate the model by considering the problem of computing how likely the additional triple is true for the task of knowledge base completion. Experiments on  and  show the excellent performance of our model and algorithm.\nTitle:\nKnowledge base completion by learning pairwise-interaction differentiated embeddings\n\nAbstract:\nThis paper proposes a simple retraining scheme to purposefully adjust unsupervised word embeddings for specific supervised tasks, such as sentence classification. Different from the current methods, which fine-tune word embeddings in training set through the supervised learning procedure, our method treats the labels of task as implicit context information to retrain word embeddings, so that every required word for the intended task obtains task-specific representation. Moreover, because our method is independent of the supervised learning process, it has less risk of over-fitting. We have validated the rationality of our method on various sentence classification tasks. The improvements of accuracy are remarkable, when only scarce training set is available.\nTitle:\nA Targeted Retraining Scheme of Unsupervised Word Embeddings for Specific Supervised Tasks.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.024, mean value for all the writers is 8.648, which makes it 0.321 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIt inevitably comes out information overload problem with the increasing available data on e-commence websites. Most existing approaches have been proposed to recommend the users personal significant and interesting items on e-commence websites, by estimating unknown rating which the user may rate the unrated item, i.e., rating prediction. However, the existing approaches are unable to perform user prediction and item prediction, since they just treat the ratings as real numbers and learn nothing about the ratings' embeddings in the training process. In this paper, motivated by relation prediction in multi-relational graph, we propose a novel embedding model, namely RPEM, to solve the problem including the tasks of rating prediction, user prediction and item prediction simultaneously for recommendation systems, by learning the latent semantic representation of the users, items and ratings. In addition, we apply the proposed model to cross-domain recommendation, which is able to realize recommendation generation in multiple domains. Empirical comparison on several real datasets validates the effectiveness of the proposed model. The data is available at https://github.com/yuzhaour/da.\nTitle:", "model_inf_time": 1.41}, {"id": "4162", "output": "Lexical Features for Spanish Question Answering", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2004. Our approach is centered in the use of context at a lexical level in order to identify possible answers to factoid questions. Such method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. The methods applied at different stages of the system and prototype architecture for question answering are described. The paper shows and discusses the results achieved with this approach.\nTitle:\nThe Use of Lexical Context in Question Answering for Spanish\n\nAbstract:\nThis paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2004. Our approach is centered on the use of context at a lexical level in order to identify possible answers to factoid questions. This method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. We describe the methods applied at different stages of the system and our prototype architecture for question answering. The paper shows and discusses the results we achieved with this approach.\nTitle:\nQuestion answering for spanish supported by lexical context annotation\n\nAbstract:\nThis paper describes the system developed by the Language Technologies Lab at INAOE for the Spanish Question Answering task at CLEF 2006. The presented system is centered in a full data- driven architecture that uses machine learning and text mining techniques to identify the most probable answers to factoid and definition questions respectively. Its major quality is that it mainly relies on the use of lexical information and avoids applying any complex language processing re- source such as named entity classifiers, parsers or ontologies. Experimental results show that the proposed architecture can be a practical solution for monolingual question answering reaching an answer precision as high as 51%.\nTitle:\nINAOE at CLEF 2006: Experiments in Spanish Question Answering.\n\nAbstract:\nQuestion Answering has become a promising research field whose aim is to provide more natural access to the information than traditional document retrieval techniques. In this work, an approach centered in the use of context at a lexical level has been followed in order to identify possible answers to short factoid questions stated by the user in natural language. The methods applied at different stages of the system as well as an architecture for question answering are described. The evaluation of this approach was made following QA@CLEF03 criteria on a corpus of over 200,000 news in Spanish. The paper shows and discusses the results achieved by the system.\nTitle:\nQuestion answering for Spanish based on lexical and context annotation\n\nAbstract:\nThis paper describes a QA system centered in a full data-driven architecture. It applies machine learning and text mining techniques to identify the most probable answers to factoid and definition questions respectively. Its major quality is that it mainly relies on the use of lexical information and avoids applying any complex language processing resources such as named entity classifiers, parsers and ontologies. Experimental results on the Spanish Question Answering task at CLEF 2006 show that the proposed architecture can be a practical solution for monolingual question answering by reaching a precision as high as 51%.\nTitle:\nUsing machine learning and text mining in question answering\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.202, mean value for all the writers is 8.648, which makes it 0.473 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper describes the prototype developed in the Language Technologies Laboratory at INAOE for the Spanish monolingual QA evaluation task at CLEF 2005. The proposed approach copes with the QA task according to the type of question to solve (factoid or definition). In order to identify possible answers to factoid questions, the system applies a methodology centered in the use of lexical features. On the other hand, the system is supported by a pattern recognition method in order to identify answers to definition questions. The paper shows the methods applied at different stages of the system, with special emphasis on those used for answering factoid questions. Then the results achieved with this approach are discussed.\nTitle:", "model_inf_time": 1.06}, {"id": "4163", "output": "NLEL-MAAT at CLEF-IP 2009", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis report presents the work carried out at NLE Lab for the QA@CLEF-2009 competition. We used the JIRS passage retrieval system, which is based on redundancy, with the assumption that it is possible to find the response to a question in a large enough document collection. The retrieved passages are ranked depending on the number, length and position of the question n-grams structures found in the passages. The best results were obtained in monolingual English, while the worst results were obtained for French. We suppose the difference is due to the question style that varies considerably from one language to another.\nTitle:\nNLEL-MAAT at CLEF-ResPubliQA.\n\nAbstract:\n\n This report presents the work carried out at NLE Lab for the QA@CLEF-2009 competition. We used the JIRS passage retrieval system, which is based on redundancy, with the assumption that it is possible to find the response to a\n question in a large enough document collection. The retrieved passages are ranked depending on the number, length and position\n of the question n-gram structures found in the passages. The best results were obtained in monolingual English, while the worst results were\n obtained for French. We suppose the difference is due to the question style that varies considerably from one language to\n another.\n \n \nTitle:\nNLEL-MAAT at ResPubliQA\n\nAbstract:\nIn this paper we describe the participation of the Universidad Polit\u00e9cnica of Valencia to the 2006 edition, which was focused on the comparison between a Passage Retrieval engine (JIRS) specifically aimed to the Question Answering task and a standard, general use search engine such as Lucene. JIRS is based on n-grams, Lucene on keywords. We participated in three monolingual tasks: Spanish, Italian and French. The obtained results show that JIRS is able to return high quality passages, especially in Spanish.\nTitle:\nN-gram vs. keyword-based passage retrieval for question answering\n\nAbstract:\nThis report describes the work done by the RFIA group at the Departamento de Sistemas Informaticos y Computacion of the Universidad Politecnica of Valencia for the 2006 edition of the CLEF Question Answering task. We participated in three monolingual tasks: Spanish, Italian and French. The system used is a slightly revised version of the one we developed for the past year. The most interesting aspect of the work is the comparison between a Passage Retrieval engine (JIRS) specifically aimed to the Question Answering task and a standard, general use search engine such as Lucene. Results show that JIRS is able to return high quality passages.\nTitle:\nThe UPV at QA@CLEF 2007.\n\nAbstract:\nThis paper presents a simple approach to the Wikipedia Question Answering pilot task in CLEF 2006. The approach ranks the snippets, retrieved using the Lucene search engine, by means of a similarity measure based on bags of words extracted from both the snippets and the articles in wikipedia. Our participation was in the monolingual English and Spanish tasks. We obtained the best results in the Spanish one.\nTitle:\nA bag-of-words based ranking method for the wikipedia question answering task\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.874, mean value for all the writers is 8.648, which makes it 0.193 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis report presents the work carried out at NLE Lab for the CLEF-IP 2009 competition. We adapted the JIRS passage retrieval system for this task, with the objective to exploit the stylistic characteristics of the patents. Since JIRS was developed for the Question Answering task and this is the first time its model was used to compare entire documents, we had to carry out some transformations on the patent documents. The obtained results are not good and show that the modifications adopted in order to use JIRS represented a wrong choice, compromising the performance of the retrieval system.\nTitle:", "model_inf_time": 1.49}, {"id": "4164", "output": "Lexical Context and Pattern Recognition for Spanish Question Answering", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2004. Our approach is centered in the use of context at a lexical level in order to identify possible answers to factoid questions. Such method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. The methods applied at different stages of the system and prototype architecture for question answering are described. The paper shows and discusses the results achieved with this approach.\nTitle:\nThe Use of Lexical Context in Question Answering for Spanish\n\nAbstract:\nThis paper describes the prototype developed in the Language Technologies Laboratory at INAOE for the Spanish monolingual QA evaluation task at CLEF 2005. The proposed approach copes with the QA task according to the type of question to solve (factoid or definition). In order to identify possible answers to factoid questions, the system applies a methodology centered in the use of lexical features. On the other hand, the system is supported by a pattern recognition method in order to identify answers to definition questions. The paper shows the methods applied at different stages of the system, with special emphasis on those used for answering factoid questions. Then the results achieved with this approach are discussed.\nTitle:\nThe role of lexical features in question answering for spanish\n\nAbstract:\nQuestion Answering has become a promising research field whose aim is to provide more natural access to the information than traditional document retrieval techniques. In this work, an approach centered in the use of context at a lexical level has been followed in order to identify possible answers to short factoid questions stated by the user in natural language. The methods applied at different stages of the system as well as an architecture for question answering are described. The evaluation of this approach was made following QA@CLEF03 criteria on a corpus of over 200,000 news in Spanish. The paper shows and discusses the results achieved by the system.\nTitle:\nQuestion answering for Spanish based on lexical and context annotation\n\nAbstract:\nThis paper describes the system developed by the Language Technologies Lab at INAOE for the Spanish Question Answering task at CLEF 2006. The presented system is centered in a full data- driven architecture that uses machine learning and text mining techniques to identify the most probable answers to factoid and definition questions respectively. Its major quality is that it mainly relies on the use of lexical information and avoids applying any complex language processing re- source such as named entity classifiers, parsers or ontologies. Experimental results show that the proposed architecture can be a practical solution for monolingual question answering reaching an answer precision as high as 51%.\nTitle:\nINAOE at CLEF 2006: Experiments in Spanish Question Answering.\n\nAbstract:\nThis paper describes the system developed by the Language Technologies Lab of INAOE for the Spanish Question Answering task at CLEF 2007. The presented system is centered in a full datadriven architecture that uses information retrieval and machine learning techniques to identify the most probable answers to definition and factoid questions respectively. The major quality of our system is that it mainly relies on the use of lexical information and avoids applying any complex language processing resource such as POS taggers, named entity classifiers, parsers or ontologies. Experimental results indicate that our approach is very effective for answering definition questions from Wikipedia. In contrast, they also reveal that it is very difficult to respond factual questions from this resource solely based on the use of lexical overlaps and redundancy.\nTitle:\nINAOE's Participation at QA@CLEF 2007.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.231, mean value for all the writers is 8.648, which makes it 0.497 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2004. Our approach is centered on the use of context at a lexical level in order to identify possible answers to factoid questions. This method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. We describe the methods applied at different stages of the system and our prototype architecture for question answering. The paper shows and discusses the results we achieved with this approach.\nTitle:", "model_inf_time": 1.24}, {"id": "4165", "output": "Achievable Rates for K-User Gaussian Interference Channels Using Lattice and Algebraic Codes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe paper studies a class of three user Gaussian interference channels. A new layered lattice coding scheme is introduced as a transmission strategy. The use of lattice codes allows for an \"alignment\" of the interference observed at each receiver. The layered lattice coding is shown to achieve more than one degree of freedom for a class of interference channels and also achieves rates which are better than the rates obtained using the Han-Kobayashi coding scheme.\nTitle:\nA Layered Lattice Coding Scheme for a Class of Three User Gaussian Interference Channels\n\nAbstract:\nThis paper shows that structured transmission schemes are a good choice for secret communication over interference networks with an eavesdropper. Structured transmission is shown to exploit channel asymmetries and thus perform better than randomly generated codebooks for such channels. For a class of interference channels, we show that an equivocation sum-rate that is within two bits of the maximum possible legitimate communication sum-rate is achievable using lattice codes.\nTitle:\nOn the secrecy rate of interference networks using structured codes\n\nAbstract:\nThis paper considers the problem of transmitting linear functions of two correlated Gaussian sources over a two-user additive Gaussian noise multiple access channel. The goal is to recover this linear function within an average mean squared error distortion criterion. Each transmitter has access to only one of the two Gaussian sources and is limited by an average power constraint. In this paper, a lattice coding scheme and two lower bounds on the achievable distortion are presented. The lattice scheme achieves within a constant of a distortion lower bound if the signal-to-noise ratio is greater than a threshold. Furthermore, for the difference of correlated Gaussian sources, uncoded transmission is shown to be worse in performance to lattice coding methods for correlation coefficients above a threshold.\nTitle:\nCommunicating Linear Functions of Correlated Gaussian Sources Over a MAC\n\nAbstract:\nThis paper considers the problem of transmitting the difference of two positively correlated Gaussian sources over a two-user additive Gaussian noise multiple access channel (MAC). The goal is to recover this difference within an average mean squared error distortion criterion. Each transmitter has access to only one of the two Gaussian sources and is limited by an average power constraint. In this work, a lattice coding scheme that achieves a distortion within a constant of a distortion lower bound is presented if the signal to noise ratio (SNR) is greater than a threshold. Further, uncoded transmission is shown to be worse in performance to lattice coding methods for correlation coefficients above a threshold. An alternative lattice coding scheme is also presented that can potentially improve on the performance of uncoded transmission.\nTitle:\nCommunicating the Difference of Correlated Gaussian Sources over a MAC\n\nAbstract:\nThis paper is motivated by a sensor network on a correlated field where nearby sensors share information, an d can thus assist rather than interfere with one another. A special class of two-user Gaussian interference channels (IFCs) is considered where one of the two transmitters knows both the messages to be conveyed to the two receivers (called the IFC with degraded message sets). Both achievability and converse arguments are provided for this scenario for a class of discrete memoryless channels with weak interference. For the case of the Gaussian weak interference channel with degraded message sets, optimality of Gaussian inputs is also shown, resulting in the capacity region of this channel.\nTitle:\nOn the Capacity of Interference Channels with Degraded Message sets\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.838, mean value for all the writers is 8.648, which makes it 0.162 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe aim of this paper is to study the achievable rates for a $K$ user Gaussian interference channels for any SNR using a combination of lattice and algebraic codes. Lattice codes are first used to transform the Gaussian interference channel (G-IFC) into a discrete input-output noiseless channel, and subsequently algebraic codes are developed to achieve good rates over this new alphabet. In this context, a quantity called efficiency is introduced which reflects the effectiveness of the algebraic coding strategy. The paper first addresses the problem of finding high efficiency algebraic codes. A combination of these codes with Construction-A lattices is then used to achieve non trivial rates for the original Gaussian interference channel.\nTitle:", "model_inf_time": 1.44}, {"id": "4166", "output": "Using Temporal Coherence for Task-Specific Focus of Attention in Autonomous Driving", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nFTP: CMU-CS-95-143.ps In many real-world tasks, the ability to focus attention on the important features of the input is crucial for good performance. In this paper a mechanism for achieving task-specific focus of attention is presented. A saliency map, which is based upon a computed expectation of the contents of the inputs at the next time step, indicates which regions of the input retina are important for performing the task. The saliency map can be used to accentuate the features which are important, and de-emphasize those which are not. The performance of this method is demonstrated on a real-world robotics task: autonomous road following. The applicability of this method is also demonstrated in a non-visual domain. Architectural and algorithmic details are provided, as well as empirical results.\nTitle:\nUsing the Representation in a Neural Network''s Hidden Layer for Task-Specific Focus of Attention\n\nAbstract:\nIn many real world tasks, only a small fraction of the available inputs are important at any particular time. This paper presents a method for ascertaining the relevance of inputs by exploiting temporal coherence and predictability. The method pro- posed in this paper dynamically allocates relevance to inputs by using expectations of their future values. As a model of the task is learned, the model is simulta- neously extended to create task-specific predictions of the future values of inputs. Inputs which are either not relevant, and therefore not accounted for in the model, or those which contain noise, will not be predicted accurately. These inputs can be de-emphasized, and, in turn, a new, improved, model of the task created. The tech- niques presented in this paper have yielded significant improvements for the vision-based autonomous control of a land vehicle, vision-based hand tracking in cluttered scenes, and the detection of faults in the etching of semiconductor wafers.\nTitle:\nUsing expectation to guide processing: a study of three real-world applications\n\nAbstract:\nIn many vision based tasks, the ability to focus attention on the important portions of a scene is crucial for good performance on the tasks. In this paper we present a simple method of achieving spatial selective attention through the use of a saliency map. The saliency map indicates which regions of the input retina are important for performing the task. The saliency map is cre- ated through predictive autoencoding. The performance of this method is demonstrated on two simple tasks which have multiple very strong distract- ing features in the input retina. Architectural extensions and application directions for this model are presented. 1 MOTIVATION Many real world tasks have the property that only a small fraction of the available input is important at any particular time. On some tasks this extra input can easily be ignored. Nonetheless, often the similarity between the important input features and the irrelevant features is great enough to interfere with task performance. 'ho examples of this phenom- ena are the famous \"cocktail party effect\", otherwise known as speech recognition in a noisy environment, and image processing of a cluttered scene. In both cases, the extrane- ous information in the input signal can be easily confused with the important features, making the task much more difficult. The concrete real world task which motivates this work is vision-based road following. In this domain, the goal is to control a robot vehicle by analyzing the scene ahead, and choos- ing a direction to travel based on the location of important features like lane marking and road edges. This is a difficult task, since the scene ahead is often cluttered with extraneous features such as other vehicle, pedestrians, trees, guardrails, crosswalks, road signs and many other objects that can appear on or around a roadway. While we have had signifi- cant success on the road following task using simple feed-forward neural networks to transform images of the road ahead into steering commands for the vehicle (Pomerleau, 1993b), these methods fail when presented with cluttered environments like those encoun- 1. For the general task of autonomous navigation, these extra features are extremely important, but for restricted task of road following, which is the focus of this paper, these features are merely distractions. Although we are addressing the more general task using the techniques described here in combination with other methods, a description of these efforts is beyond the scope of this paper.\nTitle:\nUsing a Saliency Map for Active Spatial Selective Attention: Implementation & Initial Results\n\nAbstract:\nAfter training statistical models to classify sets of data into predetermined classes, it is often difficult to interpret\n what the models have learned. This paper presents a novel approach for finding examples which lie on the decision boundaries\n of statistical models trained for classification. These examples provide insight into what the model has learned. Additionally,\n they can provide candidates for use as additional training data for improving the performance of the statistical models. By\n labeling the examples which lie on the decision boundaries, we provide information to the model in the regions in which it\n is most uncertain. The approaches presented in this paper are demonstrated on the real-world vision-based task of detecting\n faces in cluttered scenes.\n \nTitle:\nFinding Regions of Uncertainty in Learned Models: An Application to Face Detection\n\nAbstract:\nThis paper describes a simple and efficient method to make template-based object classification invariant to in-plane rotations. The task is divided into two parts: orientation discrimination and classification. The key idea is to perform the orientation discrimination before the classification. This can be accom- plished by hypothesizing, in turn, that the input image belongs to each class of interest. The image can then be rotated to maximize its similarity to the train- ing images in each class (these contain the prototype object in an upright orien- tation). This process yields a set of images, at least one of which will have the object in an upright position. The resulting images can then be classified by models which have been trained with only upright examples. This approach has been successfully applied to two real-world vision-based tasks: rotated handwritten digit recognition and rotated face detection in cluttered scenes.\nTitle:\nMaking templates rotationally invariant: an application to rotated digit recognition\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.857, mean value for all the writers is 8.648, which makes it 0.178 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nReliable vision-based control of an autonomous vehicle requires the ability to focus attention on the important features in an input scene. Previous work with an autonomous lane following system, ALVINN (Pomerleau, 1993), has yielded good results in uncluttered conditions. This paper presents an artificial neural network based learning approach for handling difficult scenes which will confuse the ALVINN system. This work presents a mechanism for achieving task-specific focus of attention by exploiting temporal coherence. A saliency map, which is based upon a computed expectation of the contents of the inputs in the next time step, indicates which regions of the input retina are important for performing the task. The saliency map can be used to accentuate the features which are important for the task, and de-emphasize those which are not.\nTitle:", "model_inf_time": 1.71}, {"id": "4167", "output": "Hereditary Harrop Logic for Security Protocol Specification", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we define a sequent calculus to formally specify, simulate, debug and verify security protocols. In our sequents we distinguish between the current knowledge of principals and the current global state of the session. Hereby, we can describe the operational semantics of principals and of an intruder in a simple and modular way. Furthermore, using proof theoretic tools like the analysis of permutability of rules, we are able to find efficient proof strategies that we prove complete for special classes of security protocols including Needham-Schroeder. Based on the results of this preliminary analysis, we have implemented a Prolog meta-interpreter which allows for rapid prototyping and for checking safety properties of security protocols, and we have applied it for finding error traces and proving correctness of practical examples.\nTitle:\nProof Theory, Transformations, and Logic Programming for Debugging Security Protocols\n\nAbstract:\nIn this paper we investigate the possible application of parameterized verification techniques to synchronization skeletons of multithreaded Java programs. As conceptual contribution, we identify a class of infinite-state abstract models, called Multi-Transfer Nets (MTNs), that preserve the main features of the semantics of concurrent Java. We achieve this goal by exploiting an interesting connection with the Broadcast Protocols of [7], and by introducing the notion of asynchronous rendez-vous. As technical contribution, we extend the symbolic verification techniques of [6] based on Covering Sharing Trees and structural invariants to MTNs. As practical contribution, we report on experimental results for verification of examples of multithreaded Java programs.\nTitle:\nTowards the Automated Verification of Multithreaded Java Programs\n\nAbstract:\nIn this paper we investigate the applicability of a bottom-up evaluation strategy for a first order fragment of linear logic [7] for the purposes of automated validation of authentication protocols. Following [11], we use multi-conclusion clauses to represent the behaviour of agents in a protocol session, and we adopt the Dolev-Yao intruder model and related message and cryptographic assumptions. Also, we use universal quantification to provide a logical and clean way to express creation of nonces. Our approach is well suited to verify properties which can be specified by means of minimality conditions. Unlike traditional approaches based on model-checking, we can reason about parametric, infinite-state systems, thus we do not pose any limitation on the number of parallel runs of a given protocol. Furthermore, our approach can be used both to find attacks and to prove correctness of protocols. We present some preliminary experiments which we have carried out using the above approach. In particular, we analyze the ffgg protocol introduced by Millen [30]. This protocol is a challenging case study in that it is free from sequential attacks, whereas it suffers from parallel attacks that occur only when at least two sessions are run in parallel.\nTitle:\nAutomated protocol verification in linear logic\n\nAbstract:\nIn this paper we investigate the applicability of a bottom-up evaluation strategy for a first-order fragment of affine linear logic that we introduced in Theory Prac.\u00a0Log.\u00a0Program.\u00a04 (2004) 1 for the purposes of automated verification of secrecy in cryptographic protocols. Following the Proceedings of the 12th Computer Security Foundations Workshop (1999) 55, we use multi-conclusion clauses to represent the behaviour of agents in a protocol session, and we adopt the Dolev\u2013Yao intruder model. In addition, universal quantification provides a formal and declarative way to express creation of nonces. Our approach is well suited to verifying properties which can be specified by means of minimal conditions. Unlike traditional approaches based on model checking, we can reason about parametric, infinite-state systems; thus we do not pose any limitation on the number of parallel runs of a protocol. Furthermore, our approach can be used both to find attacks and to verify secrecy for a protocol. We apply our method to analyse several classical examples of authentication protocols. Among them we consider the ffgg protocol (Proceedings of the Workshop on Formal Methods and Security Protocols (1999)). This protocol is a challenging case study in that it is free from sequential attacks, whereas it suffers from parallel attacks that occur only when at least two sessions are run in parallel. The other case studies are of the Otway\u2013Rees protocol and several formulations of the Needham\u2013Schroeder protocol.\nTitle:\nAutomatic verification of secrecy properties for linear logic specifications of cryptographic protocols\n\nAbstract:\nWe investigate the applicability of symbolic exploration to the automatic! verification of secrecy and authentication properties for time sensitive cryptographic protocols. Our formal specifications are given in multiset rewriting over first order atomic formulas enriched with constraints so as to uniformly model fresh name generation and validity condition of time stamps. Our verification approach is based on data structures for symbolically representing sets of configurations of an arbitrary number of parallel protocol sessions. As a case study we discuss the verification of timed authentication for the Wide Mouth Frog protocol.\nTitle:\nAutomatic Verification of Time Sensitive Cryptographic Protocols\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.136, mean value for all the writers is 8.648, which makes it 0.437 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe investigate the fragment of intuitionistic logic consisting of hereditary Harrop formulas [MNPS91] as a specification language for security protocols. In this setting, embedded implications and universal quantification provide a natural built-in mechanism to model the dynamics in the knowledge of the agents involved in a protocol. We take advantage of the system \u03bbProlog [NM88,NM99] in order to turn specifications in hereditary Harrop formulas into executable prototypes, ready to be debugged. To exploit these features, we select as main case-study the well-known Needham-Schroeder protocol [NS78]. In this paper we report on the results of our experiments and we discuss potentially interesting directions of future research.\nTitle:", "model_inf_time": 1.32}, {"id": "4168", "output": "Iterative OMP for Compressive Sensing with Reduced Measurement Requirements", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nFusion based Compressive Sensing (CS) reconstruction algorithms combine multiple CS reconstruction algorithms, which worked with different principles, to obtain a better signal estimate. Examples include Fusion of Algorithms for Compressed Sensing (FACS) and Committee Machine Approach for Compressed Sensing (CoMACS). However, these algorithms involve solving a least squares problem which may be ill-conditioned. Modified CS algorithms such as Modified Basis Pursuit (Mod-BP) ensured a sparse signal can efficiently be reconstructed when a part of its support is known. Since Mod-BP makes use of available signal knowledge to improve upon BP, we propose to employ multiple Greedy Pursuits (GPs) to derive a partial support for Mod-BP. As Mod-BP makes use of signal knowledge derived using GPs, we term our proposed algorithm as Greedy Pursuits Assisted Basis Pursuit (GPABP). Experimental results show that our proposed algorithm performs better than the state-of-the-art algorithms - FACS and its variants.\nTitle:\nGreedy pursuits assisted basis pursuit for compressive sensing\n\nAbstract:\nIn distributed compressive sensing, if one signal in a joint-sparse signal ensemble is known apriori, the remaining signals can be reconstructed using modified Compressive Sensing (CS) algorithms such as Modified Basis Pursuit (Mod-BP) which makes use of Partially Known Support (PKS). Though Mod-BP reconstructs the joint-sparse signals with high accuracy, it takes a huge amount of time to converge. This might not be desirable in some practical applications like CS reconstruction of video frames. Carrillo et al have illustrated the use of PKS in iterative greedy algorithms to improve the recovery performance at a much shorter time. However, PKS based iterative greedy algorithms are totally blind about the wrong atoms present in the PKS, which is likely for video frames. To overcome this, we propose Adaptive Backtracking Matching Pursuit (AdBMP) which makes effective use of the PKS to reconstruct the sparse signal. Experimental results show that AdBMP gives a better reconstruction accuracy compared to that of the existing PKS based iterative greedy algorithms.\nTitle:\nRecovery Of Correlated Sparse Signals Using Adaptive Backtracking Matching Pursuit\n\nAbstract:\nThis letter presents a variant of Orthogonal Matching Pursuit (OMP) method, called Backtracking-based Adaptive OMP (BAOMP), for compressive sensing and sparse signal reconstruction. As an extension of the OMP algorithm, the BAOMP method incorporates a simple backtracking technique to detect the previous chosen atoms&#39; reliability and then deletes the unreliable atoms at each iteration. Through this...\nTitle:\nBacktracking-Based Matching Pursuit Method for Sparse Signal Reconstruction.\n\nAbstract:\nThe recently emerged theory of compressive sensing (CS) has a remarkable result that signals having sparse representations in some known basis can be represented (with high probability) by taking a few random projection measurements of the signals. In this paper, we study some CS sparse reconstruction methods and propose a video object error coding method based on CS theory. The proposed system first assumes the moving objects have been segmented from background image and object-based motion compensated from the previous reconstruction frame, and then the resulting object error is encoded by using CS random matrix projection. Finally the coded measurements can be quantized to store or transmit. Experimental results demonstrate the object error blocks can be effectively recovered by using CS sparse reconstruction algorithms. This proposed method would be widely used in the object-based video compression fields.\nTitle:\nVideo object error coding method based on compressive sensing\n\nAbstract:\n\u2022An algorithm for reconstructing innovative joint-sparse signal ensemble is proposed.\u2022The algorithm utilizes multiple greedy pursuits and modified basis pursuit.\u2022The algorithm is robust to the innovation components in the joint-sparse signals.\u2022The algorithm remains stable for large-sized signal ensembles.\nTitle:\nGreedy Pursuits Assisted Basis Pursuit for reconstruction of joint-sparse signals.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.17, mean value for all the writers is 8.648, which makes it 1.299 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOrthogonal Matching Pursuit (OMP) and Basis Pursuit (BP) are two well-known recovery algorithms in compressed sensing. To recover a -dimensional -sparse signal with high probability, OMP needs number of measurements, whereas BP needs only number of measurements. In contrary, OMP is a practically more appealing algorithm due to its superior execution speed. In this piece of work, we have proposed a scheme that brings the required number of measurements for OMP closer to BP. We have termed this scheme as , which runs OMP for -iterations instead of -iterations, by choosing a value of . It is shown that guarantees a high probability signal recovery with number of measurements. Another limitation of OMP unlike BP is that it requires the knowledge of . In order to overcome this limitation, we have extended the idea of to illustrate another recovery scheme called , which runs OMP until th- signal residue vanishes. It is shown that can achieve a close to -norm recovery without any knowledge of like BP.\nTitle:", "model_inf_time": 1.44}, {"id": "4169", "output": "A Real-Time Landmine Detection System Using Ground Penetrating Radar", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nGround penetrating radar (GPR) is a widely used sensor for land mine detection. However, GPR signal return is very susceptible to ground bounce and reflection of clutter objects, which makes the detection a difficult problem to date. In this paper, we propose to utilize two-sided linear prediction (LP) to model the background interference and then employ the residue energy to generate the test statistic. It is demonstrated from real GPR data that the proposed scheme is able to significantly suppress the interference due to ground reflection and is superior to the adaptive ground bounce removal and one-sided LP methods.\nTitle:\nGeneralized two-sided linear prediction approach for land mine detection\n\nAbstract:\nGround penetrating radar (GPR) is a widely used tool for land mine detection. However, land mine detection still remains a difficult task because of the changing conditions and the strong reflection of the ground. In this paper, a generalized two-sided linear prediction model is used to estimate the background. By effectively removing the background components from the GPR signal, the residual energy is found to be more reliable to generate the test statistic for detection. Results based on real GPR data show that the proposed method is able to not only remove the ground bounce and background signal but also suppress the response from some clutter objects.\nTitle:\nA study on two-sided linear prediction approach for land mine detection\n\nAbstract:\nLandmine detection is an important and yet challenging problem remains to be solved. Ground penetrating radar (GPR) is an effective sensor to detect landmines that are made of plastic or have low metal content. Most GPR signal processing algorithms apply processing in the time (depth) domain. This paper proposes to use the frequency domain features from the GPR signal to improve the detection of weak plastic mines and to reduce the number false alarms due to clutter objects. The motivation comes from the fact that the energy density spectrum may be different between mine targets and clutter objects, although both may have strong GPR signal return in the time domain. Experimental results based on clutter lane data collected at a test site corroborate the effectiveness of the proposed spectral features to increase the accuracy for landmine detection.\nTitle:\nImproving Landmine Detection Using Frequency Domain Features From Ground Penetrating Radar\n\nAbstract:\nGround penetrating radar (GPR)-based discrimination of landmines from clutter is known to be challenging due to the wide variability of possible clutter (e.g., rocks, roots, and general soil heterogeneity). This paper discusses the use of GPR frequency-domain spectral features to improve the detection of weak-scattering plastic mines and to reduce the number of false alarms resulting from clutter. The motivation for this approach comes from the fact that landmine targets and clutter objects often have different shapes and/or composition, yielding different energy density spectrum (EDS) that may be exploited for their discrimination (this information is also present in time-domain data, but in the frequency domain we can remove a phase if desired and can reveal better spatial characteristics and therefore often achieve greater robustness). This paper first applies the finite-difference time-domain (FDTD) modeling technique to establish the theoretical foundation. The method to generate EDS from GPR measurements is then described. The consistency of the frequency-domain features is examined through two different GPRs that have different spatial sampling rates and frequency bandwidths. Experimental results from several test sites, based on GPR data collected over buried mines and emplaced buried clutter objects, corroborate the theoretical development and the effectiveness of the proposed spectral feature to increase the accuracy of landmine detection and discrimination.\nTitle:\nAn Investigation of Using the Spectral Characteristics From Ground Penetrating Radar for Landmine/Clutter Discrimination\n\nAbstract:\nThe Matching Pursuits Dissimilarity Measure (MPDM) is an effective way to to compare signals that are sparsely approximated using a Matching Pursuits method. The CAMP algorithm uses an MPDM distance measure in Competitive Agglomeration clustering to model and classify signals. The MPDM approach can only compare signals originating from a single source. Many landmine detection systems use multiple sensors to make simultaneous measurements of the same region of interest. In this paper we propose a Multimodal MPDM that can be used with CAMP to fuse signals from multiple sensors. We demonstrate the effectiveness of the Multimodal MPDM over the single sensor MPDM in improving discrimination of landmines from clutter objects.\nTitle:\nA Multimodal Matching Pursuits Dissimilarity Measure Applied To Landmine/Clutter Discrimination\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.788, mean value for all the writers is 8.648, which makes it 1.826 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose a real-time software system for landmine detection using ground-penetrating radar (GPR). The system includes an efficient and adaptive preprocessing component; a hidden Markov model-(HMM-) based detector; a corrective training component; and an incremental update of the background model. The preprocessing is based on frequency-domain processing and performs ground-level alignment and background removal. The HMM detector is an improvement of a previously proposed system (baseline). It includes additional pre-and postprocessing steps to improve the time efficiency and enable real-time application. The corrective training component is used to adjust the initial model parameters to minimize the number of misclassification sequences. This component could be used offline, or online through feedback to adapt an initial model to specific sites and environments. The background update component adjusts the parameters of the background model to adapt it to each lane during testing. The proposed software system is applied to data acquired from three outdoor test sites at different geographic locations, using a state-of-the-art array GPR prototype. The first collection was used as training, and the other two (contain data from more than 1200 m2 of simulated dirt and gravel roads) for testing. Our results indicate that, on average, the corrective training can improve the performance by about 10% for each site. For individual lanes, the performance gain can reach 50%.\nTitle:", "model_inf_time": 1.62}, {"id": "4170", "output": "Bilateral Kernel-based Region Detector for Stable Region Detection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present a method to enhance noisy depth maps using adaptive steering kernel regression based on distance transform. Data-adaptive kernel regression filters are widely used for image denoising by considering spatial and photometric properties of pixel data. In order to reduce noise in depth maps more efficiently, we adaptively refine the steering kernel regression function according to local region structures, flat and textured areas. In this work, we first generate two distance transform maps from the depth map and its corresponding color image. Then, the steering kernel is modified by a newly-designed weighing function directly related to joint distance transform. The weighting function expands the steering kernel in flat areas and shrinks it in textured areas toward local edges in the depth map. Finally, we filter the noise in the depth map with the refined steering kernel regression function. Experimental results show that our method outperforms the competing methods in objective and subjective comparisons for depth map enhancement.\nTitle:\nDepth map enhancement using adaptive steering Kernel regression based on distance transform\n\nAbstract:\nIn comparison with 2D face images, 3D face models have the advantage of being illumination and pose invariant, which provides improved capability of handling changing environments in practical surveillance. Feature detection, as the initial process of reconstructing 3D face models from 2D uncalibrated image sequences, plays an important role and directly affects the accuracy and robustness of the resulting reconstruction. In this paper, we propose an automated scene-specific selection algorithm that adaptively chooses an optimal feature detector according to the input image sequence for the purpose of 3D face reconstruction. We compare the performance of various feature detectors in terms of accuracy and robustness of the sparse and dense reconstructions. Our experimental results demonstrate the effectiveness of the proposed selection method from the observation that the chosen feature detector produces 3D reconstructed face models with superior accuracy and robustness to image noise.\nTitle:\nAutomated scene-specific selection of feature detectors for 3D face reconstruction\n\nAbstract:\nIn this paper, we present a new method to enhance depth images captured by a time-of-flight (TOF) depth sensor spatially and temporally. In practice, depth images obtained from TOF depth sensors have critical problems, such as optical noise existence, unmatched boundaries, and temporal inconsistency. In this work, we improve depth quality by performing a newly-designed joint bilateral filtering, color segmentation-based boundary refinement, and motion estimation-based temporal consistency. Experimental results show that the proposed method significantly minimizes the inherent problems of the depth images so that we can use them to generate a dynamic and realistic 3D scene.\nTitle:\nSpatial and Temporal Enhancement of Depth Images Captured by a Time-of-Flight Depth Sensor\n\nAbstract:\nHere, a versatile data-driven application independent method to extend the depth of field is presented. The principal contribution in this effort is the use of features extracted by Empirical Mode Decomposition, namely Intrinsic Mode Images, for fusion. The input images are decomposed into intrinsic mode images and fusion is performed on the extracted oscillatory modes, by means of weighing schemes that allow emphasis of focused regions in each input image. The fused image unifies information from all focal planes, while maintaining the verisimilitude of the scene. In order to validate the fusion performance of our method, we have compared our results with those of region-based and multiscale decomposition based fusion techniques. Several illustrative examples and objective comparisons are provided.\nTitle:\nExtending Depth Of Field By Intrinsic Mode Image Fusion\n\nAbstract:\nMultifocus fusion is the process of unifying focal information from a set of input images acquired with limited depth of field. In this effort, we present a general purpose multifocus fusion algorithm, which can be applied to varied applications ranging from microscopic to long range scenes. The main contribution in this paper is the segmentation of the input images into partitions based on focal connectivity. Focal connectivity is established by isolating regions in an input image that fall on the same focal plane. Our method uses focal connectivity and does not rely on physical properties like edges directly for segmentation. Our method establishes sharpness maps to the input images, which are used to isolate and attribute image partitions to input images. The partitions are mosaiced seamlessly to form the fused image. Illustrative examples of multifocus fusion using our method are shown. Comparisons against existing methods are made and the results are discussed.\nTitle:\nMultifocus Image Fusion by Establishing Focal Connectivity\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.157, mean value for all the writers is 8.648, which makes it 1.287 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we present a new method for a locally adaptive region detector called Bilateral kernel-based Region Detector (BIRD). This work is to detect stable regions from images by consecutively computing a multiscale decomposition based on the bilateral kernel. The BIRD regards a region as covariant if it exhibits predictability in its photometric distance over spatial distance. Distinctiveness and robustness across scales are achieved by selecting the extremely stable regions through sequential scales. Our method is simple and easy to implement. Experimental results show that our method outperforms competing affine region detection methods in efficiency on region detection.\nTitle:", "model_inf_time": 1.4}, {"id": "4171", "output": "Real-Time Object Tracking and Obstacle Avoidance Using a Mobile Robot with Multiple Sensors", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we present a vision system that performs tracking a human face in 3D. To achieve this we combine color and stereo cues to find likely image regions where face may exist. A greedy search algorithm examines for a face candidate focusing the ...\nTitle:\nAutomatic Face Region Tracking for Highly Accurate Face Recognition in Unconstrained Environments\n\nAbstract:\nThis paper presents a feature point tracking algorithm using optical flow under the non-prior training active feature model (NPT-AFM) framework. The proposed algorithm mainly focuses on analysis of deformable objects, and provides real-time, robust tracking. The proposed object tracking procedure can be divided into two steps: (i) optical flow-based tracking of feature points and (ii) NPT-AFM for robust tracking. In order to handle occlusion problems in object tracking. feature points inside an object are estimated instead of its shape boundary of the conventional active contour model (ACM) or active shape model (ASM), and are updated as an element of the training set for the AFM. The proposed NPT-AFM framework enables the tracking of occluded objects in complicated background. Experimental results show, that the proposed NPT-AFM-based algorithm can track deformable objects in real-time.\nTitle:\nUsing a Non-prior Training Active Feature Model\n\nAbstract:\nThe feasibility of realistic autonomous space manipulation tasks using multisensory information is shown through two experiments involving a fluid interchange system and a module interchange system. In both cases, autonomous location of the mating element, autonomous location of the guiding light target, mating, and demating of the system are performed. Specifically, vision-driven techniques were implemented that determine the arbitrary two-dimensional position and orientation of the mating elements as well as the arbitrary three-dimensional position and orientation of the light targets. The robotic system is also equipped with a force/torque sensor that continuously monitors the six components of force and torque exerted on the end effector. Using vision, force, torque, proximity, and touch sensors, the fluid interchange system and the module interchange system experiments were accomplished autonomously and successfully\nTitle:\nThe use of multisensor data for robotic applications\n\nAbstract:\nIn size preserving video tracking, the camera\u0092s focal length (zoom) is adjusted automatically to compensate for the changes in the target\u0092s image size caused by the relative motion between the camera and the target. The accurate estimation of these changes is paramount to the system performance. The existing method of choice for real-time target scale estimation applies structure from motion (SFM) based on the weak perspective projection model [1]. We design a target scale estimation algorithm with linear solution based on the more advanced paraperspective projection model. Another key problem in SFM based algorithms is the separation between foreground and background features (image corners), especially when composite camera and target motions are involved. This paper also addresses a fast foreground/background separation algorithm, the affine shape method. The resulting segmentation automatically adapts to the target\u0092s 3D geometry and motion. Experimental results illustrate the effectiveness of the proposed scale estimation and segmentation algorithms in tracking translating and rotating objects with a PTZ camera while preserving their sizes.\nTitle:\n3D Target Scale Estimation and Motion Segmentation for Size Preserving Tracking in PTZ Video\n\nAbstract:\nA general approach is presented for the integration of vision, range, proximity, and touch sensory data to derive a better estimate of the position and orientation (pose) of an object appearing in the work space. Efficient and robust methods for analyzing vision and range data to derive an interpretation of input images are discussed. Vision information analysis includes a model-based object recognition module and an image-to-world coordinate transformation module to identify the three-dimensional (3-D) coordinates of the recognized objects. The range information processing includes modules for reprocessing, segmentation, and 3-D primitive extraction. The multisensory information integration approach represents sensory information in a sensor-independent form and formulates an optimization problem to find a minimum-error solution to the problem. The capabilities of a multisensor robotic system are demonstrated by performing a number of experiments using an industrial robot equipped with several sensors of differing types\nTitle:\nDeveloping robotic systems with multiple sensors\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.22, mean value for all the writers is 8.648, which makes it 1.341 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper describes a robotic application that tracks a moving object by utilizing a mobile robot with multiple sensors. The robotic platform uses a visual camera to sense the movement of the desired object and a range sensor to help the robot detect and then avoid obstacles in real time while continuing to track and follow the desired object. In terms of real-time obstacle avoidance capacity, this paper also presents a modified potential field algorithm called Dynamic Goal Potential Field algorithm (DGPF) for this robotic application specifically. Experimental results show that the robotic and intelligent system can fulfill the requirements of tracking an object and avoiding obstacles simultaneously when the object is moving.\nTitle:", "model_inf_time": 1.59}, {"id": "4172", "output": "Constructive Solutions for Nonlinear Stabilization of Saturated Linear Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe paper revisits the local exponential stabilization and global asymptotic stabilization problems of saturated linear systems using nonlinear control laws. The proposed approach takes advantage of a rational formulation of the control action in terms of a parameter sigma, which is the solution to an implicit equation depending on the state, scheduling the control law. Constructive solutions are then proposed by considering sum-of-squares formulation of the conditions, when imposing a polynomial dependence on parameter sigma.\nTitle:\nA Polynomial Approach To Nonlinear State Feedback Stabilization Of Saturated Linear Systems\n\nAbstract:\nIn this paper, we propose modifications in the generation control in power systems to improve the economic efficiency, stability and robustness of generator regulation in real time. We first present the state-space description of a conventional power network model describing system dynamics around a nominal operating condition. We then formulate an optimization problem describing system regulation under exogenous disturbances, following which the redesign methodology is presented. The optimality, stability and delay robustness of the redesigned dynamics are studied, where we also introduce controllable loads. The performance of the redesigned control scheme is illustrated by numerical examples.\nTitle:\nRedesigning generation control in power systems: Methodology, stability and delay robustness\n\nAbstract:\nThis paper presents a method for stability analysis of switched and hybrid systems using polynomial and piece-wise polynomial Lyapunov functions. Computation of such functions can be performed using convex optimization, based on the sum of squares decomposition of multi-variate polynomials. The analysis yields several improvements over previous methods and opens up new possibilities, including the possibility of treating nonlinear vector fields and/or switching surfaces and parametric robustness analysis in a unified way.\nTitle:\nAnalysis Of Switched And Hybrid Systems - Beyond Piecewise Quadratic Methods\n\nAbstract:\nIn this paper, we design a distributed dynamic feedback controller that implements real-time economic optimization in power networks with tree topology. We consider the coupling between the dynamics of the power network and the appropriately formulated optimization problem in the real-time market, i.e., when the time-scale separation between markets and network dynamics fades. The design methodology is motivated by optimization decomposition methods in that the controller is derived from a primal-dual decomposition approach. We then prove the asymptotic stability of the overall (closed-loop) system in a scalable fashion. The performance of the controller is illustrated by numerical investigations.\nTitle:\nDistributed dynamic feedback control for smart power networks with tree topology\n\nAbstract:\nThe region of attraction of a trim point for the pitch axis of a nonlinear modeled aircraft modulated via linear dynamic inversion based controller was determined numerically. The model incorporates uncertainty in the position of center of gravity along the X-body axis. The stability regions are computed using SOSTOOLS, which converts the required sum of squares conditions to an appropriate semi definite program that is then solved using SeDuMi.\nTitle:\nAnalysis of aircraft pitch axis stability augmentation system using sum of squares optimization\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.986, mean value for all the writers is 8.648, which makes it 1.142 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe technical note revisits the local exponential stabilization and global asymptotic stabilization problems of saturated linear systems using nonlinear control laws. The proposed nonlinear control law has rational dependence on a parameter , which is computed by solving an implicit equation depending on the state. Constructive solutions are obtained, based on a sum-of-squares formulation of the proposed conditions.\nTitle:", "model_inf_time": 1.15}, {"id": "4173", "output": "Augmenting 3D Ultrasound with Statistical Shape Models for Epidural Needle Guidance", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nEpidural needle insertions and facet joint injections play an important role in spine anaesthesia. The main challenge of safe needle insertion is the deep location of the target, resulting in a narrow and small insertion channel close to sensitive anatomy. Recent approaches utilizing ultrasound (US) as a low-cost and widely available guiding modality are promising but have yet to become routinely used in clinical practice due to the difficulty in interpreting US images, their limited view of the internal anatomy of the spine, and/or inclusion of cost-intensive tracking hardware which impacts the clinical workflow.We propose a novel guidance system for spine anaesthesia. An efficient implementation allows us to continuously align and overlay a statistical model of the lumbar spine on the live 3D US stream without making use of additional tracking hardware. The system is evaluated in vivo on 12 volunteers.The in vivo study showed that the anatomical features of the epidural space and the facet joints could be continuously located, at a volume rate of 0.5 Hz, within an accuracy of 3 and 7 mm, respectively.A novel guidance system for spine anaesthesia has been presented which augments a live 3D US stream with detailed anatomical information of the spine. Results from an in vivo study indicate that the proposed system has potential for assisting the physician in quickly finding the target structure and planning a safe insertion trajectory in the spine.\nTitle:\nTowards real-time, tracker-less 3D ultrasound guidance for spine anaesthesia\n\nAbstract:\nEpidural anesthesia is a common but challenging procedure in obstetrics and surgery, especially for the obese patient. An ultrasound guidance system is proposed using a transducer-mounted camera to create 3D panorama images of the spine relative to markings on the skin. Guidance will include identification of individual vertebrae, and selection of a suitable puncture site, trajectory and depth of needle insertion. This study describes the panorama creation and preliminary testing. The camera tracks the transducer movement using a specialized strip of markers attached to the skin surface, which enables absolute position estimation of the transducer with respect to the patient over the full range of the spine. The 3D panorama image can then be resliced in various parasagittal planes to show either the target epidural spaces or the laminae. The geometric accuracy of the panoramas are validated against an optical tracking system and independent measurements by a sonographer.\nTitle:\nPanorama ultrasound for guiding epidural anesthesia: a feasibility study\n\nAbstract:\nSpinal needle injections are widely applied to alleviate back pain and for anesthesia. Current treatment is performed either blindly with palpation or using fluoroscopy or computed tomography (CT). Both fluoroscopy and CT guidance expose patients to ionizing radiation. Ultrasound (US) guidance for spinal needle procedures is becoming more prevalent as an alternative. It is challenging to use US as the sole imaging modality for intraoperative guidance of spine needle injections due to the acoustic shadows created by the bony structures of the vertebra that limit visibility of the target areas for injection. We propose registration of CT and the US images to augment anatomical visualization for the clinician during spinal interventions guided by US.\nTitle:\nA multi-vertebrae CT to US registration of the lumbar spine in clinical data\n\nAbstract:\nUltrasound image guided needle insertion is the method of choice for a wide variety of medical diagnostic and therapeutic procedures. When flexible needles are inserted in soft tissue, these needles generally follow a curved path. Segmenting the trajectory of the needles in ultrasound images will facilitate guiding them within the tissue. In this paper, a novel algorithm for curved needle segmentation in three-dimensional (3D) ultrasound images is presented. The algorithm is based on the projection of a filtered 3D image onto a two-dimensional (2D) image. Detection of the needle in the resulting 2D image determines a surface on which the needle is located. The needle is then segmented on the surface. The proposed technique is able to detect needles without any previous assumption about the needle shape, or any a priori knowledge about the needle insertion axis line.\nTitle:\nA New Scheme for Curved Needle Segmentation in Three-Dimensional Ultrasound Images.\n\nAbstract:\nWe propose an augmented reality system to identify lumbar vertebral levels to assist in spinal needle insertion for epidural anesthesia. These procedures require careful placement of a needle to ensure effective delivery of anesthetics and to avoid damaging sensitive tissue such as nerves. In this system, a trinocular camera tracks an ultrasound transducer during the acquisition of a sequence of B-mode images. The system generates an ultrasound panorama image of the lumbar spine, automatically identifies the lumbar levels in the panorama image, and overlays the identified levels on a live camera view of the patient's back. Validation is performed to test the accuracy of panorama generation, lumbar level identification, overall system accuracy, and the effect of changes in the curvature of the spine during the examination. The results from 17 subjects demonstrate the feasibility and capability of achieving an error within clinically acceptable range for epidural anaesthesia.\nTitle:\nAn Augmented Reality System for Epidural Anesthesia (AREA): Prepuncture Identification of Vertebrae.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.294, mean value for all the writers is 8.648, which makes it 2.258 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe blind placement of an epidural needle is among the most difficult regional anesthetic techniques. The challenge is to insert the needle in the mid-sagittal plane and to avoid overshooting the needle into the spinal cord. Prepuncture 2D ultrasound scanning has been introduced as a reliable tool to localize the target and facilitate epidural needle placement. Ideally, real-time ultrasound should be used during needle insertion. However, several issues inhibit the use of standard 2D ultrasound, including the obstruction of the puncture site by the ultrasound probe, low visibility of the target in ultrasound images, and increased pain due to longer needle trajectory. An alternative is to use 3D ultrasound imaging, where the needle and target could be visible within the same reslice of a 3D volume; however, novice ultrasound users (i.e., many anesthesiologists) still have difficulty interpreting ultrasound images of the spine and identifying the target epidural space. In this paper, we propose to augment 3D ultrasound images by registering a multi-vertebrae statistical shape+pose model. We use such augmentation for enhanced interpretation of the ultrasound and identification of the mid-sagittal plane for the needle insertion. Validation is performed on synthetic data derived from the CT images, and 64 in vivo ultrasound volumes.\nTitle:", "model_inf_time": 1.75}, {"id": "4174", "output": "Unsupervised Multi-Document Summarization via Word Association Strength", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe goal of automatic text summarization is to generate an abstract of a document or a set of documents. In this paper we propose a word association based method for generating summaries in a variety of languages. We show that a robust statistical method for finding associations which are specific to the given document(s) is applicable to many languages. We introduce strategies that utilize the discovered associations to effectively select sentences from the document(s) to constitute the summary. Empirical results indicate that the method works reliably in a relatively large set of languages and outperforms methods reported in MultiLing 2013.\n\n\nTitle:\nLanguage-independent multi-document text summarization with document-specific word associations.\n\nAbstract:\nWe introduce a methodology for knowledge discovery in databases (KDD) where one first discovers large collections of patterns at once, and then performs interactively retrieves subsets of the collection of patterns. The proposed methodology suits such KDD formalisms as association and episode rules, where large collections of potentially interesting rules can be found efficiently.We present methods that support interactive exploration of large collections of rules. With these methods the user can flexibly specify the focus of interest, and also iteratively refine it.We have implemented our methodology in the TASA system which discovers patterns in telecommunication alarm databases. In this paper, we give concrete examples of how to use frequent patterns in the construction of alarm correlation expert systems.\nTitle:\nA Data Mining Methodology and Its Application to Semi-Automatic Knowledge Acquisition\n\nAbstract:\nMany real world datasets are represented in the form of graphs. The classical graph properties found in the data, like cliques or independent sets, can reveal new interesting information in the data. However, such properties can be either too rare or too trivial in the given context. By relaxing the criteria of the classical properties, we can find more and totally new patterns in the data. In this paper, we define relaxed graph properties and study their use in analyzing and processing graph-based data. Especially, we consider the problem of finding self-referring groups in WWW, and give a general algorithm for mining all such patterns from a collection of WWW pages. We suggest that such self-referring groups can reveal web communities or other clusterings in WWW and also help in compression of graph-formed data.\nTitle:\nMining Relaxed Graph Properties in Internet\n\nAbstract:\nThe paper presents an approach to computational knowledge discovery through the mechanism of bisociation. Bisociative reasoning is at the heart of creative, accidental discovery (e.g., serendipity), and is focused on finding unexpected links by crossing contexts. Contextu- alization and linking between highly diverse and distributed data and knowledge sources is therefore crucial for the implementation of bisocia- tive reasoning. In the paper we explore these ideas on the problem of analysis of microarray data. We show how enriched gene sets are found by using ontology information as background knowledge in semantic sub- group discovery. These genes are then contextualized by the computation of probabilistic links to diverse bioinformatics resources. Preliminary ex- periments with microarray data illustrate the approach.\nTitle:\nBisociative Knowledge Discovery for Microarray Data Analysis\n\nAbstract:\nWe discuss the use of database methods for data mining. Recently impressive results have been achieved for some data mining problems using highly specialized and clever data structures. We study how well one can manage by using general purpose database management systems. We illustrate our ideas by investigating the use of a dbms for a well-researched area: the discovery of association rules. We present a simple algorithm, consisting of only union and intersection operations, and show that it achieves quite good performance on an efficient dbms. Our method can incorporate inheritance hierarchies to the association rule algorithm easily. We also present a technique that effectively reduces the number of database operations when searching large search spaces that contain only few interesting items. Our work shows that database techniques are promising for data mining: general architectures can achieve reasonable results.\nTitle:\nA perspective on databases and data mining\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.729, mean value for all the writers is 8.648, which makes it 0.784 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the age of big data, automatic methods for creating summaries of documents become increasingly important. In this paper we propose a novel, unsupervised method for (multi-)document summarization. In an unsupervised and language-independent fashion, this approach relies on the strength of word associations in the set of documents to be summarized. The summaries are generated by picking sentences which cover the most specific word associations of the document(s). We measure the performance on the DUC 2007 dataset. Our experiments indicate that the proposed method is the best-performing unsupervised summarization method in the state-of-the-art that makes no use of human-curated knowledge bases.\nTitle:", "model_inf_time": 1.34}, {"id": "4175", "output": "Caching VM Images for Fast Startup in IaaS Clouds", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nElastic cloud applications rely on fast virtual machine (VM) startup, e.g. when scaling out for handling increased workload. While there have been recent studies into the VM startup time in clouds, the effects of the VM image (VMI) disk size and its contents are little understood. To fill this gap, we present a detailed study of these factors on Amazon EC2. Based on our findings, we developed a novel approach for consolidating size and contents of VMIs. We then evaluated our approach with the ConPaaS VMI, an open-source Platform-as-a-Service runtime. Compared to an unmodified ConPaaS VMI, our approach results in up to four times reduction of the disk size, three times speedup for the VM startup time, and three times reduction of storage cost.\nTitle:\nReducing VM Startup Time and Storage Costs by VM Image Content Consolidation.\n\nAbstract:\nCompute clusters, consisting of many, uniformly built nodes, are used to run a large spectrum of different workloads, like tightly coupled (MPI) jobs, MapReduce, or graph-processing data-analytics applications, each of which with their own resource requirements. Many studies consistently highlight two types of under-utilized cluster resources: memory (up to 50%) and network. In this work, we take a step towards (software) resource disaggregation, and therefore increased resource utilization, by designing a memory scavenging technique that makes unused memory available to applications on other cluster nodes. We implement this technique in MemFSS, an in-memory distributed file system. The scavenging MemFSS extends its storage space by taking advantage of the unused memory and bandwidth of cluster nodes already running other tenants' applications. Our experiments show that our memory scavenging approach incurs negligible overhead (below 10%) for most tenant applications, while the compute resource comsumption of MemFSS applications is largely reduced (by 17%-74%).\nTitle:\nTowards Resource Disaggregation \u2014 Memory Scavenging for Scientific Workloads\n\nAbstract:\nMany grid applications need to transfer large amounts of data between the geographically distributed sites of a grid environment. Network heterogeneity between these sites makes throughput optimization of data transfers to multiple sites (multicast) hard or even impossible. We present a technique called balanced multicasting that uses monitoring information for both bandwidth capacity and achievable bandwidth to compute balanced multicast trees at runtime that use application-level traffic shaping at the sender side to avoid self-induced congestion. Our experimental evaluation shows that our approach outperforms existing multicast strategies by large margins.\nTitle:\nBalanced Multicasting: High-throughput Communication for Grid Applications\n\nAbstract:\nApplications on cloud infrastructures acquire virtual machines (VMs) from providers when necessary. The current interface for acquiring VMs from most providers, however, is too limiting for the tenants, in terms of granularity in which VMs can be acquired (e.g., small, medium, large, etc.), while giving very limited control over their placement. The former leads to VM underutilization, and the latter has performance implications, both translating into higher costs for the tenants. In this work, we leverage nested virtualization and a networking overlay to tackle these problems. We present Kangaroo, an Open Stack-based virtual infrastructure provider, and IPOPsm, a virtual networking switch for communication between nested VMs over different infrastructure VMs. In addition, we design and implement Skippy, the realization of our proposed virtual infrastructure API for programming Kangaroo. Our benchmarks show that through careful mapping of nested VMs to infrastructure VMs, Kangaroo achieves up to an order of magnitude better performance, with only half the cost on Amazon EC2. Further, Kangaroo's unified Open Stack API allows us to migrate an entire application between Amazon EC2 and our local Open Nebula deployment within a few minutes, without any downtime or modification to the application code.\nTitle:\nKangaroo: A Tenant-Centric Software-Defined Cloud Infrastructure\n\nAbstract:\nWe address the problem of routing packets on multiple, router-disjoint, paths in the Internet using large-scale overlay networks. Multipath routing can improve Internet QoS, by routing around congestions. This can benefit interactive and other real-time applications. One of the main problems with practically achieving router-disjoint multipath routing is the scalability limitation on the number of participating nodes in such an overlay network, caused by the large number of (expensive) topology probes required to discover relay nodes that provide high router-level path disjointness. To address this problem, we propose a novel, synthetic coordinates-based approach. We evaluate our method against alternative strategies for finding router-level disjoint alternative paths. Additionally, we empirically evaluate the distribution of path diversity in the Internet.\nTitle:\nSynthetic Coordinates for Disjoint Multipath Routing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.188, mean value for all the writers is 8.648, which makes it 0.392 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn IaaS clouds, VM startup times are frequently perceived as slow, negatively impacting both dynamic scaling of web applications and the startup of high-performance computing applications consisting of many VM nodes. A significant part of the startup time is due to the large transfers of VM image content from a storage node to the actual compute nodes, even when copy-on-write schemes are used. We have observed that only a tiny part of the VM image is needed for the VM to be able to start up. Based on this observation, we propose using small caches for VM images to overcome the VM startup bottlenecks. We have implemented such caches as an extension to KVM/QEMU. Our evaluation with up to 64 VMs shows that using our caches reduces the time needed for simultaneous VM startups to the one of a single VM.\nTitle:", "model_inf_time": 1.36}, {"id": "4176", "output": "Polynomial Decay Flocking Model", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe prove a general result of collision-avoiding flocking. The underlying model allows several forms of coupling forces and the main result ensures flocking provided the initial state of the population does not show simultaneously very different velocities, very spread positions or very close agents.\nTitle:\nA General Collision-Avoiding Flocking Framework.\n\nAbstract:\nWe describe a model for the evolution of the languages used by the agents of a society. Our main result proves convergence of these languages to a common one under certain conditions. A few special cases are elaborated in more depth.\nTitle:\nModeling Language Evolution\n\nAbstract:\nWe perform a smoothed analysis of the condition number of rectangular matrices. We prove that, asymptotically, the expected value of this condition number depends only on the elongation of the matrix and not on the center and variance of the underlying probability distribution.\nTitle:\nSmoothed Analysis of Moore-Penrose Inversion\n\nAbstract:\nWe define a condition number ( A, b, c) for a linear program min x s.t. Ax= b, x=0 and give two characterizations via distances to degeneracy and singularity. We also give bounds for the expected value, as well as for higher moments, of log ( A, b, c) when the entries of A, b and c are i.i.d. random variables with normal distribution.\nTitle:\nSolving linear programs with finite precision: I. Condition numbers and random programs\n\nAbstract:\nWe provide several machine-independent characterizations of deterministic complexity classes in the model of computation proposed by L. Blum, M. Shub and S. Smale. We provide a characterization of partial recursive functions over any arbitrary structure. We show that polynomial time over an arbitrary structure can be characterized in terms of safe recursion. We show that polynomial parallel time over an arbitrary structure can be characterized in terms of safe recursion with substitutions.\nTitle:\nImplicit Complexity over an Arbitrary Structure: Sequential and Parallel Polynomial Time\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.552, mean value for all the writers is 8.648, which makes it 0.082 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe provide a model (for both continuous and discrete time) describing the evolution of a flock. Our model is parameter- ized by a constant capturing the rate of decay\u2014which in our model is polynomial\u2014of the influence between birds in the flock as they separate in space. Our main result shows that when convergence of the flock to a common velocity is guaranteed, while for convergence is guaranteed under some condition on the initial positions and velocities of the birds only. Index Terms\u2014Consensus reaching problem, emergence, flocking.\nTitle:", "model_inf_time": 0.81}, {"id": "4177", "output": "Dynamic Core Frequency Control in Time Warp Parallel Simulation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nParallel simulations using optimistic synchronization strategies such as Time Warp, operate with no regard to global synchronization since this results in greater parallelism and lower synchronization cost. However, like virtual memory, the parallel simulators may end up thrashing instead of performing useful work. The complication in using a Time Warp simulator is then to configure it suitably for good performance and avoid thrashing. Unfortunately, the optimal configuration is not generally static among different applications or even throughout an entire run of a single application. Thus, online feedback control systems are deployed to govern the adjustment of input parameters in our Time Warp simulation kernel. The design and implementation of effective feedback control systems can be difficult; the extra processing is pure overhead that must be absorbed by any performance gains delivered. The problem is further complicated when attempting to build a simulation kernel that is designed efficiently to operate with many different applications. In this paper, we introduce a control-centric architecture that is used to monitor and manage different parts of a Time Warp simulator. Specifically, we extend concepts from control theory such as adaptive control and stability, to better understand and design hierarchically-distributed run-time control systems for Time Warp based parallel simulation.\nTitle:\nSoftware control systems for parallel simulation\n\nAbstract:\nIn Time Warp simulations, the overheads associated with rollbacks, state-saving and the communication induced by rollbacks are the chief contributors to the cost of the simulation; thus, these aspects of the simulation have been primary targets for optimizations. Unfortunately, the behavior of the Time Warp simulation is highly dynamic and greatly influenced by the application being simulated. Thus, the suggested optimizations are only effective for certain intervals of the simulation. This paper argues that the performance of Time Warp simulators benefits from a dynamic on-line decision process that selects and configures the sub-algorithms implementing the different aspects of the simulator to best match the current behavior of the simulation. In particular, we study control strategies to dynamically: (i) adjust the checkpointing (or state-saving) interval (ii) select the cancellation strategy (lazy or aggressive), and (iii) determine the policy for aggregating the application messages (an optimization that significantly improves the performance in message passing environments). The strategies have been implemented in the WARPED Time Warp simulation kernel and the performance obtained via the dynamically controlled optimizations is shown to surpass that of their best performing static counterparts.\nTitle:\nOn-line Configuration of a Time Warp Parallel Discrete Event Simulator\n\nAbstract:\nThe set of events available for execution in a Parallel Discrete Event Simulation (PDES) are known as the pending event set. In a Time Warp synchronized simulation engine, these pending events are scheduled for execution in an aggressive manner that does not strictly enforce the causal relations between events. One of the key principles of Time Warp is that this relaxed causality will result in the processing of events in a manner that implicitly satisfies their causal order without paying the overhead costs of a strict enforcement of their causal order. On a shared memory platform the event scheduler generally attempts to schedule all available events in their Least TimeStamp First (LTSF) order to facilitate event processing in their causal order. By following an LTSF scheduling policy, a Time Warp scheduler can generally process events so that: (i) the critical path of the event timestamps is scheduled as early as possible, and (ii) causal violations occur infrequently. While this works effectively to minimize rollback (triggered by causal violations), as the number of parallel threads increases, the contention to the shared data structures holding the pending events can have significant negative impacts on overall event processing throughput. This work examines the application of profile data taken from Discrete-Event Simulation (DES) models to drive the simulation kernel optimization process. In particular, we take profile data about events in the schedule pool from three DES models to derive alternate scheduling possibilities in a Time Warp simulation kernel. Profile data from the studied DES models suggests that in many cases each Logical Process (LP) in a simulation will have multiple events that can be dequeued and executed as a set. In this work, we review the profile data and implement group event scheduling strategies based on this profile data. Experimental results show that event group scheduling can help alleviate contention and improve performance. However, the size of the event groups matters, small groupings can improve performance, larger groupings can trigger more frequent causal violations and actually slow the parallel simulation.\nTitle:\nQuantitative Driven Optimization of a Time Warp Kernel.\n\nAbstract:\nThe time warp mechanism is one of the most important synchronization protocols for parallel simulation. However for most applications, the successful use of time warp requires the careful selection of time warp optimization parameters (e.g. cancellation strategies, state saving frequency, and so on). Unfortunately, the optimal setting for the simulation parameters may not hold across an application domain or even throughout the entire simulation lifetime of a single application. Consequently, several investigators have proposed the dynamic adjustment of simulation parameters over the lifetime of the simulation. The dynamic adjustment of simulation parameters requires careful design considerations. Many of these considerations are similar to the problems studied by traditional nonlinear and adaptive control theorists. These considerations include the need to implement an adjustment mechanism that converges to stable values and the need to understand how parameter adjustment affects (sampled) output values. However, the dynamic adjustment of simulation parameters occurs by using general CPU cycles and thus introduces overhead into the simulation. This overhead must be carefully controlled-too little processing and the control system may not respond correctly; too much processing and the overall simulation performance may deteriorate. We address the design of two distinct feedback control systems for dynamic parameter adjustment and describe the design considerations that were made in their construction.\nTitle:\nFeedback Control in Time Warp Synchronized Parallel Simulators\n\nAbstract:\nMulti-core and many-core processing chips are becoming widespread and are now being widely integrated into Beowulf clusters. This poses a challenging problem for distributed simulation as it now becomes necessary to extend the algorithms to operate on a platform that includes both shared memory and distributed memory hardware. Furthermore, as the number of on-chip cores grows, the challenges for developing solutions without significant contention for shared data structures grows. This is especially true for the pending event list data structures where multiple execution threads attempt to schedule the next event for execution. This problem is especially aggravated in parallel simulation, where event executions are generally fine-grained leading quickly to non-trivial contention for the pending event list. This manuscript explores the design of the software architecture and several data structures to manage the pending event sets for execution in a Time Warp synchronized parallel simulation engine. The experiments are especially targeting multi-core and many-core Beowulf clusters containing 8-core to 48-core processors. These studies include a two-level structure for holding the pending event sets using three different data structures, namely: splay trees, the STL multiset, and ladder queues. Performance comparisons of the three data structures using two architectures for the pending event sets are presented.\nTitle:\nEvent pool structures for PDES on many-core Beowulf clusters.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.465, mean value for all the writers is 8.648, which makes it 0.156 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTime Warp synchronized parallel discrete event simulators are organized to operate asynchronously and aggressively without explicit synchronization between the concurrently executing simulation objects. In place of an explicit synchronization mechanism, the concurrent simulators implement an independent but common virtual clock model and a rollback/recovery mechanism to restore causal order when out-of-order events are detected. When the critical path of execution of the simulation is balanced across this parallel threads of execution, this can result in a highly effective, lightweight synchronization mechanism to implement parallel simulation. However, imbalances in the workload across the threads can result in excessive rollback in some threads and slowed progress of the critical path. On small shared memory multi-core systems, a lowest time-stamp scheduling policy can effectively balance the workload. However, on larger many-core chips, conventional load balancing and workload migration will once again become necessary. Fortunately, emerging many-core chips contain some interesting features that can potentially be exploited to improve the performance of parallel simulations. In particular, the recently developed Intel Single-chip Cloud Computer (SCC) provides mechanisms for the runtime control of the frequency and voltage settings of the chip. Furthermore, the frequency and voltage settings are independently set within different regions (called islands) of the chip. Thus, in a Time Warp simulation, one could increase the frequency of the cores executing threads on the critical path (those experiencing infrequent rollback) and decrease the frequency of the cores executing threads off the critical path (those experiencing excessive rollback). This paper investigates the run-time control and adjustment of core frequency in some contemporary x86 multi-core processors to identify the platforms that can support the exploration of dynamic run-time control of core frequency settings. The results show that while all multi-core processors have software controllable core frequency modulation capabilities, they are generally not fully independent as the system comes under load and are therefore unsuitable for these studies. Fortunately, one processor, the AMD X6 line, provides software control for core frequencies that can be fixed (by software) even as the system operates under load. (c) 2012 Elsevier B.V. All rights reserved.\nTitle:", "model_inf_time": 1.64}, {"id": "4178", "output": "Automatic Generation of Posynomial Performance Models for Analog Integrated Circuits", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents a method to automatically generate posynomial response surface models for the performance parameters of analog integrated circuits. The posynomial models enable the use of efficient geometric programming techniques for circuit sizing and optimization. To avoid manual derivation of approximate symbolic equations and subsequent casting to posynomial format, techniques from design of experiments and response surface modeling in combination with SPICE simulations are used to generate signomial and posynomial models in an automatic way. Attention is paid to estimating the relative 'goodness-of-fit' of the generated models. Experimental results allow to assess both the quality of the generated models as well as the strengths and the limitations of the presented approach.\nTitle:\nSimulation-based automatic generation of signomial and posynomial performance models for analog integrated circuit sizing\n\nAbstract:\nThis paper presents an new direct--fitting method to generate posynomial response surface models with arbitrary constant exponents for linear and nonlinear performance parameters of analog integrated circuits. Posynomial models enable the use of efficient geometric programming techniques for circuit sizing and optimization. The automatic generation avoids the time--consuming nature and inaccuracies of handcrafted analytic model generation. The technique is based on the fitting of posynomial model templates to numerical data from SPICE simulations. Attention is paid to estimating the relative `goodness--of--fit' of the generated models. Experimental results illustrate the significantly better accuracy of the new approach.\nTitle:\nAn efficient optimization-based technique to generate posynomial performance models for analog integrated circuits\n\nAbstract:\nThis paper presents a new method to automatically generate posynomial symbolic expressions for the performance characteristics of analog integrated circuits. The coefficient set as well as the exponent set of the posynomial expression are determined based on SPICE simulation data with device-level accuracy. We will prove that this problem corresponds to solving a non-convex optimization problem without local minima. The presented method is capable of generating posynomial performance expressions for both linear and nonlinear circuits and circuit characteristics. This approach allows to automatically generate an accurate sizing model that composes a geometric program that fully describes the analog circuit sizing problem. The automatic generation avoids the time-consuming nature of hand-crafted analytic model generation. Experimental results illustrate the capabilities and effectiveness of the presented modeling technique.\nTitle:\nGeneralized Posynomial Performance Modeling\n\nAbstract:\nThis paper presents a method to automatically generate compact symbolic performance models of analog circuits with no prior specification of an equation template. The approach takes SPICE simulation data as input, which enables modeling of any nonlinear circuits and circuit characteristics. Genetic programming is applied as a means of traversing the space of possible symbolic expressions. A grammar is specially designed to constrain the search to a canonical form for functions. Novel evolutionary search operators are designed to exploit the structure of the grammar. The approach generates a set of symbolic models which collectively provide a tradeoff between error and model complexity. Experimental results show that the symbolic models generated are compact and easy to understand, making this an effective method for aiding understanding in analog design. The models also demonstrate better prediction quality than posynomials.\nTitle:\nCAFFEINE: Template-Free Symbolic Model Generation of Analog Circuits via Canonical Form Functions and Genetic Programming\n\nAbstract:\nThis paper describes the application of Least-Squares Support Vector Machine (LS-SVM) training to analog circuit performance modeling as needed for accelerated or hierarchical analog circuit synthesis. The training is a type of regression, where a function of a special form is fit to experimental performance data derived from analog circuit simulations. The method is contrasted with a feasibility model approach based on the more traditional use of SVMs, namely classification. A Design of Experiments (DOE) strategy is reviewed which forms the basis of an efficient simulation sampling scheme. The results of our functional regression are then compared to two other DOE-based fitting schemes: a simple linear least-squares regression and a regression using posynomial models. The LS-SVM fitting has advantages over these approaches in terms of accuracy of fit to measured data, prediction of intermediatedata points and reduction of free model tuning parameters.\nTitle:\nPerformance Modeling of Analog Integrated Circuits Using Least-Squares Support Vector Machines\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.302, mean value for all the writers is 8.648, which makes it 2.264 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents an overview of methods to automatically generate posynomial response surface models for the performance characteristics of analog integrated circuits based on numerical simulation data. The methods are capable of generating posynomial performance expressions for both linear and nonlinear circuits and circuit characteristics, at SPICE-level accuracy. This approach allows for automatic generation of an accurate sizing model for a circuit that composes a geometric program that fully describes the analog circuit sizing problem. The automatic generation avoids the time-consuming and approximate nature of handcrafted analytic model generation. The methods are based on techniques from design of experiments and response surface modeling. Attention is paid to estimating the relative \"goodness-of-fit\" of the generated models. Experimental results illustrate the capabilities and effectiveness of the presented methods.\nTitle:", "model_inf_time": 1.4}, {"id": "4179", "output": "Automatic Generation of Related Work Sections via Citation Analysis", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nReading scientific articles is more time-consuming than reading news because readers need to search and read many citations. This paper proposes a citation guided method for summarizing multiple scientific papers. A phenomenon we can observe is that citation sentences in one paragraph or section usually talk about a common fact, which is usually represented as a set of noun phrases co-occurring in citation texts and it is usually discussed from different aspects. We design a multi-document summarization system based on common fact detection. One challenge is that citations may not use the same terms to refer to a common fact. We thus use term association discovering algorithm to expand terms based on a large set of scientific article abstracts. Then, citations can be clustered based on common facts. The common fact is used as a salient term set to get relevant sentences from the corresponding cited articles to form a summary. Experiments show that our method outperforms three baseline methods by ROUGE metric. A summarization system that expands citations through common fact.Explore common fact phenomenon in the scientific literature.An approach to expand terms to associated terms.\nTitle:\nSummarization of scientific documents by detecting common facts in citations\n\nAbstract:\nIt is important to help researchers find valuable scientific papers from a large literature collection containing information of authors, papers and venues. Graph-based algorithms have been proposed to rank papers based on networks formed by citation and co-author relationships. This paper proposes a new graph-based ranking framework MutualRank that integrates mutual reinforcement relationships among networks of papers, researchers and venues to achieve a more synthetic, accurate and fair ranking result than previous graph-based methods. MutualRank leverages the network structure information among papers, authors, and their venues available from a literature collection dataset and sets up a unified mutual reinforcement model that involves both intra- and inter-network information for ranking papers, authors and venues simultaneously. To evaluate, we collect a set of recommended papers from websites of graduate-level computational linguistics courses of 15 top universities as the benchmark and apply different methods to estimate paper importance. The results show that MutualRank greatly outperforms the competitors including Pag-eRank, HITS and CoRank in ranking papers as well as researchers. The experimental results also demonstrate that venues ranked by MutualRank are reasonable.\nTitle:\nTowards an effective and unbiased ranking of scientific literature through mutual reinforcement\n\nAbstract:\nHuman reading process significantly influences text understanding. Previous work has proposed a measure of information by simulating human reading process with a reading aim. The measure reflects both human memory of words in mind and association between words. There are two limitations: 1) interval between documents is an important reading factor and is not considered in the simulation, 2) the usefulness of the measure is limited to text recommendation in the work. This work proposes a multi-document scanning mechanism by exploiting the interval between documents and defines a measure named Information Quantity in Text in the mechanism. The measure is applied in both text recommendation and text summarization. Experiments show the measure outperforms an entropy-based baseline in determining the reading order of text sets according to the Summary Content Unit evaluation, and performs well in multi-document summarization according to the Pyramid evaluation. Experiments also show interval between documents in the scanning mechanism improves the recommendation and the summarization.\nTitle:\nInformation Quantity in Text and Its Applications.\n\nAbstract:\nKnowing semantic links among documents is the basis for intelligent applications over large-scale document resources. Discovering these semantic links with little human interference is a challenge issue. This paper proposes an approach to automatically discover semantic links in document set based on a probabilistic documentary semantic link network model. The approach has the following advantages: 1) It supports probabilistic relational reasoning. 2) The semantic link networks and relevant rules automatically evolve. 3) It does not rely on any predefined ontology. 4) It can adapt to the update of the adopted techniques. Experiments on document sets of different types (scientific papers and Web pages) and different scales show the proposed approach feasible. The approach can be used to automatically construct semantic overlays on large document sets to support advanced applications like various relational queries on scientific documents.\nTitle:\nAutomatically Discovering Semantic Links among Documents.\n\nAbstract:\nResource Space Model is a kind of data model which can effectively and flexibly manage the digital resources in cyber-physical system from multidimensional and hierarchical perspectives. This paper focuses on constructing resource space automatically. We propose a framework that organizes a set of digital resources according to different semantic dimensions combining human background knowledge in WordNet and Wikipedia. The construction process includes four steps: extracting candidate keywords, building semantic graphs, detecting semantic communities and generating resource space. An unsupervised statistical language topic model (i.e., Latent Dirichlet Allocation) is applied to extract candidate keywords of the facets. To better interpret meanings of the facets found by LDA, we map the keywords to Wikipedia concepts, calculate word relatedness using WordNet's noun synsets and construct corresponding semantic graphs. Moreover, semantic communities are identified by GN algorithm. After extracting candidate axes based on Wikipedia concept hierarchy, the final axes of resource space are sorted and picked out through three different ranking strategies. The experimental results demonstrate that the proposed framework can organize resources automatically and effectively. Our framework combines statistical topic model with human background knowledge.Candidate axes are generated by utilizing category hierarchy of Wikipedia.Three different ranking strategies are proposed for picking out final axes.Resources are mapped to different axes and important ones can be found out.\nTitle:\nA framework for automated construction of resource space based on background knowledge\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.717, mean value for all the writers is 8.648, which makes it 0.794 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nRelated work is a component of a scientific paper, which introduces other researchers' relevant works and makes comparisons with the current author's work. Automatically generating the related work section of a writing paper provides a tool for researchers to accomplish the related work section efficiently without missing related works. This paper proposes an approach to automatically generating a related work section by comparing the main text of the paper being written with the citations of other papers that cite the same references. Our approach first collects the papers that cite the reference papers of the paper being written and extracts the corresponding citation sentences to form a citation document. It then extracts keywords from the citation document and the paper being written and constructs a graph of the keywords. Once the keywords that discriminate the two documents are determined, the minimum Steiner tree that covers the discriminative keywords and the topic keywords is generated. The summary is generated by extracting the sentences covering the Steiner tree. According to ROUGE evaluations, the experiments show that the citations are suitable for related work generation and our approach outperforms the three baseline methods of MEAD, LexRank, and ReWoS. This work verifies the general summarization method based on connotation and extension through citation.\nTitle:", "model_inf_time": 1.42}, {"id": "4180", "output": "Fast Chip-Package Co-Design for High Routability Bump Assignment", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDue to the advantage of flip-chip design in power distribution but controversial peripheral IO placement in lower design cost, redistribution layer (RDL) is usually used for such interconnection. Sometimes RDL is so congested that the capacity for routing is insufficient. Routing therefore cannot be completed within a single layer even for manual routing. Although [2] proposed a routing algorithm that uses two layers of RDLs, but in practice the required routing area is a little more than one layer. We overcome this problem by adopting the concept of pseudo single-layer. With the heuristics for routing on mapped channels and observations on staggered pins to relieve vertical constraints, the area of 2-layer routing can be minimized and the routability is 100%. Comparisons of routing results between manual design, the commercial tool, and the proposed method are presented. We have shown the effectiveness on a real industrial case: it originally required fully manual design, the proposed method can finish RDL routing automatically and effectively.\nTitle:\nOn effective flip-chip routing via pseudo single redistribution layer\n\nAbstract:\nDue to the increasing complexity of the design interactions between the chip and package, it is necessary to consider them at the same time. In order to simultaneously handle chip and package performances, co-design of chip and package is a widely adopted solution, particularly because the finger/pad locations significantly affect IR-drop of the core and the package routing. In this paper, we develop chip-package co-design techniques to determine the locations of the fingers/pads for package routability and signal integrity concerns in IC designs, this method can be used in the 2-D and stacking IC design. Our finger/pad assignment is a two-step method: we first solve the wire congestion problem in package routing, and then try to minimize the IR-drop violation and the length of the bonding wires under a compact IR-drop model. The experimental results are encouraging. Compared with the randomly optimized method, on average, our approaches reduce the maximum package density by 42% and 68% for both technologies, IR-drop by 10.61% and 4.58%; and the bonding wires is reduced by 15.66% if we use stacking chips.\nTitle:\nPackage routability- and IR-drop-aware finger/pad planning for single chip and stacking IC designs\n\nAbstract:\nDeep submicron effects drive the complication in designing chips, as well as in package designs and communications between package and board. As a result, the iterative interface design has been a time-consuming process. This paper proposes a novel and efficient approach to designating pinout for flip-chip BGA package when designing chipsets. The proposed approach can not only automate the assignment of more than 200 I/O pins on package, but also precisely evaluate package size which accommodates all pins with almost no void pin positions, as good as the one from manual design. Furthermore, the practical experience and techniques in designing such interface has been accounted for, including signal integrity, power delivery and routability. This efficient pin-out designation and package size estimation by pin-block design and floorplanning provides much faster turn around time, thus enormous improvement in meeting design schedule. The results on two real cases show that our methodology is effective in achieving almost the same dimensions in package size, compared with manual design in weeks, while simultaneously considering critical issues in package-board codesign. To the best of our knowledge, this is the first attempt in solving flip-chip pin-out placement problem in package-board codesign.\nTitle:\nFast Flip-Chip Pin-Out Designation Respin by Pin-Block Design and Floorplanning for Package-Board Codesign\n\nAbstract:\nDeep submicrometer effects drive the complication in designing chips, as well as in package designs and communications between package and board. As a result, the iterative interface design has been a time-consuming process. This paper proposes a novel and efficient approach to designating pin-out, which is a package ball chart describing pin locations for flip-chip BGA package when designing chipsets. The proposed approach can not only automate the assignment of more than 200 input/output (I/O) pins on package, but also precisely evaluate package size which accommodates all pins with almost no void pin positions, as good as the one from manual design. Furthermore, the practical experience and techniques in designing such interface has been accounted for, including signal integrity, power delivery and routability. This efficient pin-out designation and package size estimation by pin-block design and floorplanning provides much faster turn around time, thus enormous improvement in meeting design schedule. Our pin-block design contains two major parts. First, we have pin-block construction to locate signal pins within a block along the specific patterns. Six pin patterns are proposed as templates which are automatically generated according to the user-defined constraints. Second, we have pin-blocks grouping to group all pin-blocks into package boundaries. Two alternative pin-blocks grouping strategies are provided for various applications such as chipset and field-programmable gate array (FPGA). The results on two real cases show that our methodology is effective in achieving almost the same dimensions in package size, compared with manual design in weeks, while simultaneously considering critical issues and package size migration in package-board codesign.\nTitle:\nFast flip-chip pin-out designation respin for package-board codesign\n\nAbstract:\nDue to increasing complexity of design interactions between the chip, package and PCB, it is essential to consider them at the same time. Specifically the finger/pad locations affect the performance of the chip and the package significantly. In this paper, we have developed techniques in chip-package codesign to decide the locations of fingers/pads for package routability and signal integrity concerns in chip core design. Our finger/pad assignment is a two-step method: first we optimize the wire congestion problem in package routing, and then we try to minimize the IR-drop violation with finger/pad solution refinement. The experimental results are encouraging. Compared with the randomly optimized methods, our approaches reduce in average 42% and 68% of the maximum density in package and 10.61% of IR-drop for test circuits.\nTitle:\nPackage routability- and IR-drop-aware finger/pad assignment in chip-package co-design\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.543, mean value for all the writers is 8.648, which makes it 1.617 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn current chip and package designs, it is a bottleneck to simultaneously optimize both pin assignment and pin routing for different design domains (chip, package, and board). Usually the whole process costs a huge manual effort and multiple iterations thus reducing profit margin. Therefore, we propose a fast heuristic chip-package co-design algorithm in order to automatically obtain a bump assignment which introduces high routability both in RDL routing and substrate routing (100% in our real case). Experimental results show that the proposed method (inspired by board escape routing algorithms) automatically finishes bump assignment, RDL routing and substrate routing in a short time, while the traditional co-design flow requires weeks even months.\nTitle:", "model_inf_time": 1.67}, {"id": "4181", "output": "Maytag: An Interactive NLP Framework for Financial Document Analysis", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nNews aggregators rely on links and users votes to select and present subsets of the large quantity of news and opinion items generated each day. Opinion diversity in the output sets can provide several benefits. We outline a range of diversity goals and discuss user reactions to a pilot implementation that selects for diversity as well as popularity. We then describe plans for research on alternative presentations and their impacts on users.\nTitle:\nDesigning interfaces for presentation of opinion diversity\n\nAbstract:\nUsers often rely on realtime predictions in everyday contexts like riding the bus, but may not grasp that such predictions are subject to uncertainty. Existing uncertainty visualizations may not align with user needs or how they naturally reason about probability. We present a novel mobile interface design and visualization of uncertainty for transit predictions on mobile phones based on discrete outcomes. To develop it, we identified domain specific design requirements for visualizing uncertainty in transit prediction through: 1) a literature review, 2) a large survey of users of a popular realtime transit application, and 3) an iterative design process. We present several candidate visualizations of uncertainty for realtime transit predictions in a mobile context, and we propose a novel discrete representation of continuous outcomes designed for small screens, quantile dotplots. In a controlled experiment we find that quantile dotplots reduce the variance of probabilistic estimates by ~1.15 times compared to density plots and facilitate more confident estimation by end-users in the context of realtime transit prediction scenarios.\n\n\nTitle:\nWhen (ish) is My Bus?: User-centered Visualizations of Uncertainty in Everyday, Mobile Predictive Systems.\n\nAbstract:\nPersonal informatics applications support capture and access of data related to an increasing variety of dimensions of everyday life. However, such applications often fail to effectively support diagnostic self-tracking, wherein people seek to answer a specific question about themselves. Current approaches are therefore difficult, tedious, and error-prone. This workshop paper discusses our ongoing efforts to develop methods for self-experimentation in self-tracking. We examine how self-experimentation situates within existing models of personal informatics processes, discuss our current focus on personal food triggers in patients suffering from Irritable Bowel Syndrome, and highlight open challenges for self-experimentation more broadly.\nTitle:\nOpportunities and challenges for self-experimentation in self-tracking\n\nAbstract:\nCurrent models of how people use personal informatics systems are largely based in behavior change goals. They do not adequately characterize the integration of self-tracking into everyday life by people with varying goals. We build upon prior work by embracing the perspective of lived informatics to propose a new model of personal informatics. We examine how lived informatics manifests in the habits of self-trackers across a variety of domains, first by surveying 105, 99, and 83 past and present trackers of physical activity, finances, and location and then by interviewing 22 trackers regarding their lived informatics experiences. We develop a model characterizing tracker processes of deciding to track and selecting a tool, elaborate on tool usage during collection, integration, and reflection as components of tracking and acting, and discuss the lapsing and potential resuming of tracking. We use our model to surface underexplored challenges in lived informatics, thus identifying future directions for personal informatics design and research.\nTitle:\nA lived informatics model of personal informatics\n\nAbstract:\nRecent research examines how and why people abandon self tracking tools. We extend this work with new insights drawn from people reflecting on their experiences after they stop tracking, examining how designs continue to influence people even after abandonment. We further contrast prior work considering abandonment of health and wellness tracking tools with an exploration of why people abandon financial and location tracking tools, and we connect our findings to models of personal informatics. Surveying 193 people and interviewing 12 people, we identify six reasons why people stop tracking and five perspectives on life after tracking. We discuss these results and opportunities for design to consider life after self tracking.\n\n\nTitle:\nBeyond Abandonment to Next Steps: Understanding and Designing for Life after Personal Informatics Tool Use.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.364, mean value for all the writers is 8.648, which makes it 1.464 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe present a novel application of NLP and text mining to the analysis of financial documents. In particular, we describe an implemented prototype, Maytag, which combines information extraction and subject classification tools in an interactive exploratory framework. We present experimental results on their performance, as tailored to the financial domain, and some forward-looking extensions to the approach that enables users to specify classifications on the fly.\nTitle:", "model_inf_time": 1.29}, {"id": "4182", "output": "Hybrid Name Matching for Entity Resolution", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIdentifying approximately duplicate database records that refer to the same entity is essential for information integration. The authors review traditional approaches to solving this problem and present their recent experimental results on comparing, combining, and learning textual similarity measures for name matching.\nTitle:\nAdaptive Name Matching in Information Integration\n\nAbstract:\nSimilarity measures for text have historically been an important tool for solving information retrieval problems. In many interesting settings, however, documents are often closely connected to other documents, as well as other non-textual objects: for instance, email messages are connected to other messages via header information. In this paper we consider extended similarity metrics for documents and other objects embedded in graphs, facilitated via a lazy graph walk. We provide a detailed instantiation of this framework for email data, where content, social networks and a timeline are integrated in a structural graph. The suggested framework is evaluated for two email-related problems: disambiguating names in email documents, and threading. We show that reranking schemes based on the graph-walk similarity measures often outperform baseline methods, and that further improvements can be obtained by use of appropriate learning methods.\nTitle:\nContextual search and name disambiguation in email using graphs\n\nAbstract:\nWe describe a framework for representing email as well as meeting information as a joint graph. In the graph, docu- ments and meeting descriptions are connected via other non- textual objects representing the underlying structure-rich data. This framework integrates content, social networks and a timeline in a structural graph. Extended similarity metrics for objects embedded in the graph can be derived using a lazy graph walk paradigm. In this paper we evalu- ate this general framework for two meeting and email related tasks. A novel task considered is finding email-addresses of relevant attendees for a given meeting. Another task we define and evaluate is finding the full set of email-address aliases for a person, given the corresponding name string. The experimental results show promise of this approach over other possible methods.\nTitle:\nAn Email and Meeting Assistant Using Graph Walks\n\nAbstract:\nWe propose a general approach to modeling semi-supervised learning (SSL) algorithms. Specifically, we present a declarative language for modeling both traditional supervised classification tasks and many SSL heuristics, including both well-known heuristics such as co-training and novel domain-specific heuristics. In addition to representing individual SSL heuristics, we show that multiple heuristics can be automatically combined using Bayesian optimization methods. We experiment with two classes of tasks, link-based text classification and relation extraction. We show modest improvements on well-studied link-based classification benchmarks, and state-of-the-art results on relation-extraction tasks for two realistic domains.\nTitle:\nUsing Graphs of Classifiers to Impose Declarative Constraints on Semi-supervised Learning.\n\nAbstract:\nWe learn graph-based similarity measures for the task of extracting word synonyms from a corpus of parsed text. A constrained graph walk variant that has been successfully applied in the past in similar settings is shown to outperform a state-of-the-art syntactic vector-based approach on this task. Further, we show that learning specialized similarity measures for different word types is advantageous.\nTitle:\nGraph based similarity measures for synonym extraction from parsed text\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.547, mean value for all the writers is 8.648, which makes it 0.086 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nUsing an open-source, Java toolkit of name-matching methods, we experimentally compare string distance metrics on the task of matching entity names. We inves- tigate a number of different metrics proposed by differ- ent communities, including edit-distance metrics, fast heuristic string comparators, token-based distance met- rics, and hybrid methods. Overall, the best-performing method is a hybrid scheme combining a TFIDF weight- ing scheme, which is widely used in information re- trieval, with the Jaro-Winkler string-distance scheme, which was developed in the probabilistic record linkage community.\nTitle:", "model_inf_time": 1.0}, {"id": "4183", "output": "Near-Optimal Apprenticeship Learning with No Explicit Exploration", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe consider the exploration/exploitation problem in reinforcement learning (RL). The Bayesian approach to model-based RL offers an elegant solution to this problem, by considering a distribution over possible models and acting to maximize expected reward; unfortunately, the Bayesian solution is intractable for all but very restricted cases. In this paper we present a simple algorithm, and prove that with high probability it is able to perform \u03b5-close to the true (intractable) optimal Bayesian policy after some small (polynomial in quantities describing the system) number of time steps. The algorithm and analysis are motivated by the so-called PAC-MDP approach, and extend such results into the setting of Bayesian RL. In this setting, we show that we can achieve lower sample complexity bounds than existing algorithms, while using an exploration strategy that is much greedier than the (extremely cautious) exploration of PAC-MDP algorithms.\nTitle:\nNear-Bayesian exploration in polynomial time\n\nAbstract:\nWe consider the policy search approach to reinforcement learning. We show that if a \"baseline distribution\" is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a finite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem.\nTitle:\nPolicy Search by Dynamic Programming\n\nAbstract:\nIn the model-based policy search approach to reinforcement learning (RL), policies are found using a model (or \"simulator\") of the Markov decision process. However, for high-dimensional continuous-state tasks, it can be extremely difficult to build an accurate model, and thus often the algorithm returns a policy that works in simulation but not in real-life. The other extreme, model-free RL, tends to require infeasibly large numbers of real-life trials. In this paper, we present a hybrid algorithm that requires only an approximate model, and only a small number of real-life trials. The key idea is to successively \"ground\" the policy evaluations using real-life trials, but to rely on the approximate model to suggest local changes. Our theoretical results show that this algorithm achieves near-optimal performance in the real system, even when the model is only approximate. Empirical results also demonstrate that---when given only a crude model and a small number of real-life trials---our algorithm can obtain near-optimal performance in the real system.\nTitle:\nUsing inaccurate models in reinforcement learning\n\nAbstract:\nA critical issue for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or infinite state spaces, traditional planning and reinforcement learning algorithms may be inapplicable, since their running time typically grows linearly with the state space size in the worst case. In this paper we present a new algorithm that, given only a generative model (a natural and common type of simulator) for an arbitrary MDP, performs on-line, near-optimal planning with a per-state running time that has no dependence on the number of states. The running time is exponential in the horizon time (which depends only on the discount factor \u03b3 and the desired degree of approximation to the optimal policy). Our algorithm thus provides a different complexity trade-off than classical algorithms such as value iteration\u2014rather than scaling linearly in both horizon time and state space size, our running time trades an exponential dependence on the former in exchange for no dependence on the latter.Our algorithm is based on the idea of sparse sampling. We prove that a randomly sampled look-ahead tree that covers only a vanishing fraction of the full look-ahead tree nevertheless suffices to compute near-optimal actions from any state of an MDP. Practical implementations of the algorithm are discussed, and we draw ties to our related recent results on finding a near-best strategy from a given class of strategies in very large partially observable MDPs (Kearns, Mansour, & Ng. Neural information processing systems 13, to appear).\nTitle:\nA Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes\n\nAbstract:\n  We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP), given a model. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. We give a natural way of estimating the value of all policies in these transformed POMDPs. Policy search is then simply performed by searching for a policy with high estimated value. We also establish conditions under which our value estimates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng (1999), but with \"sample complexity\" bounds that have only a polynomial rather than exponential dependence on the horizon time. Our method applies to arbitrary POMDPs, including ones with infinite state and action spaces. We also present empirical results for our approach on a small discrete problem, and on a complex continuous state/continuous action problem involving learning to ride a bicycle. \nTitle:\nPEGASUS: A Policy Search Method for Large MDPs and POMDPs\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.784, mean value for all the writers is 8.648, which makes it 0.737 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe consider reinforcement learning in systems with unknown dynamics. Algorithms such as E3 (Kearns and Singh, 2002) learn near-optimal policies by using \"exploration policies\" to drive the system towards poorly modeled states, so as to encourage exploration. But this makes these algorithms impractical for many systems; for example, on an autonomous helicopter, overly aggressive exploration may well result in a crash. In this paper, we consider the apprenticeship learning setting in which a teacher demonstration of the task is available. We show that, given the initial demonstration, no explicit exploration is necessary, and we can attain near-optimal performance (compared to the teacher) simply by repeatedly executing \"exploitation policies\" that try to maximize rewards. In finite-state MDPs, our algorithm scales polynomially in the number of states; in continuous-state linear dynamical systems, it scales polynomially in the dimension of the state. These results are proved using a martingale construction over relative losses.\nTitle:", "model_inf_time": 1.42}, {"id": "4184", "output": "Minimal Nondeterministic Automata and Simulations", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe introduce the concept of finite automata over algebraic structures. We address the classical emptiness problem and its various refinements in our setting. In particular, we prove several decidability and undecidability results. We also explain the way our automata model connects with the existential first order theory of algebraic structures.\nTitle:\nDecision Problems For Finite Automata Over Infinite Algebraic Structures\n\nAbstract:\nFinite automata have been recently used as alternative, discrete models in theoretical physics, especially in problems related to the dichotomy between endophysical/intrinsic and exophysical/ extrinsic perception (see, for instance [3, 6, 18\u201321]). These studies deal with Moore experiments; the main result states that it is impossible to determine the initial state of an automaton, and, consequently, a discrete model of Heisenberg uncertainty has been suggested. For this aim the classical theory of finite automata \u2014 which considers automata with initial states \u2014 is not adequate, and a new approach is necessary. A study of finite deterministic automata without initial states is exactly the aim of this paper. We will define and investigate the complexity of various types of simulations between automata. Minimal automata will be constructed and proven to be unique up to an isomorphism. We will build our results on an extension of Myhill-Nerode technique; all constructions will make use of \u201cautomata responses\u201d to simple experiments only, i.e., no information about the internal machinery will be considered available.\nTitle:\nDeterministic automata simulation, universality and minimality\n\nAbstract:\nWe investigate partial orders that are computable, in a precisesense, by finite automata. Our emphasis is on treesand linear orders. We study the relationship between automaticlinear orders and trees in terms of rank functions thatare versions of Cantor-Bendixson rank. We prove that automaticlinear orders and automatic trees have finite rank.As an application we provide a procedure for deciding theisomorphism problem for automatic ordinals. We also investigatethe complexity and definability of infinite paths inautomatic trees. In particular, we show that every infinitepath in an automatic tree with countably many infinite pathsis a regular language.\nTitle:\nOn Automatic Partial Orders\n\nAbstract:\nA sequential automatic algebra is a structure of the type (A; f\n 1,..., f\n \n n\n ), where A is recognised by a finite automaton, and functions f\n 1, ..., f\n \n n\n  are total operations on A that are computed by input-output automata. Our input-output automata are variations of Mealy automata. We study some of\n the fundamental properties of these algebras and provide many examples. We give classification results for certain classes\n of groups, Boolean algebras, and linear orders. We also introduce different classes of sequential automatic algebras and give\n separating examples. We investigate linear orders considered as sequential automatic algebras. Finally, we outline some of\n the basic properties of sequential automatic unary algebras.\n \nTitle:\nSequential Automatic Algebras\n\nAbstract:\nThe paper studies classes of regular languages based on algebraic constraints imposed on transitions of automata and discusses issues related to specifications of these classes from algebraic, computational and logical points of view.\nTitle:\nOn algebraic and logical specifications of classes of regular languages\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 6.7, mean value for all the writers is 8.648, which makes it 1.662 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMotivated by recent applications of finite automata to theoretical physics, we study the minimization problem for nondeterministic automata (with outputs, but no initial states). We use Ehrenfeucht\u2013Fra\u0131\u0308sse-like games to model automata responses and simulations. The minimal automaton is constructed and, in contrast with the classical case, proved to be unique up to an isomorphism. Finally, we investigate the partial ordering induced by automata simulations. For example, we prove that, with respect to this ordering, the class of deterministic automata forms an ideal in the class of all automata.\nTitle:", "model_inf_time": 1.0}, {"id": "4185", "output": "Social Network Structure Extraction from Unidentified Faces in Video Clips", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nClustering approaches can alleviate the burden of tagging face identities in ad hoc video and image collections. We introduce a novel semisupervised framework for clustering face patterns into identity groups using minimal human interaction. This technique combines concepts from ensemble clustering and active learning to improve clustering accuracy. The framework actively queries the user for a soft link constraint between each pair of neighboring faces that are ambiguously matched according to the ensemble. We demonstrate the efficacy of our approach with the broadest evaluation of active face clustering algorithms to date. Our evaluations focus on data that is appropriate for human-in-the-loop face recognition, including blurry point-and-shoot videos, images of women seen before and after the application of makeup, and photographs of twins. The results indicate that ensemble-based constrained clustering algorithms are generally more robust to noise than alternative approaches. Finally, we show that the proposed clustering algorithm is more accurate and parsimonious than the current state-of-the-art.\nTitle:\nFramework for Active Clustering With Ensembles\n\nAbstract:\nIn this paper, we introduce a new system for face recognition by matching 3D face shape. This algorithm selects multiple regions of the face for matching in an attempt to reduce the effects caused by variations in expression between gallery and probe images. Experimental results are reported using the Face Recognition Grand Challenge v2.0 data set. Our results demonstrate improved performance relative to those of four previous papers using this same data set.\nTitle:\n3D Face Recognition with Region Committee Voting\n\nAbstract:\nWe propose an algorithm to generate realistic face images of both real and synthetic identities (people who do not exist) with different facial yaw, shape and resolution. The synthesized images can be used to augment datasets to train CNNs or as massive distractor sets for biometric verification experiments without any privacy concerns. Additionally, law enforcement can make use of this technique to train forensic experts to recognize faces. Our method samples face components from a pool of multiple face images of real identities to generate the synthetic texture. Then, a real 3D head model compatible to the generated texture is used to render it under different facial yaw transformations. We perform multiple quantitative experiments to assess the effectiveness of our synthesis procedure in CNN training and its potential use to generate distractor face images. Additionally, we compare our method with popular GAN models in terms of visual quality and execution time.\nTitle:\nFast Face Image Synthesis With Minimal Training\n\nAbstract:\nMany commercially available 3-D sensors suitable for face image capture employ passive or texture-assisted stereo imaging or structured illumination with a moving light stripe. These techniques require a stationary subject. We describe an initial design and evaluation of a fixed-stripe moving object 3-D scanner designed for human faces. Our method of acquisition requires the subject to walk through a static light screen generated by two laser line projectors. Triangulation and tracking applied to the video sequences captured during subject motion yield a 3-D image of the subject's face from multiple images. To demonstrate the accuracy of our initial design, a small-scale facial recognition experiment was executed. In an experiment involving 81 subjects with four images per subject on the average, we used two gallery images per subject, and we achieved 89.6% rank-one recognition using an iterative closest point (ICP)-based matching method, demonstrating the feasibility of the technique.\nTitle:\nThree-Dimensional Facial Imaging Using a Static Light Screen (SLS) and a Dynamic Subject\n\nAbstract:\nThe strong, neutral, or weak (SNoW) face impostor pairs problem is intended to explore the causes and impact of impostor face pairs that are inherently strong (easily recognized as nonmatches) or weak (possible false matches). The SNoW technique develops three partitions within the impostor score distribution of a given data set. Results provide evidence that varying degrees of impostor scores impact the overall performance of a face recognition system. This paper extends our earlier work to incorporate improvements regarding outlier detection for partitioning, explores the SNoW concept for the additional modalities of fingerprint and iris, and presents methods for how to begin to reveal the causes of weak impostor pairs. We also show a clear operational difference between strong and weak comparisons as well as identify partition stability across multiple algorithms.\nTitle:\nStrong, Neutral, or Weak: Exploring the Impostor Score Distribution\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.787, mean value for all the writers is 8.648, which makes it 0.119 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe introduce a method for extracting the social network structure for the persons appearing in a set of video clips. Individuals are unknown, and are not matched against known enrollments. An identity cluster representing an individual is formed by grouping similar-appearing faces from different videos. Each identity cluster is represented by a node in the social network. Two nodes are linked if the faces from their clusters appeared together in one or more video frames. Our approach incorporates a novel active clustering technique to create more accurate identity clusters based on feedback from the user about ambiguously matched faces. The final output consists of one or more network structures that represent the social group(s), and a list of persons who potentially connect multiple social groups. Our results demonstrate the efficacy of the proposed clustering algorithm and network analysis techniques.\nTitle:", "model_inf_time": 1.4}, {"id": "4186", "output": "Quantifying and Verifying Network Reachability", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nQuantifying and querying network reachability is important for security monitoring and auditing as well as many aspects of network management such as troubleshooting, maintenance, and design. Although attempts to model network reachability have been made, feasible solutions to computing network reachability have remained unknown. In this paper, we propose a suite of algorithms for quantifying reachability based on network configurations [mainly Access Control Lists (ACLs)] as well as solutions for querying network reachability. We present a network reachability model that considers connectionless and connection-oriented transport protocols, stateless and stateful routers/ firewalls, static and dynamic NAT, PAT, IP tunneling, etc. We implemented the algorithms in our network reachability tool called Quarnet and conducted experiments on a university network. Experimental results show that the offline computation of reachability matrices takes a few hours, and the online processing of a reachability query takes 0.075 s on average.\nTitle:\nQuantifying and Verifying Reachability for Access Controlled Networks\n\nAbstract:\nNetwork reachability is an important characteristic for understanding end-to-end network behavior and helps in detecting violations of security policies across the network. While quantifying network reachability within one administrative domain is a difficult problem in itself, performing the same computation across a network spanning multiple administrative domains presents a novel challenge. The problem of quantifying network reachability across multiple administrative domains is more difficult because the privacy of security policies of individual domains is a serious concern and needs to be protected through this process. In this paper, we propose the first cross-domain privacy-preserving protocol for quantifying network reachability. Our protocol constructs equivalent representations of the Access Control List (ACL) rules and determines network reachability while preserving the privacy of the individual ACLs. This protocol can accurately determine the network reachability along a network path through different administrative domains. We have implemented and evaluated our protocol on both real and synthetic ACLs. The experimental results show that the online processing time of an ACL containing thousands of rules is less than 25 s. Given two ACLs, each containing thousands of rules, the comparison time is less than 6 s, and the total communication cost is less than 2100 kB.\nTitle:\nPrivacy-Preserving Quantification of Cross-Domain Network Reachability\n\nAbstract:\nNetwork reachability is one of the key factors for capturing end-to-end network behavior and detecting the violation of security policies. While quantifying network reachability within one administrative domain is already difficult, quantifying network reachability across multiple administrative domains is more difficult because the privacy of security policies becomes a serious concern and needs to be protected through this process. In this paper, we propose the first cross-domain privacy-preserving protocol for quantifying network reachability. Our protocol constructs equivalent representations of the Access Control List (ACL) rules and determines network reachability while preserving the privacy of the individual ACLs. This protocol can accurately determine the network reachability along a network path through different administrative domains. We have implemented and evaluated our protocol on both real and synthetic ACLs. The experimental results show that the online processing time of an ACL with thousands of rules is less than 25 seconds, the comparison time of two ACLs is less than 6 seconds, and the communication cost between two ACLs with thousands of rules is less than 2100 KB.\nTitle:\nPrivacy-preserving cross-domain network reachability quantification\n\nAbstract:\nUnderstanding Internet traffic dynamics in large cellular networks is important for network design, troubleshooting, performance evaluation, and optimization. In this paper, we present the results from our study, which is based upon a week-long aggregated flow level mobile device traffic data collected from a major cellular operator's core network. In this study, we measure and characterize the spatial and temporal dynamics of mobile Internet traffic. We distinguish our study from other related work by conducting the measurement at a larger scale and exploring mobile data traffic patterns along two new dimensions -- device types and applications that generate such traffic patterns. Based on the findings of our measurement analysis, we propose a Zipf-like model to capture the volume distribution of application traffic and a Markov model to capture the volume dynamics of aggregate Internet traffic. We further customize our models for different device types using an unsupervised clustering algorithm to improve prediction accuracy.\nTitle:\nCharacterizing and modeling internet traffic dynamics of cellular devices\n\nAbstract:\nMost prior research on policies has focused on correctness. While correctness is an important issue, the adoption of policy-based computing may be limited if the resulting systems are not implemented efficiently and thus perform poorly. To increase the effectiveness and adoption of policy-based computing, in this paper, we propose fast policy evaluation algorithms that can be adapted to support various policy languages. In this paper, we focus on XACML policy evaluation because XACML has become the de facto standard for specifying access control policies, has been widely used on web servers, and is most complex among existing policy languages. We implemented our algorithms in a policy evaluation system called XEngine and conducted side-by-side comparison with Sun Policy Decision Point (PDP), the industrial standard for XACML policy evaluation. The results show that XEngine is orders of magnitude faster than Sun PDP. The performance difference grows almost linearly with the number of rules in an XACML policy. To our best knowledge, there is no prior work on improving XACML policy evaluation performance. This paper represents the first step in exploring this unknown space.\nTitle:\nDesigning Fast and Scalable XACML Policy Evaluation Engines\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.748, mean value for all the writers is 8.648, which makes it 0.085 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nQuantifying and querying network reachability is important for security monitoring and auditing as well as many aspects of network management such as troubleshooting, maintenance, and design. Although attempts to model network reachability have been made, feasible solutions to computing network reachability have remained unknown. In this paper, we propose a suite of algorithms for quantifying reachability based on network configurations (mainly ACLs) as well as solutions for querying network reachability. We present a comprehensive network reachability model that considers connectionless and connection-oriented transport protocols, stateless and stateful routers/firewalls, static and dynamic NAT, PAT, etc. We implemented the algorithms in our network reachability analysis tool called Quarnet and conducted experiments on a university network. Experimental results show that the offline computation of reachability matrices takes a few hours and the online processing of a reachability query takes 0.075 seconds on average.\nTitle:", "model_inf_time": 1.38}, {"id": "4187", "output": "A Memetic Algorithm for the Tool Switching Problem", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper describes a generic (meta-)cooperative optimization schema in which several agents endowed with an optimization technique (whose nature is not initially restricted) cooperate to solve an optimization problem. These agents can use a wide set of optimization techniques, including local search, population-based methods, and hybrids thereof, hence featuring multilevel hybridization. This optimization approach is here deployed on the Tool Switching Problem (ToSP), a hard combinatorial optimization problem in the area of flexible manufacturing. We have conducted an ample experimental analysis involving a comparison of a wide number of algorithms or a large number of instances. This analysis indicates that some meta-cooperative instances perform significantly better than the rest of the algorithms, including a memetic algorithm that was the previous incumbent for this problem.\nTitle:\nA memetic cooperative optimization schema and its application to the tool switching problem\n\nAbstract:\nThis work deals with memetic-computing agent-models based on the cooperative integration of search agents endowed with (possibly different) optimization strategies, in particular memetic algorithms. As a proof-of-concept of the model, we deploy it on the tool switching problem (ToSP), a hard combinatorial optimization problem that arises in the area of flexible manufacturing. The ToSP has been tackled by different algorithmic methods ranging from exact to heuristic methods (including local search meta-heuristics, population-based techniques and hybrids thereof, i.e., memetic algorithms). Here we consider an ample number of instances of this cooperative memetic model, whose agents are adapted to cope with this problem. A detailed experimental analysis shows that the meta-models promoting the cooperation among memetic algorithms provide the best overall results compared with their constituent parts (i.e., memetic algorithms and local search approaches). In addition, a parameter sensitivity analysis of the meta-models is developed in order to understand the interplay among the elements of the proposed topologies.\nTitle:\nMemetic cooperative models for the tool switching problem.\n\nAbstract:\nThe Tool Switching Problem (ToSP) is a hard combinatorial optimization problem of relevance in the field of flexible manufacturing systems (FMS), that has been tackled in the literature using both complete and heuristic methods, including local-search metaheuristics, population-based methods and hybrids thereof (e.g., memetic algorithms). This work approaches the ToSP using several hybrid cooperative models where spatially-structured agents are endowed with specific local-search/population-based strategies. Issues such as the intervening techniques and the communication topology are analyzed via an extensive empirical evaluation. It is shown that the cooperative models provide better results than their constituent parts. Furthermore, they not only provide solutions of similar quality to those returned by the memetic approach but raise interest prospects with respect to its scalability.\nTitle:\nHybrid Cooperation Models for the Tool Switching Problem\n\nAbstract:\nThis paper presents a parameterized schema for building memetic algorithms based on cross-entropy (CE) methods. This novel schema is general in nature, and features multiple probability mass functions and Lamarckian learning. The applicability of the approach is assessed by considering the Tool Switching Problem, a complex combinatorial problem in the field of Flexible Manufacturing Systems. An exhaustive evaluation (including techniques ranging from local search and evolutionary algorithms to constructive methods) provides evidence of the effectiveness of CE-based memetic algorithms.\nTitle:\nCross Entropy-Based Memetic Algorithms: An Application Study Over The Tool Switching Problem\n\nAbstract:\nThis paper deals with the construction of binary sequences with low autocorrelation, a very hard problem with many practical applications. The paper analyzes several metaheuristic approaches to tackle this kind of sequences. More specifically, the paper provides an analysis of different local search strategies, used as stand-alone techniques and embedded within memetic algorithms. One of our proposals, namely a memetic algorithm endowed with a Tabu Search local searcher, performs at the state-of-the-art, as it consistently finds optimal sequences in considerably less time than previous approaches reported in the literature. Moreover, this algorithm is also able to provide new best-known solutions for large instances of the problem. In addition, a variant of this algorithm that explores only a promising subset of the whole search space (known as skew-symmetric sequences) is also analyzed. Experimental results show that this new algorithm provides new best-known solutions for very large instances of the problem.\nTitle:\nFinding low autocorrelation binary sequences with memetic algorithms\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.922, mean value for all the writers is 8.648, which makes it 1.087 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper deals with the Tool Switching Problem (ToSP), a well-known problem in operations research. The ToSP involves determining\n a job sequence and the tools to be loaded on a machine with the goal of minimizing the total number of tool switches. This\n problem has been tackled by a number of algorithmic approaches in recent years. Here, we propose a memetic algorithm that\n combines a problem-specific permutational genetic algorithm with a hill-climbing procedure. It is shown that this combined\n approach outperforms each of the individual algorithms, as well as an ad-hoc beam search heuristic defined in the literature\n for this problem.\nTitle:", "model_inf_time": 1.31}, {"id": "4188", "output": "Key Escrow in Identity-Based Schemes for Mobile Ad Hoc Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nRecently, identity-based cryptography (IBC) schemes are considered as a tool to secure ad hoc networks. In this work we focus on the role of the Trust Authority (TA) as a key escrow, a property that is inherent to all IBC schemes. We explore the special role of key escrow in ad hoc networks and show that this role significantly differs from key escrows in other networks. We introduce a series of adversary models for dishonest TAs in ad hoc networks, including a new model where a TA uses spy nodes that record communications in the network and report them to the TA. Our analytical results show that in many ad hoc network applications the TA can be prevented from being a key escrow.\nTitle:\nShort Paper: Limitations of Key Escrow in Identity-Based Schemes in Ad Hoc Networks\n\nAbstract:\nRecently, identity-based cryptographic (IBC) schemes have been considered to secure mobile ad hoc networks (MANETs) due to their efficient key management properties. However, proposed schemes do not provide mechanisms for key revocation and key renewal. In this paper, we propose the first key revocation and key renewal mechanisms for IBC schemes that are especially designed for MANETs. In our fully self-organized revocation scheme, each node monitors nodes in communication range and securely propagates its observations. The public key of a node is revoked if a minimum number of nodes accused the node. To enable key renewal, we introduce a modified format for ID-based public keys, such that new keys can be issued for the same identity. The introduced revocation scheme is efficient because it uses pre-shared keys from the Weil pairing and messages are sent to an m-hop neighborhood instead to the entire network.\nTitle:\nKey revocation for identity-based schemes in mobile ad hoc networks\n\nAbstract:\nSecurity issue has been considered as one of the most pivotal aspects for the fifth-generation mobile network (5G) due to the increasing demands of security service as well as the growing occurrence of security threat. In this paper, instead of focusing on the security architecture in the upper layer, we investigate the secure transmission for a basic channel model in a heterogeneous network, that is, two-way relay channels. By exploiting the properties of the transmission medium in the physical layer, we propose a novel secure scheme for the aforementioned channel mode. With precoding design, the proposed scheme is able to achieve a high transmission efficiency as well as security. Two different approaches have been introduced: information theoretical approach and physical layer encryption approach. We show that our scheme is secure under three different adversarial models: (1) untrusted relay attackmodel, (2) trusted relay with eavesdropper attack model, and (3) untrusted relay with eavesdroppers attack model. We also derive the secrecy capacity of the two different approaches under the three attacks. Finally, we conduct three simulations of our proposed scheme. The simulation results agree with the theoretical analysis illustrating that our proposed scheme could achieve a better performance than the existing schemes.\nTitle:\nA Novel Secure Transmission Scheme in MIMO Two-Way Relay Channels with Physical Layer Approach.\n\nAbstract:\nThe paper introduces a novel message authentication framework over broadcast channels, where a symmetric cryptography-based physical layer assisted message authentication (PLAA) scheme is introduced in wireless networks. The proposed framework integrate the conventional message authentication schemes and the physical layer authentication mechanisms by taking advantage of temporal and spatial uniqueness in physical layer channel responses, aiming to achieving fast authentication while minimising the packet transmission overhead. Our claims through extensive analysis and simulation will be verified via comparing with public key infrastructure-based PLAA scheme and traditional upper layer authentication schemes.\nTitle:\nPhysical layer assisted authentication for distributed ad hoc wireless sensor networks.\n\nAbstract:\nIn this paper we propose a novel privacy-preserving mutual authentication protocol for RFID systems using the recently proposed ultra-lightweight cryptographic algorithm Hummingbird-2. The new protocol is resistant to the most common attacks against the security and privacy of RFID systems. Furthermore, we also address efficient implementation of the proposed protocol on a batteryless, MSP430-based WISP tag, and investigate the performance of the key search process on a laptop. Our experimental results demonstrate that the Hummingbird-2 mutual authentication protocol provides a highly effective and efficient security and privacy solution for low-cost passive RFID tags.\nTitle:\nA Lightweight Privacy-Preserving Mutual Authentication Protocol For Rfid Systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.641, mean value for all the writers is 8.648, which makes it 0.847 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nRecently, Identity-Based Cryptography (IBC) schemes have been considered as a tool to secure Mobile Ad Hoc Networks (MANETs) due to the efficient key management of the schemes. In this work, we focus on the role of the Key Generation Centre (KGC) as a key escrow, a property that is inherent to all IBC schemes. We explore the special role of key escrow in MANETs and show that this role significantly differs from key escrows in other networks. We introduce two adversary models for dishonest KGCs in MANETs, including a new spy model where a KGC uses so-called spy nodes that record communications in the network and report them to the KGC. We discuss the two faces of key escrow in MANETs, where our analytical results show that in many MANET applications the KGC can be prevented from being a key escrow. On the other hand, the results of this paper illustrate how a KGC can utilise spy nodes to monitor nodes in a MANET, as needed in some applications.\nTitle:", "model_inf_time": 1.46}, {"id": "4189", "output": "Privacy Loss Modeling in Video Surveillance", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nHuge amounts of video are being recorded every day by surveillance systems. Since video is capable of recording and preserving an enormous amount of information which can be used in many applications, it is worth examining the degree of privacy loss that might occur due to public access to the recorded video. A fundamental requirement of privacy solutions is an understanding and analysis of the inference channels than can lead to a breach of privacy. Though inference channels and privacy risks are well studied in traditional data sharing applications (e.g., hospitals sharing patient records for data analysis), privacy assessments of video data have been limited to the direct identifiers such as people's faces in the video. Other important inference channels such as location (Where), time (When), and activities (What) are generally overlooked. In this paper we propose a privacy loss model that highlights and incorporates identity leakage through multiple inference channels that exist in a video due to what, when, and where information. We model the identity leakage and the sensitive information separately and combine them to calculate the privacy loss. The proposed identity leakage model is able to consolidate the identity leakage through multiple events and multiple cameras. The experimental results are provided to demonstrate the proposed privacy analysis framework.\nTitle:\nW-privacy: understanding , and  inference channels in multi-camera surveillance video\n\nAbstract:\nPrivacy is a big concern in current video surveillance systems. Due to privacy issues, many strategic places remain unmonitored leading to security threats. The main problem with existing privacy protection methods is that they assume availability of accurate region of interest (RoI) detectors that can detect and hide the privacy sensitive regions such as faces. However, the current detectors are not fully reliable, leading to breaches in privacy protection. In this paper, we propose a privacy protection method that adopts adaptive data transformation involving the use of selective obfuscation and global operations to provide robust privacy even with unreliable detectors. Further, there are many implicit privacy leakage channels that have not been considered by researchers for privacy protection. We block both implicit and explicit channels of privacy leakage. Experimental results show that the proposed method incurs 38% less distortion of the information needed for surveillance in comparison to earlier methods of global transformation; while still providing near-zero privacy loss.\nTitle:\nAdaptive transformation for robust privacy protection in video surveillance\n\nAbstract:\nLarge-scale multimedia surveillance installations usually consist of a number of spatially distributed video cameras that are installed in a premise and are connected to a central control station, where human operators (e.g., security personnel) remotely monitor the scene images captured by the cameras. In the majority of these systems the ratio of human operators to the number of camera views is very low. This potentially raises the problem that some important events may be missed. Studies have shown that a human operator can effectively monitor only four camera views. Moreover, the visual attention of human operator drops below the acceptable level while performing the task of visual monitoring. Therefore, there is a need for the selection of the four most relevant camera views at a given time instant. This paper proposes a human-centric approach to solve the problem of dynamically selecting and scheduling the four best camera views. In the proposed approach we use a feedback camera to observe the human monitoring the surveillance camera feeds. Using this information, the system computes the operator's attention to the camera views to automatically determine the importance of events being captured by the respective cameras. This real-time non-invasive relevance feedback is then augmented with the automatic detection of events to compute the four best feeds. The experiments show the effectiveness of the proposed approach by improving the identification of important events occurring in the environment.\nTitle:\nEffective multimedia surveillance using a human-centric approach\n\nAbstract:\nToday digital video is used extensively in many applications. Sometimes a video could be treated as a top secret for an organization, for example military secrets, surveillance footage and corporate product designs, and may need to be shared among a group of people in a secure manner. Traditional data security methods such as encryption techniques are prone to single-point attack, i.e. the secret can be revealed by obtaining the decryption key from any single person. Alternatively, the secret sharing scheme provides collective control over the secrecy of information and is considered information theoretically secure. In this paper, we propose to adopt a secret sharing based approach to provide collective control over a given sensitive video. We present three methods that utilize the spatial and temporal redundancy in videos in different ways. We analyze the security of these methods and compare them for efficiency in terms of computation time and space using extensive experimentation.\nTitle:\nCollective control over sensitive video data using secret sharing\n\nAbstract:\nThis paper addresses the problem of ensuring the integrity of a digital video and presents a scalable signature scheme for video authentication based on cryptographic secret sharing. The proposed method detects spatial cropping and temporal jittering in a video, yet is robust against frame dropping in the streaming video scenario. In our scheme, the authentication signature is compact and independent of the size of the video. Given a video, we identify the key frames based on differential energy between the frames. Considering video frames as shares, we compute the corresponding secret at three hierarchical levels. The master secret is used as digital signature to authenticate the video. The proposed signature scheme is scalable to three hierarchical levels of signature computation based on the needs of different scenarios. We provide extensive experimental results to show the utility of our technique in three different scenarios--streaming video, video identification and face tampering.\nTitle:\nA scalable signature scheme for video authentication\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.463, mean value for all the writers is 8.648, which makes it 0.158 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nVideo cameras are being extensively used in many applications. Huge amounts of video are being recorded and stored everyday by surveillance systems. Any proposed application of this data raises severe privacy concerns. An assessment of privacy loss is necessary before any potential application of the data. In traditional methods of privacy modeling, researchers have focused on explicit means of identity leakage like facial information, etc. However, other implicit inference channels through which individual's an identity can be learned have not been considered. For example, an adversary can observe the behavior, look at the places visited and combine that with the temporal information to infer the identity of the person in the video. In this work, we thoroughly investigate privacy issues involved with the video data considering both implicit and explicit channels. We first establish an analogy with the statistical databases and then propose a model to calculate the privacy loss that might occur due to publication of the video data. The experimental results demonstrate the utility of the proposed model.\nTitle:", "model_inf_time": 1.28}, {"id": "4190", "output": "Point-Sampled 3D Video Acquisition and Rendering", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we present a scalable 3D video framework for capturing and rendering dynamic scenes. The acquisition system is based on multiple sparsely placed 3D video bricks, each comprising a projector, two grayscale cameras and a color camera. Relying on structured light with complementary patterns, texture images and pattern- augmented views of the scene are acquired simultaneously by time multiplexed projections and synchronized camera exposures. Using space-time stereo on the acquired pat- tern images, high-quality depth maps are extracted, whose corresponding surface samples are merged into a view- independent, point-based 3D data structure. This representa- tion allows for effective photo consistency enforcement and outlier removal, leading to a signicant decrease of visual artifacts and a high resulting rendering quality using EWA volume splatting. Our framework and its view-independent representation allow for simple and straightforward editing of 3D video. In order to demonstrate its e xibility, we show compositing techniques and spatio-temporal effects.\nTitle:\nScalable 3D video of dynamic scenes\n\nAbstract:\nWe present 3D video fragments, a dynamic point sample framework for real-time free-viewpoint video. By generalizing 2D video pixels towards 3D irregular point samples we combine the simplicity of conventional 2D video processing with the power of more complex polygonal representations for free-viewpoint video. We propose a differential update scheme exploiting the spatio-temporal coherence of the video streams of multiple cameras. Updates are issued by operators such as inserts and deletes accounting for changes in the input video images. The operators from multiple cameras are processed, merged into a 3D video stream and transmitted to a remote site. We also introduce a novel concept for camera control which dynamically selects the set of relevant cameras for reconstruction. Moreover, it adapts to the processing load and rendering platform. Our framework is generic in the sense that it works with any real-time 3D reconstruction method which extracts depth from images. The video renderer displays free-viewpoint videos using an efficient point-based splatting scheme and makes use of state-of-the-art vertex and pixel processing hardware for real-time visual processing.\nTitle:\n3D video fragments: dynamic point samples for real-time free-viewpoint video\n\nAbstract:\nIn this paper: we present a coding framework addressing image-space compression for free-viewpoint video. Our framework is bused on time-varying 3D point samples which represent real-world objects. The 3D point samples are obtained after a geometrical rcconstruction from multiple pre-recorded video sequences and thus allow for arbitrary viewpoints daring playback. The encoding of the data is performed as an off-line process and is not time-critical. The decoding however, must support for real-time rendering of the dynamic 3D data. We introduce a compression framework which encodes multiple point attributes like depth and color into progressive streams. The reference data structure is aligned on the original camera input images and thus enables for easy view-dependent decoding. A novel differential coding approach permits random access in constant time throughout the entire data set and thus enables arbitrary viewpoint trajectories in both time and space.\nTitle:\nUnconstrained Free-Viewpoint Video Coding\n\nAbstract:\nMulti-view reconstruction aims at computing the geometry of a scene observed by a set of cameras. Accurate 3D reconstruction of dynamic scenes is a key component for a large variety of applications, ranging from special effects to telepresence and medical imaging. In this paper we propose a method based on Moving Least Squares surfaces which robustly and efficiently reconstructs dynamic scenes captured by a calibrated set of hybrid color+depth cameras. Our reconstruction provides spatio-temporal consistency and seamlessly fuses color and geometric information. We illustrate our approach on a variety of real sequences and demonstrate that it favorably compares to state-of-the-art methods.\nTitle:\nSpatio-temporal geometry fusion for multiple hybrid cameras using moving least squares surfaces\n\nAbstract:\n3D video billboard clouds reconstruct and represent a dynamic three-dimensional scene using displacement-mapped billboards. They consist of geometric proxy planes augmented with detailed displacement maps and combine the generality of geometry-based 3D video with the regularization properties of image-based 3D video. 3D video billboards are an iinage-based representation placed in the disparity space of the acquisition cameras and thus provide a regular sampling of the scene with a uniform error model. We propose a general geometry filtering,framework which generates time-coherent models and removes reconstruction and quantization noise as well as calibration errors. This replaces the complex and time-consuming sub-pixel matching process in stereo reconstruction with a bilateral filter Rendering is performed using a GPU-accelerated algorithm which generates consistent view-dependent geometry and textures for each individual frame. In addition, we present a semi-automatic approach for modeling dynamic three-dimensional scenes with a set of multiple 3D video billboards clouds.\nTitle:\n3d Video Billboard Clouds\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.045, mean value for all the writers is 8.648, which makes it 1.368 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents a point-sampled approach for capturing 3D video footage and subsequent re-rendering of real-world scenes. The acquisition system is composed of multiple sparsely placed 3D video bricks. The bricks contain a low-cost projector, two grayscale cameras and a high-resolution color camera. To improve on depth calculation we rely on structured light patterns. Texture images and pattern-augmented views of the scene are acquired simultaneously by time multiplexed projections of complementary patterns and synchronized camera exposures. High-resolution depth maps are extracted using depth-from-stereo algorithms performed on the acquired pattern images. The surface samples corresponding to the depth values are merged into a view-independent, point-based 3D data structure. This representation allows for efficient post-processing algorithms and leads to a high resulting rendering quality using enhanced probabilistic EWA volume splatting. In this paper, we focus on the 3D video acquisition system and necessary image and video processing techniques.\nTitle:", "model_inf_time": 1.37}, {"id": "4191", "output": "Cardiac Beat Observation from 3D Body Surface Shape Using Active Stereo with Waved-Grid Pattern Projection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper describes a method for automatic detection of contractions in the small bowel through analyzing Wireless Capsule Endoscopic images. Based on the characteristics of contraction images, a coherent procedure that includes analyzes of the temporal and spatial features is proposed. For temporal features, the image sequence is examined to detect candidate contractions through the changing number of edges and an evaluation of similarities between the frames of each possible contraction to eliminate cases of low probability. For spatial features, descriptions of the directions at the edge pixels are used to determine contractions utilizing a classification method. The experimental results show the effectiveness of our method that can detect a total of 83% of cases. Thus, this is a feasible method for developing tools to assist in diagnostic procedures in the small bowel.\nTitle:\nContraction detection in small bowel from an image sequence of wireless capsule endoscopy.\n\nAbstract:\nWe propose a method which refines the range measurement of range finders by computing correspondences of vertices of multiple range images acquired from various viewpoints. Our method assumes that a range image acquired by a laser rangefinder has anisotropic error distribution which is parallel to the ray direction. Thus, we find the corresponding points of range images along with the ray direction. We iteratively converge range images to minimize the distance of corresponding points. We demonstrate the effectiveness of our method by presenting the experimental results of artificial and real range data. Also, we show that our method refines a 3D shape more accurately as opposed to that achieved by using the Gaussian filter.\nTitle:\nIterative refinement of range images with anisotropic error distribution\n\nAbstract:\nRecognizing intestinal contractions from wireless capsule endoscopy (WCE) image sequences provides a non-invasive method of measurement, and suggests a solution to the problems of traditional techniques for assessing intestinal motility. Based on the characteristics of contractile patterns and information on their frequencies, the contractions can be investigated using essential image features extracted from WCE videos. In this study, we proposed a coherent three-stage procedure using temporal and spatial features. The possible contractions are recognized by changes in the edge structure of the intestinal folds in Stage 1 and evaluating similarity features in consecutive frames in Stage 2. In order to take account of the properties of contraction frequency, we consider that the possible contractions are located within windows including consecutive frames. The size of these contraction windows is adjusted according to the passage of the WCE. These procedures aim to exclude as many non-contractions as possible. True contractions are determined through spatial analysis of directional information in Stage 3. Using the proposed method, 81% of true contractions are detected with a 37% false alarm rate for evaluations in the experiments. The overall performance of this method is better than that of previous methods, in terms of both the quality and quantity indices. The results suggest feasible data for further clinical applications.\nTitle:\nDetection of contractions in adaptive transit time of the small bowel from wireless capsule endoscopy videos.\n\nAbstract:\nIn this paper, we propose a method to reconstruct the shapes of moving objects. The proposed method is a projector-camera system that reconstructs a shape from a single image where a static pattern is cast by a projector, such a method is ideal for acquisition of moving objects at a high frame rate. The issues tackled in this paper are as follows: 1) realize one-shot 3D reconstruction with a single-colored pattern, and 2) obtain accurate shapes by finding correspondences in sub-pixel accuracy. To achieve these goals, we propose the following methods: 1) implicit encoding of projector information by a grid of wave lines, 2) grid-based stereo between projector pattern and camera images to determine unique correspondences, 3) (quasi-)pixel-wise interpolations and optimizations to reconstruct dense shapes, and 4) a single-colored pattern contributes to simplify pattern projecting devices compared to color-coded methods. In the experiment, we show the proposed method is efficient to solve the issues above.\nTitle:\nGrid-Based Active Stereo with Single-Colored Wave Pattern for Dense One-shot 3D Scan\n\nAbstract:\nDense 3D reconstruction of fast moving objects could contribute to various applications such as body structure analysis, accident avoidance, and so on. In this paper, we propose a technique based on a one-shot scanning method, which reconstructs 3D shapes for each frame of a high frame-rate video capturing the scenes projected by a static pattern. To avoid instability of image processing, we restrict the number of colors used in the pattern to less than two. The proposed technique comprises (1) an efficient algorithm to eliminate ambiguity of projected parallel-line patterns by using intersection points, (2) a batch reconstruction algorithm of multiple frames by using spatio-temporal constraints, and (3) an efficient detection method of color-encoded grid pattern based on de Bruijn sequence. In the experiments, the line detection algorithm worked effectively and the dense reconstruction algorithm produces accurate and robust results. We also show the improved results by using temporal constraints. Finally, the dense reconstructions of fast moving objects in a high frame-rate video are presented.\nTitle:\nDense 3D Reconstruction from High Frame-Rate Video Using a Static Grid Pattern\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.946, mean value for all the writers is 8.648, which makes it 1.961 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose a method to observe cardiac beat from 3D shape information of body surface by using the active stereo with waved-grid pattern projection, and report preliminary experiments to evaluate validities of the proposed method. By comparing results of our method with those of electrocardiogram (ECG), we confirmed sufficient correspondences between peak intervals of depth changes between contiguous frames measured by the active stereo and R-R intervals measured by ECG. We proposed the visualization of the spatial distribution of depth change plotted on the 3D shape of chest surface. We confirm that the spatial phase difference, which is caused by heart pump ability, appears in the 3-D shape change of chest surface.\nTitle:", "model_inf_time": 1.84}, {"id": "4192", "output": "Ontology-Based Competitive Intelligence Mining in Neuroscience", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAutomated discovery and extraction of biological relations from online documents, particularly MEDLINE texts, has become essential and urgent because such literature data are accumulated in a tremendous growth. In this paper, we present an ontology-based framework of biological relation extraction system. This framework is unified and able to extract several kinds of relations such as gene-disease, gene-gene, and protein-protein interactions etc. The main contributions of this paper are that we propose a two-level pattern learning algorithm and organize patterns hierarchically.\nTitle:\nONBRIRES: Ontology-Based Biological Relation Extraction System\n\nAbstract:\nIn this paper we present an extractive system that automatically generates gene summaries from the biomedical literature. The proposed text summarization system selects and ranks sentences from multiple MEDLINE abstracts by exploiting gene-specific information and similarity relationships between sentences. We evaluate our system on a large dataset of 7,294 human genes and 187,628 MEDLINE abstracts using Recall-Oriented Understudy for Gisting Evaluation (ROUGE), a widely used automatic evaluation metric in the text summarization community. Two baseline methods are used for comparison. Experimental results show that our system significantly outperforms the other two methods with regard to all ROUGE metrics. A demo website of our system is freely accessible at http://60.195.250.72/onbires/summary.jsp.\nTitle:\nTowards automatic generation of gene summary\n\nAbstract:\nPatents are critical for a company to protect its core technologies. Effective patent mining in massive patent databases can provide companies with valuable insights to develop strategies for IP management and marketing. In this paper, we study a novel patent mining problem of automatically discovering core patents (i.e., patents with high novelty and influence in a domain). We address the unique patent vocabulary usage problem, which is not considered in traditional word-based statistical methods, and propose a topic-based temporal mining approach to quantify a patent's novelty and influence. Comprehensive experimental results on real-world patent portfolios show the effectiveness of our method.\nTitle:\nFinding nuggets in IP portfolios: core patent mining through textual temporal analysis\n\nAbstract:\nPatenting is one of the most important ways to protect company's core business concepts and proprietary technologies. Analyzing large volume of patent data can uncover the potential competitive or collaborative relations among companies in certain areas, which can provide valuable information to develop strategies for intellectual property (IP), R&D, and marketing. In this paper, we present a novel topic-driven patent analysis and mining system. Instead of merely searching over patent content, we focus on studying the heterogeneous patent network derived from the patent database, which is represented by several types of objects (companies, inventors, and technical content) jointly evolving over time. We design and implement a general topic-driven framework for analyzing and mining the heterogeneous patent network. Specifically, we propose a dynamic probabilistic model to characterize the topical evolution of these objects within the patent network. Based on this modeling framework, we derive several patent analytics tools that can be directly used for IP and R&D strategy planning, including a heterogeneous network co-ranking method, a topic-level competitor evolution analysis algorithm, and a method to summarize the search results. We evaluate the proposed methods on a real-world patent database. The experimental results show that the proposed techniques clearly outperform the corresponding baseline methods.\nTitle:\nPatentMiner: topic-driven patent analysis and mining\n\nAbstract:\nMulti-relation Question Answering is a challenging task, due to the requirement of elaborated analysis on questions and reasoning over multiple fact triples in knowledge base. In this paper, we present a novel model called Interpretable Reasoning Network that employs an interpretable, hop-by-hop reasoning process for question answering. The model dynamically decides which part of an input question should be analyzed at each hop; predicts a relation that corresponds to the current parsed results; utilizes the predicted relation to update the question representation and the state of the reasoning process; and then drives the next-hop reasoning. Experiments show that our model yields state-of-the-art results on two datasets. More interestingly, the model can offer traceable and observable intermediate predictions for reasoning analysis and failure diagnosis.\nTitle:\nAn Interpretable Reasoning Network for Multi-Relation Question Answering.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.661, mean value for all the writers is 8.648, which makes it 0.011 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nNeuroscience becomes a strategic growing field for both academic institutions and industrial companies because of their profound impact on human health, clinical therapy, and basic research such as brain informatics and cognitive science. To track activities of the rapidly developing field and observe new trends, neuroscientists must keep up-to-date with all the relevant information on the internet. In this paper, we present an ontology-based mining system, which is able to probe Competitive Intelligence in Neuroscience by using a well-defined ontology. It is able to support decision making by searching neuroscientific discoveries semantically and tracking new trends statistically. The experiments performed on 15,433,668 MEDLINE articles yield evidence for the feasibility and validity of our system.\nTitle:", "model_inf_time": 1.26}, {"id": "4193", "output": "A Generative Probabilistic Model for Multi-Label Classification with Correlated Labels", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nExisting relation classification methods that rely on distant supervision assume that a bag of sentences mentioning an entity pair are all describing a relation for the entity pair. Such methods, performing classification at the bag level, cannot identify the mapping between a relation and a sentence, and largely suffers from the noisy labeling problem. in this paper, we propose a novel model for relation classification at the sentence level from noisy data. The model has two modules: an instance selector and a relation classifier. The instance selector chooses high-quality sentences with reinforcement learning and feeds the selected sentences into the relation classifier, and the relation classifier makes sentence level prediction and provides rewards to the instance selector. The two modules are trained jointly to optimize the instance selection and relation classification processes. Experiment results show that our model can deal with the noise of data effectively and obtains better performance for relation classification at the sentence level.\nTitle:\nReinforcement Learning for Relation Classification From Noisy Data.\n\nAbstract:\nMultinomial Naive Bayes with Expectation Maximization (MNB-EM) is a standard semi-supervised learning method to augment Multinomial Naive Bayes (MNB) for text classification. Despite its success, MNB-EM is not stable, and may succeed or fail to improve MNB. We believe that this is because MNB-EM lacks the ability to preserve the class distribution on words. In this paper, we propose a novel method to augment MNB-EM by leveraging the word-level statistical constraint to preserve the class distribution on words. The word-level statistical constraints are further converted to constraints on document posteriors generated by MNB-EM. Experiments demonstrate that our method can consistently improve MNB-EM, and outperforms state-of-art baselines remarkably.\nTitle:\nSemi-Supervised Multinomial Naive Bayes for Text Classification by Leveraging Word-Level Statistical Constraint.\n\nAbstract:\nAbstract Picking up appropriate classification algorithms for a given data set is very important and useful in practice. One of the most challenging issues for algorithm selection is how to characterize different data sets. Recently, we extracted the structural information of a data set to characterize itself. Although these kinds of characteristics work well in identifying similar data sets and recommending appropriate classification algorithms, the extraction method can only be applied to binary data sets and its performance is not high. Thus, in this paper, an improved data set characterization method is proposed to address these problems. For the purpose of evaluating the effectiveness of the improved method on algorithm recommendation, the unsupervised learning method EM is employed to build the algorithm recommendation model. Extensive experiments with 17 different types of classification algorithms are conducted upon 84 public UCI data sets; the results demonstrate the effectiveness of the proposed method.\nTitle:\nAn improved data characterization method and its application in classification algorithm recommendation\n\nAbstract:\nThe class imbalance problems have been reported to severely hinder classification performance of many standard learning algorithms, and have attracted a great deal of attention from researchers of different fields. Therefore, a number of methods, such as sampling methods, cost-sensitive learning methods, and bagging and boosting based ensemble methods, have been proposed to solve these problems. However, these conventional class imbalance handling methods might suffer from the loss of potentially useful information, unexpected mistakes or increasing the likelihood of overfitting because they may alter the original data distribution. Thus we propose a novel ensemble method, which firstly converts an imbalanced data set into multiple balanced ones and then builds a number of classifiers on these multiple data with a specific classification algorithm. Finally, the classification results of these classifiers for new data are combined by a specific ensemble rule. In the empirical study, different class imbalance data handling methods including three conventional sampling methods, one cost-sensitive learning method, six Bagging and Boosting based ensemble methods, our previous method EM1vs1 and two fuzzy-rule based classification methods were compared with our method. The experimental results on 46 imbalanced data sets show that our proposed method is usually superior to the conventional imbalance data handling methods when solving the highly imbalanced problems. HighlightsWe propose a novel ensemble method to handle imbalanced binary data.The method turns imbalanced data learning into multiple balanced data learning.Our method usually performs better than the conventional methods on imbalanced data.\nTitle:\nA novel ensemble method for classifying imbalanced data\n\nAbstract:\nTraditional sentiment analysis mainly considers binary classifications of reviews, but in many real-world sentiment classification problems, non-binary review ratings are more useful. This is especially true when consumers wish to compare two products, both of which are not negative. Previous work has addressed this problem by extracting various features from the review text for learning a predictor. Since the same word may have different sentiment effects when used by different reviewers on different products, we argue that it is necessary to model such reviewer and product dependent effects in order to predict review ratings more accurately. In this paper, we propose a novel learning framework to incorporate reviewer and product information into the text based learner for rating prediction. The reviewer, product and text features are modeled as a three-dimension tensor. Tensor factorization techniques can then be employed to reduce the data sparsity problems. We perform extensive experiments to demonstrate the effectiveness of our model, which has a significant improvement compared to state of the art methods, especially for reviews with unpopular products and inactive reviewers.\nTitle:\nIncorporating reviewer and product information for review rating prediction\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.545, mean value for all the writers is 8.648, which makes it 0.088 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTraditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces. In the scenario of multi-label classification, most of the classifiers simply assume the predefined classes are independently distributed, which would definitely hinder the classification performance when there are intrinsic correlations between the classes. In this article, we propose a generative probabilistic model, the Correlated Labeling Model (CoL Model), to formulate the correlation between different classes. The CoL model is presented to capture the correlation between classes and the underlying structures via the latent random variables in a supervised manner. We develop a variational procedure to approximate the posterior distribution and employ the EM algorithm for the empirical Bayes parameter estimation. In our evaluations, the proposed model achieved promising results on various data sets.\nTitle:", "model_inf_time": 1.66}, {"id": "4194", "output": "Evolving Neuro-developmental Programs for Learning Checkers", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nBiological neurons are extremely complex cells whose morphology grows and changes in response to the external environment. Yet, artificial neural networks (ANNs) have represented neurons as simple computational devices. It has been evident for a long time that ANNs have learning abilities that are insignificant compared with some of the simplest biological brains. We argue that we understand enough neuroscience to create much more sophisticated models. In this paper, we report on our attempts to do this.We identify and evolve seven programs that together represents a neuron which grows post evolution into a complete 'neurological' system. The network that occurs by running the programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change. We have evaluated the capability of these networks for playing the game of checkers. Our method has no board evaluation function, no explicit learning rules and no human expertise at playing checkers is used. The learning abilities of these networks are encoded at a genetic level rather than at the phenotype level of neural connections.\nTitle:\nIn search of intelligent genes: the cartesian genetic programming computational neuron (CGPCN)\n\nAbstract:\nThis paper presents a method for co-evolving neuro-inspired developmental programs for playing checkers. Each player's program is represented by seven chromosomes encoding digital circuits, using a form of genetic programming, called Cartesian Genetic Programming (CGP). The neural network that occurs by running the genetic programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to situations encountered on the checkers board. The results show that, after a number of generations, by playing each other the agents play much better than those from earlier generations. Such learning abilities are encoded at a geneticlevel rather than at the phenotype level of neural connections.\nTitle:\nCoevolution of Neuro-developmental Programs That Play Checkers\n\nAbstract:\nAlthough artificial neural networks have taken their inspiration from natural neurological systems, they have largely ignored the genetic basis of neural functions. Indeed, evolutionary approaches have mainly assumed that neural learning is associated with the adjustment of synaptic weights. The goal of this paper is to use evolutionary approaches to find suitable computational functions that are analogous to natural sub-components of biological neurons and demonstrate that intelligent behavior can be produced as a result of this additional biological plausibility. Our model allows neurons, dendrites, and axon branches to grow or die so that synaptic morphology can change and affect information processing while solving a computational problem. The compartmental model of a neuron consists of a collection of seven chromosomes encoding distinct computational functions inside the neuron. Since the equivalent computational functions of neural components are very complex and in some cases unknown, we have used a form of genetic programming known as Cartesian genetic programming (CGP) to obtain these functions. We start with a small random network of soma, dendrites, and neurites that develops during problem solving by repeatedly executing the seven chromosomal programs that have been found by evolution. We have evaluated the learning potential of this system in the context of a well-known single agent learning problem, known as Wumpus World. We also examined the harder problem of learning in a competitive environment for two antagonistic agents, in which both agents are controlled by independent CGP computational networks (CGPCN). Our results show that the agents exhibit interesting learning capabilities.\nTitle:\nEvolution of Cartesian Genetic Programs for Development of Learning Neural Architecture\n\nAbstract:\nThe majority of artificial neural networks are static and lifeless and do not change themselves within a learning environment. In these models learning is seen as the process of obtaining the strengths of connections between neurons (i.e. weights). We refer to this as the 'synaptic dogma'. This is in marked contrast with biological networks which have time dependent morphology and in which practically all neural aspects can change or be shaped by mutual interactions and interactions with an external environment. Inspired by this and many aspects of neuroscience, we have designed a new kind of neural network. In this model, neurons are represented by seven evolved programs that model particular components and aspects of biological neurons (dendrites, soma, axons, synapses, electrical and developmental behaviour). Each network begins as a small randomly generated network of neurons. When the seven programs are run, the neurons, dendrites, axons and synapses can increase or decrease in number and change in interaction with an external environment. Our aim is to show that it is possible to evolve programs that allow a network to learn through experience (i.e. encode the ability to learn). We report on our continuing investigations in the context of learning how to play checkers.\nTitle:\nBreaking the Synaptic Dogma: Evolving a Neuro-inspired Developmental Network\n\nAbstract:\nThe brain has long been seen as a powerful analogy from which novel computational techniques could be devised. However, most artificial neural network approaches have ignored the genetic basis of neural functions. In this paper we describe a radically different approach. We have devised a compartmental model of a neuron as a collection of seven chromosomes encoding distinct computational functions representing aspects of real neurons. This model allows neurons, dendrites, and axon branches to grow, die and change while solving a computational problem. This also causes the synaptic morphology to change and affect the information processing. Since the appropriate computational equivalent functions of neural computation are unknown, we have used a form of genetic programming known as Cartesian Genetic Programming (CGP) to obtain these functions. We have evaluated the learning potential of this system in the context of solving a well known agent based learning scenario, known as wumpus world and obtained promising results.\nTitle:\nA developmental model of neural computation using cartesian genetic programming\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.466, mean value for all the writers is 8.648, which makes it 0.698 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose a new form of Cartesian Genetic Programming (CGP) that develops into a computational network capable of learning. The developed network architecture is inspired by the brain. When the genetically encoded programs are run, a networks develops consisting of neurons, dendrites, axons, and synapses which can grow, change or die. We have tested this approach on the task of learning how to play checkers. The novelty of the research lies mainly in two aspects: Firstly, chromosomes are evolved that encode programs rather than the network directly and when these programs are executed they build networks which appear to be capable of learning and improving their performance over time solely through interaction with the environment. Secondly, we show that we can obtain learning programs much quicker through co-evolution in comparison to the evolution of agents against a minimax based checkers program. Also, co-evolved agents show significantly increased learning capabilities compared to those that were evolved to play against a minimax-based opponent.\nTitle:", "model_inf_time": 1.48}, {"id": "4195", "output": "SIM-RSO-CCA Secure Public Key Encryption from Standard Assumptions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present novel constructions of chosen-ciphertext secure (CCA secure) key encapsulation mechanism (KEM) from chosen-plaintext secure (CPA secure) KEM in the standard model. It is already known that CCA secure public key encryption (PKE) can be generically constructed from CPA secure PKE and ((simulation-sound) non-interactive zero-knowledge proof) via the Naor-Yung or Dolev-Dwork-Naor transforms. Thus, one can also immediately construct CCA secure PKE from CPA secure KEM by converting CPA secure KEM into CPA secure PKE and transforming it to be CCA secure PKE. However, such a construction seems redundant since in general PKE is less efficient than KEM and it would be more efficient if we can directly construct CCA secure KEM from CPA secure KEM without intermediating CPA secure PKE. In this work, we propose new variants of the Naor-Yung and Dolev-Dwork-Naor transforms that directly convert CPA secure KEM into CCA secure KEM, and show that our proposed schemes are more efficient than the above straightforward constructions. For example, when instantiating from the decision linear assumption, ciphertext size of our Naor-Yung variant consists of 34 group elements while that of the straightforward construction consists of 47 group elements. Furthermore, we also propose another variant of the Dolev-Dwork-Naor transform from multiple KEM and show that a KEM which is obtained from Wee's extractable hash proof system can also be considered as an efficient construction of multiple KEM.\nTitle:\nEfficient variants of the Naor-Yung and Dolev-Dwork-Naor transforms for CCA secure key encapsulation mechanism\n\nAbstract:\nWhether it is possible to construct a chosen ciphertext secure (CCA secure) public key encryption (PKE) scheme only from a chosen plaintext secure (CPA secure) one is a fundamental open problem, and the best known positive results regarding this problem are the constructions of so-called bounded CCA secure schemes. Since we can achieve the best possible security in the bounded CCA security notions, in order to further tackle the problem, we would need other new security notions that capture intermediate security notions that lie between CPA and CCA security. Motivated by this situation, we focus on \"parallel\" decryption queries (originally introduced by Bellare and Sahai) for the extension of bounded CCA security, and introduce a new security notion which we call mixed CCA security. It captures security against adversaries that make single and parallel decryption queries in a predetermined order, where each parallel query can contain unboundedly many ciphertexts. Moreover, how the decryption oracle is available before and after the challenge is also taken into account in this new security definition, which enables us to capture existing major security notions that lie between CPA and CCA security in a unified security notion. We investigate the relations among mixed CCA security notions, and show a necessary and sufficient condition of implications/separations between any two notions in mixed CCA security. We also show two black-box constructions of PKE schemes with improved security only using CPA secure schemes as building blocks.\nTitle:\nParallel decryption queries in bounded chosen ciphertext attacks\n\nAbstract:\nIn this paper, we show two new constructions of chosen ciphertext secure (CCA secure) public key encryption (PKE) from general assumptions. The key ingredient in our constructions is an obfuscator for point functions with multi-bit output (MBPF obfuscators, for short), that satisfies some (average-case) indistinguishability-based security, which we call AIND security, in the presence of hard-to-invert auxiliary input. Specifically, our first construction is based on a chosen plaintext secure PKE scheme and an MBPF obfuscator satisfying the AIND security in the presence of computationally hard-to-invert auxiliary input. Our second construction is based on a lossy encryption scheme and an MBPF obfuscator satisfying the AIND security in the presence of statistically hard-to-invert auxiliary input. To clarify the relative strength of AIND security, we show the relations among security notions for MBPF obfuscators, and show that AIND security with computationally (resp. statistically) hard-to-invert auxiliary input is implied by the average-case virtual black-box (resp. virtual grey-box) property with the same type of auxiliary input. Finally, we show that a lossy encryption scheme can be constructed from an obfuscator for point functions (point obfuscator) that satisfies re-randomizability and a weak form of composability in the worst-case virtual grey-box sense. This result, combined with our second generic construction and several previous results on point obfuscators and MBPF obfuscators, yields a CCA secure PKE scheme that is constructed solely from a re-randomizable and composable point obfuscator. We believe that our results make an interesting bridge that connects CCA secure PKE and program obfuscators, two seemingly isolated but important cryptographic primitives in the area of cryptography.\nTitle:\nChosen Ciphertext Security via Point Obfuscation.\n\nAbstract:\nIn this paper, we present the first generic construction of a chosen-ciphertext (CCA) secure uni-directional proxy re-encryption (PRE) scheme. In particular, full CCA security (i.e., not relaxed CCA security such as replayable CCA security) of our proposed scheme is proven even against powerful adversaries that are given a more advantageous attack environment than in all previous works, and furthermore, random oracles are not required. To achieve such strong security, we establish a totally novel methodology for designing PRE based on a specific class of threshold encryption. Via our generic construction, we present the first construction that is CCA secure in the standard model.\nTitle:\nGeneric construction of chosen ciphertext secure proxy re-encryption\n\nAbstract:\nIn this paper, we introduce and study a new cryptographic primitive that we call puncturable key encapsulation mechanism (PKEM), which is a special class of KEMs that satisfy some functional and security requirements that, combined together, imply chosen ciphertext security (CCA security). The purpose of introducing this primitive is to capture certain common patterns in the security proofs of the several existing CCA secure public key encryption (PKE) schemes and KEMs based on general cryptographic primitives which (explicitly or implicitly) use the ideas and techniques of the Dolev-Dwork-Naor (DDN) construction (STOC'91), and \"break down\" the proofs into smaller steps, so that each small step is easier to work with/verify/understand than directly tackling CCA security. To see the usefulness of PKEM, we show(1) how several existing constructions of CCA secure PKE/KEMconstructed based on general cryptographic primitives can be captured as a PKEM, which enables us to understand these constructions via a unified framework, (2) their connection to detectable CCA security (Hohenberger et al. EUROCRYPT'12), and (3) a new security proof for a KEM-analogue of the DDN construction from a set of assumptions: sender non-committing encryption (SNCE) and non-interactive witness indistinguishable proofs. Then, as our main technical result, we show how to construct a PKEM satisfying our requirements (and thus a CCA secure KEM) from a new set of general cryptographic primitives: SNCE and symmetric key encryption secure for key-dependent messages (KDM secure SKE). Our construction realizes the \"decryptthen-re-encrypt\"-style validity check of a ciphertext which is powerful but in general has a problem of the circularity between a plaintext and a randomness. We show how SNCE and KDMsecure SKE can be used together to overcome the circularity. We believe that the connection among three seemingly unrelated notions of encryption primitives, i.e. CCA security, the sender non-committing property, and KDM security, to be of theoretical interest.\nTitle:\nConstructing and Understanding Chosen Ciphertext Security via Puncturable Key Encapsulation Mechanisms.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.133, mean value for all the writers is 8.648, which makes it 0.414 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the situation where there are one sender and multiple receivers and the sender transmits ciphertexts of correlated plaintexts, a receiver selective opening (RSO) attack for a public key encryption (PKE) scheme considers adversaries that can corrupt some of the receivers and get their secret keys and plaintexts. Security against RSO attacks for a PKE scheme ensures confidentiality of ciphertexts of uncorrupted receivers. Simulation-based RSO security against chosen ciphertext attacks (SIM-RSO-CCA) is the strongest security notion in all RSO attack scenarios. Jia, Lu, and Li (2016) [19] proposed the first SIM-RSO-CCA secure PKE scheme. However, their scheme used indistinguishability obfuscation, which is not known to be constructed from any standard computational assumption. In this paper, we give two contributions for constructing SIM-RSO-CCA secure PKE from standard computational assumptions. Firstly, we propose a generic construction of SIM-RSO-CCA secure PKE using an IND-CPA secure PKE scheme and a non-interactive zero-knowledge proof system satisfying one-time simulation soundness. Secondly, we propose an efficient and concrete construction of SIM-RSO-CCA secure PKE based on the decisional Diffie-Hellman (DDH) assumption. Moreover, we give a method for efficiently expanding the plaintext space of the DDH-based construction. By applying this method to the construction, we obtain the first DDH-based SIM-RSO-CCA secure PKE scheme supporting a super-polynomially large plaintext space with compact ciphertexts.\nTitle:", "model_inf_time": 1.99}, {"id": "4196", "output": "Modeling and Routing Control in Realistic ISP Topologies", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMeasurement studies on the Internet topology show that connectivities of nodes exhibit power---law attribute, but it is apparent that only the degree distribution does not determine the network structure, and especially true when we study the network---related control like routing control. In this paper, we first reveal structures of the router---level topologies using the working ISP networks, which clearly indicates ISP topologies are highly clustered; a node connects two or more nodes that also connected each other, while not in the existing modeling approaches. Based on this observation, we develop a new realistic modeling method for generating router---level topologies. In our method, when a new node joins the network, the node likely connects to the nearest nodes. In addition, we add the new links based on the node utilization in the topology, which corresponds to an enhancement of network equipments in ISP networks. With appropriate parameters, important metrics, such as the a cluster coefficient and the number of node-pairs that pass through nodes, exhibit the similar value of the actual ISP topology while keeping the degree distribution of resulting topology to follow power---law.\nTitle:\nAnalyzing and Modeling Router---Level Internet Topology\n\nAbstract:\nIn the Internet, a statistical perspective of global traffic flows has been considered as an important key to network management. Nonetheless, it is expensive or sometime difficult to measure statistics of each flow separately. Therefore, it is of practical importance to infer unobservable statistical characteristics of individual flows from characteristics of the aggregated-flows that are easily measured at some links (router interfaces) in the network. In this paper, we propose a new approach to such inference problems, and provide some examples of inferring unobservable arrival rates of packets on each flow from measurement of the aggregated-flows. Our method is applicable to cases not covered by the existing methods for the OD traffic matrix inference. We also show simulation results, which indicate potential of our approach.\nTitle:\nInferring Traffic Flow Characteristics from Aggregated-Flow Measurement\n\nAbstract:\nIn the Internet, because of huge scale and distributed administration, it is of practical importance to infer network-internal characteristics that cannot be measured directly. We propose a general method of determining characteristics of links from given characteristics of end-to-end paths. Our method can be applied to an arbitrary path-topology. Furthermore we show the general conditions that the link characteristics to be inferred must satisfy. Packet loss and queuing delay time are shown to satisfy them. Case studies which our method can treat are also provided\nTitle:\nInferring link characteristics from end-to-end path measurements\n\nAbstract:\nIn traffic engineering (TE), it is vital to take traffic characteristics of the flows into account in appropriately assigning the flows to multiple network paths to achieve better delay performance as a whole in order to effectively distribute traffic flows over the paths. This paper presents a novel traffic characteristic-aware flow assignment method to reduce the queuing delay in a fundamental case where two types of flows with distinct traffic characteristics (e.g., burstiness) are distributed into two paths. First, we extensively analyze the queuing delays in assigning flows in the manner of various combinations of flows in terms of minimizing the worst queuing delay among two paths and show that it is not easy to find the optimal flow assignment when the paths have different bandwidths. Second, we propose an on-line flow assignment method for the different-bandwidth paths and show that the numerical simulation with the method finds a nearly optimal flow assignment and outperforms up to 40% compared with the conventional path-bandwidth-based flow assignment. Our evaluation suggests that considering the traffic characteristics in the flow distribution over multiple paths significantly improves the delay performance when the flows have distinct characteristics.\nTitle:\nFlow assignment method with\u00a0traffic characteristics over multiple paths for\u00a0reducing queuing delay\n\nAbstract:\nWe consider a location-aware store-carry-forward routing scheme based on node density estimation (LA Routing in short), which adopts different message forwarding strategies depending on node density at contact locations where two nodes encounter. To do so, each node estimates a node density distribution based on information about contact locations. In this paper, we clarify how the estimation accuracy affects the performance of LA Routing. We also examine the performance of LA Routing when it applies to networks with homogeneous node density. Through simulation experiments, we show that LA Routing is fairly robust against the accuracy of node density estimation and its performance is comparable with Probabilistic Routing even in the case that that node density is homogeneous.\nTitle:\nLocation-Aware Store-Carry-Forward Routing Based On Node Density Estimation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.053, mean value for all the writers is 8.648, which makes it 2.052 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMeasurement studies on the Internet topology show that connectivities of nodes exhibit power-law attribute, but it is apparent that only the degree distribution does not determine the network structure, and especially true when we study the network-related control like routing control. In this paper, we first reveal structures of the router-level topologies using the working ISP networks, which clearly indicates ISP topologies are highly clustered; a node connects two or more nodes that also connected each other, while not in the existing modeling approaches. Based on this observation, we develop a new realistic modeling method for generating router-level topologies. In our method, when a new node joins the network, the node likely connects to the nearest nodes. In addition, we add the new links based on the node utilization in the topology, which corresponds to an enhancement of network equipments in ISP networks. With appropriate parameters, important metrics, such as the a clustering coefficient and the amount of traffic that pass through nodes, exhibit the similar value of the actual ISP topology while keeping the degree distribution of resulting topology to follow power-law. We then apply the routing control method to the ISP topologies and show that the optimal routing method gives much smaller maximum link utilization (about 1/3) compared with the minimum hop routing which is often used in the operating networks. Accordingly, we examine a heuristic routing method suitable to the ISP topologies with consideration of technology constraints of IP routers. The evaluation results show that our modeling method can be actually used for evaluations on routing control.\nTitle:", "model_inf_time": 1.38}, {"id": "4197", "output": "Triangular Spamming: A Stealthy and Efficient Technique", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAccurately identifying spam campaigns launched by a large number of bots in a botnet allows for accurate spam campaign signature generation and hence is critical to defeating spamming botnets. The straight-forward approach of clustering all spam containing the same label such as an URL into a campaign can be easily defeated by techniques such as simple obfuscations of URLs. In this paper, we perform a comprehensive study of content-agnostic characteristics of spam campaigns, e.g. duration and source-network distribution of spammers, in order to ascertain whether and how they can assist the simple label-based clustering methods in identifying campaigns and generating campaign signatures. In particular, from a five-month trace collected by a relay sinkhole, we manually identified and then analyzed seven URL-based botnet spam campaigns consisting of 52 million spam messages sent over 2.09 million SMTP connections originated from over 150,000 non-proxy spamming hosts and destined to about 200,000 end domains. Our analysis shows that the spam campaigns, when observed from large destination domains, exhibit durations far longer than the five-day period as reported in a recent study. We analyze the implications of this finding on spam campaign signature generation. We further study other characteristics of these long-lasting campaigns. Our analysis reveals several new findings regarding workload distribution, sending patterns, and coordination among the spamming machines.\nTitle:\nBotnet spam campaigns can be long lasting: evidence, implications, and analysis\n\nAbstract:\nWe present novel and practical techniques to accurately detect IP prefix hijacking attacks in real time to facilitate mitigation. Attacks may hijack victim's address space to disrupt network services or perpetrate malicious activities such as spamming and DoS attacks without disclosing identity. We propose novel ways to significantly improve the detection accuracy by combining analysis of passively collected BGP routing updates with data plane ingerprints of suspicious prefixes. The key insight is to use data plane information in the form of edge network ingerprinting to disambiguate suspect IP hijacking incidences based on routing anomaly detection. Conflicts in data plane ingerprints provide much more definitive evidence of successful IP pre- fix hijacking. Utilizing multiple real-time BGP feeds, we demonstrate the ability of our system to distinguish between legitimate routing changes and actual attacks. Strong correlation with addresses that originate spam emails from a spam honeypot confirms the accuracy of our techniques.\nTitle:\nAccurate Real-time Identification of IP Prefix Hijacking\n\nAbstract:\nUnderstanding the spammer behavior is a critical step in the long-lasting battle against email spams. Previous studies have focused on setting up honeypots or email sinkholes containing destination mailboxes for spam collection. A spam trace collected this way offers the limited viewpoint from a single organizational domain and hence is short of reflecting the global behavior of spammers. In this paper, we present a spam analysis study using sinkholes based on open relays. A relay sinkhole offers a unique vantage point in spam collection: it has the broader view of spam originated from multiple spam origins destined to mailboxes belonging to multiple organizational domains. The trace collected using this methodology opens the door to study spammer behaviors that were difficult to do using spam collected from a single organization. Seeing the aggregate behavior of spammers allows us to systematically separate High-Volume Spammers (HVS, e.g. direct spammers) from Low-Volume Spammers (LVS, e.g. low-volume bots in a botnet). Such a separation in turn gives rise to the notion of \"spam campaigns\", which reveals how LVS appear to coordinate with each other to share the spamming workload among themselves. A detailed spam campaign analysis holds the promise of finally reverse engineering the workload distribution strategies by the LVS coordinator.\nTitle:\nPeeking into spammer behavior from a unique vantage point\n\nAbstract:\nInternet routing events are known to introduce severe disruption to applications. So far effective diagnosis of routing events has relied on proprietary ISP data feeds, resulting in limited ISP-centric views not easily accessible by customers or other ISPs. In this work, we propose a novel approach to diagnosing significant routing events associated with any large networks from the perspective of end systems. Our approach is based on scalable, collaborative probing launched from end systems and does not require proprietary data from ISPs. Using a greedy scheme for event correlation and cause inference, we can diagnose both interdomain and intradomain routing events. Unlike existing methods based on passive route monitoring, our approach can also measure the impact of routing events on end-to-end network performance. We demonstrate the effectiveness of our approach by studying five large ISPs over four months. We validate its accuracy by comparing with the existing ISP-centric method and also with events reported on NANOG mailing lists. Our work is the first to scalably and accurately diagnose routing events associated with large networks entirely from end systems.\nTitle:\nEffective diagnosis of routing disruptions from end systems\n\nAbstract:\nWe present a measurement study analyzing DDoS attacks from multiple data sources, relying on both direct measurements of flow-level information, and more traditional indirect measurements using backscatter analysis. Understanding the nature of DDoS attacks is critically important to the development of effective counter measures to this pressing problem. While much of the community's current understanding of DDoS attacks result from indirect measurements, our analysis suggests that such studies do not give a comprehensive view of DDoS attacks witnessed in today's Internet. Specifically, our results suggest little use of address spoofing by attackers, which imply that such attacks will be invisible to indirect backscatter measurement techniques. Further, at the detailed packet-level characterization (e.g., attack destination ports), there are significant differences between direct and indirect measurements. Thus, there is tremendous value in moving towards direct observations to better understand DDoS attacks. Direct measurements additionally provide information inaccessible to indirect measurements, enabling us to better understand how to defend against attacks. We find that for 70% of the attacks fewer than 50 source ASes are involved and a relatively small number of ASes produce nearly 72% of the total attack volume. This suggests that network providers can reduce a substantial volume of malicious traffic with targeted deployment of DDoS defenses.\nTitle:\nAnalyzing large DDoS attacks using multiple data sources\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.939, mean value for all the writers is 8.648, which makes it 0.248 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSpam is increasingly accepted as a problem associated with compromised hosts or email accounts. This problem not only makes the tracking of spam sources difficult but also enables a massive amount of illegitimate or unwanted emails to be disseminated quickly. Various attempts have been made to analyze, backtrack, detect, and prevent spam using both network as well as content characteristics. However, relatively less attention has been given to understanding how spammers actually carry out their spamming activities from a network angle. Spammers\u2019 network behavior has significant impact on spammers\u2019 common goal, sending spam in a stealthy and efficient manner. Our work thoroughly investigates a fairly unknown spamming technique we name as triangular spamming that exploits routing irregularities of spoofed IP packets. It is highly stealthy and efficient in that triangular spamming enables 1) exploiting bandwidth diversity of botnet hosts to carry out spam campaigns effectively without divulging precious high-bandwidth hosts and 2) bypassing the current SMTP traffic blocking policies. Despite its relative obscurity, its use has been confirmed by the network operator community. Through carefully devised probing techniques and actual deployment of triangular spamming on Planetlab (a wide-area distributed testbed), we investigate the feasibility, impact of triangular spamming and propose practical detection and prevention methods. From our probing experiments, we found that 97% of the networks which block outbound SMTP traffic are vulnerable to triangular spamming and only 44% of them are listed on Spamhaus Policy Blocking List (PBL).\nTitle:", "model_inf_time": 1.61}, {"id": "4198", "output": "Substrate Network Design for Efficient Virtual Network Embedding", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nRunning multiple virtual networks, customized for different performance objectives, is a promising way to support diverse applications over a shared substrate. Despite being simple, a static division of resources between virtual networks can be highly inefficient, while dynamic resource allocation runs the risk of instability. This paper uses optimization theory to show that adaptive resource allocation can be stable and can maximize the aggregate performance across the virtual networks. In the DaVinci architecture, each substrate link periodically reassigns bandwidth shares between its virtual links; while at a smaller timescale, each virtual network runs a distributed protocol that maximizes its own performance objective independently. Numerical experiments with a mix of delay-sensitive and throughput-sensitive traffic show that the bandwidth shares converge quickly to the optimal values. We demonstrate that running several custom protocols in parallel and allocating resource adaptively can be more efficient, more flexible, and easier to manage than a compromise \"one-size-fits-all\" design.\nTitle:\nDaVinci: dynamically adaptive virtual networks for a customized internet\n\nAbstract:\nThis paper advocates a different approach to reduce routing convergence--side-stepping the problem by avoiding it in the first place! Rather than recomputing paths after temporary topology changes, we argue for a separation of timescale between offline computation of multiple diverse paths and online spreading of load over these paths. We believe decoupling failure recovery from path computation leads to networks that are inherently more efficient, more scalable, and easier to manage.\nTitle:\nDynamic route recomputation considered harmful\n\nAbstract:\nThe complexity of network management is widely recognized as one of the biggest challenges facing the Internet today. Point solutions for individual problems further increase system complexity while not addressing the underlying causes. In this paper, we argue that many network-management problems stem from the same root cause---the need to maintain consistency between the physical and logical configuration of the routers. Hence, we propose VROOM (Virtual ROuters On the Move), a new network-management primitive that avoids unnecessary changes to the logical topology by allowing (virtual) routers to freely move from one physical node to another. In addition to simplifying existing network-management tasks like planned maintenance and service deployment, VROOM can also help tackle emerging challenges such as reducing energy consumption. We present the design, implementation, and evaluation of novel migration techniques for virtual routers with either hardware or software data planes. Our evaluation shows that VROOM is transparent to routing protocols and results in no performance impact on the data traffic when a hardware-based data plane is used.\nTitle:\nVirtual routers on the move: live router migration as a network-management primitive\n\nAbstract:\nSoftware Defined Networks (SDNs) support diverse network policies by offering direct, network-wide control over how switches handle traffic. Unfortunately, many controller platforms force applications to grapple simultaneously with end-to-end connectivity constraints, routing policy, switch memory limits, and the hop-by-hop interactions between forwarding rules. We believe solutions to this complex problem should be factored in to three distinct parts: (1) high-level SDN applications should define their end-point connectivity policy on top of a \"one big switch\" abstraction; (2) a mid-level SDN infrastructure layer should decide on the hop-by-hop routing policy; and (3) a compiler should synthesize an effective set of forwarding rules that obey the user-defined policies and adhere to the resource constraints of the underlying hardware. In this paper, we define and implement our proposed architecture, present efficient rule-placement algorithms that distribute forwarding policies across general SDN networks while managing rule-space constraints, and show how to support dynamic, incremental update of policies. We evaluate the effectiveness of our algorithms analytically by providing complexity bounds on their running time and rule space, as well as empirically, using both synthetic benchmarks, and real-world firewall and routing policies.\nTitle:\nOptimizing the \"one big switch\" abstraction in software-defined networks\n\nAbstract:\nThe Internet would be more efficient and robust if routers could flexibly divide traffic over multiple paths. Often, having one or two extra paths is sufficient for customizing paths for different applications, improving security, reacting to failures, and balancing load. However, support for Internet-wide multipath routing faces two significant barriers. First, multipath routing could impose significant computational and storage overhead in a network the size of the Internet. Second, the independent networks that comprise the Internet will not relinquish control over the flow of traffic without appropriate incentives. In this article, we survey flexible multipath routing techniques that are both scalable and incentive compatible. Techniques covered include: multihoming, tagging, tunneling, and extensions to existing Internet routing protocols.\nTitle:\nToward internet-wide multipath routing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.408, mean value for all the writers is 8.648, which makes it 1.058 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nNetwork virtualization is a powerful way to run multiple architectures or experiments simultaneously on a shared infrastructure. However, making efficient use of the underlying resources requires effective techniques for virtual network embedding--mapping each virtual network to specific nodes and links in the substrate network. Since the general embedding problem is computationally intractable, past research restricted the problem space to allow efficient solutions, or focused on designing heuristic algorithms. In this paper, we advocate a different approach: rethinking the design of the substrate network to enable simpler embedding algorithms and more efficient use of resources, without restricting the problem space. In particular, we simplify virtual link embedding by: i) allowing the substrate network to split a virtual link over multiple substrate paths and ii) employing path migration to periodically re-optimize the utilization of the substrate network. We also explore node-mapping algorithms that are customized to common classes of virtual-network topologies. Our simulation experiments show that path splitting, path migration,and customized embedding algorithms enable a substrate network to satisfy a much larger mix of virtual networks\nTitle:", "model_inf_time": 1.29}, {"id": "4199", "output": "Space-Code Bloom Filter for Efficient Per-Flow Traffic Measurement", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nPer-flow traffic measurement is critical for usage accounting, traffic engineering, and anomaly detection. Previous methodologies are either based on random sampling (e.g., Cisco's NetFlow), which is inaccurate, or only account for the \"elephants\". Our paper introduces a novel technique for measuring per-flow traffic approximately, for all flows regardless of their sizes, at very high-speed (say, OC192+). The core of this technique is a novel data structure called Space Code Bloom Filter (SCBF). A SCBF is an approximate representation of a multiset; each element in this multiset is a traffic flow and its multiplicity is the number of packets in the flow. SCBF employs a Maximum Likelihood Estimation (MLE) method to measure the multiplicity of an element in the multiset. Through parameter tuning, SCBF allows for graceful tradeoff between measurement accuracy and computational and storage complexity. SCBF also contributes to the foundation of data streaming by introducing a new paradigm called blind streaming. We evaluated the performance of SCBF on packet traces gathered from a tier-1 ISP backbone and through mathematical analysis. Our preliminary results demonstrate that SCBF achieves reasonable mea-surement accuracy with very low storage and computational complexity.\nTitle:\nSpace-code bloom filter for efficient traffic flow measurement\n\nAbstract:\nEstimation of traffic matrices, which provide critical input for network capacity planning and traffic engineering, has recently been recognized as an important research problem. Most of the previous approaches infer traffic matrix from either SNMP link loads or sampled NetFlow records. In this work, we design novel inference techniques that, by statistically correlating SNMP link loads and sampled NetFlow records, allow for much more accurate estimation of traffic matrices than obtainable from either information source alone, even when sampled NetFlow records are available at only a subset of ingress. Our techniques are practically important and useful since both SNMP and NetFlow are now widely supported by vendors and deployed in most of the operational IP networks. More importantly, this research leads us to a new insight that SNMP link loads and sampled NetFlow records can serve as \"error correction codes\" to each other. This insight helps us to solve a challenging open problem in traffic matrix estimation, \"How to deal with dirty data (SNMP and NetFlow measurement errors due to hardware/software/transmission problems)?\" We design techniques that, by comparing notes between the above two information sources, identify and remove dirty data, and therefore allow for accurate estimation of the traffic matrices with the cleaned dat.We conducted experiments on real measurement data obtained from a large tier-1 ISP backbone network. We show that, when full deployment of NetFlow is not available, our algorithm can improve estimation accuracy significantly even with a small fraction of NetFlow data. More importantly, we show that dirty data can contaminate a traffic matrix, and identifying and removing them can reduce errors in traffic matrix estimation by up to an order of magnitude. Routing changes is another a key factor that affects estimation accuracy. We show that using them as the a priori, the traffic matrices can be estimated much more accurately than those omitting the routing change. To the best of our knowledge, this work is the first to offer a comprehensive solution which fully takes advantage of using multiple readily available data sources. Our results provide valuable insights on the effectiveness of combining flow measurement and link load measurement.\nTitle:\nRobust traffic matrix estimation with imperfect information: making use of multiple data sources\n\nAbstract:\nThe overall performance of a firewall is crucial in enforcing and administrating security, especially when the network is under attack. The continuous growth of the Internet, coupled with the increasing sophistication of the attacks, is placing stringent demands on firewall performance. In this paper, we describe a traffic-aware optimization framework to improve the operational cost of firewalls. Based on this framework, we design a set of tools that inspect and analyze both multidimensional firewall rules and traffic logs and construct the optimal equivalent firewall rules based on the observed traffic characteristics. To the best of our knowledge, this work is the first to use traffic characteristics in firewall optimization. Furthermore, we develop a novel adaptation mechanism that dynamically detects anomalous traffic behavior and adaptively alters the firewall rules to avoid serious performance degradation due to the traffic anomaly. To evaluate the performance of our approaches, we collected a large set of firewall rules and traffic logs at tens of enterprise networks managed by a Tier-1 service provider. Our evaluation results find these approaches very effective. In particular, we achieve more than 10 fold performance improvement by using the proposed traffic-aware firewall optimization.\nTitle:\nTraffic-Aware Firewall Optimization Strategies\n\nAbstract:\nThe use of peer-to-peer (P2P) applications is growing dramaticaliy, particularly for sharing large video/audio files and software. In this paper, we analyze P2P traffic by measuring flow-level information collected at multiple border routers across a large ISP network, and report our investigation of three popular P2P systems -- FastTrack, Gnutella, and DirectConnect. We characterize the P2P traffic observed at a single ISP and its impact on the underlying network. We observe very skewed distribution in the traffic across the network at different levels of spatial aggregation (IP, prefix, AS). All three P2P systems exhibit significant dynamics at short times scale and particularly at the IP address level Still, the fraction of P2P traffic contributed by each prefix is much more stable than the corresponding distribution of either Web traffic or overall traffic. The high volume and good stability properties of P2P traffic indicates that the P2P workload is a good candidate for being managed via application-specific layer-3 traffic engineering in an ISP's network.\nTitle:\nAnalyzing peer-to-peer traffic across large networks\n\nAbstract:\nUnderstanding Internet traffic dynamics in large cellular networks is important for network design, troubleshooting, performance evaluation, and optimization. In this paper, we present the results from our study, which is based upon a week-long aggregated flow level mobile device traffic data collected from a major cellular operator's core network. In this study, we measure and characterize the spatial and temporal dynamics of mobile Internet traffic. We distinguish our study from other related work by conducting the measurement at a larger scale and exploring mobile data traffic patterns along two new dimensions -- device types and applications that generate such traffic patterns. Based on the findings of our measurement analysis, we propose a Zipf-like model to capture the volume distribution of application traffic and a Markov model to capture the volume dynamics of aggregate Internet traffic. We further customize our models for different device types using an unsupervised clustering algorithm to improve prediction accuracy.\nTitle:\nCharacterizing and modeling internet traffic dynamics of cellular devices\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.22, mean value for all the writers is 8.648, which makes it 0.488 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nPer-flow traffic measurement is critical for usage accounting, traffic engineering, and anomaly detection. Previous methodologies are either based on random sampling (e.g., Cisco's NetFlow), which is inaccurate, or only account for the \"elephants.\" We introduce a novel technique for measuring per-flow traffic approximately, for all flows regardless of their sizes, at very high-speed (say, OC768). The core of this technique is a novel data structure called Space-Code Bloom Filter (SCBF). A SCBF is an approximate representation of a multiset; each element in this multiset is a traffic flow and its multiplicity is the number of packets in the flow. The multiplicity of an element in the multiset represented by SCBF can be estimated through either of two mechanisms-maximum-likelihood estimation or mean value estimation. Through parameter tuning, SCBF allows for graceful tradeoff between measurement accuracy and computational and storage complexity. SCBF also contributes to the foundation of data streaming by introducing a new paradigm called blind streaming. We evaluate the performance of SCBF through mathematical analysis and through experiments on packet traces gathered from a tier-1 ISP backbone. Our results demonstrate that SCBF achieves reasonable measurement accuracy with very low storage and computational complexity. We also demonstrate the application of SCBF in estimating the frequency of keywords at a search engine-demonstrating the applicability of SCBF to other problems that can be reduced to multiset membership queries\nTitle:", "model_inf_time": 1.76}, {"id": "41100", "output": "Maximal Permissive Control Synthesis with Invariant Requirements", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe propose a new technique for controlled system synthesis on non-deterministic automata for requirements in modal logic. Synthesis, as defined in this paper, restricts a behavioral specification of the uncontrolled system such that it satisfies a given logical expression, while adhering to the rules dictated by supervisory control such as maximal permissiveness and controllability. The applied requirement formalism extends Hennessy-Milner logic with the invariant and reachability modalities from G\u00f6del-L\u00f6b logic, and is therefore able to express a broad range of control requirements, such as marker state reachability and deadlock-freeness. This paper contributes to the field of control synthesis by achieving maximal permissiveness in a non-deterministic context for control requirements in modal logic, and treatment of controllability via partial bisimulation. We present a well-defined and complete derivation of the synthesis result, which is supported further by computer-verified proofs created using the Coq proof assistant. The synthesis method is also presented in algorithmic form, including an analysis of its computational complexity. We show that the proposed synthesis theory allows full expressibility of Ramadge-Wonham supervisory control theory and we illustrate its applicability in two small industrial case studies, including an analysis with regard to scalability.\nTitle:\nMaximally permissive controlled system synthesis for non-determinism and modal logic.\n\nAbstract:\nWe present a rule format for structural operational semantics to guarantee that the associated labelled transition system is bounded nondeterministic.\nTitle:\nStructural operational semantics and bounded nondeterminism\n\nAbstract:\nModeling arbitrary connectivity changes of mobile ad hoc networks (MANETs) makes application of automated formal verification challenging. We introduced constrained labeled transition systems (CLTSs) as a semantic model to represent mobility. To model check MANET protocol with respect to the underlying topology and connectivity changes, we here introduce a branching-time temporal logic interpreted over CLTSs. The temporal operators, from Action Computation Tree Logic with an unless operator, are parameterized by multi-hop constraints over topologies, to express conditions on successful scenarios of a MANET protocol. We moreover provide a bisimilarity relation with the same distinguishing power for CLTSs as our logical framework.\nTitle:\nModel Checking MANETs with Arbitrary Mobility.\n\nAbstract:\nThis paper studies nested simulation and nested trace semantics over the language BCCSP, a basic formalism to express finite process behaviour. It is shown that none of these semantics affords finite (in)equational axiomatizations over BCCSP. In particular, for each of the nested semantics studied in this paper, the collection of sound, closed (in)equations over a singleton action set is not finitely based.\nTitle:\nNested semantics over finite trees are equationally hard\n\nAbstract:\nThis paper confirms a conjecture of Bergstra and Klop's from 1984 by establishing that the process algebra obtained by adding an auxiliary operator proposed by Hennessy in 1981 to the recursion free fragment of Milner's Calculus of Communicating Systems is not finitely based modulo bisimulation equivalence. Thus, Hennessy's merge cannot replace the left merge and communication merge operators proposed by Bergstra and Klop, at least if a finite axiomatization of parallel composition modulo bisimulation equivalence is desired.\nTitle:\nCCS with Hennessy's merge has no finite-equational axiomatization\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.486, mean value for all the writers is 8.648, which makes it 0.138 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents a novel approach to adapt a behavioral model in order to satisfy a requirement in Hennessy-Milner Logic, including an additional box modality operator, expressing an invariant formula. Control system synthesis, as defined in this way, retains all non-invalidating behavior, and thereby guarantees maximal permissiveness for supervisory control. This research extends earlier work by embracing a broader synthesized logic, enabling synthesis with respect to invariant formulas for non-deterministic behavioral models. All definitions and proofs in this paper have been computer verified using the Coq proof assistant.\nTitle:", "model_inf_time": 1.22}, {"id": "41101", "output": "Boolean Matrix Representations and Decomposition of Covering Approximation Operators", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCovering-based rough approximation space is an important generalization of classical rough approximation space. Matrix is not only a classical field in mathematics, but also a powerful tool in computer science. Recently, boolean matrices have been employed to define the sixth type of covering-based approximation operators. In this paper, we use matrices to describe the other five types of approximation operators. These descriptions can be obtained via two types of boolean product operations. The equivalent descriptions of the first type of approximation operators with indiscernible neighborhood are studied. The fourth covering-based upper approximation operator is described by predecessor neighborhood, which is generated from a relation induced by a covering. These equivalent descriptions about the fourth and the first type of approximation operators are used to obtain the representations of these three approximation operators by matrices. This work provides a more general approach for covering-based approximations through matrix.\nTitle:\nCovering-based approximation operators by boolean matrix\n\nAbstract:\nAs the matrix can compactly represent numeric data, simplify problem formulation and reduce time complexity, it has many applications in most of the scientific fields. For this purpose, some types of generalized rough sets have been connected with matrices. However, covering-based rough sets which play an important role in data mining and machine learning are seldom connected with matrices. In this paper, we define three composition operations of coverings and study their characteristic matrices; Moreover, the relationships between the characteristic matrices and covering approximation operators are investigated. First, for a covering, an existing matrix representation of indiscernible neighborhoods called the type-1 characteristic matrix of the covering is recalled and a new matrix representation of neighborhoods called the type-2 characteristic matrix of the covering is proposed. Second, considering the importance of knowledge fusion and decomposition, we define three types of composition operations of coverings. Specifically, their type-1 and type-2 characteristic matrices are studied. Finally, we also explore the representable properties of covering approximation operators with respect to any covering generated by each composition operation. It is interesting to find that three types of approximation operators, which are induced by each type of composition operation of coverings, can be expressed as the Boolean product of a coefficient matrix and a characteristic vector. These interesting results suggest the potential for studying covering-based rough sets by matrix approaches.\nTitle:\nCharacteristic matrices of compound operations of coverings and their relationships with rough sets.\n\nAbstract:\nRough set theory is a useful tool for data mining. It is based on equivalence relations and has been extended to covering-based generalized rough set. This paper studies three kinds of covering generalized rough sets for dealing with the vagueness and granularity in information systems. First, we examine the properties of approximation operations generated by a covering in comparison with those of the Pawlak's rough sets. Then, we propose concepts and conditions for two coverings to generate an identical lower approximation operation and an identical upper approximation operation. After the discussion on the interdependency of covering lower and upper approximation operations, we address the axiomization issue of covering lower and upper approximation operations. In addition, we study the relationships between the covering lower approximation and the interior operator and also the relationships between the covering upper approximation and the closure operator. Finally, this paper explores the relationships among these three types of covering rough sets.\nTitle:\nOn Three Types of Covering-Based Rough Sets\n\nAbstract:\nGraph theoretical ideas are highly utilized by computer science fields especially data mining. In this field, a data structure can be designed in the form of graph. Covering is a widely used form of data representation in data mining and covering-based rough sets provide a systematic approach to this type of representation. In this paper, we study the connectedness of graphs through covering-based rough sets and apply it to connected matroids. First, we present an approach to inducing a covering by a graph, and then study the connectedness of the graph from the viewpoint of covering approximation operators. Second, we construct a graph from a matroid, and find the matroid and the graph have the same connectedness, which makes us to use covering-based rough sets to study connected matroids. In summary, this paper provides a new approach to studying graph theory and matroid theory.\nTitle:\nConnectedness of graphs and its application to connected matroids through covering-based rough sets.\n\nAbstract:\nRough set theory is an efficient and essential tool for dealing with vagueness and granularity in information systems. Covering-based rough set theory is proposed as a significant generalization of classical rough sets. Matroid theory is a vital structure with high applicability and borrows extensively from linear algebra and graph theory. In this paper, one type of covering-based approximations is studied from the viewpoint of Eulerian matroids. First, we explore the circuits of an Eulerian matroid from the perspective of coverings. Second, this type of covering-based approximations is represented by the circuits of Eulerian matroids. Moreover, the conditions under which the covering-based upper approximation operator is the closure operator of a matroid are presented. Finally, a matroidal structure of covering-based rough sets is constructed. These results show many potential connections between covering-based rough sets and matroids.\nTitle:\nCovering-Based Rough Sets on Eulerian Matroids.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.378, mean value for all the writers is 8.648, which makes it 0.23 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCovering-based rough sets provide an efficient means of dealing with covering data, which occur widely in practical applications. Boolean matrix decomposition has frequently been applied to data mining and machine learning. In this paper, three types of existing covering approximation operators are represented by Boolean matrices, and then used in Boolean matrix decomposition. First, we define two characteristic matrices of a covering. Through these Boolean characteristic matrices, three types of existing covering approximation operator are concisely and equivalently represented. Second, these operator representations are applied to Boolean matrix decomposition, which has a close relationship with nonnegative matrix factorization, a popular and efficient technique for machine learning. We provide a sufficient and necessary condition for a square Boolean matrix to decompose into the Boolean product of another matrix and its transpose. We then develop an algorithm for this Boolean matrix decomposition. Finally, these three covering approximation operators are axiomatized using Boolean matrices. This work presents an interesting viewpoint from which to investigate covering-based rough set theory and its applications.\nTitle:", "model_inf_time": 1.38}, {"id": "41102", "output": "BB84 Quantum Key Distribution Using Fainted Pulses and QPSK Modulation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose a preprocessing method to improve Side Channel Attacks (SCAs) on Dual-rail with Precharge Logic (DPL) countermeasure family. The strength of our method is that it uses intrinsic characteristics of the countermeasure: classical methods fail when the countermeasure is perfect, whereas our method still works and enables us to perform advanced attacks. We have experimentally validated the proposed method by attacking a DES cryptoprocessor embedded in a Field Programmable Gates Array (FPGA), and protected by the Wave Dynamic Differential Logic (WDDL) countermeasure. This successful attack, unambiguous as the full key is retrieved, is the first to be reported.\nTitle:\nSuccessful attack on an FPGA-based WDDL DES cryptoprocessor without place and route constraints\n\nAbstract:\nWe suggest, in a methodological manner, the use of Wavelet transforms to improve side channel analysis (SCA). The proposed applications are involved in several side channel analysis aspects: storage of traces, patterns detection and noise filtering. We show that all these aspects are useful to improve evaluation of information leakages from embedded devices. In particular, we show how wavelets favour practical secret key recovery.\nTitle:\nWavelet Transform Based Pre-processing for Side Channel Analysis\n\nAbstract:\n  This article presents an asynchronous FPGA architecture for implementing cryptographic algorithms secured against physical cryptanalysis. We discuss the suitability of asynchronous reconfigurable architectures for such applications before proceeding to model the side channel and defining our objectives. The logic block architecture is presented in detail. We discuss several solutions for the interconnect architecture, and how these solutions can be ported to other flavours of interconnect (i.e. single driver). Next We discuss in detail a high speed asynchronous configuration chain architecture used to configure our asynchronous FPGA with simulation results, and we present a 3 X 3 prototype FPGA fabricated in 65 nm CMOS. Lastly we present experiments to test the high speed asynchronous configuration chain and evaluate how far our objectives have been achieved with proposed solutions, and we conclude with emphasis on complementary FPGA CAD algorithms, and the effect of CMOS variation on Side-Channel Vulnerability. \nTitle:\nA Secure Asynchronous FPGA Architecture, Experimental Results and Some Debug Feedback\n\nAbstract:\nThe Expectation-Maximization (EM) algorithm is studied to perform channel estimation in a low cost and high data rate impulse radio UWB receiver. The system under consideration uses a pulse position modulation with a simple analog energy detector., In order to overcome the problems inherent to high data rates, such as inter-symbol interferences, a probabilistic equalizer is used. The EM algorithm and the equalizer are embedded into the loop of an iterative channel decoder. This permits to refine both the channel parameters and the signal probability at each iteration. We present numerical results performed on the channel models from the IEEE 802.15.3a task group. These results show that the EM algorithm contributes to achieve data rate greater than 100Mb/s with a simple impulse radio UWB receiver.\nTitle:\nEm Channel Estimation In A Low-Cost Uwb Receiver Based On Energy Detection\n\nAbstract:\nIn this paper, we answer the question of what are the necessary conditions under which Correlation Power Attack (CPA), that essentially targets embedded cryptographic implementations, is optimal with regards to attacks that exploit the same leakage model. For this purpose, we offer an in-depth theoretical study which aims at determining the conditions under which the Pearson correlation coefficient is maximized. Moreover, we propose theoretical metrics to practically verify the validity of those conditions. Besides, we illustrate our theoretical study by an experiment on real electromagnetic traces acquired from a DES cryptographic implementation.\nTitle:\nOn the optimality of correlation power attack on embedded cryptographic systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.143, mean value for all the writers is 8.648, which makes it 0.422 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this work we present the principles and experimental demonstration of a BB84 quantum key distribution (QKD) one way system using fainted pulses and a quadrature phase-shift-keying (QPSK) format including a time- multiplexed unmodulated carrier reference at Aliceu0027s end, and a differential homodyne reception at Bobu0027s end. We also describe the secure electronics interface subsystem concept for the interaction with upper layers in an IP application.\nTitle:", "model_inf_time": 1.49}, {"id": "41103", "output": "Improving Near Duplicate Image Retrieval with Coherent Codebooks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe detection of near duplicate images in large databases, such as the ones of popular social networks, digital investigation archives, and surveillance systems, is an important task for a number of image forensics applications. In digital investigation, hashing techniques are commonly used to index large quantities of images for the detection of copies belonging to different archives. In the last few years, different image hashing techniques based on the Bags of Visual Features paradigm appeared in literature. Recently, this paradigm has been augmented by using multiple descriptors (e.g., Bags of Visual Phrases) in order to exploit the coherence between different feature spaces. In this paper we propose to further improve the Bags of Visual Phrases approach considering the coherence between feature spaces not only at the level of image representation, but also during the codebook generation phase. Also we introduce a novel image database specifically designed for the development and benchmarking of near duplicate image retrieval techniques. The dataset consists of more than 3,300 images depicting more than 500 different scenes having at least three real near duplicates. The dataset has a huge variability in terms of geometric and photometric transformations between scenes and their corresponding near duplicates. Finally, we suggest a method to compress the proposed image representation for storage purposes. Experiments show the effectiveness of the proposed near duplicate retrieval technique, which outperforms the original Bags of Visual Phrases approach.\nTitle:\nAligning codebooks for near duplicate image detection\n\nAbstract:\nFood recognition is an interesting and challenging problem with applications in medical, social and anthropological research areas. The high variability of food images makes the recognition task difficult for current state-of-the-art methods. It has been proved that the exploitation of multiple features to capture complementary aspects of the image contents is useful to improve the discrimination of different food items. In this paper we exploit an image representation based on the consensus among visual vocabularies built on different feature spaces. Starting from a set of visual codebooks, a consensus clustering technique is used to build a consensus vocabulary used to represent food pictures with a Bag-of-Visual-Words paradigm. This new representation is employed together with a SVM for recognition purpose.\nTitle:\nFood Recognition Using Consensus Vocabularies.\n\nAbstract:\nThe distribution of digital images with the classic and newest technologies available on Internet (e.g., emails, social networks, digital repositories) has induced a growing interest on systems able to protect the visual content against malicious manipulations that could be performed during their transmission. One of the main problems addressed in this context is the authentication of the image received in a communication. This task is usually performed by localizing the regions of the image which have been tampered. To this aim the received image should be first registered with the one at the sender by exploiting the information provided by a specific component of the forensic hash associated with the image. In this paper we propose a robust alignment method which makes use of an image signature based on the Bag of Features paradigm. The alignment is based on a voting procedure in the parameter space of the model used to recover the geometric transformation occurred into the manipulated image. Experiments show that the proposed approach obtains good margin in terms of performances with respect to state-of-the art methods.\nTitle:\nRobust image registration and tampering localization exploiting bag of features based forensic signature\n\nAbstract:\nThe distribution of digital images with the classic and newest technologies available on Internet (e.g., emails, social networks, digital repositories) has induced a growing interest on systems able to protect the visual content against malicious manipulations that could be performed during their transmission. One of the main problems addressed in this context is the authentication of the image received in a communication. This task is usually performed by localizing the regions of the image which have been tampered. To this aim the received image should be first registered with the one at the sender by exploiting the information provided by a specific component of the forensic hash associated with the image. In this paper we propose a robust alignment method which makes use of an image hash component based on the Bag of Visual Words paradigm. The proposed signature is attached to the image before transmission and then analyzed at destination to recover the geometric transformations which have been applied to the received image. The estimator is based on a voting procedure in the parameter space of the geometric model used to recover the transformation occurred to the received image. Experiments show that the proposed approach obtains good margin in terms of performances with respect to state-of-the art methods.\nTitle:\nA robust forensic hash component for image alignment\n\nAbstract:\nThe widespread use of classic and newest technologies available on Internet (e.g., emails, social networks, digital repositories) has induced a growing interest on systems able to protect the visual content against malicious manipulations that could be performed during their transmission. One of the main problems addressed in this context is the authentication of the image received in a communication. This task is usually performed by localizing the regions of the image which have been tampered. To this aim the aligned image should be first registered with the one at the sender by exploiting the information provided by a specific component of the forensic hash associated to the image. In this paper we propose a robust alignment method which makes use of an image hash component based on the Bag of Features paradigm. The proposed signature is attached to the image before transmission and then analyzed at destination to recover the geometric transformations which have been applied to the received image. The estimator is based on a voting procedure in the parameter space of the model used to recover the geometric transformation occurred into the manipulated image. The proposed image hash encodes the spatial distribution of the image features to deal with highly textured and contrasted tampering patterns. A block-wise tampering detection which exploits an histograms of oriented gradients representation is also proposed. A non-uniform quantization of the histogram of oriented gradient space is used to build the signature of each image block for tampering purposes. Experiments show that the proposed approach obtains good margin of performances with respect to state-of-the art methods.\nTitle:\nRobust Image Alignment for Tampering Detection\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.719, mean value for all the writers is 8.648, which makes it 0.061 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nImage retrieval from large databases, such as popular social networks, collections of surveillance images and videos, or digital investigation archives, is a very important task for a number of applications. In digital investigation, hashing techniques are commonly used to index large quantities of images to detect copies from different archives. In the last few years, a number of image hashing techniques based on the Bags of Visual Words paradigm have been proposed. Recently, this paradigm has been augmented by using multiple descriptors (Bags of Visual Phrases) to exploit the coherence between different feature spaces. In this paper we propose to further improve the Bags of Visual Phrases approach exploiting the coherence between feature spaces not only in the image representation, but also in the codebooks generation. Experiments performed on real and synthetic near duplicate image datasets show the effectiveness of the proposed approach, which outperforms the original Bags of Visual Phrases approach.\nTitle:", "model_inf_time": 1.54}, {"id": "41104", "output": "Finding Interesting Association Rules", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nOne of the major problems in the field of knowledge discovery (or data mining) is the interestingness problem. Past research and applications have found that, in practice, it is all too easy to discover a huge number of patterns in a database. Most of these patterns are actually useless or uninteresting to the user. But due to the huge number of patterns, it is difficult for the user to comprehend them and to identify those interesting to him/her. To prevent the user from being overwhelmed by the large number of patterns, techniques are needed to rank them according to their interestingness. In this paper, we propose such a technique, called the user-expectation method. In this technique, the user is first asked to provide his/her expected patterns according to his/her past knowledge or intuitive feelings. Given these expectations, the system uses a fuzzy matching technique to match the discovered patterns against the user's expectations, and then rank the discovered patterns according to the matching results. A variety of rankings can be performed for different purposes, such as to confirm the user's knowledge and to identify unexpected patterns, which are by definition interesting. The proposed technique is general and interactive\nTitle:\nFinding interesting patterns using user expectations\n\nAbstract:\nOne of the important problems in data mining is the evalua- tion of subjective interestingness of the discovered rules. Past research has found that in many real-life applications it is easy to generate a large number of rules from the data- base, but most of the rules are not useful or interesting to the user. Due to the large number of rules, it is difficult for the user to analyze them manually in order to identify those interesting ones. Whether a rule is of interest to a user de- pends on his/her existing knowledge of the domain, and his/her interests. In this paper, we propose a technique that analyzes the discovered rules against a specific type of ex- isting knowledge, which we call general impressions, to help the user identify interesting rules. We first propose a representation language to allow general impressions to be specified. We then present some algorithms to analyze the discovered classification rules against a set of general im- pressions. The results of the analysis tell us which rules con- form to the general impressions and which rules are unex- pected. Unexpected rules are by definition inte resting.\nTitle:\nUsing General Impressions to Analyze Discovered Classification Rules\n\nAbstract:\nData repositories are constantly evolving and techniques are needed to reveal the dynamic behaviors in the data that might be useful to the user. Existing temporal association rules mining algorithms consider time as another dimension and do not describe the behavior of rules over time. In this work, we introduce the notion of trend fragment to facilitate the analysis of relationships among rules. Two algorithms are proposed to find the relationships among rules. Experiment results on both synthetic and real-world datasets indicate that our approach is scalable and effective.\nTitle:\nDiscovering Trends and Relationships among Rules\n\nAbstract:\nClassification rule mining aims to discover a small set of rules in the database that forms an accurate classifier. Association rule mining finds all the rules existing in the database that satisfy some minimum support and minimum confidence constraints. For association rule mining, the target of discovery is not pre-determined, while for classification rule mining there is one and only one pre-determined target. In this paper, we propose to integrate these two mining techniques. The integration is done by focusing on mining a special subset of association rules, called class association rules (CARs). An efficient algorithm is also given for building a classifier based on the set of discovered CARs. Experimental results show that the classifier built this way is, in general, more accurate than that produced by the state-of-the-art classification system C4.5. In addition, this integration helps to solve a number of problems that exist in the current classification systems.\nTitle:\nIntegrating Classification and Association Rule Mining\n\nAbstract:\nBuilding predictive models and finding useful rules are two important tasks of data mining. While building predictive models has been well studied, finding useful rules for action still presents a major problem. A main obstacle is that many data mining algorithms often produce too many rules. Existing research has shown that most of the discovered rules are actually redundant or insignificant. Pruning techniques have been developed to remove those spurious and/or insignificant rules. In this paper, we argue that being a significant rule (or a non-redundant rule), however, does not mean that it is a potentially useful rule for action. Many significant rules (unpruned rules) are in fact not actionable. This paper studies this issue and presents an efficient algorithm to identify these non-actionable rules. Experiment results on many real-life datasets show that the number of non-actionable rules is typically quite large. The proposed technique thus enables the user to focus on fewer rules and to be assured that the remaining rules are non-redundant and potentially useful for action.\nTitle:\nIdentifying non-actionable association rules\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.437, mean value for all the writers is 8.648, which makes it 1.033 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAssociation rules, a class of important regularities in databases, have proven very useful in practical applications, but association-rule-mining algorithms tend to produce huge numbers of rules, most of which are of no interest. Users have considerable difficulty manually analyzing so many rules to identify the truly interesting ones. To solve that problem, we have developed a new approach to help them find interesting rules (in particular, unexpected rules) from a set of discovered association rules. This interestingness analysis system (IAS) leverages the user's existing domain knowledge to analyze discovered associations and then rank discovered rules according to various interestingness criteria, such as conformity and various types of unexpectedness. This article describes how we have implemented this technique and used it successfully in a number of applications.\nTitle:", "model_inf_time": 1.15}, {"id": "41105", "output": "Decentralized Coordination Constraints for Distributed Reinforcement Learning", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose to guide reinforcement learning (RL) with expert coordination knowledge for multi-agent problems managed by a central controller. The aim is to learn to use expert coordination knowledge to restrict the joint action space and to direct exploration towards more promising states, thereby improving the overall learning rate. We model such coordination knowledge as constraints and propose a two-level RL system that utilizes these constraints for online applications. Our declarative approach towards specifying coordination in multi-agent learning allows knowledge sharing between constraints and features (basis functions) for function approximation. Results on a soccer game and a tactical real-time strategy game show that coordination constraints improve the learning rate compared to using only unary constraints. The two-level RL system also outperforms existing single-level approach that utilizes joint action selection via coordination graphs.\nTitle:\nCoordination guided reinforcement learning\n\nAbstract:\nRelational representations have great potential for rapidly generalizing learned knowledge in large Markov decision processes such as multi-agent problems. In this work, we introduce relational temporal difference learning for the distributed case where the communication links among agents are dynamic. Thus no critical components of the system should reside in any one agent. Relational generalization among agents' learning is achieved through the use of partially bound relational features and a message passing scheme. We further describe how the proposed concepts can be applied to distributed reinforcement learning methods that use value functions. Experiments were conducted on soccer and real-time strategy game domains with dynamic communication. Results show that our methods improve goal achievement in online learning with a greatly decreased number of parameters to learn when compared with existing distributed learning methods.\nTitle:\nDistributed relational temporal difference learning\n\nAbstract:\nCollaborative filtering have become increasingly important with the development of Web 2.0. Online shopping service providers aim to provide users with quality list of recommended items that will enhance user satisfaction and loyalty. Matrix factorization approaches have become the dominant method as they can reduce the dimension of the data set and alleviate the sparsity problem. However, matrix factorization approaches are limited because they depict each user as one preference vector. In practice, we observe that users may have different preferences when purchasing different subsets of items, and the periods between purchases also vary from one user to another. In this work, we propose a probabilistic approach to learn latent clusters in the large user-item matrix, and incorporate temporal information into the recommendation process. Experimental results on a real world dataset demonstrate that our approach significantly improves the conversion rate, precision and recall of state-of-the-art methods.\nTitle:\nUtilizing Purchase Intervals in Latent Clusters for Product Recommendation\n\nAbstract:\nData repositories are constantly evolving and techniques are needed to reveal the dynamic behaviors in the data that might be useful to the user. Existing temporal association rules mining algorithms consider time as another dimension and do not describe the behavior of rules over time. In this work, we introduce the notion of trend fragment to facilitate the analysis of relationships among rules. Two algorithms are proposed to find the relationships among rules. Experiment results on both synthetic and real-world datasets indicate that our approach is scalable and effective.\nTitle:\nDiscovering Trends and Relationships among Rules\n\nAbstract:\nMany real world objects have states that change over time. By tracking the state sequences of these objects, we can study their behavior and take preventive measures before they reach some undesirable states. In this paper, we propose a new kind of pattern called progressive confident rules to describe sequences of states with an increasing confidence that lead to a particular end state. We give a formal definition of progressive confident rules and their concise set. We devise pruning strategies to reduce the enormous search space. Experiment result shows that the proposed algorithm is efficient and scalable. We also demonstrate the application of progressive confident rules in classification.\nTitle:\nMining progressive confident rules\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.468, mean value for all the writers is 8.648, which makes it 1.007 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper we present a distributed reinforcement learning system that leverages on expert coordination knowledge to improve learning in multi-agent problems. We focus on the scenario where agents can communicate with their neighbors but this communication structure and the number of agents may change over time. We express coordination knowledge as constraints to reduce the joint action space for exploration. We introduce an extra learning level to learn when to make use of these constraints. This extra level is decentralized among the agents, making it suitable for our communication restrictions. Experiment results on tactical real-time strategy and soccer games show that our system is effective in online learning as opposed to existing methods that use individual constraints on agents and coordinated action selection.\nTitle:", "model_inf_time": 1.21}, {"id": "41106", "output": "Guided Mutation for Cooperative Coevolutionary Algorithms", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn coevolution, species are coevolving in a way that the genetic changes of one species in response to another species are reciprocal. One class of coevolution is cooperative coevolution in which species collaborate to solve the problems. The fitness of an individual in a species is assigned based on how well its collaboration with other individuals of another species can perform. As an extension of evolutionary algorithms (EAs), cooperative revolutionary algorithms (CCEAs) operate similar to EAs, except during fitness evaluations. In this paper, we focus on genetic variation operations of a CCEA: mutations. We present how to bias mutations in cooperative coevolution and compare the performance of a CCEA adopting biasing mutations (CCEA-BM) and a conventional CCEA in which all individuals are encoded in binary representations. Our experimental study shows that biasing mutations can improve the performance of a CCEA on function optimization, in particular when high orders of binary representations are used.\nTitle:\nBiasing mutations in cooperative coevolution\n\nAbstract:\nAbstract In this paper, we study the cooperative coevolutionary algorithms (CCEAs) for dynamic optimization. We introduce the CCEAs with two popular types of individuals: (1) random immigrants (RIs) that increase the diversity for changing environments, and (2) elitist individuals that increase the local convergence to the optima. The CCEAs are evaluated on a standard suite of benchmark problems and are compared with evolution strategies (ES). Our experimental results show that the CCEAs are efficient in locating and tracking optima in dynamic environments. They are superior to the ES when the RI individuals and the elitist individuals are used. In addition, we empirically investigate how the CCEAs perform with different parameter settings. These settings include collaboration methods, the use of plus\u2013comma selections, and the number of RI individuals and elitist individuals. We also investigate the CCEAs that use a mutative \u03c3-self adaptation. The CCEAs perform the best when they use the best collaboration method and the plus selection. The use of the mutative \u03c3-self adaptation is insignificant. Our results also show that the CCEAs are more scalable than the ES in dynamic environments.\nTitle:\nCooperative coevolutionary algorithms for dynamic optimization: an experimental study.\n\nAbstract:\nCurrent recommendation methods are mainly classified into content-based, collaborative filtering and hybrid methods. These methods are based on similarity measurements among items or users. In this paper, we investigate recommendation systems from a new perspective based on object typicality and propose a novel typicality-based recommendation approach. Experiments show that our method outperforms compared methods on recommendation quality.\nTitle:\nRecommendation based on object typicality\n\nAbstract:\nGENET is a local search approach with a neural network connectionist architecture for solving constraint satisfaction problems by iterative improvement and incorporates a learning strategy to escape local minima. In this paper, a method within the framework of propagation of posted new constraints and based on the progressive stochastic search of GENET for solving the job shop scheduling constraint satisfaction optimization problem (JSSCSOP) will be presented. The experimental results show that the performance of our method gets competitive when the domain of each variable is not big, even if the size of the problem instances increases.\nTitle:\nApplying GENET to the JSSCSOP\n\nAbstract:\nIn this paper, we study a class of resource allocation problems with changing resource capacities. The system consists of competitive agents that have to choose among several resources to complete their tasks. The objective of the resource allocation is that agents can adapt to the dynamic environment autonomously and make good utilisation of resources. We propose an adaptive strategy for agents to use in the resource allocation system with time-varying capacities. This strategy is based on individual agent's experience and prediction. Simulations show that agents using the adaptive strategy as a whole can adapt effectively to the changing capacity levels and result in better resource utilisation than those proposed in previous work. Finally, we also investigate how the parameters affect the performance of the strategy.\nTitle:\nAn Adaptive Strategy for Resource Allocation with Changing Capacities\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.747, mean value for all the writers is 8.648, which makes it 0.938 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this study, a mutation method called \"guided mutation\" is proposed. In guided mutation, each individual maintains two mutation control parameters: mutation direction and mutation step size. Guidance in mutations is provided by continuous updates on these parameters during the evolutionary process. We apply guided mutations to cooperative coevolutionary algorithm (CCEA), and compare its performance of optimizing nine common problem domains with those of other CCEAs. Our results show that guided mutations can improve the performance of a CCEA in some problem domains. We discuss the implications of our results and suggest some directions for future research.\nTitle:", "model_inf_time": 1.17}, {"id": "41107", "output": "GreenLA: An Energy-Efficient Linear Algebra Library", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nParallelizing H.264/AVC decoding on multicore architectures is challenged by its inherent structural and functional dependencies at both frame and macro-block levels, as macro-blocks and certain frame types must be decoded in a sequential order. So far, dynamic scheduling scheme with recursive tail submit [1], as one of the best existing algorithms, provides a good throughput performance by exploiting macro-block level parallelism and mitigating global queue contention. Nevertheless, it fails to achieve an optimal performance due to 1) the use of global queue, which incurs substantial synchronization overhead when the number of cores increases and 2) the unawareness of cache locality with respect to the underlying hierarchical core/cache topology that results in unnecessary latency, communication cost and load imbalance. In this paper, we propose an adaptive dynamic scheduling scheme that employs multiple local queues to reduce lock contention, and assigns tasks in a cache locality aware and load-balancing fashion so that neighboring macro-blocks are preferably dispatched to nearby cores. We design, implement and evaluate our scheme on a 32-core cc-NUMA SGI server. Compared to existing alternatives by running real benchmark applications, we observe that our scheme produces higher throughput and lower latency with more balanced workload and less communication cost.\nTitle:\nAn Adaptive Dynamic Scheduling Scheme for H.264/AVC Decoding on Multicore Architecture\n\nAbstract:\nL7-filter is a significant component in Linux's QoS framework that classifies network traffic based on application layer data. It enables subsequent distribution of network resources in respect to the priority of applications. Considerable research has been reported to deploy multi-core architectures for computationally intensive applications. Unfortunately, the proliferation of multi-core architectures has not helped fast packet processing due to: 1) the lack of efficient parallelism in legacy network programs, and 2) the non-trivial configuration for scalable utilization on multi-core servers. In this paper, we propose a highly scalable parallelized L7-filter system architecture with affinity-based scheduling on a multi-core server. We start with an analytical study of the system architecture based on an offline design. Similar to Receive Side Scaling (RSS) in the NIC, we develop a model to explore the connection level parallelism in L7-filter and propose an affinity-based scheduler to optimize system scalability. Performance results show that our optimized L7-filter has superior scalability over the naive multithreaded version. It improves system performance by about 50% when all the cores are deployed.\nTitle:\nA scalable multithreaded L7-filter design for multi-core servers\n\nAbstract:\nGeneral purpose computing on GPUs have became increasingly popular over the last decade. Scientific applications with SIMD computation characteristics show considerable performance improvements when run on these massively parallel architectures. However, data dependencies across thread blocks significantly impact the degree of achievable parallelism by requiring global synchronization across multi-processors (SMs) inside the GPU. In order to efficiently run applications with inter-block data dependencies, we need fine-granular `task-based execution models' that will treat SMs inside GPU as stand-alone parallel processing units. Such a scheme will enable efficient execution by utilizing all internal computation elements inside GPU and eliminating unnecessary waits during global barriers. In this paper, we propose a new, dynamic and `all-in-GPU' task execution framework for executing both regular and irregular data-dependent applications on GPUs. Our run-time eliminates the need for global synchronization and minimize inter-SM communication through distributed queues. In our preliminary experiments run on a Tesla c2050 GPU, we have obtained up to 62% more speedup when compared to centralized queue approach. The overhead of system has been measured as low as 5%.\nTitle:\nA paradigm shift in GP-GPU computing: task based execution of applications with dynamic data dependencies\n\nAbstract:\nSince multicore systems offer greater performance via parallelism, future computing is progressing towards use of multicore machines with large number of cores. However, the performance of emerging multithreaded programs often does not scale to fully utilize the available cores. Therefore, simultaneously running multiple multithreaded applications becomes inevitable to fully exploit the computing potential of such machines. However, maximizing the performance and throughput on multicore machines in the presence of multiple multithreaded programs is a challenge for the OS. We have observed that the state-of-the-art contention management algorithms fail to effectively coschedule multithreaded programs on multicore machines. To address the above challenge, we present ADAPT, a scheduling framework that continuously monitors the resource usage of multithreaded programs and adaptively coschedules them such that they interfere with each other's performance as little as possible. In addition, ADAPT selects appropriate memory allocation and scheduling policies according to the workload characteristics. We have implemented ADAPT on a 64-core Supermicro server running Solaris 11 and evaluated it using 26 multithreaded programs including the TATP database application, SPECjbb2005, and programs from Phoenix, PARSEC, and SPEC OMP suites. The experimental results show that ADAPT substantially improves total turnaround time and system utilization relative to the default Solaris 11 scheduler.\nTitle:\nADAPT: A framework for coscheduling multithreaded programs\n\nAbstract:\nCurrent state-of-the-art task scheduling algorithms for network packet processing schedule the program into a parallel-pipeline topology on network processors to maximize the throughput. However, there has been no existing work targeting power budget for packet processing on off-the-shelf multicore architectures. As energy consumption, reliability and cooling cost for packet processing systems become increasingly important, it is necessary to integrate power-awareness into a scheduler to meet the power budget. In this paper, we propose a novel scheduling algorithm to optimize both throughput and latency given a power budget for network packet processing on multicore architectures. This algorithm addresses power-aware parallel-pipeline scheduling problem by applying per-core DVFS to optimally adjust frequency on each core. We implement our algorithm on an AMD machine with two Quad-Core Opteron 2350 processors and compare the results with existing algorithms given the same power budget. For six real packet processing applications, our algorithm improves throughput and reduces latency by an average of 64.6% and 25.2%, respectively.\nTitle:\nOptimizing Throughput and Latency under Given Power Budget for Network Packet Processing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.747, mean value for all the writers is 8.648, which makes it 0.084 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWhile many linear algebra libraries have been developed to optimize their performance, no linear algebra library considers their energy efficiency at the library design time. In this paper, we present GreenLA - an energy efficient linear algebra software package that leverages linear algebra algorithmic characteristics to maximize energy savings with negligible overhead. GreenLA is (1) energy efficient: it saves up to several times more energy than the best existing energy saving approaches that do not modify library source codes; (2) high performance: its performance is comparable to the highly optimized linear algebra library MAGMA; and (3) transparent to applications: with the same programming interface, existing MAGMA users do not need to modify their source codes to benefit from GreenLA. Experimental results demonstrate that GreenLA is able to save up to three times more energy than the best existing energy saving approaches while delivering similar performance compared to the state-of-the-art linear algebra library MAGMA.\nTitle:", "model_inf_time": 1.54}, {"id": "41108", "output": "An iterative design approach to community engagement through interactive prototypes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper explores how people communicate in reference to local interests and suggests information and communication technology (ICT) design for enhancement of local community networks. Qualitative data was gathered from participant observations of local community collective action and open interviews with active community members. Data analysis revealed concepts, leading to categories in relation to local interactions and interests. Design suggestions consider introducing people to local community private-strategic activity via public displays that indicate simple entry points to active participation, and creating information collections according to local community perspectives for long-term reference.\nTitle:\nA qualitative analysis of local community communications\n\nAbstract:\nGrowing participation is a key challenge for the viability of sustainability initiatives, many of which require enactment at a local community level in order to be effective. This paper undertakes a review of technology assisted carpooling in order to understand the challenge of designing participation and consider how mobile social software and interface design can be brought to bear. It was found that while persuasive technology and social networking approaches have roles to play, critical factors in the design of carpooling are convenience, ease of use and fit with contingent circumstances, all of which require a use-centred approach to designing a technological system and building participation. Moreover, the reach of technology platform-based global approaches may be limited if they do not cater to local needs. An approach that focuses on iteratively designing technology to support and grow mobile social ridesharing networks in particular locales is proposed. The paper contributes an understanding of HCI approaches in the context of other designing participation approaches.\nTitle:\nDesigning participation in agile ridesharing with mobile social software\n\nAbstract:\nAgile ridesharing aims to utilise the capability of social networks and mobile phones to facilitate people to share vehicles and travel in real time. However the application of social networking technologies in local communities to address issues of personal transport faces significant design challenges. In this paper we describe an iterative design-based approach to exploring this problem and discuss findings from the use of an early prototype. The findings focus upon interaction, privacy and profiling. Our early results suggest that explicitly entering information such as ride data and personal profile data into formal fields for explicit computation of matches, as is done in many systems, may not be the best strategy. It might be preferable to support informal communication and negotiation with text search techniques.\nTitle:\nInteraction, privacy and profiling considerations in local mobile social software: a prototype agile ride share system\n\nAbstract:\nDesigning technologies to support community communication in local communities of place is a considerable challenge. This research compares and contrasts two approaches: (i) Supporting a community organisation to develop their own IT and (ii) Deploying of a digital noticeboard in the local built environment with the aim of fostering broad community communication. The challenges of appropriating the built environment for public use and soliciting community information for public use are discussed.\nTitle:\nGetting to the nub of neighbourhood interaction\n\nAbstract:\nThe design of applications for dynamic ridesharing or carpooling is often formulated as a matching problem of connecting people with an aligned set of transport needs within a reasonable interval of time and space. This problem formulation relegates social connections to being secondary factors. Technology assisted ridesharing applications that put the matching problem first have revealed that they suffer from being unable to address the factor of social comfort, even after adding friend features or piggybacking on social networking sites. This research aims to understand the fabric of social interactions through which ridesharing happens. We take an online observation approach in order to understand the fabric of social interactions for ridesharing that is happening in highly subscribed online groups of local residents. This understanding will help researchers to identify design challenges and opportunities to support ridesharing in local communities. This paper contributes a fundamental understanding of how social interactions and social comfort precede rideshare requests in local communities.\nTitle:\nUnderstanding the fabric of social interactions for ridesharing through mining social networking sites\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.753, mean value for all the writers is 8.648, which makes it 1.796 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper explores the possibility of a grass roots approach to engaging people in community change initiatives by designing simple interactive exploratory prototypes for use by communities over time that support shared action. The prototype is gradually evolved in response to community use, fragments of data gathered through the prototype, and participant feedback with the goal of building participation in community change initiatives. A case study of a system to support ridesharing is discussed. The approach is compared and contrasted to a traditional IT systems procurement approach.\nTitle:", "model_inf_time": 1.26}, {"id": "41109", "output": "Event-Driven Linear System Modeling for Dataflow Program Optimization", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper describes a methodology for the optimization of portable parallel signal processing applications specified by dataflow programs. The use of dataflow as a programming model for signal processing applications targeting parallel platforms provides an important advantage versus the traditional sequential programming paradigm: the portability of parallelism. The paper introduce a design space exploration methodology for exploring alternative implementations in which abstract traces of a program, representing the actual data dependencies of its parts, are first constructed and then analyzed to guide the refactoring and mapping and of the signal processing applications to best match its intended parallel target. The methodology is demonstrated and evaluated in an at-size case study of an MPEG-4 video decoder.\nTitle:\nOptimization of portable parallel signal processing applications by design space exploration of dataflow programs\n\nAbstract:\nHeterogeneous parallel systems are becoming mainstream computing platforms nowadays. One of the main challenges the development community is currently facing is how to fully exploit the available computational power when porting existing programs or developing new ones with available techniques. In this direction, several design space exploration methods have been presented and extensively adopted. However, defining the feasible design space of a dynamic dataflow program still remains an open issue. This paper proposes a novel methodology for defining such a space through a serial execution. Homotopy theoretic methods are used to demonstrate how the design space of a program can be reconstructed from its serial execution trajectory. Moreover, the concept of dependencies graph of a dataflow program defined in the literature is extended with the definition of two new kinds of dependencies - the Guard Enable and Disable - and the 3-tuple notion needed to represent them.\nTitle:\nRepresenting Guard Dependencies in Dataflow Execution Traces\n\nAbstract:\nThe natural representation of data streams, parallelism, and composition has made dataflow an attractive programming model for expressing a wide range of stream and media processing applications, and has led MPEG and ISO to base their latest video coding standards on this model. This paper describes and compares methodologies and metrics for the optimization of signal processing algorithms represented as dataflow programs. Our approach is based on the analysis of traces and addresses some of the complexity challenges that arise from the very large data sets that are required for evaluating real-world applications. The methodology and experimental results are demonstrated and evaluated in two at-size case studies, an MPEG-4 SP and an AVC/H.264 video decoders.\nTitle:\nProfiling of Dataflow Programs Using Post Mortem Causation Traces\n\nAbstract:\nThe paper introduces a new methodology for pipeline synthesis with applications to data flow high level system design. The pipeline synthesis is applied to dataflow programs whose operators are translated into graphs and dependencies relations that are then processed for the pipeline architecture optimization. For each pipeline-stage time, a minimal number of pipeline stages is first determined and then an optimal assignment of operators to stages is generated with the objective of minimizing the total pipeline register size. The obtained \"optimal\" pipeline schedule is automatically transformed back into a dataflow program that then can be synthesized to efficient hardware implementations. Two new pipeline scheduling:\"least cost search branch and bound\", and an heuristic technique have been developed. The first algorithm yields global optimum solutions for middle size designs, whereas the second one generates close-to-optimal solutions for large designs. Experimental results on FPGA designs show that the total pipeline register size gain in a range up to 4.68x can be achieved. The new algorithms overcome the known downward and upward direction dataflow graph traversal algorithms concerning the amount of pipeline register size by up to 100% on average.\nTitle:\nSynthesis and Optimization of Pipelines for HW Implementations of Dataflow Programs\n\nAbstract:\nThe increasing ubiquity of heterogeneous parallel computing platforms nowadays creates the challenge to fully exploit the available computational power when porting existing programs or developing new applications with portability in mind. Existing design space exploration methods focus on specialized applications amenable to compile-time analysis. Real-world applications, however, tend to exhibit complex behavior that depends on input data and even timing. This paper proposes a methodology for creating a finite (approximate) representation of the design space of general streaming applications, based on detailed tracking of a serial run of the program. Homotopy theoretic methods are used to demonstrate how the design space of a program can be reconstructed from its serial execution trajectory. Moreover, the concept of a dependency graph of a dataflow program defined in the literature is extended with the definition of two new kinds of dependencies - the Guard Enable and Disable - and the 3-tuple notion needed to represent them.\nTitle:\nSystems design space exploration by serial dataflow program execution\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.928, mean value for all the writers is 8.648, which makes it 1.092 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nExecution trace graph analysis of dataflow programs has been demonstrated to be an effective way for exploring and optimizing the design space of many core applications. In this work a novel transformation from the execution trace graph to an event driven linear system is proposed. It is also illustrated how the trace space of can be effectively reduced and well known system control techniques can be efficiently used in order to find close to optimal solutions. In particular, the problem of finding a bounded buffer size configuration is proposed and solved using a model predictive controller. Two design examples, a JPEG and an MPEG HEVC decoder have been used to demonstrate the effectiveness of the approach.\nTitle:", "model_inf_time": 1.43}, {"id": "41110", "output": "From High-Level Goals to Primitive Commands: A Reconfigurable Architecture for Robot Doorway Navigation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nComplex tasks are usually described as high-level goals, leaving out the details on how to achieve them. However, to control a robot, details must be provided. Having the robot move itself to and through an unknown, and possibly narrow, doorway is an example of such a task. The authors illustrate the difficulty of such a task using actual data from a real robot. The authors show how the transformation from high-level goals to primitive commands can be performed at execution time and they propose an architecture based on reconfigurable objects that contain domain knowledge and knowledge about the sensors and actuators available. The authors then show how their approach is used in solving the illustrated task\nTitle:\nWhy is it so difficult for a robot to pass through a doorway using ultrasonic sensors?\n\nAbstract:\nComplex robot tasks are usually described as high level goals, with no details on how to achieve them. However, details must be provided to generate primitive commands to control a real robot. A sensor explication concept that makes details explicit from general commands is presented. We show how the transformation from high-level goals to primitive commands can be performed at execution time and we propose an architecture based on reconfigurable objects that contain domain knowledge and knowledge about the sensors and actuators available. Our approach is based on two premises: 1) plan execution is an information gathering process where determining what information is relevant is a great part of the process; and 2) plan execution requires that many details are made explicit. We show how our approach is used in solving the task of moving a robot to and through an unknown, and possibly narrow, doorway; where sonic range data is used to find the doorway, walls, and obstacles. We illustrate the difficulty of such a task using data from a large number of experiments we conducted with a real mobile robot. The laboratory results illustrate how the proper application of knowledge in the integration and utilization of sensors and actuators increases the robustness of plan execution.\nTitle:\nSensor explication: knowledge-based robotic plan execution through logical objects.\n\nAbstract:\nWe would like robots to recognize and handle situations that do not conform with normal operating conditions. We want to be able to do this without having to consider explicitly errors caused by missing or defective parts, or by malfunctioning. To this end we present the detailed design of a system in which the controller of the robot takes advantage of large knowledge bases to ensure proper execution of the robot task. Real time considerations played a large role in our design.\nTitle:\nThe role of knowledge in the architecture of a robust robot control\n\nAbstract:\nReduced cost of robotic hardware enables the use of teams of robots instead of a single device. Multi-robot approaches promise faster results and more robust systems as each individual robot becomes dispensable. Given higher numbers of robots, writing dependable control software becomes more complex and thus more expensive. Consequently, a software architecture that is readily applied to new missions becomes essential. In the following, an architecture for distributed control of a team of heterogeneous mobile robots is introduced. Design as well as implementation details are presented. A distinguishing feature of the architecture is its versatility in handling resources. An example application for a surveillance task is discussed.\nTitle:\nA robot team for surveillance tasks: Design and architecture\n\nAbstract:\nTask allocation is ubiquitous in computer science and robotics, yet some problems have received limited attention in the computer science and AI community. Specifically, we will focus on multi-robot task allocation problems when tasks have time windows or ordering constraints. We will outline the main lines of research and open problems.\nTitle:\nMulti-Robot Allocation of Tasks with Temporal and Ordering Constraints.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.457, mean value for all the writers is 8.648, which makes it 0.163 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nComplex tasks are usually described as high-level goals, leaving out the details on how to achieve them. However, to control a robot, the task must be described in terms of primitive commands for the robot. Having the robot move itself to and through an unknown, and possibly narrow, doorway is an example of such a task. It is shown how the transformation from high-level goals to primitive commands can be performed at execution time and an architecture is proposed based on reconfigurable objects that contain domain knowledge and knowledge about the sensors and actuators available. The approach is illustrated using actual data from a real robot.\nTitle:", "model_inf_time": 1.69}, {"id": "41111", "output": "Association Rule Mining for Effective and Scalable Web Personalization", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWeb usage mining, possibly used in conjunction with standard approaches to personalization such as collaborative filtering, can help address some of the shortcomings of these techniques, including reliance on subjective user ratings, lack of scalability, and poor performance in the face of high-dimensional and sparse data. However, the discovery of patterns from usage data by itself is not sufficient for performing the personalization tasks. The critical step is the effective derivation of good quality and useful (i.e., actionable) \u201caggregate usage profiles\u201d from these patterns. In this paper we present and experimentally evaluate two techniques, based on clustering of user transactions and clustering of pageviews, in order to discover overlapping aggregate profiles that can be effectively used by recommender systems for real-time Web personalization. We evaluate these techniques both in terms of the quality of the individual profiles generated, as well as in the context of providing recommendations as an integrated part of a personalization engine. In particular, our results indicate that using the generated aggregate profiles, we can achieve effective personalization at early stages of users' visits to a site, based only on anonymous clickstream data and without the benefit of explicit input by these users or deeper knowledge about them.\nTitle:\nDiscovery and Evaluation of Aggregate Usage Profiles for Web Personalization\n\nAbstract:\nPersonalization based on Web usage mining can enhance the effectiveness and scalability of collaborative filtering. However, without semantic knowledge about the underlying domain, such systems cannot recommend different types of complex objects based in their underlying properties and attributes. This paper provides an overview of approaches for incorporating semantic knowledge into Web usage mining and personalization. processes. We present two general approaches to integrate semantic knowledge extracted from the content features of pages into the usage-based personalization process. Next, we present a general framework of integrating domain ontologies with Web Usage Mining and Personalization. In each case, we discuss how semantic knowledge is leveraged and represented in the preprocessing and pattern discovery phases, as well as how it is used to enhance usage-based personalization.\nTitle:\nA Road Map to More Effective Web Personalization: Integrating Domain Knowledge with Web Usage Mining\n\nAbstract:\nThe open nature of collaborative recommender systems allows attackers who inject biased profile data to have a significant impact on the recommendations produced. Standard memory-based collaborative filtering algorithms, such as k- nearest neighbor, have been shown to be quite vulnerable to such attacks. In this paper, we examine the robustness of model-based recommendation algorithms in the face of profile injection attacks. In particular, we consider two recommendation algorithms, one based on k-means clustering and the other based on Probabilistic Latent Semantic Analysis (PLSA). These algorithms aggregate similar users into user segments that are compared to the profile of an active user to generate recommendations. Traditionally, model-based algorithms have been used to alleviate the scalability problems associated with memory-based recommender systems. We show, empirically, that these algorithms also offer significant improvements in stability and robustness over the standard k- nearest neighbor approach when attacked. Furthermore, our results show that, particularly, the PLSA-based approach can achieve comparable recommendation accuracy.\nTitle:\nModel-based collaborative filtering as a defense against profile injection attacks\n\nAbstract:\nCollaborative recommendation is effective at representing a user's overall interests and tastes, and finding peer users that can provide good recommendations. However, it remains a challenge to make collaborative recommendation sensitive to a user's specific context and to the changing shape of user interests over time. Our approach to building context-sensitive collaborative recommendation is a hybrid one that incorporates semantic knowledge in the form of a domain ontology. User profiles are defined relative to the ontology, giving rise to an ontological user profile. In this paper, we describe how ontological user profiles are learned, incrementally updated, and used for collaborative recommendation. Using book rating data, we demonstrate that this recommendation algorithm offers improved coverage, diversity, personalization, and cold-start performance while at the same time enhancing recommendation accuracy.\nTitle:\nImproving the effectiveness of collaborative recommendation with ontology-based user profiles\n\nAbstract:\nThe open nature of collaborative recommender systems present a security problem. Attackers that cannot be readily distinguished from ordinary users may inject biased profiles, degrading the objectivity and accuracy of the system over time. The standard user-based collaborative filtering algorithm has been shown quite vulnerable to such attacks. In this paper, we examine relevance measures that complement neighbor similarity and their influence on algorithm robustness. In particular, we consider two techniques, significance weighting and trust weighting, that attempt to calculate the utility of a neighbor with respect to rating prediction. Such techniques have been used to improve prediction accuracy in collaborative filtering. We show that significance weighting, in particular, also results in improved robustness under profile injection attacks.\nTitle:\nImpact of relevance measures on the robustness and accuracy of collaborative filtering\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.375, mean value for all the writers is 8.648, which makes it 0.233 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTo engage visitors to a Web site at a very early stage (i.e., before registration or authentication), personalization tools must rely primarily on clickstream data captured in Web server logs. The lack of explicit user ratings as well as the sparse nature and the large volume of data in such a setting poses serious challenges to standard collaborative filtering techniques in terms of scalability and performance. Web usage mining techniques such as clustering that rely on offline pattern discovery from user transactions can be used to improve the scalability of collaborative filtering, however, this is often at the cost of reduced recommendation accuracy. In this paper we propose effective and scalable techniques for Web personalization based on association rule discovery from usage data. Through detailed experimental evaluation on real usage data, we show that the proposed methodology can achieve better recommendation effectiveness, while maintaining a computational advantage over direct approaches to collaborative filtering such as the k-nearest-neighbor strategy.\nTitle:", "model_inf_time": 1.43}, {"id": "41112", "output": "Statistical Analysis of Source Localization in Reverberant Environments", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe main obstacle for designing robust microphone-based source localization systems is the effects of room reverberation. Utilizing results from statistical room acoustics, we analyze the performance of GCC-based methods for time-delay estimation. An interesting outcome of the analysis is that the so-called PHAT time-delay estimator is shown to be optimal among a class of cross-correlation based time-delay estimators.\nTitle:\nAnalysis of time-delay estimation in reverberant environments.\n\nAbstract:\nReverberant environments pose a challenge to speech acquisition from distant microphones. Approaches using microphone arrays have met with limited success. Recent research using audio-visual sensors for tasks such as speaker localization has shown improvement over traditional audio-only approaches. Using computer vision techniques we can estimate the orientation of the speaker's head in addition to the location of the speaker. In this paper we study the utility of using the head pose information for effective beamforming and clean speech acquisition from distant microphones. The improvements in speech recognition accuracy relative to that of a close talking microphone are presented and the results provide sufficient motivation for incorporating head pose information in beamforming techniques.\nTitle:\nRole of head pose estimation in speech acquisition from distant microphones\n\nAbstract:\nScene understanding in the context of a smart meeting room involves the extraction of various kinds of cues at different levels of semantic abstraction. Specifically, hu- man activity in a scene is usually monitored using arrays of audio and visual sensors. Tasks such as person local- ization and tracking, speaker ID, focus of attention detec- tion, speech recognition and affective state recognition are among them. In this paper we demonstrate a system that ex- tracts such information by synergistically combining the in- formation from the various tasks to support each other. We exploit the fact that the output of one kind of human activ- ity analysis task contains valuable information for another such block and by interconnecting them, a robust system re- sults. We demonstrate this in a smart meeting room context equipped with 3 cameras and 16 microphones. The system performs the tasks of person tracking, head pose estimation, beamforming, speaker ID and speech recognition using au- dio and visual cues. The novelty lies in putting together the tasks such that they can provide relevant information to one another. We evaluate the performance of our system and present results for tasks such as keyword spotting and track- ing re-identification on real-world meeting scenes collected in our audio-visual testbed.\nTitle:\nHierarchical audio-visual cue integration framework for activity analysis in intelligent meeting rooms\n\nAbstract:\nIn this paper we proposed to solve the eye detection and localization problem under a general statistical model based object detection framework. A binary tree representation is used to discover the objects\u9a74 underlying statistical structure. Tree structures enable us to describe the object local statistical structure in a coarse-to-fine fashion. Each subtree explains the statistics for certain local substructure. The tree is built in a top-down fashion. Subsets with negligible conditional independency are found by k-means clustering using mutual information. The conditionally independent features are separated into different subtrees, while more dependent features are tended to appear close in the tree. The distribution of the object can be learned accordingly. Gaussian mixture in the independent component analysis (ICA) subspace is used to model the distribution of each high dependent feature subset, where each independent component explains the local substructure. The use of tree structure enables us to learn the distribution recursively by applying Bayesian criterion. Substantial experiments were done to evaluate the performance over the eyes detection accuracy as well as the localization ability. Experimental results show a better detection accuracy than the Viisage system with a reasonable localization ability, which validate the algorithm.\nTitle:\nA Binary Tree for Probability Learning in Eye Detection\n\nAbstract:\nAs part of human-centered driver assist framework for holistic multimodal sensing, we present an evaluation of independent vector analysis for speaker recognition task inside an automotive vehicle. Independent component analysis-based blind source separation algorithms have attracted attentions in recent years in the application of speech separation and enhancement. Compared to the traditional beamforming technique, the blind source separation method may typically require less number of microphones and perform better under reverberant environment. We recorded two speakers in the driver and front-passenger seats talking simultaneously inside a car and used independent vector analysis to separate the two speech signals. In the speaker recognition task, we show that by training the model with the speech signals from the IVA process, our system is able to achieve 95 % accuracy from a 1-second speech segment.\nTitle:\nIn-Vehicle Speaker Recognition Using Independent Vector Analysis\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.118, mean value for all the writers is 8.648, which makes it 1.254 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nRoom reverberation is typically the main obstacle for designing robust microphone-based source localization systems. The purpose of the paper is to analyze the achievable performance of acoustical source localization methods when room reverbera- tion is present. To facilitate the analysis, we apply well known results from room acoustics to develop a simple but useful statistical model for the room transfer function. The properties of the statistical model are found to correlate well with results from real data measurements. The room transfer function model is further applied to analyze the statistical properties of some existing methods for source local- ization. In this respect we consider especially the asymptotic error variance and the probability of an anomalous estimate. A note- worthy outcome of the analysis is that the so-called PHAT time- delay estimator is shown to be optimal among a class of cross- correlation based time-delay estimators. To verify our results on the error variance and the outlier probability we apply the image method for simulation of the room transfer function.\nTitle:", "model_inf_time": 1.38}, {"id": "41113", "output": "A Fast Binary to Modulo M Translator Based on the \"Casting Out Nine\" Rule", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we study multicasting in the self-routing multistage interconnection networks (MINs) for asynchronous transfer mode (ATM) switch architectures. Many B-ISDN applications require multicast connections in addition to conventional point-to-point connections. This paper presents a novel approach to support multicast connection, on the basis of a restricted address encoding scheme which constructs a short fixed-size multicast header and a recursive scheme that recycles a multicast packet one or more times through the network to send it to the desired destinations. The proposed two-phase multicast algorithm provides deadlock-free multiple multicast connections in MIN-based ATM switches. The emphasis is on analyzing the performance of an unbuffered MIN-based switch using the multicast algorithm in terms of network throughput. The proposed algorithm can be easily applied to buffered MIN-based ATM switches.\nTitle:\nPerformance Analysis of a Multicast Switch Based on Multistage Interconnection Networks\n\nAbstract:\nWe study fault-tolerant multicasting in multistage interconnection networks (MINs) for constructing large-scale multicomputers. In addition to point-to-point routing among processor nodes, efficient multicasting is critical to the performance of multicomputers. This paper presents a new approach to provide fault-tolerant multicasting, which employs the restricted header encoding schemes. The proposed approach is based on a recursive scheme in order to send a multicast packet to the desired destinations detouring faulty element(s). In the proposed fault-tolerant multicasting, a multicast packet is routed to its own destinations in only two passes through the MIN having a number of faulty elements by exploiting its nonblocking property\nTitle:\nFault-tolerant multicasting in multistage interconnection networks\n\nAbstract:\nA multistage interconnection network is a suitable class of interconnection architecture for constructing large-scale multicomputers. Broadcast and multicast communication are fundamental in supporting collective communication operations such as reduction and barrier synchronization. In this paper, we propose a new multicast technique in wormhole-switched bidirectional multistage banyan networks for constructing large-scale multicomputers. To efficiently support broadcast and multicast with simple additional hardware without deadlock, we propose a two-phase multicast algorithm which takes only two transmissions to perform a broadcast and a multicast to an arbitrary number of desired destinations. We encode a header as a cube and adopt the most upper input link first scheme with periodic priority rotation as arbitration mechanism on contented output links. We coalesce the desired destination addresses into multiple number of cubes. And then, we evaluate the performance of the proposed algorithm by simulation. The proposed two-phase multicast algorithm makes a significant improvement in terms of latency. It is noticeable that the two-phase algorithm keeps broadcast latency as efficient as the multicast latency of fanout $2^m$ where $m$ is the minimum integer satisfying $2^m \\geq \\sqrt{N}$($N$ is a network size).\nTitle:\nTwo-phase multicast in wormhole-switched bidirectional multistage Banyan networks\n\nAbstract:\nIn this letter, we propose a novel IEEE 802.11 WLAN based MAC to achieve fairness and efficiency for large groups of users. Unlike the current MAC, the proposed scheme excludes nodes which had an access to the channel recently from the contention by controlling a value of CW. The proposed scheme does not only improves throughput performance by reducing contention effectively but also realizes access fairness regardless of the number of contending users. The simulation results show that the proposed scheme can be a nice alternative for future MAC under large population of users.\nTitle:\nAQM for Weighted Fairness in Wireless LANs\n\nAbstract:\nWe discuss the multicast communication in the self-routing multistage interconnection network (MIN) for constructing the internal architecture of asynchronous transfer mode (ATM) switches. Many applications of ATM switches require multicast communications in addition to conventional point-to-point communications. This paper presents a novel approach to supporting multicast communication, on the basis of the recursive scheme that recycles a multicast packet one or more times through the network to reach at desired destinations. We also propose cost-effective multicast algorithms providing deadlock-freedom in MIN-based ATM switches. The proposed algorithms require a small and fixed number of recycling passes and a reasonable number of links used. The proposed algorithms can be easily applicable to buffered MIN-based ATM switches.\nTitle:\nNovel algorithms for multicast communication in self-routing MIN-based ATM switches\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.536, mean value for all the writers is 8.648, which makes it 1.611 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA fast binary to modulo M translation, where M is a prime number, is necessary in many applications. The prime memory system of an array processor which allows parallel, conflict-free access to various slices of a data array is one such application. We review existing modulo M translators, including the one used in the Burroughs Scientific Processor (BSP). We then propose a very simple and fast translator for a restricted but very useful class of M which includes, as a subclass, the form suggested by P. Budnik and D. J. Kuck ( IEEE Trans. Comput . C-20, 12 (Dec. 1971), 1566\u20131569). Our basic idea stems from the traditional \u201ccasting out nine\u201d rule, which has been used as a quick method for checking the accuracy of remainders in the decimal number system. By generalizing this rule and extending it for the binary number system, we have obtained a new modulo M translator. The new modulo M translator is built upon a set of modulo adders, and is simpler and faster than previous modulo M translators such as the one employed in the BSP.\nTitle:", "model_inf_time": 1.72}, {"id": "41114", "output": "On Efficiently Scheduling Series-Parallel Dags for Area Maximization", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA new quality metric, called area, is introduced for schedules that execute dags, i.e., computations having intertask dependencies. Motivated by the temporal unpredictability encountered when computing over the Internet, the goal under the new metric is to maximize the average number of tasks that are eligible for execution at each step of a computation. Area-maximization is a weakening of IC-optimality, which strives to maximize the number of eligible tasks at every step of the computation. In contrast to IC-optimal schedules, area-maximizing schedules exist for every dag. For dags that admit IC-optimal schedules, all area-maximizing schedules are IC-optimal, and vice versa. The basic properties of this metric are derived in this paper, and tools for efficiently crafting area-maximizing schedules for large classes of computationally significant dags are developed. Several of these results emerge from a close connection between area-maximizing scheduling and the MAX Linear-Arrangement Problem for Dags.\nTitle:\nOn scheduling dags to maximize area\n\nAbstract:\nEarlier work has developed the underpinnings of a theory of scheduling\n\tcomputations having intertask dependencies - modeled via dags - for\n\tInternet-based computing. The goal of the schedules produced is to\n\trender tasks eligible for execution at the maximum possible rate.\n\tThis goal aims: (a) to utilize remote clients' computational resources\n\twell, by always having work to allocate to an available client; (b)\n\tto lessen the likelihood of the \"gridlock\" that ensues when a computation\n\tstalls for lack of eligible tasks. The dags handled by the theory\n\tthus far are those that can be constructed from a given collection\n\tof bipartite building-block dags via the operation of dagcomposition.\n\tThe current paper extends the range of applicability of the theory\n\tby significantly expanding the repertoire of building-block dags\n\tthat the scheduling algorithms can handle. Thereby, the theory can\n\tnow schedule large classes of \"expansive\" and \"reductive\" dags optimally.\nTitle:\nOn Scheduling Expansive and Reductive Dags for Internet-Based Computing\n\nAbstract:\nMany modern computing platforms, including \"aggressive\" multicore architectures, proposed exascale architectures, and many modalities of Internetbased computing are \"task hungry\"--their performance is enhanced by always having as many tasks eligible for allocation to processors as possible. The AREAOriented scheduling (AO-scheduling) paradigm for computations with intertask dependencies--modeled as DAGs--was developed to address the \"hunger\" of such platforms, by executing an input DAG so as to render tasks eligible for execution quickly. AO-scheduling is a weaker, but more robust, successor to IC-scheduling. The latter renders tasks eligible for execution maximally fast--a goal that is not achievable for many DAGs.AO-scheduling coincides with IC-scheduling on DAGs that admit optimal IC-schedules--and optimal AO-scheduling is possible for all DAGs. The computational complexity of optimal AO-scheduling is not yet known; therefore, this goal is replaced here by a multi-phase heuristic that produces optimal AO-schedules for series-parallel DAGs but possibly suboptimal schedules for general DAGs. This paper employs simulation experiments to assess the computational benefits of AO-scheduling in a variety of scenarios and on a range of DAGs whose structure is reminiscent of ones encountered in scientific computing. The experiments pit AO-scheduling against a range of heuristics, fromlightweight ones such as FIFO scheduling to computationally more intensive ones that mimic IC-scheduling's local decisions. The observed results indicate that AO-scheduling does enhance the efficiency of task-hungry platforms, by amounts that vary according to the availability patterns of processors and the structure of the DAG being executed.\nTitle:\nAssessing the computational benefits of AREA-oriented DAG-scheduling\n\nAbstract:\nMany modern computing platforms are \"task-hungry\": their performance is enhanced by always having as many tasks available for execution as possible. IC-scheduling, a master-worker framework for executing static computations that have intertask dependencies (modeled as DAGS), was developed with precisely the goal of rendering a computation-dag's tasks eligible for execution at the maximum possible rate. The current paper addresses the problem of enhancing IC-scheduling so that it can accommodate the varying computational resources of different workers, by clustering a computation-DAG'S tasks, while still producing eligible (now, clustered) tasks at the maximum possible rate. The task-clustering strategies presented exploit the structure of the computation being performed, ranging from a strategy that works for any DAG, to ones that build increasingly on the explicit structure of the dag being scheduled.\nTitle:\nOn Clustering Dags For Task-Hungry Computing Platforms\n\nAbstract:\nEarlier work has developed the underpinnings of IC-Scheduling Theory,\n\ta frame- work for scheduling computations having intertask dependencies|modeled\n\tvia dags- for Internet-based computing. The goal of the schedules\n\tproduced is to render tasks eligible for execution at the maximum\n\tpossible rate, with the dual aim of: (a) utiliz- ing remote clients'\n\tcomputational resources well, by always having work to allocate to\n\tan available client; (b) lessening the likelihood of a computation's\n\tstalling for lack of eligible tasks. The dags handled by the Theory\n\tthus far are those that can be decomposed into a given collection\n\tof bipartite building-block dags via the operation of dag-decomposition.\n\tA basic tool in constructing schedules is a relation B, which allows\n\tone to \"prioritize\" the scheduling of a complex dag's building blocks.\n\tThe current paper extends IC-Scheduling Theory in two ways: by expanding\n\tsignificantly the repertoire of dags that the Theory can schedule\n\toptimally, and by allowing one sometimes to shortcut the algorithmic\n\tprocess required to find optimal schedules. The expanded repertoire\n\tnow allows the Theory to schedule optimally, among other dags, a\n\tlarge range of dags that are either \"expansive,\" in the sense that\n\tthey grow outward from their sources, or \"reductive,\" in the sense\n\tthat they grown inward toward their sinks. The algorithmic shortcuts\n\tallow one to \"read off\" an optimal schedule for a dag from a given\n\toptimal schedule for the dag's dual, which is obtained by reversing\n\tall arcs (thereby exchanging the roles of sources and sinks).\nTitle:\nAdvances in IC-Scheduling Theory: Scheduling Expansive and Reductive Dags and Scheduling Dags via Duality\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.016, mean value for all the writers is 8.648, which makes it 0.539 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nEarlier work introduced a new optimization goal for DAG schedules: the \"AREA\" of the schedule. AREA-maximizing schedules are intended for computational environments--such as Internet-based computing and massively multicore computers--that benefit from DAG-schedules that produce executioneligible tasks as fast as possible. The earlier study of AREA-maximizing schedules showed how to craft such schedules efficiently for DAGs that have the structure of trees and other, less well-known, families of DAGs. The current paper extends the earlier work by showing how to efficiently craft AREA-maximizing schedules for series-parallel DAGs, a family that arises, e.g., in multi-threaded computations. The tools that produce the schedules for series-parallel DAGs promise to apply also to other large families of computationally significant DAGs.\nTitle:", "model_inf_time": 1.71}, {"id": "41115", "output": "Adaptive Locality Weighted Multisource Joint Sparse Representation for Remote Sensing Classification", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA major difficulty of remote sensing image classification comes from the lack of quality training data. In this paper, we propose a domain adaptation algorithm to alleviate this problem by leveraging labeled data from an auxiliary data source. The proposed approach aims to align the class distributions of two domains while enhancing their discriminability. Separate transformations are used to proj...\nTitle:\nDomain Adaptation for Robust Classification of Disparate Hyperspectral Images.\n\nAbstract:\nHyperspectral imagery has emerged as a popular sensing modality for a variety of applications, and sparsity-based methods were shown to be very effective to deal with challenges coming from high dimensionality in most hyperspectral classification problems. In this paper, we challenge the conventional approach to hyperspectral classification that typically builds sparsity-based classifiers directly...\nTitle:\nMorphologically Decoupled Structured Sparsity for Rotation-Invariant Hyperspectral Image Analysis.\n\nAbstract:\nThe developments in sensor technology have made the high-resolution hyperspectral remote sensing data available to the remote sensing analyst for ground-cover classification and target recognition tasks. The inherent high dimensionality of such data sets and the limited ground-truth data availability in many real-life operating scenarios necessitate such hyperspectral classification systems to emp...\nTitle:\nDecision-Level Fusion of Spectral Reflectance and Derivative Information for Robust Hyperspectral Land Cover Classification.\n\nAbstract:\nThe 2008 Data Fusion Contest organized by the IEEE Geoscience and Remote Sensing Data Fusion Technical Committee deals with the classification of high-resolution hyperspectral data from an urban area. Unlike in the previous issues of the contest, the goal was not only to identify the best algorithm but also to provide a collaborative effort: The decision fusion of the best individual algorithms wa...\nTitle:\nDecision Fusion for the Classification of Hyperspectral Data: Outcome of the 2008 GRS-S Data Fusion Contest\n\nAbstract:\nExploiting disparate features from potentially different data sources with multiple-kernel based machine learning is a promising approach for analyzing geo-spatial data. A mixture-of-kernel approach can facilitate construction of a more effective training data pool with Active Learning (AL). In addition, this could alleviate the computational burden in AL implementations. Kernel based learning requires hyperparameter tuning for model selection. Further, an optimal function is required to integrate different features or data sources appropriately in the kernel induced space. Both kernel parameters and kernel combination functions may need to be tuned at each AL learning step, which is potentially very time-consuming. In this paper, a novel multiple kernel active learning algorithm is proposed that promises enhanced classification, improved AL performance, and a mechanism for automatic selection of kernel weights in the mixture-of-kernels. We demonstrate the usefulness of the proposed framework with results for both feature fusion and sensor fusion tasks.\nTitle:\nMultiple kernel active learning for robust geo-spatial image analysis\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.982, mean value for all the writers is 8.648, which makes it 1.991 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we propose an adaptive locality weighted multisource joint sparse representation classification (ALWMJ-SRC) model for the classification of multisource remote sensing data. Although the notion of multitask joint sparsity has been recently developed for data fusion and has shown to be effective for various applications, in this paper, we suggest that there are important limitations s...\nTitle:", "model_inf_time": 1.22}, {"id": "41116", "output": "Minimum Mobile Guarded Guards in Grids", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA grid P is a connected union of vertical and horizontal segments. A mobile guard is a guard which is allowed to move along a grid segment, thus a point x is seen by a mobile guard g if either x is on the same segment as g or x is on a grid segment crossing g. A set of mobile guards is weakly cooperative if at any point on its patrol, every guard can be seen by at least one other guard. In this paper we discuss the classes of polygon-bounded grids and simple grids for which we propose a quadratic time algorithm for solving the problem of finding the minimum weakly cooperative guard set (MinWCMG). We also provide an O(nlogn) time algorithm for the MinWCMG problem in horizontally or vertically unobstructed grids. Next, we investigate complete rectangular grids with obstacles. We show that as long as both dimensions of a grid are larger than the number of obstacles k, k+2 weakly cooperative mobile guards always suffice to cover the grid. Finally, we prove that the MinWCMG problem is NP-hard even for grids in which every segment crosses at most three other segments. Consequently, the minimum k-periscope guard problem for 2D grids is NP-hard as well, and this answers the question posed by Gewali and Ntafos [L.P. Gewali, S. Ntafos, Covering grids and orthogonal polygons with periscope guards, Computational Geometry: Theory and Applications 2 (1993) 309-334].\nTitle:\nCooperative mobile guards in grids\n\nAbstract:\nIn this paper we deal with one of the art gallery problems, namely the problem of fault tolerant guarding of grids, which is defined as the problem of finding two disjoint guard sets in a grid. Although determining the existence of such a structure is easy in general grids, the task of minimising the number of guards taken over both teams is shown to be NP-hard even for subcubic grids. Moreover, we propose a 6/5-approximation algorithm for solving the fault tolerant guard problem in grids.\nTitle:\nFault tolerant guarding of grids\n\nAbstract:\nA team of  mobile robots is deployed on a weighted graph whose edge weights represent distances. The robots move perpetually along the domain, represented by all points belonging to the graph edges, without exceeding their maximum speed. The robots need to patrol the graph by regularly visiting all points of the domain. In this paper, we consider a team of robots (patrolmen), at most  of which may be unreliable, i.e., they fail to comply with their patrolling duties. What algorithm should be followed so as to minimize the maximum time between successive visits of every edge point by a reliable patrolman? The corresponding measure of efficiency of patrolling called  has been widely accepted in the robotics literature. We extend it to the case of untrusted patrolmen; we denote by  the maximum time that a point of the domain may remain unvisited by reliable patrolmen. The objective is to find patrolling strategies minimizing . We investigate this problem for various classes of graphs. We design optimal algorithms for line segments, which turn out to be surprisingly different from strategies for related patrolling problems proposed in the literature. We then use these results to study general graphs. For Eulerian graphs , we give an optimal patrolling strategy with idleness , where || is the sum of the lengths of the edges of . Further, we show the hardness of the problem of computing the idle time for three robots, at most one of which is faulty, by reduction from 3-edge-coloring of cubic graphs\u2014a known NP-hard problem. A byproduct of our proof is the investigation of classes of graphs minimizing idle time (with respect to the total length of edges); an example of such a class is known in the literature under the name of Kotzig graphs.\nTitle:\nWhen Patrolmen Become Corrupted: Monitoring a Graph Using Faulty Mobile Robots.\n\nAbstract:\nConsider an orthogonal grid of streets and avenues in a Manhattan-like city populated by stationary sensor modules at some crossings and mobile robots that can serve as relays of information that the modules exchange. Both module-module and module-robot communication is limited to a straight line of sight along a row or a column of the grid. We present a number of distributed algorithms for the robots to establish a connected network of a given set S of modules by moving to suitable locations in the grid and serving as relays. It is shown that the number of robots required to connect the modules depends not only on the number c of connected components in the visibility graph of S, but also on the degree of symmetry in S. In most cases, our algorithms use the worst case optimal number of robots for a given c.\nTitle:\nForming a connected network in a grid by asynchronous and oblivious robots\n\nAbstract:\nWe study the problem of mapping an unknown environment represented as an unlabelled undirected graph. A robot (or automaton) starting at a single vertex of the graph G has to traverse the graph and return to its starting point building a map of the graph in the process. We are interested in the cost of achieving this task (whenever possible) in terms of the number of edge traversal made by the robot. Another optimization criteria is to minimize the amount of information that the robot has to carry when moving from node to node in the graph. We present efficient algorithms for solving map construction using a robot that is not allowed to mark any vertex of the graph, assuming the knowledge of only an upper bound on the size of the graph. We also give universal algorithms (independent of the size of the graph) for map construction when only the starting location of the robot is marked. Our solutions apply the technique of universal exploration sequences to solve the map construction problem under various constraints. We also show how the solution can be adapted to solve other problems such as the gathering of two identical robots dispersed in an unknown graph.\nTitle:\nConstructing a map of an anonymous graph: applications of universal sequences\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.465, mean value for all the writers is 8.648, which makes it 0.156 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA set of mobile guards in a grid is guarded if at any point on its patrol segment every guard can be seen by at least one other guard. Herein we discuss a class of polygon-bounded grids and simple grids for which we propose a quadratic time algorithm for solving the problem of finding the minimum set of mobile guarded guards (the MinMGG problem). Recall that the MinMGG problem is NP-hard even for grids every segment of which crosses at most three other segments. We also provide an O(n log n) time algorithm for the MinMGG problem in horizontally or vertically unobstructed grids. Finally, we investigate complete rectangular grids with obstacles. We show that if both the vertical and the horizontal sizes of the grid are larger than the number of obstacles k, k+2 mobile guarded guards always suffice to cover the grid.\nTitle:", "model_inf_time": 1.43}, {"id": "41117", "output": "A Flexible Granularity Approach to Hardware/Software Partitioning", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nHigh-level estimation techniques are of paramount importance for design decisions like hardware/software partitioning or design space explorations. In both cases an appropriate compromise between accuracy and computation time determines about the feasibility of those estimation techniques. In this paper we present high-level estimation techniques for hardware effort and hardware/software communication time. Our techniques deliver fast results at sufficient accuracy. Furthermore, it is shown in which way these techniques are applied in order to cope with contradictory design goals like performance constraints and hardware effort constraints. As a solution, we present a cost function for the purpose of hardware/software partitioning that offers a dynamic weighting of its components. The conducted experiments show that the usage of our estimation techniques in conjunction with their efficient combination leads to reasonable hardware/software implementations as opposed to approaches that consider single constraints only.\nTitle:\nHigh-Level Estimation Techniques For Usage In Hardware/Software Co-Design\n\nAbstract:\nComputer aided hardware/software partitioning is one of the keychallenges in hardware/software co-design. While previous approacheshave used a fixed granularity, i.e. the size of the partitioningobjects was fixed, we present a partitioning approach thatdynamically determines the partitioning granularity to adapt optimizationsteps to application properties and to intermediate optimizationresults. Experiments with simulated annealing optimizationshow a faster convergence and far better adaptability to costfunction variations than in previous experiments with fixed granularity.\nTitle:\nA hardware/software partitioner using a dynamically determined granularity\n\nAbstract:\nAn important presupposition for HW/SW partitioning are sophisticated estimation algorithms at a high level of abstraction that obtain high quality results. Therefore the granularities of estimation and partitioning have to be adapted adequately. In this paper we discuss the effects that arise when the granularities of partitioning and estimation are not adapted in a necessary way. Furthermore we present our solution that allows to choose different levels of granularities adapted to the estimation and partitioning phase. The experiments show that this refinement in estimation at a high level of abstraction leads to an improvement (in terms of run-time and chip area) of the whole mixed HW/SW system.\nTitle:\nThe Interplay of Run-Time Estimation and Granularity in HW/SW Partitioning\n\nAbstract:\nDesign space exploration of embedded systems typically focuses on classical design goals such as cost, timing, buffer sizes, and power consumption. Robustness criteria, i.e. sensitivity of the system to variations of properties like execution and transmission delays, input data rates, CPU clock rates, etc., has found less attention despite its practical relevance. In this paper we introduce multi-dimensional robustness metrics, expressing the static and dynamic design robustness of a given system, the former assuming a fixed parameter configuration, and the latter including parameter adaptations as response to property variations. Additionally, we propose a metric measuring the robustness gain that can be achieved through system reconfigurability. Since determining multi-dimensional robustness is computationally expensive we introduce efficient exploration methods based on a stochastic sensitivity analysis technique capable of deriving upper and lower robustness bounds for a given system with low computational effort. We demonstrate the robustness optimization methods by means of a small but realistic case study.\nTitle:\nMethods for multi-dimensional robustness optimization in complex embedded systems\n\nAbstract:\nRobustness and optimality are becoming the key principles in designing efficient and reliable state-of-the-art multi-processor real-time systems. However, due to complex inter-processor dependencies, the variation of local system parameters may have unpredictable system level impact including timing anomalies. In this context, the former heuristic optimization approaches used at resource level become less suitable for distributed systems with heterogeneous components and dynamic scheduling techniques. Techniques from exploration theory can better address the optimization problems but suffer from huge search spaces in general. In this paper, we present constructive methods for pointing out those system configurations that lead to anomalous behavior of the performance metrics. These are then used to guide the exploration process and reduce the search space, thereby increasing efficiency and making the approach applicable in practice. As a result, detailed information about anomalies can be quickly obtained and heavily exploited in system optimization, which we demonstrate using comprehensible examples.\nTitle:\nScheduling Anomaly Detection and Optimization for Distributed Systems with Preemptive Task-Sets\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.292, mean value for all the writers is 8.648, which makes it 0.549 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nHardware/software partitioning is a key issue in the design of embedded systems when performance constraints have to be met and chip area and/or power dissipation are critical. For that reason, diverse approaches to automatic hardware/software partitioning have been proposed since the early 1990s. In all approaches so far, the granularity during partitioning is fixed, i.e., either small system parts (e.g., base blocks) or large system parts (e.g., whole functions/processes) can be swapped at once during partitioning in order to find the best hardware/software tradeoff. Since the deployment of a fixed granularity is likely to result in suboptimum solutions, we present the first approach that features a flexible granularity during hardware/software partitioning. Our approach is comprehensive in so far that the estimation techniques, our multigranularity performance estimation technique described here in detail, that control partitioning, are adapted to the flexible partitioning granularity. In addition, our multilevel objective function is described. It allows us to tradeoff various design constraints/goals (performance/hardware area) against each other. As a result, our approach is applicable to a wider range of applications than approaches with a fixed granularity. We also show that our approach is fast and that the obtained hardware/software partitions are much more efficient (in terms of hardware effort, for example) than in cases where a fixed granularity is deployed.\nTitle:", "model_inf_time": 1.44}, {"id": "41118", "output": "Efficient Obstacle-Avoiding Rectilinear Steiner Tree Construction", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDesign decisions made during high-level synthesis usually have great impacts on the later design stages. In this paper, We present a general framework, which plans for the clock skew scheduling in physical design stages during register binding in high-level synthesis. Our proposed technique pursues the optimality of the native objective functions of the register binding problem. At the same time, it ensures not invalidating the subsequent clock skew scheduling for optimizing the clock period. We use the switching power as the native objective of our register binding problem. The problem is first formulated as a MILP problem. An acceleration scheme based on the concept of weakly compatible edge set(WCES) is proposed to speed up the MILP solver to obtain the optimal solution. Then, we present our heuristic algorithm to reduce the running time further. The experimental results show that on average our acceleration scheme can speed up the solver by 8.6 times, and our heuristic is 70 times faster than the solver with a 5.25% degradation of the native objective. The minimum and maximum degradation among our benchmark set are 0.82% and 12.2% respectively.\n\n\nTitle:\nEarly planning for clock skew scheduling during register binding\n\nAbstract:\nClock skew scheduling is a useful sequential circuit optimization method. The run time efficiency of this problem becomes crucial if it must be repeated iteratively in a higher level optimization. The widely recognized Burns' algorithm proposed to solve this problem suffers from high runtime complexity, which makes it unsuitable to be deployed in iterative optimization loops. This algorithm is based on the general concept of primal-dual optimization. In this paper, we demonstrate that a more efficient approach to the clock skew scheduling problem can be developed by designing a new algorithm using the same primal-dual optimization concept. The basic idea of the algorithm is to avoid creating new admissible graph and recalculating \u00c2\u00bf values for each iteration of the primal-dual optimization. The asymptotic runtime efficiency of our algorithm is of O(|V||E| + |V|2log|V|), which is improved from O(|V|2|E|) thanks to the heap data structure used in our proposed algorithm. The experimental results show that our algorithm is on average 95 times faster than Burns' implementation. In best case, we can observe as much as 189 times speedup.\nTitle:\nA revisit to the primal-dual based clock skew scheduling algorithm\n\nAbstract:\nEfficient use of limited available resources on an FPGA remains a crucial problem for synthesizing pipelined designs. Resource sharing addresses this challenge. In this paper, we propose resource sharing techniques that can be incorporated into an automated synthesis flow to generate pipelined designs. Given a synthesized pipelined design, we create a direct relationship between available time slack on modules and the multiplexing overhead due to sharing. This flexibility is maximally exploited without violating any throughput constraints. We propose different techniques to address resource sharing problems of varying restrictions. Specifically, we propose an optimal algorithm for Constant-Slack Resource Sharing and a heuristic for the general Intra-Pipeline Stage Resource Sharing. On an average the demand on arithmetic functional units can be reduced by 39.5% for a set of benchmarks from the multimedia domain using our resource sharing technique.\nTitle:\nResource sharing in pipelined CDFG synthesis\n\nAbstract:\nMost of the FPGA's area and delay are due to routing. Considering routability at earlier steps of the CAD How would both yield better quality and faster design process. In this paper, we discuss the metrics that affect routability in packing logic into clusters. We are presenting a routability-driven clustering method for cluster-based FPGAs. Our method packs LUTs into logic clusters while incorporating routability metrics into a cost function. Based on our routability model, the routability in timing-driven packing algorithm is analyzed. We integrate our routability model into a timing-driven packing algorithm. Our method yields up to 50% improvement in terms of the minimum number of routing tracks compared to VPack (16.5% on average). The average routing area improvement is 27% over VPack and 12% over t-VPack.\nTitle:\nRoutability-Driven Packing: Metrics And Algorithms For Cluster-Based FPGAs\n\nAbstract:\nTapered buffers are widely used in CMOS integrated circuits to drive large capacitive loads. During the design of a tapered buffer, there are several design objectives to consider including delay, area, and power consumption. Existing methods produce suboptimal solutions considering multiple metrics, largely because they decouple different metrics during the design phase, which restricts the solution space for the combined metric. In this paper, a new algorithm is proposed to derive the optimal solution for a unified metric through comprehensive exploration of the solution space. Compared with existing methods, our method yields as high as 18.8% (9.0% on average) improvement in a unified design metric optimizing area, delay, and power simultaneously. The proposed algorithm also reduces buffer power consumption under delay constraints. The power reduction over existing alternatives is as much as 48.1% (28.7% on average).\nTitle:\nA Comprehensive Tapered buffer optimization algorithm for unified design metrics\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.839, mean value for all the writers is 8.648, which makes it 0.163 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nObstacle-avoiding Steiner tree construction is a fundamental problem in VLSI physical design. In this paper, we provide a new approach for rectilinear Steiner tree construction in the presence of obstacles. We propose a novel algorithm, which generates sparse obstacle-avoiding spanning graphs efficiently. We design a fast algorithm for the minimum terminal spanning tree construction, which is the bottleneck step of several existing approaches in terms of running time. We adopt an edge-based heuristic, which enables us to perform both local and global refinement, leading to Steiner trees with small lengths. The time complexity of our algorithm is O(nlogn). Hence, our technique is the most efficient one to the best of our knowledge. Experimental results on various benchmarks show that our algorithm achieves 25.8 times speedup on average, while the average length of the resulting obstacle-avoiding rectilinear Steiner trees is only 1.58% larger than the best existing solution\nTitle:", "model_inf_time": 1.43}, {"id": "41119", "output": "Improving Neural Network Accuracy and Rule Extractability Through Discretized Input Augmentation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present an approach for sample selection using an ensemble of neural networks for credit scoring. The ensemble determines samples that can be considered outliers by checking the classification accuracy of the neural networks on the original training data samples. Those samples that are consistently misclassified by the neural networks in the ensemble are removed from the training dataset. The remaining data samples are then used to train and prune another neural network for rule extraction. Our experimental results on publicly available benchmark credit scoring datasets show that by eliminating the outliers, we obtain neural networks with higher predictive accuracy and simpler in structure compared to the networks that are trained with the original dataset. A rule extraction algorithm is applied to generate comprehensible rules from the neural networks. The extracted rules are more concise than the rules generated from networks that have been trained using the original datasets.\nTitle:\nUsing Sample Selection To Improve Accuracy And Simplicity Of Rules Extracted From Neural Networks For Credit Scoring Applications\n\nAbstract:\nA new algorithm for neural network pruning is presented. Using this algorithm, networks with small number of connections and high accuracy rates for breast cancer diagnosis are obtained. We will then describe how rules can be extracted from a pruned network by considering only a finite number of hidden unit activation values. The accuracy of the extracted rules is as high as the accuracy of the pruned network. For the breast cancer diagnosis problem, the concise rules extracted from the network achieve an accuracy rate of more than 95% on the training data set and on the test data set.\nTitle:\nExtracting rules from pruned networks for breast cancer diagnosis.\n\nAbstract:\nThis paper proposes a GRG (Greedy Rule Generation) algorithm, a new method for generating classification rules from a data set with discrete attributes. The algorithm is \"greedy\" in the sense that at every iteration, it searches for the best rule to generate. The criteria for the best rule include the number of samples and the size of subspaces that it covers, as well as the number of attributes in the rule. This method is employed for extracting rules from neural networks that have been trained and pruned for solving classification problems. The classification rules are extracted from the neural networks using the standard decompositional approach. Neural networks with one hidden layer are trained and the proposed GRG algorithm is applied to their discretized hidden unit activation values. Our experimental results show that neural network rule extraction with the GRG method produces rule sets that are accurate and concise. Application of GRG directly on three medical data sets with discrete attributes also demonstrates its effectiveness for rule generation.\nTitle:\nGreedy rule generation from discrete data and its use in neural network rule extraction.\n\nAbstract:\nFeature selection is an integral part of most learning algorithms. Due to the existence of irrelevant and redundant attributes, by selecting only the relevant attributes of the data, higher predictive accuracy can be expected from a machine learning method. In this paper, we propose the use of a three-layer feedforward neural network to select those input attributes that are most useful for discriminating classes in a given set of input patterns. A network pruning algorithm is the foundation of the proposed algorithm. By adding a penalty term to the error function of the network, redundant network connections can be distinguished from those relevant ones by their small weights when the network training process has been completed. A simple criterion to remove an attribute based on the accuracy rate of the network is developed. The network is retrained after removal of an attribute, and the selection process is repeated until no attribute meets the criterion for removal. Our experimental results suggest that the proposed method works very well on a wide variety of classification problems.\nTitle:\nNeural-network feature selector.\n\nAbstract:\nArtificial neural networks have been successfully applied to a variety of business application problems involving classification and regression. They are especially useful for regression problems as they do not require prior knowledge about the data distribution. In many applications, it is desirable to extract knowledge from trained neural networks so that the users can gain a better understanding of the solution. Existing research works have focused primarily on extracting symbolic rules for classification problems with few methods devised for regression problems. In order to fill this gap, we propose an approach to extract rules from neural networks that have been trained to solve regression problems. The extracted rules divide the data samples into groups. For all samples within a group, a linear function of the relevant input attributes of the data approximates the network output. The approach is illustrated with two examples on various application problems. Experimental results show that the proposed approach generates rules that are more accurate than the existing methods based on decision trees and linear regression.\nTitle:\nAn approach to generate rules from neural networks for regression problems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.164, mean value for all the writers is 8.648, which makes it 0.44 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe classification and prediction accuracy of neural networks can be improved when they are trained with discretized continuous attributes as additional inputs. Such input augmentation makes it easier for the network weights to form more accurate decision boundaries when the data samples of different classes in the data set are contained in distinct hyper-rectangular subregions in the original input space. In this paper, we present first how a neural network can be trained with augmented discretized inputs. The additional inputs are obtained by dividing the original interval of each continuous attribute into subintervals of equal length. The network is then pruned to remove most of the discretized inputs as well as the original continuous attributes as long as the network still achieves a minimum preset accuracy requirement. We then discuss how comprehensible classification rules can be extracted from the pruned network by analyzing the activations of the network hidden units and the weights of the network connections that remain in the pruned network. Our experiments on artificial data sets show that the rules extracted from the neural networks can perfectly replicate the class membership rules used to create the data perfectly. On real-life benchmark data sets, neural networks trained with augmented discretized inputs are shown to achieve better accuracy than neural networks trained with the original data.\nTitle:", "model_inf_time": 1.69}, {"id": "41120", "output": "Quadrilateral Meshes in Computer Graphics", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nHigh-order and regularly sampled surface representations are more efficient and compact than general meshes and considerably simplify many geometric modeling and processing algorithms. A number of recent algorithms for conversion of arbitrary meshes to regularly sampled form (typically quadrangulation) aim to align the resulting mesh with feature lines of the geometry. While resulting in a substantial improvement in mesh quality, feature alignment makes it difficult to obtain coarse regular patch partitions of the mesh. In this paper, we propose an approach to constructing patch layouts consisting of small numbers of quadrilateral patches while maintaining good feature alignment. To achieve this, we use quadrilateral T-meshes, for which the intersection of two faces may not be the whole edge or vertex, but a part of an edge. T-meshes offer more flexibility for reduction of the number of patches and vertices in a base domain while maintaining alignment with geometric features. At the same time, T-meshes retain many desirable features of quadrangulations, allowing construction of high-order representations, easy packing of regularly sampled geometric data into textures, as well as supporting different types of discretizations for physical simulation.\nTitle:\nFeature-aligned T-meshes\n\nAbstract:\nWe present a method to globally parameterize a surface represented by height maps over a set of planes (range images). In contrast to other parametrization techniques, we do not start with a manifold mesh. The parametrization we compute defines a manifold structure, it is seamless and globally smooth, can be aligned to geometric features and shows good quality in terms of angle and area preservation, comparable to current parametrization techniques for meshes. Computing such global seamless parametrization makes it possible to perform quad remeshing, texture mapping and texture synthesis and many other types of geometry processing operations. Our approach is based on a formulation of the Poisson equation on a manifold structure defined for the surface by the range images. Construction of such global parametrization requires only a way to project surface data onto a set of planes, and can be applied directly to implicit surfaces, nonmanifold surfaces, very large meshes, and collections of range scans. We demonstrate application of our technique to all these geometry types.\nTitle:\nGlobal parametrization of range image sets\n\nAbstract:\nVarious applications of global surface parametrization benefit from the alignment of parametrization isolines with principal curvature directions. This is particularly true for recent parametrization-based meshing approaches, where this directly translates into a shape-aware edge flow, better approximation quality, and reduced meshing artifacts. Existing methods to influence a parametrization based on principal curvature directions suffer from scale-dependence, which implies the necessity of parameter variation, or try to capture complex directional shape features using simple 1D curves. Especially for non-sharp features, such as chamfers, fillets, blends, and even more for organic variants thereof, these abstractions can be unfit. We present a novel approach which respects and exploits the 2D nature of such directional feature regions, detects them based on coherence and homogeneity properties, and controls the parametrization process accordingly. This approach enables us to provide an intuitive, scale-invariant control parameter to the user. It also allows us to consider non-local aspects like the topology of a feature, enabling further improvements. We demonstrate that, compared to previous approaches, global parametrizations of higher quality can be generated without user intervention.\nTitle:\nScale-Invariant Directional Alignment of Surface Parametrizations.\n\nAbstract:\nIn this paper we describe an approach to the construction of curvature-continuous surfaces with arbitrary control meshes using subdivision. Using a simple modification of the widely used Loop subdivision algorithm we obtain perturbed surfaces which retain the overall shape and appearance of Loop subdivision surfaces but no longer have flat spots or curvature singularities at extraordinary vertices. Our method is computationally efficient and can be easily added to any existing subdivision code.\nTitle:\nConstructing curvature-continuous surfaces by blending\n\nAbstract:\nDiscrete curvature and shape operators, which capture complete information about directional curvatures at a point, are essential in a variety of applications: simulation of deformable two-dimensional objects, variational modeling and geometric data processing. In many of these applications, objects are represented by meshes. Currently, a spectrum of approaches for formulating curvature operators for meshes exists, ranging from highly accurate but computationally expensive methods used in engineering applications to efficient but less accurate techniques popular in simulation for computer graphics.We propose a simple and efficient formulation for the shape operator for variational problems on general meshes, using degrees of freedom associated with normals. On the one hand, it is similar in its simplicity to some of the discrete curvature operators commonly used in graphics; on the other hand, it passes a number of important convergence tests and produces consistent results for different types of meshes and mesh refinement.\nTitle:\nComputing Discrete Shape Operators On General Meshes\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 6.819, mean value for all the writers is 8.648, which makes it 1.56 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTriangle meshes have been nearly ubiquitous in computer graphics, and a large body of data structures and geometry processing algorithms based on them has been developed in the literature. At the same time, quadrilateral meshes, especially semi-regular ones, have advantages for many applications, and significant progress was made in quadrilateral mesh generation and processing during the last several years. In this survey we discuss the advantages and problems of techniques operating on quadrilateral meshes, including surface analysis and mesh quality, simplification, adaptive refinement, alignment with features, parametrisation and remeshing.\nTitle:", "model_inf_time": 1.28}, {"id": "41121", "output": "Interactive Rendering of Large Volume Datasets", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose a new technique for hardware accelerated multi-resolution geometry synthesis. The level of detail for a given viewpoint is created on-the-fly, allowing for an almost unlimited model resolution in rendering without excessive memory usage. The models consist of regularly sampled rectangular patches that are subdivided hierarchically by a programmable shader in order to create different levels of resolution. The approach is inherently parallel and lends itself to an implementation on vector processor-like parallel architectures. We demonstrate this property by an implementation on programmable graphics hardware. This implementation shows a substantial performance benefit over a CPU-based implementation by up to more than an order of magnitude. We apply the framework to rendering of smooth surfaces and to rendering of complexly structured fractal landscapes using a novel multi-channel fractal subdivision technique. Due to the hardware acceleration, it is possible to perform interactive editing and walkthroughs of such scenes in real-time.\nTitle:\nHardware accelerated multi-resolution geometry synthesis\n\nAbstract:\nWe present a novel multi-resolution point sample rendering algorithm for keyframe animations. The algorithm accepts triangle meshes of arbitrary topology as input which are animated by specifying different sets of vertices at keyframe positions. A multi-resolution representation consisting of prefiltered point samples and triangles is built to represent the animated mesh at different levels of detail. We introduce a novel sampling and stratification algorithm to efficiently generate suitable point sample sets,for moving triangle meshes. Experimental results demonstrate that the new data structure can be used to render highly complex keyframe animations like crowd scenes in real-time.\nTitle:\nMulti-Resolution Rendering Of Complex Animated Scenes\n\nAbstract:\nIn this paper we propose a hardware accelerated ray-casting architecture for multi-resolution volumetric datasets. The architecture is targeted at rendering very large datasets with limited voxel memory resources for both cases where the working set of a frame does or does not fit into the voxel memory. We describe the multi-resolution model used to organize the volume data, especially the wavelet based compression scheme. An efficient hardware implementation of the wavelet decompression is presented and the considerations for volume memory management are discussed. By incorporating the wavelet decompression in hardware a multiple of the decompression bandwidth compared to a PC can be achieved We also show that the impact of our multi-resolution scheme on the actual ray-casting pipeline is minimal.\nTitle:\nA hardware architecture for multi-resolution volume rendering\n\nAbstract:\nWe present a new data structure for rendering highly complex virtual environments of arbitrary topology. The special feature of our approach is that it allows an interactive navigation in very large scenes (30 GB/400 million polygons in our benchmark scenes) that cannot be stored in main memory, but only on a local or remote hard disk. Furthermore, it allows interactive rendering of substantially more complex scenes by instantiating objects.For the computation of an approximate image of the scene, a sampling technique is used. In the preprocessing, a so-called sample tree is built whose nodes contain randomly selected polygons from the scene. This tree only uses space that is linear in the number of polygons. In order to produce an image of the scene, the tree is traversed and polygons stored in the visited nodes are rendered. During the interactive walkthrough, parts of the sample tree are loaded from local or remote hard disk.We implemented our algorithm in a prototypical walkthrough system. Analysis and experiments show that the quality of our images is comparable to images computed by the conventional z-buffer algorithm regardless of the scene topology.\nTitle:\nThe Randomized Sample Tree: A Data Structure for Interactive Walk-Throughs in Externally Stored Virtual Environments\n\nAbstract:\nWe present a new technique for reconstructing a single shape and its nonrigid motion from 3D scanning data. Our algorithm takes a set of time-varying unstructured sample points that capture partial views of a deforming object as input and reconstructs a single shape and a deformation field that fit the data. This representation yields dense correspondences for the whole sequence, as well as a completed 3D shape in every frame. In addition, the algorithm automatically removes spatial and temporal noise artifacts and outliers from the raw input data. Unlike previous methods, the algorithm does not require any shape template but computes a fitting shape automatically from the input data. Our reconstruction framework is based upon a novel topology-aware adaptive subspace deformation technique that allows handling long sequences with complex geometry efficiently. The algorithm accesses data in multiple sequential passes, so that long sequences can be streamed from hard disk, not being limited by main memory. We apply the technique to several benchmark datasets, significantly increasing the complexity of the data that can be handled efficiently in comparison to previous work.\nTitle:\nEfficient reconstruction of nonrigid shape and motion from real-time 3D scanner data\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.25, mean value for all the writers is 8.648, which makes it 1.193 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe present a new algorithm for rendering very large volume data sets at interactive framerates on standard PC hardware. The algorithm accepts scalar data sampled on a regular grid as input. The input data is converted into a compressed hierarchical wavelet representation in a preprocessing step. During rendering, the wavelet representation is decompressed on-the-fly and rendered using hardware texture mapping. The level of detail used for rendering is adapted to the local frequency spectrum of the data and its position relative to the viewer. Using a prototype implementation of the algorithm we were able to perform an interactive walkthrough of large data sets such as the visible human on a single of-the-shelf PC.\nTitle:", "model_inf_time": 1.18}, {"id": "41122", "output": "Lossless Compression of Tetrahedral Meshes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present a meshing and rendering framework for large unstructured tetrahedral meshes which is based on a collection of segments that form a multi resolution model. Each segment contains several thousand tetrahedra and covers either a part of the original tetrahedral mesh or a simplified version of the mesh. The mesh can be adapted locally at run time to viewing and classification parameters by replacing segments with other segments. Dependencies between segments are stored in a hierarchical graph and ensure a consistent mesh at any time. So, we extend the concept of multi-triangulations from triangle meshes to tetrahedral meshes and show how hierarchical segments can be constructed easily for huge tetrahedral meshes.The segments are stored in a simple, but efficient compressed format which not only saves disc space but also allows for an easy calculation of the incidency information between segments at decompression time. A visualization system exploits the multi-triangulation to increase the interactivity of direct volume rendering, isosurface extraction and vector field visualization such that large meshes can be explored on standard PCs.\nTitle:\nSegment-based tetrahedral meshing and rendering\n\nAbstract:\nIn this paper we introduce a new compressed representation for the connectivity of a triangle mesh. We present local compression and decompression algorithms which are fast enough for real time ap- plications. The achieved space compression rates keep pace with the best rates reported for any known global compression algorithm. These nice properties have great benefits for several important ap- plications. Naturally, the technique can be used to compress triangle meshes without significant delay before they are stored on external devices or transmitted over a network. The presented decompression algorithm is very simple allowing a possible hardware realization of the decompression algorithm which could significantly increase the rendering speed of pipelined graphics hardware.\nTitle:\nReal time compression of triangle mesh connectivity\n\nAbstract:\nInteractive exploration of huge tetrahedral meshes is required by many applications but the limitations of the current hardware do not allow for the full dataset to be rendered at interactive frame rates. Multi resolution representations are an important tool to adapt the tetrahedral mesh complexity to the current viewing parameters in real time rendering environments.We present a meshing framework that builds a compact multi resolution representation for large tetrahedral meshes. A preprocessing step simplifies the mesh into a binary vertex hierarchy which is used at run time to adapt the mesh to viewing parameters. Exploiting the redundancy in the connectivity information of the mesh enables us to store the vertex hierarchy compactly such that a vertex split or edge collapse can be performed by knowing incremental updates only.We integrated this multiresolution representation into a volume rendering environment that supports direct volume rendering as well as a new point based rendering approach.\nTitle:\nView-dependent tetrahedral meshing and rendering\n\nAbstract:\nWe present a physically-based fluid simulation with dynamic grid refinement on parallel SIMD graphics hardware. The irregular and dynamic structure of an adaptive grid requires sophisticated memory access patterns as well as a decomposition of the problem for parallel processing and the distribution of tasks to multiple threads. In this paper, we focus on the representation and management of the dynamic grid on the graphics device for an efficient parallelization of the advection step and the iterative solving of the Poisson equation. In order to achieve high performance, we utilize the hardware's capabilities like fast cache access and trilinear filtering. Furthermore, expensive data transfer between host and device is minimized to avoid a major bottleneck. We report results on the inherent overhead of the dynamic grid compared to an equivalent Cartesian grid. In addition, a visual simulation of smoke is presented with radiosity-based illumination and volume ray casting at interactive frame rates.\nTitle:\nDynamic grid refinement for fluid simulations on parallel graphics architectures\n\nAbstract:\nIn this paper we propose a novel surface reconstruction technique based on Bayesian statistics: The measurement process as well as prior assumptions on the measured objects are modeled as probability distributions and Bayes' rule is used to infer a reconstruction of maximum probability. The key idea of this paper is to define both measurements and reconstructions as point clouds and describe all statistical assumptions in terms of this finite dimensional representation. This yields a discretization of the problem that can be solved using numerical optimization techniques. The resulting algorithm reconstructs both topology and geometry inform of a well-sampled point cloud with noise removed. In a final step, this representation is then converted into a triangle mesh. The proposed approach is conceptually simple and easy to extend. We apply the approach to reconstruct piecewise-smooth surfaces with sharp features and examine the performance of the algorithm on different synthetic and real-world data sets.\nTitle:\nBayesian Point Cloud Reconstruction\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.71, mean value for all the writers is 8.648, which makes it 0.8 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn recent years, substantial progress has been achieved in the area of volume visualization on irregular grids, which is mainly based on tetrahedral meshes. Even moderately fine tetrahedral meshes consume several mega-bytes of storage. For archivation and transmission compression algorithms are essential. In scientific applications lossless compression schemes are of primary interest. This paper introduces a new lossless compression scheme for the connectivity of tetrahedral meshes. Our technique can handle all tetrahedral meshes in three dimensional euclidean space even with non manifold border. We present compression and decompression algorithms which consume for reasonable meshes linear time in the number of tetrahedra. The connectivity is compressed to less than 2.4 bits per tetrahedron for all measured meshes. Thus a tetrahedral mesh can almost be reduced to the vertex coordinates, which consume in a common representation about one quarter of the total storage space.We complete our work with solutions for the compression of vertex coordinates and additional attributes, which might be attached to the mesh.\nTitle:", "model_inf_time": 1.28}, {"id": "41123", "output": "Influence of Handover Mechanisms and Transmission Delay on SFN Gain", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nFor broadcast networks, the Single-Frequency Network (SFN) mode is an alternative to the well-known Multi-Frequency Network (MFN) mode, where instead of transmitters operating at different frequencies, all base stations use the same frequency. Besides the optimal frequency reuse, it is usually expected that the more homogeneous distribution of received signal strength reception in an SFN will improve the quality of service. Nevertheless, it should be noted that not all the locations within the service area will benefit from the SFN configuration. Some areas will show a degraded quality caused by the SFN echoes. In this paper, the SFN gain is defined as a parameter describing potential gain or interference. An unambiguous methodology to obtain the actual SFN gain is presented and the variation of the gain is investigated for a DVB-H network as a function of the signal strength difference received from different transmitters. This SFN gain can be used for coverage planning of future broadcast networks.\nTitle:\nOn the Methodology for Calculating SFN Gain in Digital Broadcast Systems\n\nAbstract:\nThis paper investigates the actual measured performance of an 802.16-based system. A measurement methodology for evaluating the performance is proposed, which is then used for studying and comparing the results of different scenarios. More specifically, the influences of changing the modem height from 2.5 m to 6 m and base station height from 15 m to 45 m are analyzed and discussed in this paper, and it will be shown that only the latter one has a significant effect on the coverage and the performance. Furthermore, the relationship between the carrier-to-interference-noise ratio (CINR) and the wireless link throughput is analyzed, and a semi-empirical model will be proposed. The model remains identical for the different scenarios, emphasizing the usefulness of the obtained model. Indoor reception is also analyzed, and it will be shown that the reception at locations close to the window is approximately 7 dB higher than deeper inside the building. A profound statistical analysis is then performed to validate both the performance measurements and the model, and excellent correspondence is obtained. Finally, as the system supports link adaptive modulation and coding, the results of its effectiveness are discussed in this paper.\nTitle:\nField measurements and performance analysis of an 802.16 system in a suburban environment\n\nAbstract:\nIn this paper, the performance of a system based on IEEE 802.16-2004 is analyzed for different conditions. The influence of different parameters of the system is investigated. Channel models will be compared as well as the use of MIMO systems. The throughput and range are determined for different diversity schemes for a realistic scenario at 3500 MHz for business applications.\nTitle:\nInfluence Of Channel Models And Mimo On The Performance Of A System Based On Ieee 802.16\n\nAbstract:\nThe performance of different modulation schemes of a broadband fixed wireless 802.16 (WiMAX) system in a sector is experimentally determined and compared to link budget calculations using different path loss models. A link budget based on path loss measurements of the actual WiMAX signal at 3.5GHz for a typical residential scenario in a suburban environment is determined. The link budget calculations show a reasonable agreement with actual performance measurements with certified WiMAX modems. Carefulness is advised when these models are used for the actual deployment of a WiMAX network in a specific area.\nTitle:\nComparison of the link budget with experimental performance of a WiMAX system\n\nAbstract:\nIn this paper, the influence of intra-network interference on the QoS in WLANs has been assessed.\nTitle:\nInfluence of intra-network interference on quality of service in wireless LANs\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 12.649, mean value for all the writers is 8.648, which makes it 3.414 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSingle Frequency Networks (SFNs) are often deployed for their optimal frequency reuse and the more homogeneous distribution of the field strength in the covered area. Different methodologies have already been proposed to calculate the so-called SFN gain over Multi Frequency Networks (MFNs), but so far, the influence of (MFN) handover mechanisms on the gain values has not yet been investigated. Also, it can be expected that in SFNs, the gain values will depend on the transmission delay difference of the signals from the different transmitters in the SFN. This paper will first assess the influence of a handover mechanism on previously obtained SFN gain values. Secondly, it will be investigated if the transmission delay difference is a good predictor for the SFN gain. This paper further clarifies the SFN concept from a network planner's point of view and aids in understanding what a network planner should take into account when deploying an SFN.\nTitle:", "model_inf_time": 1.41}, {"id": "41124", "output": "Throughput Optimization Strategies for Wireless LANs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nLarge scale growth of wireless networks and the scarcity of the electromagnetic spectrum are imposing more interference to the wireless terminals which jeopardize the Quality of Service offered to the end users. In order to address this kind of performance degradation, this paper proposes a novel experimentally verified cognitive decision engine which aims at optimizing the throughput of IEEE 802.11 links in presence of homogeneous IEEE 802.11 interference. The decision engine is based on a surrogate model that takes the current state of the wireless network as input and makes a prediction of the throughput. The prediction enables the decision engine to find the optimal configuration of the controllable parameters of the network. The decision engine was applied in a realistic interference scenario where utilization of the cognitive decision engine outperformed the case where the decision engine was not deployed by a worst case improvement of more than 100%.\nTitle:\nThroughput optimization of wireless LANs by surrogate model based cognitive decision making\n\nAbstract:\nIn this paper, the performance of a system based on IEEE 802.16-2004 is analyzed for different conditions. The influence of different parameters of the system is investigated. Channel models will be compared as well as the use of MIMO systems. The throughput and range are determined for different diversity schemes for a realistic scenario at 3500 MHz for business applications.\nTitle:\nInfluence Of Channel Models And Mimo On The Performance Of A System Based On Ieee 802.16\n\nAbstract:\nDue to the precipitous growth of wireless networks and the paucity of spectrum, more interference is imposed to the wireless terminals which constraints their performance. In order to preserve such performance degradation, this paper proposes a framework which uses cognitive radio techniques for quality of service (QoS) management of wireless local area networks (LANs). The framework incorporates radio environment maps as input to a cognitive decision engine that steers the network to optimize its QoS parameters such as throughput. A novel experimentally verified heuristic physical model is developed to predict and optimize the throughput of wireless terminals. The framework was applied to realistic stationary and time-variant interference scenarios where an average throughput gain of 344% was achieved in the stationary interference scenario and 70% to 183% was gained in the time-variant interference scenario.\nTitle:\nA cognitive QoS management framework for WLANs.\n\nAbstract:\nIn this paper, the influence of intra-network interference on the QoS in WLANs has been assessed.\nTitle:\nInfluence of intra-network interference on quality of service in wireless LANs\n\nAbstract:\nThe power consumption of wireless access networks is an important issue. In this paper, the power consumption of Long Term Evolution (LTE) base stations is optimized. We consider the city of Ghent, Belgium with 75 possible LTE base station locations. We optimize the network towards two objectives: the coverage maximization and the power consumption minimization. We propose a new Barebones Self-adaptive Differential Evolution. The results of the proposed method indicate the advantages and applicability of our approach.\nTitle:\nOptimization of Power Consumption in 4G LTE Networks Using a Novel Barebones Self-adaptive Differential Evolution Algorithm.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 12.141, mean value for all the writers is 8.648, which makes it 2.98 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThanks to the active development of IEEE 802.11, the performance of wireless local area networks (WLANs) is improving by every new edition of the standard facilitating large enterprises to rely on Wi-Fi for more demanding applications. The limited number of channels in the unlicensed industrial scientific medical frequency band however is one of the key bottlenecks of Wi-Fi when scalability and robustness are points of concern. In this paper we propose two strategies for the optimization of throughput in wireless LANs: a heuristic derived from a theoretical model and a surrogate model based decision engine.\nTitle:", "model_inf_time": 1.06}, {"id": "41125", "output": "Joint FEC Coding and Linear Precoding for MIMO Systems with Antenna Correlation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe study the uplink transmission in multiple-input multiple-output (MIMO) systems with antenna correlation. We focus on schemes that require only channel covariance information at the transmitter (CCIT), which involves lower cost than full channel state information at the transmitter (CSIT). We start from mutual information analysis and show that a simple CCIT-based scheme, referred to as statistical water-filling (SWF), can perform close to the optimal full CSIT-based one in MIMO systems with more receive antennas than transmit ones. We then focus on the implementation of SWF in practically coded systems. An iterative linear minimum mean squared error (LMMSE) receiver is assumed and an extrinsic information transfer (EXIT) chart type curve matching technique is developed based on Hadamard precoding techniques. Simulation results show that the proposed scheme can obtain significant performance improvement compared to the conventional equal power transmission. Finally, we show that the proposed scheme is also very efficient in multi-user uplink MIMO systems with distributed channel information.\nTitle:\nTransmitter Design for Uplink MIMO Systems With Antenna Correlation\n\nAbstract:\nIn this paper, we present a transmission scheme for a multiple-input multiple-output (MIMO) quasi-static fading channel with imperfect channel state information at the transmitter (CSIT). In this scheme, we develop a precoder structure to exploit the available CSIT and apply spatial coupling for further performance enhancement. We derive an analytical evaluation method based on extrinsic information transfer (EXIT) functions, which provides convenience for our precoder design. Furthermore, we observe an area property indicating that, for a spatially coupled system, the iterative receiver can perform error-free decoding even the original uncoupled system has multiple fixed points in its EXIT chart. This observation implies that spatial coupling is useful to alleviate the uncertainty in CSIT which causes difficulty in designing LDPC code based on the EXIT curve matching technique. Numerical results are presented, showing an excellent performance of the proposed scheme in MIMO fading channels with imperfect CSIT.\nTitle:\nSpatially Coupled Ldpc Coding And Linear Precoding For Mimo Systems\n\nAbstract:\nThis paper is concerned with precoder design for multiple-input multiple-output (MIMO) systems with iterative equalization. We first consider the case of no channel state information at the transmitter (CSIT). Based on evolution analysis, we derive the optimized precoder that minimizes the bit error rate (BER) of the system. We show that, with the optimized precoder, the linear precoding and iterative equalization scheme can achieve a genie-aided performance upper bound at high signal-to-noise ratio (SNR). We further consider the precoder design with perfect CSIT. We show that the precoder design problem reduces to a convex power-allocation problem that can be efficiently solved using standard convex programming tools. Numerical results are provided to demonstrate the performance advantages of the proposed scheme over its counterparts.\nTitle:\nPrecoder design for MIMO systems with iterative equalization\n\nAbstract:\nIn this paper, we present a joint forward-error-correction (FEC) coding and linear precoding scheme for multiple-input multiple-output (MIMO) and inter-symbol interference (ISI) channels with imperfect channel state information at the transmitter (CSIT). We first study the performance of ideally coded systems. We focus on an average power gain (APG) method that can achieve capacity in the two extreme cases of no CSIT and perfect CSIT. In the more general case, the performance of the APG method improves progressively with the CSIT quality. We then consider the implementation of this APG method in a practically coded system. We propose a unified scheme involving beamforming, water-filling, and diversity coding. The core of the new scheme is a joint FEC coding and linear precoding strategy at the transmitter and an iterative detection process at the receiver. Simulation results demonstrate that the proposed scheme can achieve significant performance gain by efficiently utilizing the available CSIT.\nTitle:\nJoint FEC coding and linear precoding for MIMO ISI channels\n\nAbstract:\nWe propose a space-time coding scheme for efficient transmission over multiple-input multiple-output (MIMO) channels without channel state information at the transmitter (CSIT). The proposed scheme involves linear precoding (LP) at the transmitter and iterative linear minimum mean-square error (LMMSE) detection at the receiver. We develop a procedure to jointly optimize the forward-error-control (FEC) coding and LP, taking into consideration of the iterative detection process. Our analysis shows that the proposed scheme can perform close to the outage capacity of MIMO channels.\nTitle:\nSpace-time linear precoding and iterative LMMSE detection for MIMO channels without CSIT\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.011, mean value for all the writers is 8.648, which makes it 0.31 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe study transmissions in multiple-input multiple-output (MIMO) systems with antenna correlation. We focus on joint forward error correction (FEC) encoder and linear precoder design based on channel covariance information at the transmitter (CCIT). We aim to optimize the system performance using the extrinsic information transfer (EXIT) chart type curve matching principle. By adopting a Hadamard precoding technique, we show that, the EXIT chart type curve of the precoded system is asymptotically determined by the channel correlation matrix at the transmitter as the number of receive antennas tends to infinity. The encoder-precoder curve matching can be made asymptotically accurate even in the lack of full channel state information at the transmitter (CSIT). Excellent performance based on this strategy is demonstrated by simulation results in systems with only a moderately large number of receive antennas.\nTitle:", "model_inf_time": 1.53}, {"id": "41126", "output": "Linear-Time Algorithms for Wiener and Szeged Indices of Benzenoid Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this note we present some new results on distances in benzenoids. An algorithm is presented which, for a given benzenoid system G bounded by a simple circuit Z with n vertices, computes the Wiener index of G in O( n ) time. Also we show that benzenoid systems have a convenient dismantling scheme, which can be derived by applying breadth-first search to their dual graphs. Our last result deals with the clustering problem of sets of atoms of benzenoids systems. We show how the k -means clustering algorithm (for points in Euclidean space) can be efficiently implemented in the case of benzenoids.\nTitle:\nDistances in benzenoid systems: further developments\n\nAbstract:\nWe show that the molecular graph G of a benzenoid hydrocarbon admits an isometric embedding into the Cartesian product of three trees T-1, T-2, and T-3 defined by three directions of the host hexagonal grid. Namely, to every vertex v of G one can associate an ordered triplet (v(1), v(2), v(3)) with v(i) being a vertex of T-i(i = 1, 2, 3), such that the graph-theoretic distance between two vertices u, v of G equals the sum of respective tree-distances between u(i) and v(i). This labeling of the vertices of G can be obtained in O(n) time. As an application of this result we present an optimal O(Pt) time algorithm for computing the diameter of the graph G of a benzenoid system with n vertices.\nTitle:\nOn Distances In Benzenoid Systems\n\nAbstract:\nThe median set of a graph G with weighted vertices comprises the vertices minimizing the average weighted distance to the vertices of G. We characterize the graphs in which, with respect to any nonnegative vertex weights, median sets always induce connected subgraphs. The characteristic conditions can be tested in polynomial time (by employing linear programming) and are immediately verified for a number of specific graph classes.\nTitle:\nGraphs with Connected Medians\n\nAbstract:\nIn this note we show that, for each chordal graph G, there is a tree T such that T is a spanning tree of the square G\n2 of G and, for every two vertices, the distance between them in T is not larger than the distance in G plus two. Moreover, we prove that, if G is a strongly chordal graph or even a dually chordal graph, then there exists a spanning tree T of G which is an additive 3-spanner as well as a multiplicative 4-spanner of G. In all cases the tree T can be computed in linear time.\nTitle:\nDistance Approximating Trees for Chordal and Dually Chordal Graphs (Extended Abstract)\n\nAbstract:\nIn a graph G=(V, E), the eccentricity e(v) of a vertex v is max{d(v, u)\u2236u \u2208 V}. The center of a graph is the set of vertices with minimum eccentricity. A graph G is chordal if every cycle of length at least four has a chord. We present an algorithm which computes in linear time a central\n vertex of a chordal graph. The algorithm uses the metric properties of chordal graphs and Tarjan and Yannakakis linear-time\n test for graph chordality.\n \nTitle:\nA Linear-Time Algorithm for Finding a Central Vertex of a Chordal Graph\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.303, mean value for all the writers is 8.648, which makes it 0.294 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA linear time algorithm is presented which, for a given benzenoid system G, computes the Wiener index of G. The algorithm is based on an isometric embedding of G into the Cartesian product of three trees, combined with the notion of the Wiener index of vertex-weighted graphs. An analogous approach yields also a linear algorithm for computing the Szeged index of benzenoid systems.\nTitle:", "model_inf_time": 1.46}, {"id": "41127", "output": "Sparse FFT-Based Face Recognition Using Compressive Sensing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper proposes a new video face recognition (FR) method that is designed for significantly improving FR via adaptive fusion of multiple face features (belonging to the same subject) acquired from a face sequence of video frames. In this paper, we derive an upper bound for recognition error arising from the proposed weighted feature fusion to justify theoretically its effectiveness for recognition from videos. In addition, in order to compute the optimal weights of face features to be fused, we develop a novel weight determination solution based on fuzzy membership function and quality measurement for face images. Using four public video databases, the effectiveness of the proposed method has been successfully evaluated under the conditions that are similar to those in real-world video FR applications. Furthermore, our method is simple and straightforward to implement.\nTitle:\nFace Feature Weighted Fusion Based on Fuzzy Membership Degree for Video Face Recognition\n\nAbstract:\nThis paper presents a new facial expression recognition (FER) which exploits the effectiveness of color information and sparse representation. For extracting face feature, we compute color vector differences between color pixels so that they can effectively capture change of face appearance (e.g., skin texture). Through comparative and extensive experiment using two public FER databases (DBs), we validate that our color texture features are suited to the sparse representation for improving FER accuracy. Specifically, our color texture features can considerably improve the recognition accuracy obtained by sparse representation compared with other features (e.g., Local Binary Pattern (LBP)) under realistic recognition conditions (e.g., low-resolution faces). It is also shown that the use of our features can yield high discrimination capability and sparsity, justifying the high recognition accuracies obtained. Further, the proposed FER outperforms five other state-of-the-art FER methods.\nTitle:\nUsing color texture sparsity for facial expression recognition.\n\nAbstract:\nIn this work, we propose a new and novel framework for improving the performance of linear feature extraction (LFE) algorithms, characterized by the Bayesian error probability (BEP) in the extracted feature domain. The proposed framework relies on optimizing a tight quadratic approximation to the BEP in the transformed space with respect to the transformation matrix. Applied to many synthetic multi-class Gaussian classification problems, the proposed optimization procedure significantly improves the classification performance when it is initialized by popular LFE matrices such as the Fisher linear discriminant analysis.\nTitle:\nImproving Classification Performance Of Linear Feature Extraction Algorithms\n\nAbstract:\nThis paper proposes a novel weighted feature fusion in color face recognition (FR) to automatically annotate faces in personal videos. In the proposed FR method, multiple face images (belonging to the same subject) are clustered from a sequence of video frames. To facilitate a complementary effect on improving annotation performance, the grouped faces are combined using the proposed weighted feature fusion. In addition, we make effective use of facial color feature to cope with decrease in annotation performance due to a low-resolution face in personal videos. To evaluate the effectiveness of proposed FR method, more than 40,000 video frames for 10 real-world personal videos are collected from an existing online video sharing website. Experimental results show that the proposed FR method significantly improves annotation performance obtained using conventional grayscale image based FR methods.\nTitle:\nFace annotation for online personal videos using color feature fusion based face recognition\n\nAbstract:\nIn this paper, we propose a novel feature representation based on color-based Local Binary Pattern (LBP) texture analysis for face recognition (FR). The proposed method exploits both color and texture discriminative features of a face image for FR purpose. We evaluate the proposed feature using three public face databases: CMU-PIE, Color FERET, and XM2VTSDB. Experimental results show that the results of the proposed feature impressively better than the results of grayscale LBP and color features. In particular, it is shown that the proposed feature is highly robust against severe variations in illumination and spatial resolution.\nTitle:\nUsing colour local binary pattern features for face recognition\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.169, mean value for all the writers is 8.648, which makes it 0.445 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper proposes a novel face recognition algorithm that utilizes a sparse Fast Fourier Transform (FFT)-based feature extraction method. In our algorithm, we use Compressive Sampling (CS) theory two times. First, in the feature extraction process for extracting the feature vectors from a face images, and second, in the classification process where the CS reconstruction is used for selecting true classes. As a result, a significant reduction in the dimensionality of the signals is achieved. Extensive and comparative experiments have been conducted to evaluate the performance of the proposed scheme. The experiment results show that the combined Compressive Sensing and Sparse Representation Classification (SRC) achieves a high recognition accuracy, while maintaining a reasonable computational complexity.\nTitle:", "model_inf_time": 1.26}, {"id": "41128", "output": "Approximate Reasoning about Information Changes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe introduce basic notions related to granular computing, namely the information granule syntax and semantics as well as the inclusion and closeness (similarity) relations of granules. Different information sources (units, agents) are equipped with two kinds of operations on information granules: operations transforming tuples of information granules definable by a given agent into information granules definable by this agent and approximation operations for computing by agents approximations of information granules delivered by other agents. More complex granules are constructed by means of these operations and approximation operations from some input information granules. The construction of information granules is described by expressions called terms. We discuss a problem of synthesis of robust terms, i.e., descriptions of information granules, satisfying a given specification. This is an important problem for granular computing and its applications for spatial reasoning or knowledge discovery and data mining. (C) 2001 John Wiley & Sons, Inc.\nTitle:\nInformation Granules: Towards Foundations Of Granular Computing\n\nAbstract:\nWe discuss information granule calculi as a basis of granular computing. They are defined by constructs like information granules, basic relations of inclusion and closeness between information granules as well as operations on them. The exact interpretation between granule languages of different information sources (agents) often does not exist. Hence (rough) inclusion and closeness of granules are considered instead of their equality. Examples of all the basic constructs of information granule calculi are presented. The construction of more complex information granules is described by expressions called terms. We discuss the synthesis problem of robust terms, i.e., descriptions of information granules, satisfying a given specification in a satisfactory degree. We also present a method for synthesis of information granules represented by robust terms (approximate schemes of reasoning) by means of decomposition of specifications for such granules. The discussed problems of granular computing are of special importance for many applications, in particular related to spatial reasoning as well as to knowledge discovery and data mining.\nTitle:\nGranular Computing: A Rough Set Approach\n\nAbstract:\nWe present an approach based on calculi of information granules as a basis for approximate reasoning in intelligent systems. Approximate reasoning schemes are defined by means of information granule construction schemes satisfying some robustness constraints. In distributed environments such schemes are extended to rough neural networks. Problems of learning in rough neural networks from experimental data and background knowledge are discussed. The approach is based on rough mereology.\nTitle:\nToward Intelligent Systems: Calculi of Information Granules\n\nAbstract:\n\n We propose to use complex information granules to extract patterns from data in distributed environment. These patterns can\n be treated as a generalization of association rules.\n \n \nTitle:\nInformation Granules in Distributed Environment\n\nAbstract:\nThe aim of the paper is to present basic notions related to granular computing, namely the information granule syntax and semantics as well as the inclusion and closeness (similarity) relations of granules. In particular, we discuss how to define approximation d complex granule sets using the above notions.\nTitle:\nApproximation of Information Granule Sets\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.978, mean value for all the writers is 8.648, which makes it 0.572 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe discuss basic concepts for approximate reasoning about information changes. Any rule for reasoning about information changes specifies how changes of information granules from the rule premise influence changes of information granules from the rule conclusion. Changes in information granules can be measured, e.g., using expressions analogous to derivatives. We illustrate our approach by means of information maps and information granules defined in such maps.\nTitle:", "model_inf_time": 0.88}, {"id": "41129", "output": "Side-Channel Cube Attack on NOEKEON", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose an efficient method for extracting simple low-degree equations (e.g. quadratic ones) in addition to the linear ones, obtainable from the original cube attack by Dinur and Shamir at EUROCRYPT 2009. This extended cube attack can be successfully applied even to cryptosystems in which the original cube attack may fail due to the attacker's inability in finding sufficiently many independent linear equations. As an application of our extended method, we exhibit a side channel cube attack against the PRESENT block cipher using the Hamming weight leakage model. Our side channel attack improves upon the previous work of Yang, Wang and Qiao at CANS 2009 from two aspects. First, we use the Hamming weight leakage model which is a more relaxed leakage assumption, supported by many previously known practical results on side channel attacks, compared to the more challenging leakage assumption that the adversary has access to the \"exact\" value of the internal state bits as used by Yang et al. Thanks to applying the extended cube method, our attack has also a reduced complexity compared to that of Yang et al. Namely, for PRESENT-80 (80-bit key variant) as considered by Yang et al., our attack has a time complexity 216 and data complexity of about 213 chosen plaintexts; whereas, that of Yang et al. has time complexity of 232 and needs about 215 chosen plaintexts. Furthermore, our method directly applies to PRESENT-128 (i.e. 128-bit key variant) with time complexity of 264 and the same data complexity of 213 chosen plaintexts.\nTitle:\nExtended cubes: enhancing the cube attack by extracting low-degree non-linear equations\n\nAbstract:\nIn this paper, we investigate the security of the KATAN family of block ciphers against differential fault attacks. KATAN consists of three variants with 32, 48 and 64-bit block sizes, called KATAN32, KATAN48 and KATAN64, respectively. All three variants have the same key length of 80 bits. We assume a single-bit fault injection model where the adversary is supposed to be able to corrupt a single random bit of the internal state of the cipher and this fault injection process can be repeated (by resetting the cipher); i.e., the faults are transient rather than permanent. First, we determine suitable rounds for effective fault injections by analyzing distributions of low-degree (mainly, linear and quadratic) polynomial equations obtainable using the cube and extended cube attack techniques. Then, we show how to identify the exact position of faulty bits within the internal state by precomputing difference characteristics for each bit position at a given round and comparing these characteristics with ciphertext differences (XOR of faulty and non-faulty ciphertexts) during the online phase of the attack. The complexity of our attack on KATAN32 is 259 computations and about 115 fault injections. For KATAN48 and KATAN64, the attack requires 255 computations (for both variants), while the required number of fault injections is 211 and 278, respectively.\nTitle:\nFault analysis of the KATAN family of block ciphers\n\nAbstract:\nIn this paper, we present an adaptively secure identity-based broadcast encryption system featuring constant sized ciphertext in the standard model. The size of the public key and the private keys of our system are both linear in the maximum number of receivers. In addition, our system is fully collusion-resistant and has stateless receivers. Compared with the state-of-the-art, our scheme is well optimized for the broadcast encryption. The computational complexity of decryption of our scheme depends only on the number of receivers, not the maximum number of receivers of the system. Technically, we employ dual system encryption technique and our proposal offers adaptive security under the general subgroup decisional assumption. Our scheme demonstrates that the adaptive security of the schemes utilizing a composite order group can be proven under the general subgroup decisional assumption, while many existing systems working in a composite order group are secure under multiple subgroup decision assumptions. We note that this finding is of an independent interest, which may be useful in other scenarios.\nTitle:\nAdaptively Secure Identity-Based Broadcast Encryption With a Constant-Sized Ciphertext\n\nAbstract:\nIn this paper, we suggest a new method for cryptanalysis of the basic structures of the block ciphers having SP network structure. The concept of the substitution difference is introduced and the distribution characteristics of substitution distances in an S-box is developed. This gives clues for cryptanalysis of the cipher.We then examine if this method is applicable to cryptanalysis of Rijndael. We present the method for cryptanalysis of the first round of Rijndael including the initial Round-Key addition part in order to illustrate our new method.\nTitle:\nA New Cryptanalytic Method Using the Distribution Characteristics of Substitution Distances\n\nAbstract:\nLEX is a stream cipher based on the round transformation of the AES block cipher, and it was selected for the final phase evaluation of the eSTREAM project. LEX is 2.5 times faster than AES both in software and in hardware. In this paper, we present a differential fault attack on LEX. The fault model assumes that the attacker is able to flip a random bit of the internal state of the cipher but cannot control the exact location of the induced fault. Our attack requires 40 faults, and recovers the secret key with 216 operations.\nTitle:\nDifferential fault analysis of LEX\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.543, mean value for all the writers is 8.648, which makes it 0.09 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we investigate the security of the NOEKEON block cipher against side channel cube attacks. NOEKEON was proposed by Daemen et al. for the NESSIE project. The block size and the key size are both 128 bits. The cube attack, introduced by Dinur and Shamir at EUROCRYPT 2009, is a new type of algebraic cryptanalysis. The attack may be applied if the adversary has access to a single bit of information that can be represented by a low degree multivariate polynomial over GF(2) of secret and public variables. In the side channel attack model, the attacker is assumed to have access to some leaked information about the internal state of the cipher as well as the plaintext and ciphertext. Adopting the notion of a single bit leakage as formalized by Dinur and Shamir, we assume that the attacker has only one bit of information about the intermediate state after each round. Using this side channel attack model, we show that it is possible to extract 60 independent linear equations over 99 (out of 128) key variables. To recover the whole 128-bit key, the attack requires only about 210 chosen plaintext and O(268) time complexity.\nTitle:", "model_inf_time": 1.53}, {"id": "41130", "output": "The Dunbar Number in Online Social Networks: Rationality, Aggression, and Evolution", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nInferring potential links is a fundamental problem in social networks. In the link recommendation problem, the aim is to suggest a list of potential people to each user, ordered by the preferences of the user. Although various approaches have been developed to solve this problem, the difficulty of producing a ranking list with high precision at the top -- the most important consideration for real world applications -- remains largely an open problem. In this work, we propose two top-k link recommendation algorithms which focus on optimizing the top ranked links. For this purpose, we define a cost-sensitive ranking loss which penalizes the mistakes at the top of a ranked list more than the mistakes at the bottom. In particular, we propose a log loss, derive its surrogate, and formulate a top-k link recommendation model by optimizing this surrogate loss function based upon latent features. Moreover, we extend this top-k link recommendation model by incorporating both the latent features and explicit features of the network. Finally, an efficient learning scheme to learn the model parameters is provided. We conduct empirical studies based upon four real world datasets, i.e., Wikipedia, CondMat, Epinions, and MovieLens 1M, of which the largest network contains more than 70 thousand nodes and over one million links. Our experiments demonstrate that the proposed algorithms outperform several state-of-the-art methods.\nTitle:\nTop-k Link Recommendation in Social Networks\n\nAbstract:\nHashing plays a pivotal role in nearest-neighbor searching for large-scale image retrieval. Recently, deep learning-based hashing methods have achieved promising performance. However, most of these deep methods involve discriminative models, which require large-scale, labeled training datasets, thus hindering their real-world applications. In this paper, we propose a novel strategy to exploit the ...\nTitle:\nUnsupervised Semantic-Preserving Adversarial Hashing for Image Search.\n\nAbstract:\nFunctional magnetic resonance imaging (fMRI) has provided an invaluable method of investing real time neuron activities. Statistical tools have been developed to recognise the mental state from a batch of fMRI observations over a period. However, an interesting question is whether it is possible to estimate the real time mental states at each moment during the fMRI observation. In this paper, we address this problem by building a probabilistic model of the brain activity. We model the tempo-spatial relations among the hidden high-level mental states and observable low-level neuron activities. We verify our model by experiments on practical fMRI data. The model also implies interesting clues on the task-responsible regions in the brain.\nTitle:\nA probabilistic model for discovering high level brain activities from fMRI\n\nAbstract:\nRecent years have witnessed a surge of interest in graph-based transductive image classification. Existing simple graph-based transductive learning methods only model the pairwise relationship of images, however, and they are sensitive to the radius parameter used in similarity calculation. Hypergraph learning has been investigated to solve both difficulties. It models the high-order relationship of samples by using a hyperedge to link multiple samples. Nevertheless, the existing hypergraph learning methods face two problems, i.e., how to generate hyperedges and how to handle a large set of hyperedges. This paper proposes an adaptive hypergraph learning method for transductive image classification. In our method, we generate hyperedges by linking images and their nearest neighbors. By varying the size of the neighborhood, we are able to generate a set of hyperedges for each image and its visual neighbors. Our method simultaneously learns the labels of unlabeled images and the weights of hyperedges. In this way, we can automatically modulate the effects of different hyperedges. Thorough empirical studies show the effectiveness of our approach when compared with representative baselines.\nTitle:\nAdaptive hypergraph learning and its application in image classification.\n\nAbstract:\nSemi-supervised ranking is a relatively new and important learning problem inspired by many applications. We propose a novel graph-based regularized algorithm which learns the ranking function in the semi-supervised learning framework. It can exploit geometry of the data while preserving the magnitude of the preferences. The least squares ranking loss is adopted and the optimal solution of our model has an explicit form. We establish error analysis of our proposed algorithm and demonstrate the relationship between predictive performance and intrinsic properties of the graph. The experiments on three datasets for recommendation task and two quantitative structure-activity relationship datasets show that our method is effective and comparable to some other state-of-the-art algorithms for ranking.\nTitle:\nGeneralization performance of magnitude-preserving semi-supervised ranking with graph-based regularization\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.879, mean value for all the writers is 8.648, which makes it 0.656 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nRecent years have witnessed the explosive growth of online social networks (OSNs). They provide powerful IT-innovations for online social activities such as organizing contacts, publishing content, and sharing interests between friends who may never meet before. As more and more people become active users of OSNs, one may ponder questions such as (1) Do OSNs indeed improve our sociability? (2) To what extent can we expand our offline social spectrum in OSNs? (3) Can we identify some interesting user behaviors in OSNs? Our work in this paper attempts to answer these interesting questions. First, we systematically validate the existence of a new Dunbar@?s number in OSNs, which is ranging from 200 to 300 empirically. To reach this, we conduct local-structure analysis as well as user-interaction analysis on extensive real-world OSNs. Second, based on this new number, we divide OSN users into two categories: the rational and the aggressive, and find that rational users intend to develop close and reciprocated relationship, whereas aggressive users have no consistent behaviors. Third, we propose a simple model to highlight the constraints of time and cognition that may affect the evolution of OSNs heavily. Finally, we discuss the potential use of our findings for viral marketing and privacy management in OSNs.\nTitle:", "model_inf_time": 1.63}, {"id": "41131", "output": "User-Level Tools for Personal RFID Data Management in the RFID Ecosystem", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe success of RFID in supply chain management is leading many to consider more personal and pervasive deployments of this technology. Unlike industrial settings, however, deployments that involve humans raise new and critical problems related to privacy, security, uncertainty, and a more diverse and evolving set of applications. At the University of Washington, we are deploying a building-wide RFID-based infrastructure with hundreds of antennas and thousands of tags. Our goal is to uncover the issues of pervasive RFID deployments and devise techniques for addressing these issues before such deployments become common place. In this paper, we present the challenges encountered and lessons learned during a smaller-scale pilot deployment of the system. We show some preliminary results and, for each challenge, discuss how we addressed it or how we are planning on addressing it.\nTitle:\nChallenges for Pervasive RFID-Based Infrastructures\n\nAbstract:\nRadio Frequency IDentification (RFID) deployments are becoming increasingly popular in both industrial and consumer-oriented settings. To effectively exploit and operate such deployments, important challenges must be addressed, from managing RFID data streams to handling limitations in reader accuracy and coverage. Furthermore, deployments that support pervasive computing raise additional issues related to user acceptance and system utility. To better understand these challenges, we conducted a four-week study of a building-scale EPC Class-1 Generation-2 RFID deployment, the \"RFID Ecosystem\", with 47 readers (160 antennas) installed throughout an 8,000 square meter building. During the study, 67 participants having over 300 tags accessed the collected RFID data through applications including an object finder and a friend tracker and several tools for managing personal data. We found that our RFID deployment produces a very manageable amount of data overall, but with orders of magnitude difference among various participants and objects. We also find that the tag detection rates tend to be low with high variance across the type of tag, participant and object. Users need expert guidance to effectively mount their tags and are encouraged by compelling applications to wear tags more frequently. Finally, probabilistic modeling and inference techniques promise to enable more complex applications by smoothing over gaps and errors in the data, but must be applied with care as they add significant computational and storage overhead.\nTitle:\nLongitudinal study of a building-scale RFID ecosystem\n\nAbstract:\nThe Cascadia system provides RFID-based pervasive computing applications with an infrastructure for specifying, extracting and managing meaningful high-level events from raw RFID data. Cascadia allows application developers and even users to specify events of interest using either a declarative query language or a graphical interface with an intuitive visual language. Cascadia then effectively extracts these events from data in spite of the unreliability of RFID technology and the inherent ambiguity in event extraction. We demonstrate Cascadia's technique through a digital diary application in the form of a calendar. Cascadia automatically populates the calendar with meaningful events for the user. We use data collected in a building-wide RFID deployment.\nTitle:\nA demonstration of Cascadia through a digital diary application\n\nAbstract:\nWe present PEEX, a system that enables applications to define and extract meaningful probabilistic high-level events from RFID data. PEEX effectively copes with errors in the data and the inherent ambiguity of event extraction.\nTitle:\nProbabilistic Event Extraction from RFID Data\n\nAbstract:\nCascadia is a system that provides RFID-based pervasive computing applications with an infrastructure for specifying, extracting and managing meaningful high-level events from raw RFID data. Cascadia provides three important services. First, it allows application developers and even users to specify events using either a declarative query language or an intuitive visual language based on direct manipulation. Second, it provides an API that facilitates the development of applications which rely on RFID-based events. Third, it automatically detects the specified events, forwards them to registered applications and stores them for later use (e.g., for historical queries). We present the design and implementation of Cascadia along with an evaluation that includes both a user study and measurements on traces collected in a building-wide RFID deployment. To demonstrate how Cascadia facilitates application development, we built a simple digital diary application in the form of a calendar that populates itself with RFID-based events. Cascadia copes with ambiguous RFID data and limitations in an RFID deployment by transforming RFID readings into probabilistic events. We show that this approach outperforms deterministic event detection techniques while avoiding the need to specify and train sophisticated models.\nTitle:\nCascadia: A System for Specifying, Detecting, and Managing RFID Events\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.964, mean value for all the writers is 8.648, which makes it 0.584 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAt the University of Washington, the RFID ecosystem creates a microcosm for the Internet of Things. The authors developed a suite of Web-based, user-level tools and applications designed to empower users by facilitating their understanding, management, and control of personal RFID data and privacy settings. They deployed these applications in the RFID ecosystem and conducted a four-week user study to measure trends in adoption and utilization of the tools and applications as well as users' qualitative reactions.\nTitle:", "model_inf_time": 1.51}, {"id": "41132", "output": "Amortized Sub-linear Augmenting Paths", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nConsider an n-vertex, m-edge, undirected graph with maximum flow value v. We give a new \u00d5(m+nv)-time maximum flow algorithm based on finding augmenting paths in random samples of the edges of residual graphs. After assigning certain special sampling probabilities to edges in \u00d5(m) time, our algorithm is very simple: repeatedly find an augmenting path in a random sample of edges from the residual graph.\nTitle:\nRandom sampling in residual graphs\n\nAbstract:\nWe present a randomized linear-time algorithm to find a minimum spanning tree in a connected graph with edge weights. The algorithm uses random sampling in combination with a recently discovered linear-time algorithm for verifying a minimum spanning tree. Our computational model is a unit-cost random-access machine with the restriction that the only operations allowed on edge weights are binary comparisons.\nTitle:\nA randomized linear-time algorithm to find minimum spanning trees\n\nAbstract:\nWe significantly improve known time bounds for solving the minimum cut problem on undirected graphs. We use a \"semiduality\" between minimum cuts and maximum spanning tree packings combined with our previously developed random sampling techniques. We give a randomized (Monte Carlo) algorithm that finds a minimum cut in an m-edge, n-vertex graph with high probability in O(m log3 n) time. We also give a simpler randomized algorithm that finds all minimum cuts with high probability in O(m log3 n) time. This variant has an optimal RNC parallelization. Both variants improve on the previous best time bound of O(n2 log3 n). Other applications of the tree-packing approach are new, nearly tight bounds on the number of near-minimum cuts a graph may have and a new data structure for representing them in a space-efficient manner.\nTitle:\nMinimum cuts in near-linear time\n\nAbstract:\nThe all-pairs shortest paths problem in weighted graphs is investigated. An algorithm called the hidden paths algorithm, which finds these paths in time O(m*+n n/sup 2/ log n), where m* is the number of edges participating in shortest paths, is presented. It is argued that m* is likely to be small in practice, since m*=O(n log n) with high probability for many probability distributions on edge weights. An Omega (mn) lower bound on the running time of any path-comparison-based algorithm for the all-pairs shortest paths problem is proved.\nTitle:\nFinding the hidden path: time bounds for all-pairs shortest paths\n\nAbstract:\nWe describe random sampling techniques for approximately solving problems that involve cuts and flows in graphs. We give a near-linear-time randomized combinatorial construction that transforms any graph on n vertices into an O(nlogn)-edge graph on the same vertices whose cuts have approximately the same value as the original graph's. In this new graph, for example, we can run the (O) over tilde (m(3/2))-time maximum flow algorithm of Goldberg and Rao to find an s-t minimum cut in (O) over tilde (n(3/2)) time. This corresponds to a (1 + epsilon)-times minimum s-t cut in the original graph. A related approach leads to a randomized divide-and-conquer algorithm producing an approximately maximum flow in (O) over tilde (m root n) time. Our algorithm can also be used to improve the running time of sparsest cut approximation algorithms from (O) over tilde (mn) to (O) over tilde (n(2)) and to accelerate several other recent cut and flow algorithms. Our algorithms are based on a general theorem analyzing the concentration of random graphs' cut values near their expectations. Our work draws only on elementary probability and graph theory.\nTitle:\nRandomized Approximation Schemes for Cuts and Flows in Capacitated Graphs\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.388, mean value for all the writers is 8.648, which makes it 0.222 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAbstract Consider an n-vertex, m-edge, undirected graph with maximum,flow value v. We give a method,to find augmenting paths in such a graph in amortized sub-linear O n,v,time per path. This lets us improve the time bound,of the classic augmenting,path algorithm to O m,nv, 5 , which is also the best known time bound for bipartite matching.\nTitle:", "model_inf_time": 1.26}, {"id": "41133", "output": "Ethical considerations for FuturICT:  A Value Sensitive Design approach.", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n\u2022We propose an automatic mechanism to measure the privacy risks of users\u2019 data released in social networks.\u2022We propose decentralized information exchange protocols for privacy-aware social network users.\u2022Our protocols rely on the notion of co-utility to ensure that the information exchange is self-enforcing and mutually beneficial.\u2022We also rely on decentralized reputation management to minimize the reluctance of users to exchange their data.\nTitle:\nCo-utile disclosure of private data in social networks.\n\nAbstract:\nPeer-to-peer scenarios are increasingly important in globalized information societies. To guide the interaction between (rational) agents in these P2P settings, self-enforcing collaborative protocols are desirable, so that agents have no motivation to deviate from them. Moreover, to increase social welfare and ensure the success of protocols, it would be desirable to make the collaboration between agents mutually beneficial. In this paper, we present the notion of co-utility, which refers to protocols in which mutual help is the best rational option to take, even for purely selfish agents. This concept is illustrated through several P2P scenarios, framed both in the information society and in the physical world, for which we present and discuss co-utile self-enforcing protocols.\nTitle:\nCo-utility - Self-enforcing collaborative protocols with mutual help.\n\nAbstract:\nAn overview of the activities of the UNESCO Chair in Data Privacy is first given. One of these activities is research. We focus on the research conducted to conciliate security and privacy in vehicular ad hoc networks (VANETs) and, specifically, in VANET announcements.\nTitle:\nThe UNESCO chair in data privacy research in vehicular networks\n\nAbstract:\nThis paper explores the challenges raised by big data in privacy-preserving data management. First, we examine the conflicts raised by big data with respect to preexisting concepts of private data management, such as consent, purpose limitation, transparency and individual rights of access, rectification and erasure. Anonymization appears as the best tool to mitigate such conflicts, and it is best implemented by adhering to a privacy model with precise privacy guarantees. For this reason, we evaluate how well the two main privacy models used in anonymization (k-anonymity and \\(\\varepsilon \\)-differential privacy) meet the requirements of big data, namely composability, low computational cost and linkability.\nTitle:\nBig Data Privacy: Challenges to Privacy Principles and Models\n\nAbstract:\nBefore releasing anonymized microdata (individual data) it is essential to evaluate whether: i) their utility is high enough for their release to make sense; ii) the risk that the anonymized data result in disclosure of respondent identity or respondent attribute values is low enough. Utility and disclosure risk measures are used for the above evaluation, which normally lack a common theoretical framework allowing to trade off utility and risk in a consistent way. We explore in this paper the use of information-theoretic measures based on the notion of mutual information.\nTitle:\nMeasuring risk and utility of anonymized data using information theory\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.418, mean value for all the writers is 8.648, which makes it 0.196 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe pervasive use of information and communication technology (ICT) in modern societies enables countless opportunities for individuals, institutions, businesses and scientists, but also raises difficult ethical and social problems. In particular, ICT helped to make societies more complex and thus harder to understand, which impedes social and political interventions to avoid harm and to increase the common good. To overcome this obstacle, the large-scale EU flagship proposal FuturICT intends to create a platform for accessing global human knowledge as a public good and instruments to increase our understanding of the information society by making use of ICT-based research. In this contribution, we outline the ethical justification for such an endeavor. We argue that the ethical issues raised by FuturICT research projects overlap substantially with many of the known ethical problems emerging from ICT use in general. By referring to the notion of Value Sensitive Design, we show for the example of privacy how this core value of responsible ICT can be protected in pursuing research in the framework of FuturICT. In addition, we discuss further ethical issues and outline the institutional design of FuturICT allowing to address them.\nTitle:", "model_inf_time": 1.41}, {"id": "41134", "output": "Declarative Reflection Patterns for Enhanced Knowledge Acquisition", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCurrent knowledge acquisition tools have limited understanding of how users enter knowledge and how acquired knowledge is used, and provide limited assistance in organizing various knowledge authoring tasks. Users have to make up for these shortcomings by keeping track of past mistakes, current status, potential new problems, and possible courses of actions by themselves. In this paper, we present a novel extension to existing knowledge acquisition tools where the system organizes the episodes of past interactions through a set of declarative meta-level patterns and improves its suggestions based on relevant episodes. In particular, we focus on 1) assessing the level of confidence in suggesting an action, 2) suggesting how a knowledge authoring action can be done based on successful past actions, and 3) monitoring dynamic changes in the environment to suggest relevant modifications in the knowledge base. A preliminary study with varying synthetic user interactions shows that this meta-level assessment may reduce the number of incorrect suggestions, prevent some of the user mistakes and improve the overall problem solving results.\nTitle:\nMeta-level patterns for interactive knowledge capture\n\nAbstract:\nCurrent knowledge acquisition tools are oblivious to the process or strategy that the user may be following in entering new knowledge and unaware of their progress during a session. Users have to make up for these shortcomings by keeping track of the status, progress, potential problems and possible courses of actions by themselves. We present a novel extension to existing systems that 1) keeps track of past problem solving episodes and relates them to user entered knowledge, 2) assesses the current status of the knowledge and the problem solving using such relations, and 3) provides assistance to the user based on the assessment. We applied the approach in developing an intelligent assistant for decision making tasks. The resulting interaction shows that the system helps the user understand the progress and guides the knowledge authoring process in terms of making the knowledge more useful, adapting the knowledge to dynamic changes over time, and making the overall problem solving more successful.\nTitle:\nMemory based meta-level reasoning for interactive knowledge capture\n\nAbstract:\nKnowledge acquisition research concerned with the development of knowledge acquisition tools is in need of a methodological approach to evaluation. This paper describes experimental methodology to conduct studies and experiments of users modifying knowledge bases with knowledge acquisition tools. The paper also reports on the lessons learned from several experiments that have been performed using this methodology. The hope is that it will help others design user evaluations of knowledge acquisition tools. Ideas are discussed for improving the current methodology and some open issues that remain.\nTitle:\nUser Studies of Knowledge Acquisition Tools: Methodology and Lessons Learned\n\nAbstract:\nThis paper describes an integrated acquisition interface that includes several techniques previously developed to support users in various ways as they add new knowledge to an intelligent system. As a result of this integration, the individual techniques can take better advantage of the context in which they are invoked and provide stronger guidance to users. We describe the current implementation using examples from a travel planning domain, and demonstrate how users can add complex knowledge to the system.\nTitle:\nAn integrated environment for knowledge acquisition\n\nAbstract:\nCurrent tools for interactive knowledge capture have little or no learning aptitude. They are mostly oblivious to the process or strategy that the user may be following in entering new knowledge, unaware of their progress during a session, and ignorant of typical skills expected from a good student. We present an approach to make acquisition interfaces more proactive by extending them with: 1) goals that represent what remains to be learned, 2) strategies to achieve these goals and acquire further knowledge, and 3) awareness of the current status of the body of knowledge learned. The resulting interaction shows that the system is aware of its progress towards acquiring the new knowledge, and moves forward by understanding what acquisition goals and strategies to pursue.\nTitle:\nProactive dialogue for interactive knowledge capture\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.866, mean value for all the writers is 8.648, which makes it 0.186 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCurrent knowledge acquisition tools have limited understanding of how users enter knowledge and how acquired knowledge is used, and provide limited assistance in organizing various knowledge authoring tasks. In this paper, we present a novel extension to existing knowledge acquisition tools where the system 1) captures the episodes of knowledge acquisition and knowledge use through a set of declarative reflection patterns 2) performs assessment on how to improve the future knowledge acquisition and knowledge use based on captured episodes, and 3) provides assistance to the users by combining the assessment results.\nTitle:", "model_inf_time": 1.16}, {"id": "41135", "output": "Hardware Transactional Memory: Supporting Large Transactions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nTransactional Memory (TM) is an emerging technology which promises to make parallel programming easier compared to earlier lock based approaches. However, as with any form of speculation, Transactional Memory too wastes a considerable amount of energy when the speculation goes wrong and transaction aborts. For Transactional Memory this wastage will typically be quite high because programmer will often mark a large portion of the code to be executed transactionally[4].\nTitle:\nClock gate on abort: Towards energy-efficient hardware Transactional Memory\n\nAbstract:\nTransactional Memory (TM) has been proposed as an alternative concurrency mechanism for the shared memory parallel programming model. Its main goal is to make parallel programming for Chip Multiprocessors (CMPs) easier than using the traditional lock synchronization constructs, without compromising the performance and the scalability. This topic has received substantial research attention and several TM designs have been proposed using various TM benchmarks. We believe that the evaluation of TM proposals would be more solid if it included realistic applications, that address on-going TM research issues, and that provide the potential for straightforward comparison against locks. In this paper, we introduce RMS-TM, a Transactional Memory benchmark suite composed of seven real-world applications from the Recognition, Mining and Synthesis (RMS) domain. In addition to featuring current TM research issues such as nesting and I/O and system calls inside transactions, the RMS-TM applications also provide a mix of short and long transactions with small/large read and write sets with low/medium/high contention rates. These characteristics, as well as providing lock-based versions of the applications, make RMS-TM a useful TM tool. Current TM benchmarks do not explore all these features. In our evaluation with selected STM and HTM systems, we find that our benchmark suite is also scalable, which is useful for evaluating TM designs on high core counts.\nTitle:\nRMS-TM: a comprehensive benchmark suite for transactional memory systems\n\nAbstract:\nHardware Transactional Memory (HTM) gives software developers the opportunity to write parallel programs more easily compared to any previous programming method, and yields better performance than most previous lock-based synchronizations. Current implementations of HTM perform very well with small transactions. But when a transaction overflows the cache, these implementations either abort the transaction as unsuitable for HTM, and let software takeover, or revert to some much more inefficient hash-like in-memory structure, usually located in the userspace. We present a fast, scalable solution that has virtually no limit on transaction size, has low transactional read and write overhead, works with physical addresses, and doesn't require any changes inside the cache subsystem. This paper presents an HTMOS - Operating System (OS) and Architecture modifications that leverage the existing OS Virtual Memory mechanisms, to support unbounded transaction sizes, and provide transaction execution speed that does not decrease when transaction grows.\nTitle:\nHardware transactional memory with operating system support, HTMOS\n\nAbstract:\nWriting applications that benefit from the massive computational power of future multicore chip multiprocessors will not be an easy task for mainstream programmers accustomed to sequential algorithms rather than parallel ones. This article presents a survey of transactional memory, a mechanism that promises to enable scalable performance while freeing programmers from some of the burden of modifying their parallel code.\nTitle:\nTransactional Memory: An Overview\n\nAbstract:\nIn this paper we present the design and implementation of TM box: An MPSoC built to explore trade-offs in multicore design space and to evaluate parallel programming proposals such as Transactional Memory (TM). Our flexible system, comprised of MIPS R3000-compatible cores is easily modifiable to study different architecture, library and operating system extensions. For this paper we evaluate a 16-core Hybrid Transactional Memory implementation based on the Tiny STM-ASF proposal on a Virtex-5 FPGA and we accelerate three benchmarks written to investigate TM.\nTitle:\nTMbox: A Flexible and Reconfigurable 16-Core Hybrid Transactional Memory System\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.102, mean value for all the writers is 8.648, which makes it 0.466 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTransactional Memory (TM) is a promising paradigm for parallel programming. TM allows a thread to make a series of memory accesses as a single, atomic, transaction, while avoiding deadlocks, livelocks, and other problems commonly associated with lock-based programming. In this paper we explore Hardware support for TM (HTM). In particular, we explore how HTM can efficiently support transactions of nearly unlimited size.\nTitle:", "model_inf_time": 1.16}, {"id": "41136", "output": "A P-complete problem and its succinct language representations", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA variant of the Circuit Value Problem is introduced, in which every gate implements the NOR function @?(x@?y), and one of the inputs of every kth gate must be the (k-1)th gate. The problem, which remains P-complete, is encoded as a simple formal language over a two-letter alphabet, which can be succinctly represented by language equations of several types. Using this representation, a conjunctive grammar with 8 rules, a Boolean grammar with 5 rules and an LL(1) Boolean grammar with 8 rules for this language are constructed. Another encoding of the problem is represented by a trellis automaton with 11 states and a linear conjunctive grammar with 20 rules.\nTitle:\nA simple P-complete problem and its language-theoretic representations\n\nAbstract:\nA restricted case of the Circuit Value Problem known as the Sequential NOR Circuit Value Problem was recently used to obtain very succinct examples of conjunctive grammars, Boolean grammars and language equations representing P-complete languages (Okhotin, \"A simple P-complete problem and its representations by language equations\", MCU 2007). In this paper, a new encoding of the same problem is proposed, and a trellis automaton (one-way real-time cellular automaton) with 11 states solving this problem is constructed.\nTitle:\nRepresenting A P-Complete Problem By Small Trellis Automata\n\nAbstract:\nThis paper demonstrates that the P-complete language of yes-instances of Circuit Value Problem under a suitable encoding can be generated by a linear conjunctive grammar, or, equivalently, accepted by a triangular trellis automaton. This result has several implications on the properties of the languages generated by conjunctive grammars of the general form and on the relationship between the abstract models of parallel computation.\nTitle:\nThe hardest linear conjunctive language\n\nAbstract:\nThis paper studies systems of language equations that are resolved with respect to variables and contain the operations of concatenation, union and intersection. Every system of this kind is proved to have a least fixed point, and the equivalence of these systems to conjunctive grammars is established. This allows us to obtain an algebraic characterization of the language family generated by conjunctive grammars.\nTitle:\nConjunctive Grammars and Systems of Language Equations\n\nAbstract:\nThe family of languages generated by Boolean grammars and usable with recursive descent parsing is studied. It is demonstrated that Boolean LL languages over a unary alphabet are regular, while Boolean LL subsets of \u03a3*a* obey a certain periodicity property, which, in particular, makes the language {anb2n | n \u2265 0} nonrepresentable. It is also shown that {anbncs | n \u2265 0, s \u2208 {a, b}} is not generated by any linear conjunctive LL grammar, while linear Boolean LL grammars cannot generate {anbnc* | n \u2265 0}.\nTitle:\nExpressive power of LL(k) Boolean grammars\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.545, mean value for all the writers is 8.648, which makes it 0.941 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA variant of Circuit Value Problem over the basis of Peirce's arrow (NOR) is introduced, in which one of the inputs of every k-th gate must be the (k - 1)-th gate. The problem, which remains P-complete, is encoded as a simple formal language over a two-letter alphabet. It is shown that this language can be naturally and succinctly represented by language equations from several classes. Using this representation, a small conjunctive grammar and an even smaller LL(1) Boolean grammar for this language are constructed.\nTitle:", "model_inf_time": 1.21}, {"id": "41137", "output": "Structural Proof-Theory of Nelson's Paraconsistent Logic", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIt is known that logical systems with the property of paraconsistency can deal with inconsistency-tolerant and uncertainty reasoning more appropriately than systems which are non-paraconsistent. It is also known that the logic BI of bunched implications is useful for formalizing resource-sensitive reasoning. In this paper, a paraconsistent extension PBI of BI is studied. The logic PBI is thus intended to formalize an appropriate combination of inconsistency-tolerant reasoning and resource-sensitive reasoning. A Gentzen-type sequent calculus SPBI for PBI is introduced, and the cut-elimination and decidability theorems for SPBI are proved. An extension of the Grothendieck topological semantics for BI is introduced for PBI, and the completeness theorem with respect to this semantics is proved.\nTitle:\nInconsistency-Tolerant Bunched Implications\n\nAbstract:\nIn this paper, a way of constructing many-valued paraconsistent logics with weak double negation axioms is proposed. A hierarchy of weak double negation axioms is addressed in this way. The many-valued paraconsistent logics constructed are defined as Gentzen-type sequent calculi. The completeness and cut-elimination theorems for these logics are proved in a uniform way. The logics constructed are also shown to be decidable.\nTitle:\nA Hierarchy of Weak Double Negations\n\nAbstract:\nParaconsistent quantum logic, a hybrid of minimal quantum logic and paraconsistent four-valued logic, is introduced as Gentzen-type sequent calculi, and the cut-elimination theorems for these calculi are proved. This logic is shown to be decidable through the use of these calculi. A first-order extension of this logic is also shown to be decidable. The relationship between minimal quantum logic and paraconsistent four-valued logic is clarified, and a survey of existing Gentzen-type sequent calculi for these logics and their close relatives is addressed.\nTitle:\nProof Theory of Paraconsistent Quantum Logic.\n\nAbstract:\nInconsistency-tolerant reasoning and paraconsistent logic are of growing importance not only in Knowledge Representation, AI and other areas of Computer Science, but also in Philosophical Logic. In this paper, a new logic, paraconsistent linear-time temporal logic (PLTL), is obtained semantically from the linear-time temporal logic LTL by adding a paraconsistent negation. Some theorems for embedding PLTL into LTL are proved, and PLTL is shown to be decidable. A Gentzentype sequent calculus PLT\u03c9 for PLTL is introduced, and the completeness and cut-elimination theorems for this calculus are proved. In addition, a display calculus \u03b4PLT\u03c9 for PLTL is defined.\nTitle:\nA Paraconsistent Linear-time Temporal Logic\n\nAbstract:\nIn this paper, a single-antecedent/succedent sequent calculus NL for first-order Nelsonian paraconsistent quantum logic is investigated. The logic under consideration is regarded as a combination of both Nelson's paraconsistent four-valued logic and Dalla Chiara and Giuntini's paraconsistent quantum logic. The duality and cut-elimination theorems for NL are proved. Decidability, some constructive properties, some constructible falsity properties, and Craig interpolation property are shown for NL. An extend NL with some naive comprehension rules in the naive set theory is also investigated.\nTitle:\nFirst-Order Nelsonian Paraconsistent Quantum Logic\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.549, mean value for all the writers is 8.648, which makes it 0.938 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe aim of this paper is to obtain a theoretical foundation of inconsistency-tolerant (or paraconsistent) reasoning by presenting a comprehensive study of the structural proof-theory of David Nelson's paraconsistent logic. Inconsistency handling has a growing importance in Computer Science since inconsistencies may frequently occur in knowledge-based and intelligent information systems. Paraconsistent, inconsistency-tolerant logics have been studied to cope with such inconsistencies. In this paper, proof systems for Nelson's paraconsistent logic N4 are comprehensively studied. The logic N4 is a fundamental system and known to be a common basis for various extended and useful paraconsistent logics. Some basic theorems including cut-elimination, normalization and completeness are uniformly proved using various embedding theorems. A variety of sequent calculi and natural deduction systems for N4 and some closely related systems are presented and compared.\nTitle:", "model_inf_time": 1.35}, {"id": "41138", "output": "A Decidable Safety Problem for Dynamically Typed Access Matrix Models", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose two new constructions of identity-based proxy re-encryption (IB-PRE). The most important feature of our schemes is that we no longer need the semi-trust assumption on the proxy. Moreover, we describe the IND-PrID-CCA/CPA security models for an IB-PRE in a single-hop scenario, and then give a general analysis on the relationship between the IND-PrID-CPA security model and the desirable PRE properties: unidirectionality, collusion \"safeness\" and nontransitivity. Our first scheme has no ciphertext expansion through the re-encryption and is proven IND-PrID-CPA secure in the random oracle model. The second one achieves the IND-PrID-CCA security.\nTitle:\nNew identity-based proxy re-encryption schemes to prevent collusion attacks\n\nAbstract:\nIn this paper, we construct a strongly unforgeable ID-based signature scheme without random oracles.4 The signature size of our scheme is smaller than that of other schemes based on varieties of the Diffie-Hellman problem or the discrete logarithm problem. The security of the scheme relies on the difficulty to solve three problems related to the Diffie-Hellman problem and a one-way isomorphism.\nTitle:\nStrongly Unforgeable ID-Based Signatures without Random Oracles\n\nAbstract:\nWe propose computational, declarative definitions of the concepts of weak and strong trust relations between interacting agents, and trust domains of trust-related agents in distributed systems. Our definitions yield computational complexity results for deciding potential and actual trust relationships and membership in trust domains. We instantiate our trust concepts in four major applications of trust, namely: Trusted Third Parties (TTPs), the Web of Trust, Public-Key Infrastructures (PKIs), and Identity-Based Cryptography. Finally, we point out computational means for building trust, and by that, building up trust relations and trust domains. Our defining principle for weak and strong trust is (common) belief in and knowledge of agent correctness, respectively\nTitle:\nFormal definitions and complexity results for trust relations and trust domains fit for TTPs, the web of trust, PKIs, and ID-based cryptography\n\nAbstract:\nWe propose new public-key encryption schemes based on the conjugacy search problems (CSP) over noncommutative monoids. Under the newly developed cryptographic assumptions, our basic construction is proven IND-CPA secure in the standard model. Then, we describe two extensions: The first is proven IND-CCA secure in the random oracle model, while the second achieves the IND-CCA security in the standard model. Finally, our proposal is instantiated by using the monoid of matrices over truncated multivariable polynomials over rings. Meanwhile, we also give a discussion on the possibility to instantiate our schemes with braid groups.\nTitle:\nNew constructions of public-key encryption schemes from conjugacy search problems\n\nAbstract:\nA sanitizing signature scheme, which is a variant of digital signatures, enables a trusted party named sanitizer to modify parts of signed documents without corresponding to the signer. A lot of sanitizing signature schemes have been proposed since 2001. However these schemes are suitable for table-style documents, but none of them are considered for story-style documents. We consider that the solution to this problem is adding index to the sanitized document in the signing phase. Thus, a verifier can verify the validity of the document and understand the story in the help of the indexes, even though some messages have been masked. In this paper, we propose a new sanitizing signature scheme with indexing that realizes index-based verification and holds its privacy.\nTitle:\nA Sanitizing Signature Scheme with Indexing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.644, mean value for all the writers is 8.648, which makes it 0.85 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe safety problem in access matrix models determines whether a given subject can eventually obtain access privilege to a given object. Generally speaking, the safety problem is, unfortunately undecidable. Not much is known about protection systems for which the safety problem is decidable, except for strongly constrained systems (e.g., monotonic systems). Therefore, we propose the Dynamic-Typed Access Matrix (DTAM) Model, which extends the Typed Access Matrix model of Sandhu by allowing the type of an object to change dynamically. The DTAM model has an advantage that it can describe non-monotonic protection systems for which the safety problem is decidable. In particular, with further restrictions, we can show that the problem becomes NP-hard. In this paper, we formally define the DTAM model and then discuss various aspects of it thoroughly.\nTitle:", "model_inf_time": 1.36}, {"id": "41139", "output": "Automatic Resource Selection in Software Outsourcing: A Generalized Rough Set Approach", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nTwo recent trends are reshaping the research landscape in business process management. One such trend is the adoption of process-driven application integration by major e-business middleware vendors, and the other is the advancement of web services as a universal computing platform. In this paper, we investigate the impact of web services on business process technologies and present our viewpoints on research directions in business process management in the presence of web services. Finally, we introduce the papers published in this Special Issue on Web Service and Process Management.\nTitle:\nWeb services and process management: a union of convenience or a new area of research? Editorial\n\nAbstract:\nMany e-commerce firms provide live-chat capability on their Web sites to promote product sales and to offer customer support. With increasing traffic on e-commerce Web sites, providing such live-chat services requires a good allocation of service resources to serve the customers. When resources are limited, firms may consider employing priority-processing and reserving resources for high-value customers. In this article, we model a reserve-based priority-processing policy for e-commerce systems that have imperfect customer classification. Two policy decisions considered in the model are: (1) the number of agents exclusively reserved for high-value customers, and (2) the configuration of the classification system. We derive explicit expressions for average waiting times of high-value and low-value customer classes and define a total waiting cost function. Through numerical analysis, we study the impact of these two policy decisions on average waiting times and total waiting costs. Our analysis finds that reserving agents for high-value customers may have negative consequences for such customers under imperfect classification. Further, we study the interaction between the two policy decisions and discuss how one decision should be modified with respect to a change in the other one in order to keep the waiting costs minimized.\nTitle:\nLive-chat agent assignments to heterogeneous e-customers under imperfect classification\n\nAbstract:\nMobile business is becoming a reality due to ubiquitous Internet connectivity, popular mobile devices, and widely available cloud services. However, characteristics of the mobile environment, such as mobility, unpredictability, and variation of mobile network&#39;s signal strength, present challenges in selecting optimal services for composition. Traditional QoS-aware methods that select individual se...\nTitle:\nMobility-Enabled Service Selection for Composite Services.\n\nAbstract:\nAnalyzing business policies for discovering and validating business process models is a critical task in modern organizations, which is currently done in an ad hoc manner due to a lack of systematic methodologies. In this paper, we propose a novel methodology called Policy-Driven Process Mapping (PDPM) for extracting process models from business policy documents. Our research objective is to make process discovery from policy documents more systematic with fewer structural and semantic errors. To the best of our knowledge, PDPM is the first formal approach to discovering process models from business policies.\nTitle:\nPolicy-Driven Process Mapping (PDPM): Discovering process models from business policies\n\nAbstract:\nThe process of system design often begins with the selection of an appropriate reference model. Model selection necessitates a good understanding of the system to be developed, as well as of the reference models available for that particular type of systems. In this paper, we propose a conceptual framework for comparing reference models, based on an elaboration of a linguistics-based classification approach. This framework is applied to the comparative analysis of two well known reference models for electronic commerce.\nTitle:\nEvaluating the quality of reference models\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.973, mean value for all the writers is 8.648, which makes it 0.277 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nToday's economic reality is forcing firms to become increasingly more efficient in managing their resource functions. Outsourcing has moved to the mainstream of business development and promised to be one of the many enterprise strategies for cost-effective service delivery. Proper screening and automatic selection of outsource partners are critical to the business. Resource selection is one of the most important steps in outsourcing decision-making processes. This paper considers a data intensive selection problem in outsourcing software development projects. We analyze the properties of the resource selection problem and propose some criteria for an automatic resource selection model. A naive model, a traditional rough set model, and a generalized rough set (GRS) model are introduced and the advantages and disadvantages of each model are compared. Experimental results indicate that the GRS model is superior to other models.\nTitle:", "model_inf_time": 1.35}, {"id": "41140", "output": "Cauchy Unital Tree Valuation Monoids and Kleene's Theorem", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe investigate weighted automata with discounting and their behaviors over semirings and finitely generated graded monoids. We characterize the discounted behaviors of weighted automata precisely as rational formal power series with a discounted form of the Cauchy product. This extends a classical result of Kleene-Schutzenberger. Here we show that the very special case of Schutzenberger's result for free monoids over singleton alphabets suffices to deduce our generalization.\nTitle:\nWeighted automata with discounting\n\nAbstract:\nIn this paper we prove Kleene\u2019s result for formal tree series over a commutative semiring A (which is not necessarily complete or continuous or idempotent), i.e., the class of formal tree series over A which are accepted by weighted tree automata, and the class of rational tree series over A are equal. We show the result by direct automata-theoretic constructions and prove their correctness.\nTitle:\nA Kleene Theorem for Weighted Tree Automata\n\nAbstract:\nWe introduce a new behavior of weighted unranked tree automata. We prove a characterization of this behavior by two fragments of weighted MSO logic and thereby provide a solution of an open equivalence problem of Droste and Vogler. The characterization works for valuation monoids as weight structures; they include all semirings and, in addition, enable us to cope with average.\nTitle:\nWeighted Unranked Tree Automata over Tree Valuation Monoids and Their Characterization by Weighted Logics.\n\nAbstract:\nWe introduce multi-valued B\u00fcchi and Muller automata over distributive lattices and a multi-valued MSO logic for infinite words. For this logic, we prove the expressive equivalence of \u03c9-recognizable and MSO-definable infinitary formal power series over distributive lattices with negation function. Then we consider multi-valued Muller tree automata and a multi-valued MSO logic for trees over distributive lattices. For this logic, we establish a version of Rabin's theorem for infinitary tree series.\nTitle:\nMulti-Valued MSO Logics OverWords and Trees\n\nAbstract:\nQuantitative automata computing the maximal average consumption of resources are currently intensively investigated. We introduce Conway hemirings and show that a few equational axioms suffice to imply a Kleene theorem for the possible behaviors of quantitative automata characterizing them as rational series, and we derive several further natural identities for rational operations on such series. We also obtain a more abstract Kleene theorem for Conway hemiring automata. This extends classical results of Conway and Sch\u00fctzenberger for recognizable languages resp. semiring-weighted automata.\nTitle:\nWeighted finite automata over hemirings.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.456, mean value for all the writers is 8.648, which makes it 1.017 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCauchy unital tree valuation monoids are introduced as weight structures for weighted tree automata. Rational tree series over this kind of monoids are defined and Kleene's classical theorem is proved for this setting: a tree series over a Cauchy unital tree valuation monoid is recognizable if and only if it is rational.\nTitle:", "model_inf_time": 1.3}, {"id": "41141", "output": "Improving GeoDiverse Routing Heuristics for Resilient Optical Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWith the increasing frequency of natural disasters and intentional attacks that challenge telecommunication networks, vulnerability to cascading and regional-correlated challenges is escalating. Given the high complexity and large traffic load of optical networks, these correlated challenges cause substantial damage to reliable network communication. In this paper, we propose a network vulnerability identification mechanism and study different vulnerability scales using real-world optical network data. We further propose geographical diversity and incorporate it into a new graph resilience metric cTGGD (compensated Total Geographical Graph Diversity), which is capable of characterising and differentiating resiliency levels among different optical fibre networks. It is shown to be an effective resilience level indicator under regional network challenges or attacks. We further propose two heuristics for solving the path geodiverse problem (PGD) in which the calculation of a number of geographically separated paths is required. Geodiverse paths can be used to circumvent physical challenges such as large-scale disasters in telecommunication networks. We present the GeoDivRP routing protocol with two new routing heuristics implemented, which provides the end nodes with multiple geographically diverse paths. Our protocol demonstrates better performance compared to OSPF when the network is subject to area-based challenges. We have analysed the mechanism by which the attackers could use to maximise the attack impact with a limited budget and demonstrate the effectiveness of restoration plans.\nTitle:\nAnalysing GeoPath diversity and improving routing performance in optical networks\n\nAbstract:\nWith the increasing frequency of natural disasters and intentional attacks that challenge communication networks, vulnerability to cascading, and regionalcorrelated challenges is escalating. Given the high complexity and large traffic load of communication networks, these correlated challenges cause substantial damage to reliable network communication. In this work, we extend the GeoDivRP routing protocol to consider delay-skew requirement when using multiple geographically diverse paths for telecommunication networks under area-based challenges. We present a flow-diverse minimum-cost routing multicommodity flow problem. Furthermore, we present a nonlinear delay-skew optimization problem to balance between delay and traffic skew on paths. We investigate the tradeoff between the delay and skew in choosing multiple geodiverse paths. We implement GeoDivRP in ns-3 to use the optimized paths given by the two optimization solutions and demonstrate their effectiveness compared to open shortest path first Equal-Cost Multi-Path routing in terms of overall link utilization. It guarantees the delay-skew constraint provided by the upper layer while satisfies the traffic demand imposed by multiple routing commodities in the telecommunication networks. c 2015 Wiley Periodicals, Inc. NETWORKS, Vol. 66(4), 335-346 2015\nTitle:\nGeodiverse routing with path delay and skew requirement under area\u2010based challenges\n\nAbstract:\nIn this paper, we address the network virtualization problem of embedding a unique shortest path-based IP topology using lightpaths in a wavelength-routed network. We present an integer linear programming formulation and propose a 2-phase heuristic approach to solve this problem. We extend the model and the heuristic by addressing survivability in an integrated cross-layer framework, where the objective is to allocate a light-path topology that remains connected in the event of any single physical link failure while providing the IP network with unique shortest paths for all node-pairs. We consider a number of measures to show effectiveness of our approach and to discuss the impact on normal and survivable topology design, in terms of the number of transreceivers deployed.\nTitle:\nEmbedding IP Unique Shortest Path Topology on a Wavelength-Routed Network: Normal and Survivable Design\n\nAbstract:\nIn current network routing domains, routing information exchange usually lacks protection based on confidentiality. This makes network routing vulnerable to a variety of security attacks. In this paper, we present a framework to provide confidentiality for a link state routing protocol. This framework involves creation of a trust structure among routers as well as key management. Routing information is encrypted so that it can be accessed only by authorized routers. We present an implementation framework for our approach by extending Open Shortest Path First (OSPF), a commonly deployed link-state routing protocol. Based on our performance assessment, we have found that the additional cost in implementing our scheme has fairly moderate impact on the overall performance.\nTitle:\nOn Providing Confidentiality In Link State Routing Protocol\n\nAbstract:\nWe demonstrate the benefits of traffic engineering by studying three realistic network models derived from an actual service provider network. We evaluate traffic engineering in the presence of QoS-based routing schemes compared with Destination-Based Routing, the default routing behavior for the Internet. We also simulate prioritization of important traffic flows by implementing priority in one or more of the path caching, path ordering, and actual route selection phases of the constraint-based routing framework. We observe that traffic engineering can provide 20-50% network capacity savings. We also observe that prioritization in more than one phase of constraint-based routing can provide even more significant benefits.\nTitle:\nBenefits of traffic engineering using QoS routing schemes and network controls\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.434, mean value for all the writers is 8.648, which makes it 2.377 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose two heuristics for solving the path geodiverse problem (PGD), in which the calculation of a number of geographically separated paths is required. The geodiverse paths can be used to circumvent physical challenges such as large-scale disasters in telecommunication networks. The heuristics we propose for solving PGD have significantly less complexity compared to the optimal algorithm we previously used while still performing well by returning multiple geodiverse paths for each node pair. The geodiverse paths contribute to providing resilience against regional challenges. We present the GeoDivRP routing protocol with two new routing heuristics implemented, which provide the end nodes with multiple geographically diverse paths and demonstrates better performance compared to OSPF when the network is subject to area-based challenges.\nTitle:", "model_inf_time": 1.48}, {"id": "41142", "output": "Summarizing Transactional Databases with Overlapped Hyperrectangles", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nTransactional data are ubiquitous. Several methods, including frequent itemset mining and co-clustering, have been proposed to analyze transactional databases. In this work, we propose a new research problem to succinctly summarize transactional databases. Solving this problem requires linking the high level structure of the database to a potentially huge number of frequent itemsets. We formulate this problem as a set covering problem using overlapped hyperrectangles (a concept generally regarded as tile according to some existing papers); we then prove that this problem and its several variations are NP-hard, and we further reveal its relationship with the compact representation of a directed bipartite graph. We develop an approximation algorithm Hyper which can achieve a logarithmic approximation ratio in polynomial time. We propose a pruning strategy that can significantly speed up the processing of our algorithm, and we also propose an efficient algorithm Hyper+ to further summarize the set of hyperrectangles by allowing false positive conditions. Additionally, we show that hyperrectangles generated by our algorithms can be properly visualized. A detailed study using both real and synthetic datasets shows the effectiveness and efficiency of our approaches in summarizing transactional databases.\nTitle:\nSummarizing transactional databases with overlapped hyperrectangles\n\nAbstract:\nIn this work, we study a visual data mining problem: Given a set of discovered overlapping submatrices of interest, how can we order the rows and columns of the data matrix to best display these submatrices and their relationships? We find this problem can be converted to the hypergraph ordering problem, which generalizes the traditional minimal linear arrangement (or graph ordering) problem and then we are able to prove the NP-hardness of this problem. We propose a novel iterative algorithm which utilize the existing graph ordering algorithm to solve the optimal visualization problem. This algorithm can always converge to a local minimum. The detailed experimental evaluation using a set of publicly available transactional datasets demonstrates the effectiveness and efficiency of the proposed algorithm.\nTitle:\nOverlapping Matrix Pattern Visualization: A Hypergraph Approach\n\nAbstract:\nVarious network monitoring and performance evaluation schemes generate considerable amount of traf- fic, which affects network performance. In this paper we describe a method for minimizing network monitoring overhead based on Shortest Path Tree (SPT) protocol. We describe two different variations of the problem: the A- Problem and the E-Problem and prove that finding optimal solutions for both A -a ndE-problems is NP -hard. We also show that in general, an A-problem solution requires a significantly higher network overhead than an E-problem solution. We propose optimal approximation algorithms for the A -a ndE-problems and few different heuristics for the E-problem. Namely, we show that one can compute in polynomial time an O(ln|V |)-approximate solution for each of these problems. We analyze the performance of our approximation algorithms and heuristics on large graphs generated using Power-Law model. Performance results show that our heuristic algorithms for both problems achieve from 50% to 90% improvement in the network overhead comparatively with earlier algorithms that appeared in literature.\nTitle:\nEffective Monitor Placement in Internet Networks\n\nAbstract:\nA distance-k matching in a graph G is matching M in which the distance between any two edges of M is at least k. A distance-2 matching is more commonly referred to as an induced matching. In this paper, we show that when G is weakly chordal, the size of the largest induced matching in G is equal to the minimum number of co-chordal subgraphs of G needed to cover the edges of G, and that the co-chordal subgraphs of a minimum cover can be found in polynomial time. Using similar techniques, we show that the distance-k matching problem for k > 1 is tractable for weakly chordal graphs when k is even, and is NP-hard when k is odd. For dually chordal graphs, we use properties of hypergraphs to show that the distance-k matching problem is solvable in polynomial time whenever k is odd, and NP-hard when k is even. Motivated by our use of hypergraphs, we define a class of hypergraphs which lies strictly in between the well studied classes of acyclic hypergraphs and normal hypergraphs.\nTitle:\nNew Min-Max Theorems For Weakly Chordal And Dually Chordal Graphs\n\nAbstract:\nWe develop efficient parameterized, with additive error, approximation algorithms for the (Connected) r-Domination problem and the (Connected) p-Center problem for unweighted and undirected graphs. Given a graph G, we show how to construct a (connected) (r+O(\u03bc))-dominating set D with |D|\u2264|D\u204e| efficiently. Here, D\u204e is a minimum (connected) r-dominating set of G and \u03bc is our graph parameter, which is the tree-breadth or the cluster diameter in a layering partition of G. Additionally, we show that a +O(\u03bc)-approximation for the (Connected) p-Center problem on G can be computed in polynomial time. Our interest in these parameters stems from the fact that in many real-world networks, including Internet application networks, web networks, collaboration networks, social networks, biological networks, and others, and in many structured classes of graphs these parameters are small constants.\nTitle:\nParameterized Approximation Algorithms for some Location Problems in Graphs.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.644, mean value for all the writers is 8.648, which makes it 0.003 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTransactional data are ubiquitous. Several methods, including frequent itemsets mining and co-clustering, have been proposed to analyze transactional databases. In this work, we propose a new research problem to succinctly summarize transactional databases. Solving this problem requires linking the high level structure of the database to a potentially huge number of frequent itemsets. We formulate this problem as a set covering problem using overlapped hyperrectangles; we then prove that this problem and its several variations are NP-hard. We develop an approximation algorithm HYPER which can achieve a ln(k) + 1 approximation ratio in polynomial time. We propose a pruning strategy that can significantly speed up the processing of our algorithm. Additionally, we propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive conditions. A detailed study using both real and synthetic datasets shows the effectiveness and efficiency of our approaches in summarizing transactional databases.\nTitle:", "model_inf_time": 1.53}, {"id": "41143", "output": "Design and Implementation of a Dual-Axis Planar Maglev Positioning System with Adaptive Sliding-Mode Control", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, a novel design, control and implementation of a three degree-of-freedom (DOF) compact positioner is presented with high resolution in tens of nanometer-scale precision positioning and millimeter-level long travel range. According to the serial flexure mechanism design, whose motion comes from the elastic deformation of the flexure and the force allocation of five pairs of electromagnetic coils and permanent magnets, the precision positioner enables both horizontal and vertical actuations resulting in x-, y-, and z-motions respectively. Next, in order to improve the transient response and to suppress the vibration of the flexure suspension mechanism, an eddy current damper (ECD) is applied as a passive and noncontact resistance to vibration. Moreover, a laser interferometer sensing system is implemented to improve the positioning resolution of the stage.To maintain stability and robustness of the precision system, a decentralized adaptive sliding mode controller (DASMC) was implemented to overcome the overall situations of unmodeled system dynamics and external noises. From the experimental results, satisfactory performance has been observed, which means that the designated objectives of this research have been successfully attained, namely, (1) long working range, (2) high positioning resolution and (3) compact design.\nTitle:\nDesign And Implementation Of An Electromagnetically Damped Positioner With Flexure Suspension\n\nAbstract:\nIn this paper, a compact and three degree-of-freedoms (DOFs) micropositioner with large travel ranges is presented. The design of the micropositioner utilizes the monolithic parallel flexure mechanism with the built-in electromagnetic actuators and uses the optical sensors to achieve the object of 3-DOF precise motion. An adaptive sliding-mode controller is proposed to let the system is more robust and stable in positioning. The developed robust control architecture consists of three components: 1) sliding mode controller, 2) adaptive law, and 3) force allocation. From the provided experimental results, satisfactory performances of the hereby developed system, including stiffness and precision, have been successfully demonstrated\nTitle:\nDesign and implementation of a new 3-DOF electromagnetic micropositioner utilizing flexure mechanism\n\nAbstract:\nIn this paper, we present the design, control and implementation of a novel, compact and three degree-of-freedom (DOF) precise flexure-mechanism electromagnetic-actuating positioning system with submiocrometer-scale precise positioning capability and millimeter-level large travel range. The design of the positioner utilizes the monolithic parallel flexure mechanism with the built-in electromagnetic actuators and the eddy-current sensors to achieve the 3-DOF motion. The positioner presented herein shows the planar travel range of 1mmx1mm with a position resolution of 300 nm. To let the compact system be more robust and stable in positioning, we propose an adaptive sliding-mode controller. We will demonstrate the satisfactory performance of the positioning system, including stiffness and precision, with theoretical analysis and experimental results.\nTitle:\nA New Design Of 3-Dof Flexure-Mechanism Positioner With Electromagnetic Technology\n\nAbstract:\nWe analyze the dynamics of a magnetic guiding system and derive its analytical model with full degrees-of-freedom (DOFs). Then, an adaptive controller which deals with unknown parameters is proposed to regulate the five DOFs in this system. The guiding system including sensors and drive systems is has been implemented. From the experimental results, satisfactory performance including stiffness and resolution was achieved. This validates the design of system hardware and demonstrates the feasibility of the developed controller\nTitle:\nModeling and controller design of a MAGLEV guiding system for application in precision positioning\n\nAbstract:\nIn this paper, we have developed a novel large measurement-range atomic force microscopy (AFM) system performing the tapping mode operation. This system consists of a compact/low-cost scanning probe-type sensing system ( z-scanner) and a hybrid xy-scanner. To achieve precision measurement through image scan of given samples, a thorough mathematical modeling is established first, and an advanced robust adaptive controller is then proposed, which can deal with unknown parameters, cross-talk effects, external disturbances, and unknown hysteresis phenomena. The salient properties of the resulting closed-loop AFM system includes long traveling range, high precision, and fast response after integrating two kinds of actuations. To demonstrate and qualify the scanning capability of the proposed system, systematic experiments have been conducted.\nTitle:\nModeling and Controller Design of a Precision Hybrid Scanner for Application in Large Measurement-Range Atomic Force Microscopy\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.455, mean value for all the writers is 8.648, which makes it 2.395 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, a new planar magnetic levitation (maglev) positioning system is proposed, which is capable of executing dual-axis planar motions purely involving magnetic forces. Functionally, such a mechanism behaves like a planar XY table with micrometer precision. Specifically, in this system, a new structure with an adaptive sliding-mode control (ASMC) algorithm is described, which aims to achieve the following three goals: 1) a large moving range (millimeter level); 2) precise positioning (micrometer level); and 3) fast response. The system consists of a moving carrier platform, six permanent magnets (PMs) attached to the carrier, and six electromagnets mounted on a fixed base. After exploring the characteristics of the magnetic forces between PMs and electromagnets, the general 6-DOF dynamic model of this system is derived and analyzed. Then, because of the naturally unstable behavior inherent in maglev systems, the proposed ASMC guarantees satisfactory performance of the maglev system. Experiments have successfully demonstrated the feasibility and effectiveness of the overall system.\nTitle:", "model_inf_time": 1.87}, {"id": "41144", "output": "Research Challenges for Online Quality Prediction in Service-Oriented Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWith the increasing number of services published on the Web, it is useful to derive desirable service execution plans by identifying relevant and reliable services via an intelligent and automated recommendation process. Different from other proposals, this paper proposes to exploit time-dependant relationships among web services and their quality of service to make better recommendations. We employ time-aware Bayesian networks to reveal time dependency relationships for quality of service from service logs. Service selection is then guided using the latest time-step quality of service of related services. The short listed services can be further evaluated by business social trust paths to identify their trustworthiness. Our experiments demonstrate the effectiveness of our proposed service ranking method.\nTitle:\nWeb Service Recommendations Based on Time-Aware Bayesian Networks\n\nAbstract:\nService recommendation systems have been trying to utilize context-aware information to recommend services that better meet the needs of the service consumers. However, current context-aware service recommendation techniques are mainly based on individual intelligence or the local knowledge of users, and do not take into consideration the common knowledge among different users. To address this, re...\nTitle:\nEfficient Role Mining for Context-Aware Service Recommendation Using a High-Performance Cluster.\n\nAbstract:\nIn this paper, we propose an adaptive quality recommendation mechanism to help software service providers understand the dynamism of quality demand from the majority of requesters accurately. The unique feature of our approach is that based on the intra-cluster proximity index (called the icp-index) that we propose, the granularity of service clustering can be adjusted dynamically to meet the wide variation of service providers who want to target their services to different groups of clients. Experiments show that our approach is more accurate and flexible to identify the need of service quality of requesters than existing solutions such as simple averaging, minimum-maximum-mean, or traditional interval-range data clustering.\nTitle:\nAdaptive Quality Recommendation Mechanism for Software Service Provisioning\n\nAbstract:\nIn this paper, we propose an automatic, self-adaptive trust model for component-based service-oriented architecture that is based on Bayesian model. The focus of this model is the \"quantitative trust on action\". Both the computation model and its updating mechanism are given. Through our implementation of this model using Monte-Carlo algorithm, we show the feasibility and practicability of our proposal as well as its ability to rectify any unfair advertisement and to track the capability of highly dynamic service\nTitle:\nQuantitative Trust Based on Actions\n\nAbstract:\nIn this paper, we investigate the trust problem that the data retrieval and reuse processes bring to web content. We argue that this trust problem is due to the inappropriate setup of web server, heterogeneity and presentation demand of web clients, value-added functions (including caching) in the network, and insufficient trust-related support in current web architecture and protocols. To quantify our arguments on the trust issue of web content, modeling and monitoring of real data on Internet was conducted to quantify the seriousness of its error situation.\nTitle:\nContent Delivered on Internet - How Much Can We Trust?\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.363, mean value for all the writers is 8.648, which makes it 0.243 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOnline quality prediction allows service-oriented systems to anticipate the need for adaptation and thus to prevent the actual occurrence of failures or to mitigate upcoming failures. Such proactive adaptation capabilities are increasingly relevant for future service-oriented systems, which need to cope with limited control over third-party services, as well as rapidly changing usage contexts. Initial, promising results have been achieved for what concerns online quality prediction for service-oriented systems. However, there are many challenging issues remaining that call for concrete solutions. In this paper we present a set of research challenges identified by the research community that may be worth investigating in the coming years.\nTitle:", "model_inf_time": 1.25}, {"id": "41145", "output": "Shortest Travel Route Planning for Mobile Data Gathering in Wireless Sensor Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe emerging wireless energy transfer technology enables charging sensor batteries in a wireless sensor network (WSN) and maintaining perpetual operation of the network. Recent breakthrough in this area has opened up a new dimension to the design of sensor network protocols. In the meanwhile, mobile data gathering has been considered as an efficient alternative to data relaying in WSNs. However, time variation of recharging rates in wireless rechargeable sensor networks imposes a great challenge in obtaining an optimal data gathering strategy. In this paper, we propose a framework of joint Wireless Energy Replenishment and anchor-point based Mobile Data Gathering (WerMDG) in WSNs by considering various sources of energy consumption and time-varying nature of energy replenishment. To that end, we first determine the anchor point selection and the sequence to visit the anchor points. We then formulate the WerMDG problem into a network utility maximization problem which is constrained by flow conversation, energy balance, link and battery capacity and the bounded sojourn time of the mobile collector. Furthermore, we present a distributed algorithm composed of cross-layer data control, scheduling and routing subalgorithms for each sensor node, and sojourn time allocation subalgorithm for the mobile collector at different anchor points. Finally, we give extensive numerical results to verify the convergence of the proposed algorithm and the impact of utility weight on network performance.\nTitle:\nMobile data gathering with Wireless Energy Replenishment in rechargeable sensor networks\n\nAbstract:\nEnvironmental energy harvesting technologies have provided potential for battery-powered wireless sensor networks to have perpetual network operations. To design a robust network that can adapt to not only temporal but also spatial variations of ambient energy sources, in this paper, we utilize mobility to circumvent communication bottlenecks, by employing a mobile data collector, called SenCar. We propose a two-stage approach for mobile data collection. In the first stage, SenCar makes stops at a subset of selected sensor locations to collect data packets in a multi-hop fashion. We provide a selection algorithm to search for sensor locations with most residual energy while guaranteeing a bounded tour length. Then we design a distributed data gathering algorithm to achieve maximum network utility by adjusting data rates, link scheduling and flow routing that adapts to spatial temporal environmental energy variations. The effectiveness and efficiency of the proposed algorithms are validated by extensive numerical results.\nTitle:\nEnergy-efficient mobile data collection in energy-harvesting wireless sensor networks\n\nAbstract:\nNonuniform energy consumption is an inherent problem in wireless sensor networks characterized by multi-hop routing and many-to-one traffic pattern. Such unbalanced energy dissipation can significantly reduce network lifetime. In this paper, we study the problem of prolonging network lifetime in large scale wireless sensor networks where a mobile sink gathers data periodically along the predefined path and each sensor node uploads its data to the mobile sink over a multi-hop communication path. For this problem, we propose a heuristic topology control algorithm with time complexity O(n(m + n log n)), where n and m are the number of nodes and edges in the network, respectively, and further discuss how to refine our algorithm to satisfy practical requirements such as distributed computing and transmission timeliness. Theoretical analysis and experimental results show that our algorithm is superior to several earlier algorithms for extending network lifetime.\nTitle:\nTopology Control for Maximizing Network Lifetime in Wireless Sensor Networks with Mobile Sink\n\nAbstract:\nIn this paper, we propose a joint subcarrier pairing and power allocation (JS2PA) scheme with fairness based on the Intelligent Water Drop (IWD) optimization method for Orthogonal Frequency Division Multiple Access (OFDMA) cooperative relay networks. The proposed scheme consists of a subcarrier pairing and selection algorithm and a power allocation algorithm. We first formulate the JS2PA problem as a mixed integer programming problem aiming to maximize the total network utility under the constraints of the total and individual power, subcarrier fairness requirement and pairing. To solve the non-convex JS2PA problem, firstly, we propose a subcarrier pairing and selection algorithm based on Hungarian method so as to select the appropriate subcarrier pairs for relaying. Secondly, we provide a power allocation algorithm based on the IWD method (PA-MIWD) in which water drops act as the agents to find the optimal power allocation for each node. Finally, we conduct simulations to validate the proposed algorithms and the results show that the proposed JS2PA scheme outperforms the existing methods in terms of convergence and total network utility.\nTitle:\nJoint Subcarrier Pairing and Power Allocation in OFDMA Cooperative Relay Networks\n\nAbstract:\nIn this paper, we consider lossy mobile ad hoc networks where the data rate of a given flow becomes lower and lower along its routing path, and propose a cross-layer rate-effective network utility maximization (RENUM) framework by taking into account the lossy nature of wireless links and the constraints of rate outage probability and average delay. In the proposed framework, the utility is associated with the effective rate received at the destination node of each flow instead of the injection rate at the source of the flow. We then present a distributed joint transmission rate, link power and average delay control algorithm, in which explicit broadcast message passing is required for power allocation algorithm. The proposed algorithm is shown through numerical simulations to outperform other network utility maximization algorithms without rate outage probability/average delay constraints, leading to a higher effective rate, lower power consumption and delay.\nTitle:\nOptimal and distributed resource allocation in lossy mobile ad hoc networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.36, mean value for all the writers is 8.648, which makes it 2.314 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we propose a shortest travel route planning scheme that takes into account the spatial characteristics of wireless transmissions for mobile data gathering in wireless sensor networks. We formulate the shortest travel route problem (STRP) as a covering salesman problem (CSP), which is regarded as a mixed integer nonlinear programming and also as a non- convex programming problem. To solve the STRP problem, we propose a heuristic algorithm named decomposition algorithm (DA), which decomposes the STRP problem into two subproblems: access sequence problem and position determining problem. We conduct extensive simulation to verify the effectiveness of the proposed algorithm and show that the DA algorithm can plan the shortest travel route in large scale WSNs other than small scale WSNs by the classical traveling salesman problem (TSP) algorithms.\nTitle:", "model_inf_time": 1.53}, {"id": "41146", "output": "3-Majority Consensus is Faster than 2-Choices for Many Colors", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe study a Plurality Consensus process in which each of n anonymous agents of a communication network supports an initial opinion (a colorchosen from a finite set [k]) and, at every time step, he can revise his color according to a random sample of neighbors. The goal (of the agents) is to let the process converge to the stable configuration where all nodes support the plurality color. It is assumed that the initial color configuration has a sufficiently large bias s, that is, the number of nodes supporting the plurality color exceeds the number of nodes supporting any other color by an additive value s. We consider a basic model in which the network is a clique and the update rule (called here the 3-majority dynamics) of the process is that each agent looks at the colors of three random neighbors and then applies the majority rule(breaking ties uniformly at random). We prove a tight bound on the convergence time which grows as \u0398klog n for a wide range of parameters k and n. This linear-in-k dependence implies an exponential time-gap between the plurality consensus processand the median process studied in [7]. A natural question is whether looking at more (than three) random neighbors can significantly speed up the process. We provide a negative answer to this question: in particular, we show that samples of polylogarithmic size can speed up the process by a polylogarithmic factor only.\nTitle:\nSimple dynamics for plurality consensus\n\nAbstract:\nWe study Plurality Consensus in the GOSSIP Model over a network of n anonymous agents. Each agent supports an initial opinion or color. We assume that at the onset, the number of agents supporting the plurality color exceeds that of the agents supporting any other color by a sufficiently-large bias, though the initial plurality itself might be very far from absolute majority. The goal is to provide a protocol that, with high probability, brings the system into the configuration in which all agents support the (initial) plurality color. We consider the Undecided-State Dynamics, a well-known protocol which uses just one more state (the undecided one) than those necessary to store colors. We show that the speed of convergence of this protocol depends on the initial color configuration as a whole, not just on the gap between the plurality and the second largest color community. This dependence is best captured by a novel notion we introduce, namely, the monochromatic distance md(c) which measures the distance of the initial color configuration c from the closest monochromatic one. In the complete graph, we prove that, for a wide range of the input parameters, this dynamics converges within O(md(c) log n) rounds. We prove that this upper bound is almost tight in the strong sense: Starting from any color configuration c, the convergence time is \u03a9(md(c)). Finally, we adapt the Undecided-State Dynamics to obtain a fast, random walk-based protocol for plurality consensus on regular expanders. This protocol converges in O(md(c) polylog(n)) rounds using only polylog(n) local memory. A key-ingredient to achieve the above bounds is a new analysis of the maximum node congestion that results from performing n parallel random walks on regular expanders. All our bounds hold with high probability.\nTitle:\nPlurality consensus in the gossip model\n\nAbstract:\nWe consider the following distributed consensus problem: Each node in a complete communication network of size n initially holds an opinion, which is chosen arbitrarily from a finite set \u03a3. The system must converge toward a consensus state in which all, or almost all nodes, hold the same opinion. Moreover, this opinion should be valid, i.e., it should be one among those initially present in the system. This condition should be met even in the presence of a malicious adversary who can modify the opinions of a bounded subset of nodes, adaptively chosen in every round.\n\nWe consider the 3-majority dynamics: At every round, every node pulls the opinion from three random neighbors and sets his new opinion to the majority one (ties are broken arbitrarily). Let k be the number of valid opinions. We show that, if k \u2264 n\u03b1, where \u03b1 is a suitable positive constant, the 3-majority dynamics converges in time polynomial in k and log n with high probability even in the presence of an adversary who can affect up to o([EQUATION]) nodes at each round.\n\nPreviously, the convergence of the 3-majority protocol was known for |\u03a3| = 2 only, with an argument that is robust to adversarial errors. On the other hand, no anonymous, uniform-gossip protocol that is robust to adversarial errors was known for |\u03a3| > 2.\n\n\nTitle:\nStabilizing Consensus with Many Opinions\n\nAbstract:\nConsensus and Broadcast are two fundamental problems in distributed computing, whose solutions have several applications. Intuitively, Consensus should be no harder than Broadcast , and this can be rigorously established in several models. Can Consensus be easier than Broadcast? In models that allow noiseless communication, we prove a reduction of (a suitable variant of) Broadcast to binary Consensus, that preserves the communication model and all complexity parameters such as randomness, number of rounds, communication per round, etc., while there is a loss in the success probability of the protocol. Using this reduction, we get, among other applications, the first logarithmic lower bound on the number of rounds needed to achieve Consensus in the uniform GOSSIP model on the complete graph. The lower bound is tight and, in this model, Consensus and Broadcast are equivalent. We then turn to distributed models with noisy communication channels that have been studied in the context of some bio-inspired systems. In such models, only one noisy bit is exchanged when a communication channel is established between two nodes, and so one cannot easily simulate a noiseless protocol by using error-correcting codes. An \u2126(e \u22122 n) lower bound on the number of rounds needed for Broadcast is proved by Boczkowski et al. [PLOS Comp. Bio. 2018] in one such model (noisy uniform PULL, where e is a parameter that measures the amount of noise). We prove an O(e \u22122 log n) upper bound for binary Consensus in such model, thus establishing an exponential gap between the number of rounds necessary for Consensus versus Broadcast. We also prove a new O(e \u22122 n log n) upper bound for Broadcast in this model.\nTitle:\nConsensus Needs Broadcast in Noiseless Models but can be Exponentially Easier in the Presence of Noise.\n\nAbstract:\nWe study the following synchronous process that we call repeated balls-into-bins. The process is started by assigning n balls to n bins in an arbitrary way. Then, in every subsequent round, one ball is chosen according to some fixed strategy (random, FIFO, etc) from each non-empty bin, and re-assigned to one of the n bins uniformly at random. This process corresponds to a non-reversible Markov chain and our aim is to study its self-stabilization properties with respect to the maximum(bin) load and some related performance measures. We define a configuration (i.e., a state) legitimate if its maximum load is O(log n). We first prove that, starting from any legitimate configuration, the process will only take on legitimate configurations over a period of length bounded by any polynomial in n, with high probability (w.h.p.). Further we prove that, starting from any configuration, the process converges to a legitimate configuration in linear time, w.h.p. This implies that the process is self-stabilizing w.h.p. and, moreover, that every ball traverses all bins in O(n log2 n) rounds, w.h.p. The latter result can also be interpreted as an almost tight bound on the cover time for the problem of parallel resource assignment in the complete graph.\nTitle:\nSelf-Stabilizing Repeated Balls-into-Bins.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.034, mean value for all the writers is 8.648, which makes it 0.329 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe study consensus processes on the complete graph of n nodes. Initially, each node supports one up to n different opinions. Nodes randomly and in parallel sample the opinions of constantly many nodes. Based on these samples, they use an update rule to change their own opinion. The goal is to reach consensus, a configuration where all nodes support the same opinion. We compare two well-known update rules: 2-Choices and 3-Majority. In the former, each node samples two nodes and adopts their opinion if they agree. In the latter, each node samples three nodes: If an opinion is supported by at least two samples the node adopts it, otherwise it randomly adopts one of the sampled opinions. Known results for these update rules focus on initial configurations with a limited number of colors (say n1/3), or typically assume a bias, where one opinion has a much larger support than any other. For such biased configurations, the time to reach consensus is roughly the same for 2-Choices and 3-Majority. Interestingly, we prove that this is no longer true for configurations with a large number of initial colors. In particular, we show that 3-Majority reaches consensus with high probability in O(n3/4 \u00b7 log7/8 n) rounds, while 2-Choices can need \u03a9(n / log n) rounds. We thus get the first unconditional sublinear bound for 3-Majority and the first result separating the consensus time of these processes. Along the way, we develop a framework that allows a fine-grained comparison between consensus processes from a specific class. We believe that this framework might help to classify the performance of more consensus processes.\nTitle:", "model_inf_time": 2.04}, {"id": "41147", "output": "Greedily Improving Betweenness Centrality in a Network", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe closeness and the betweenness centralities are two well-known measures of importance of a vertex within a given complex network. Having high closeness or betweenness centrality can have positive impact on the vertex itself: hence, in this paper we consider the problem of determining how much a vertex can increase its centrality by creating a limited amount of new edges incident to it. We first prove that this problem does not admit a polynomial-time approximation scheme unless $$P=NP$$, and we then propose a simple greedy approximation algorithm with an almost tight approximation ratio, whose performance is then tested on synthetic graphs and real-world networks.\nTitle:\nGreedily Improving Our Own Centrality in A Network\n\nAbstract:\nThe closeness centrality is a well-known measure of importance of a vertex within a given complex network. Having high closeness centrality can have positive impact on the vertex itself: hence, in this paper we consider the optimization problem of determining how much a vertex can increase its centrality by creating a limited amount of new edges incident to it. We will consider both the undirected and the directed graph cases. In both cases, we first prove that the optimization problem does not admit a polynomial-time approximation scheme (unless P &equals; NP), and then propose a greedy approximation algorithm (with an almost tight approximation ratio), whose performance is then tested on synthetic graphs and real-world networks.\nTitle:\nGreedily Improving Our Own Closeness Centrality in a Network.\n\nAbstract:\n  Closeness is an important centrality measure widely used in the analysis of real-world complex networks. In particular, the problem of selecting the k most central nodes with respect to this measure has been deeply analyzed in the last decade. However, even for not very large networks, this problem is computationally intractable in practice: indeed, Abboud et al have recently shown that its complexity is strictly related to the complexity of the All-Pairs Shortest Path (in short, APSP) problem, for which no subcubic \"combinatorial\" algorithm is known. In this paper, we propose a new algorithm for selecting the k most closeness central nodes in a graph. In practice, this algorithm significantly improves over the APSP approach, even though its worst-case time complexity is the same. For example, the algorithm is able to compute the top k nodes in few dozens of seconds even when applied to real-world networks with millions of nodes and edges. We will also experimentally prove that our algorithm drastically outperforms the most recently designed algorithm, proposed by Olsen et al. Finally, we apply the new algorithm to the computation of the most central actors in the IMDB collaboration network, where two actors are linked if they played together in a movie. \nTitle:\nFast and Simple Computation of Top-k Closeness Centralities\n\nAbstract:\nIn this paper we propose a new algorithm for computing the diameter of directed unweighted graphs. Even though, in the worst case, this algorithm has complexity O(nm), where n is the number of nodes and m is the number of edges of the graph, we experimentally show that in practice our method works in O(m) time. Moreover, we show how to extend our algorithm to the case of directed weighted graphs and, even in this case, we present some preliminary very positive experimental results.\nTitle:\nOn computing the diameter of real-world directed (weighted) graphs\n\nAbstract:\nThe (Gromov) hyperbolicity is a topological property of a graph, which has been recently applied in several different contexts, such as the design of routing schemes, network security, computational biology, the analysis of graph algorithms, and the classification of complex networks. Computing the hyperbolicity of a graph can be very time consuming: indeed, the best available algorithm has running-time O(n(3.69)), which is clearly prohibitive for big graphs. In this paper, we provide a new and more efficient algorithm: although its worst-case complexity is O(n(4)), in practice it is much faster, allowing, for the first time, the computation of the hyperbolicity of graphs with up to 200,000 nodes. We experimentally show that our new algorithm drastically outperforms the best previously available algorithms, by analyzing a big dataset of real-world networks. Finally, we apply the new algorithm to compute the hyperbolicity of random graphs generated with the Erdos-Renyi model, the Chung-Lu model, and the Configuration Model.\nTitle:\nOn Computing the Hyperbolicity of Real-World Graphs\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.449, mean value for all the writers is 8.648, which makes it 0.17 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nBetweenness is a well-known centrality measure that ranks the nodes according to their participation in the shortest paths of a network. In several scenarios, having a high betweenness can have a positive impact on the node itself. Hence, in this article, we consider the problem of determining how much a vertex can increase its centrality by creating a limited amount of new edges incident to it. In particular, we study the problem of maximizing the betweenness score of a given node\u2014Maximum Betweenness Improvement (MBI)\u2014and that of maximizing the ranking of a given node\u2014Maximum Ranking Improvement (MRI). We show that MBI cannot be approximated in polynomial-time within a factor (1\u22121/2e) and that MRI does not admit any polynomial-time constant factor approximation algorithm, both unless P=NP. We then propose a simple greedy approximation algorithm for MBI with an almost tight approximation ratio and we test its performance on several real-world networks. We experimentally show that our algorithm highly increases both the betweenness score and the ranking of a given node and that it outperforms several competitive baselines. To speed up the computation of our greedy algorithm, we also propose a new dynamic algorithm for updating the betweenness of one node after an edge insertion, which might be of independent interest. Using the dynamic algorithm, we are now able to compute an approximation of MBI on networks with up to 105 edges in most cases in a matter of seconds or a few minutes.\nTitle:", "model_inf_time": 1.44}, {"id": "41148", "output": "Packing Cuts in Undirected Graphs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe study the complexity and approximability of Cut Packing and Cycle Packing. For Cycle Packing, we show that the problem is APX-hard but can be approximated within a factor of O(log n) by a simple greedy approach. Essentially the same approach achieves constant approximation for \"dense\" graphs. We show that both problems are NP-hard for planar graphs. For Cut Packing we show that, given a graph G the maximum cut packing is always between \u03b1(G) and 2\u03b1(G). We then derive new or improved polynomial-time algorithms for Cut Packing for special classes of graphs.\nTitle:\nPacking Cycles and Cuts in Undirected Graphs\n\nAbstract:\nGiven an undirected graph G with n nodes and m edges, we address the problem of finding a largest collection of edge-disjoint cycles in G. The problem, dubbed CYCLE PACKING, is very closely related to a few genome rearrangement problems in computational biology. In this paper, we study the complexity and approximability of CYCLE PACKING, about which very little is known although the problem is natural and has practical applications. We show that the problem is APX- hard but can be approximated within a factor of O(logn) by a simple greedy approach. We do not know whether the O(log n) factor is tight, but we give a nontrivial example for which the ratio achieved by greedy is not constant, namely \u03a9(\u221alogn/(loglogn)). We also show that, for \"not too sparse\" graphs, i.e., graphs for which m = \u03a9 (n1+1/t+\u03b4) for some positive integer t and for any fixed \u03b4 0, we can achieve an approximation arbitrarily close to 2t/3 in polynomial time. In particular, for any \u03b5 0, this yields a 4/3 + \u03b5 approximation when m = \u03a9(n3/2+\u03b4), therefore also for dense graphs. Finally, we briefly discuss a natural linear programming relaxation for the problem.\nTitle:\nPacking cycles in undirected graphs\n\nAbstract:\nAbstract In the valley-free path model, a path in a given directed graph is valid if it consists of a sequence of forward edges followed by a sequence of backward edges. This model is motivated by routing policies of autonomous,systems in the Internet. We give a 2-approximation algorithm for the problem of computing a maximum number of edgeor vertex-disjoint valid paths between two given vertices s and t, and we show that no better approximation ratio is possible unless P = NP. Furthermore, we give a 2-approximation algorithm for the problem of computing a minimum vertex cut that separates s and t with respect to all valid paths and prove that the problem is APX-hard. The corresponding problem for edge cuts is shown to be polynomial-time solvable. For the multiway variant of the cut problem, we give a 4-approximation algorithm. We present additional results for acyclic graphs.\nTitle:\nCuts and Disjoint Paths in the Valley-Free Model\n\nAbstract:\nIn the valley-free path model, a path in a given directed graph is valid if it consists of a sequence of forward edges followed by a sequence of backward edges. This model is motivated by BGP routing policies of autonomous systems in the Internet. Robustness considerations lead to the problem of computing a maximum number of disjoint paths between two nodes, and the minimum size of a cut that separates them. We study these problems in the valley-free path model. For the problem of computing a maximum number of edge- or vertex-disjoint valid paths between two given vertices s and t, we give a 2-approximation algorithm and show that no better approximation ratio is possible unless P = NP. For the problem of computing a minimum vertex cut that separates s and t with respect to all valid paths, we give a 2-approximation algorithm and prove that the problem is APX-hard. The corresponding problem for edge cuts is shown to be polynomial-time solvable. We present additional results for acyclic graphs.\nTitle:\nCuts and disjoint paths in the valley-free path model of internet BGP routing\n\nAbstract:\nIn this paper, we improve the bounds for computing a network decomposition, which is a basic notion in distributed graph algorithms, distributively and deterministically. Our algorithm computes an $(n^{\\epsilon(n)},(n^{\\epsilon(n)})$-decomposition in $O(n^{\\epsilon(n)})$ time, where $\\epsilon(n)=O(1/ \\sqrt{\\log n})$. As a corollary we obtain improved deterministic bounds for distributively computing several graph structures such as maximal independent sets and $\\Delta$-vertex colorings. We also show that the class of graphs $\\cal G$ whose maximum degree is $O(n^{\\delta(n)})$, where $\\delta(n)=O(1/\\log \\log n)$, is complete for the task of computing a near-optimal decomposition, i.e., a $(\\log n \\log n)$-decomposition, in $O($polylog$(n))$ time. This is a corollary of a more general characterization, which pinpoints the weak points of existing network decomposition algorithms. Completeness is to be intended in the following sense: if we have an algorithm $\\cal A$ that computes an optimal decomposition in $O($polylog$(n))$ time for graphs in $\\cal G$, then we can compute an optimal decomposition in $O($polylog $(n))$ time for all graphs.\nTitle:\nOn the Complexity of Distributed Network Decomposition\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.435, mean value for all the writers is 8.648, which makes it 1.035 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe address the problem of finding the largest collection of edge-disjoint cuts in an undirected graph, dubbed CUT PACKING, focusing on its complexity, about which very little is known. We show a very close relationship with INDEPENDENT SET, namely, for the same graph G, the size of the largest cut packing of G is at least the independence number of G, and at most twice that number. This implies that any approximation guarantee for INDEPENDENT SET immediately extends to CUT PACKING within a factor of 2. In particular, this yields a 2-approximation algorithm for CUT PACKING in perfect graphs. We then present polynomial-time algorithms for several classes of perfect (and related) graphs, including triangulated graphs and their complements, bipartite graphs and their complements, and Seymour graphs. Finally, we discuss various linear programming relaxations for the problem, finding combinatorial dual problems Of CUT PACKING and characterizing the cases in which duality is strong. (C) 2004 Wiley Periodicals, Inc.\nTitle:", "model_inf_time": 1.3}, {"id": "41149", "output": "Feature-Based Interaction Maps for Texture Analysis and Segmentation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nPairwise pixel interactions have proved to be a powerful tool in feature based [1,2] and model based [3,4] texture analysis. Various successful applications of the feature-based interaction map (FBIM) approach have already been presented [1,2,5,6]. Different aspects and components of the approach have been discussed, but no self-contained description of the FBIM method has been published yet. This paper provides a comprehensive up-to-date survey of the approach, including all major algorithms and a series of systematic experimental studies that demonstrate the capabilities of the approach.\nTitle:\nTexture analysis using feature-based pairwise interaction maps\n\nAbstract:\nRegular structures, flat and non-flat, are perceived as regular in a wide range of viewing angles and under varying illumination. In this papers, we exploit this simple observation and develop an invariant measure of pattern regularity. The measure is the maximum of the regularity values obtained for different directions within the pattern. We demonstrate that the regularity feature introduced is reasonably stable under weak-perspective of real non-flat structures. The feature is consistent with human perception of texture regularity. It is used for regularity-based image filtering. Examples of invariant detection of periodic structures are shown. Finally, structural defects in regular textures are detected as locations of low regularity.\nTitle:\nPattern Regularity as a Visual Key\n\nAbstract:\n . Pairwise pixel interactions have proved to be a powerful toolin feature based [3,7] as well model based [9] texture analysis. Differentaspects and components of the feature based interaction map (FBIM)approach have already been discussed, but no self-contained descriptionof the FBIM has been published yet. This paper provides a comprehensiveup-to-date survey of the approach, including major algorithms anda series of experimental studies that demonstrate the capabilities of the... \nTitle:\nTexture Analysis Using Pairwise Interaction Maps\n\nAbstract:\nWe address the problem of dynamic texture (DT) classification using optical flow features. Optical flow based approaches dominate among the currently available DT recognition methods. We introduce rotation- and scale-invariant DT features based on local image distortions computed via optical flow. Then we describe an SVD-based method for measuring the degree of temporal periodicity of a dynamic texture. Finally, we present the results of a DT classification study that compares the performances of different flow features for normal and complete optical flows.\nTitle:\nDynamic Texture Recognition Using Optical Flow Features And Temporal Periodicity\n\nAbstract:\nThis study aims at building photorealistic 3D models of real-world objects. We discuss the problem of combining a 3D textureless model obtained by 3D scanner, with optical images that provide textural information of the object. Recently, we have proposed a novel method to register an uncalibrated image pair to a 3D surface model. After registration, the images are mapped to the surface. However, as the images show different parts of the objects, partial overlapping textures can only be extracted from them. Combining the images into a complete texture map that covers the entire object is not trivial. We present a method to build photorealistic 3D models that includes algorithms for data registration and for merging multiple texture maps using surface flattening. Experimental results on real and synthetic data are shown.\nTitle:\nData fusion for photorealistic 3d models\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.13, mean value for all the writers is 8.648, which makes it 0.442 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe have recently introduced a new tool for texture analy- sis called feature based interaction map (FBIM). The FBIM approach can be efficiently used to assess fundamental structural properties of textures such as anisotropy, sym- metry, orientation and regularity (4). It has been demon- strated (5) that the FBIM is suitable for rotation-invariant texture classification of patterns with regular, weak regu- lar, or linear structure. In this paper, we show how the in- teraction map can be applied as astructural filter for seg- mentation, detection of textured objects and texture defects, analysis of oriented structures and shape-from-texture. The power of the FBIM filter is in its unique capability to grasp the structure of pixel interactions typical for a given texture pattern. To efficiently use this capability, we propose a fast running implementation of the FBIM algorithm and present pilot experimental results demonstrating the potential of the FBIM approach in diverse tasks and applications. 1\nTitle:", "model_inf_time": 1.26}, {"id": "41150", "output": "A Fast QoS-Based Resource Distribution Framework", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe problem of maximizing system utility by allocating a single finite resource to satisfy discrete Quality of Service (QoS) requirements of multiple applications along multiple QoS dimensions was studied previously. In this paper we consider the more complex problem of apportioning multiple finite resources to satisfy the QoS needs of multiple applications along multiple QoS dimensions. In other words, each application, such as video-conferencing, needs multiple resources to satisfy its QoS requirements. We evaluate and compare three strategies to solve this provably NP-hard problem. We show that dynamic programming and mixed integer programming compute optimal solutions to this problem but exhibit very long running times. We then adapt the mixed integer programming problem to yield near-optimal results with smaller running times. Finally, we present an approximation algorithm based on a local search technique that is less than 5% away from the optimal solution but which is more than two orders of magnitude faster. Perhaps more significantly, the local search technique turns out to be very scalable and robust as the number of resources required by each application increases\nTitle:\nA scalable solution to the multi-resource QoS problem\n\nAbstract:\nDynamic Quality-of-Service (QoS) management has been shown to be an effective way to make an efficient use of systems resources, such as computing, communication or energy. This is particularly important in resource-contrained embedded systems, such as vehicles, multimedia devices, etc.. Deploying dynamic QoS management requires using an appropriate schedulability test that is fast enough and ensures continued schedulability while the system adapts its configuration. In this paper we consider four utilization-based tests with release jitter, a particularly relevant feature in distributed systems, three of which were recently proposed and one is added in this work. We carry out an extensive comparison using random task sets to characterize their relative merits and we show a case study where multiple video streams are dynamically managed in a fictitious automotive application using such schedulability bounds.\nTitle:\nOn the schedulability analysis for dynamic QOS management in distributed embedded systems\n\nAbstract:\nCurrent spacecraft systems generally have monolithic structures, but a \"fractionated\" architecture is being considered for next generation spacecrafts. A fractionated spacecraft system is a cluster of independent modules that communicate wirelessly to maintain cluster flight formations and realize the functions usually performed by a monolithic satellite. The envisioned benefits of the fractionated approach include enhanced responsiveness, greater flexibility, robustness and co-existence of multiple missions from different sources with varying degree of trust. The fractionated architecture, however, introduces significant new challenges from the perspective of resource allocation and management. The mobile nature of the clusters and the modules within the cluster implies that the network topology is highly time-varying. A cluster with multiple missions can require messages to be transmitted across the network with varying degrees of QoS requirements such as timeliness and data delivery reliability. The system must determine the appropriate and timely resource allocation for these missions. In this paper, we address these resource allocation challenges by introducing an abstraction of dynamic graphs, and extending the QoS based Resource Allocation Model (Q-RAM) to operate on these dynamic graphs. We develop a mechanism to decompose a dynamic graph into multiple static sub-graphs using which the resource allocation problem is partitioned into multiple sub-problems within each of these static sub-graphs. We have experimentally evaluated our solution by building a simulation framework called SatSim, which can handle a variety of satellite configurations and mobility models. The proposed solution is shown to achieve a near-optimal solution for the resource allocation problem in time-varying networks, while reducing time complexity significantly.\nTitle:\nQoS-Based Resource Allocation for Next-Generation Spacecraft Networks\n\nAbstract:\nDynamic real-time systems such as phased-array radars must manage multiple resources, satisfy energy constraints and make frequent on-line scheduling decisions. These systems are hard to manage because task and system requirements change rapidly (e.g. in radar systems, the targets/tasks in the sky are moving continuously) and must satisfy a multitude of constraints. Their highly dynamic nature and stringent time constraints lead to complex cross-layer interactions in these systems. Therefore, the design of such systems has long been a conservative and/or unpredictable mixture of pre-computed schedules, pessimistic resource allocations, cautious energy usage and operator intuition. In this paper, we present an integrated approach that simultaneously maximizes overall system utility, performs task scheduling and satisfies multi-resource constraints. Using a phased-array radar system, we show that our approach can reconfigure settings of 100 tracks at every 0.7 sec in real-time, and performs within 0.1% of the achievable optimal solution.\nTitle:\nIntegrated Resource Management and Scheduling with Multi-Resource Constraints\n\nAbstract:\nWe present a QoS management framework that enables us to quantitatively measure QoS, and to analytically plan and allocate resources. In this model, end users' quality preferences are considered when system resources are apportioned across multiple applications such that the net utility that accrues to the end-users is maximized. In \\cite{RLLS97:2}\\cite{RLLS98}, we primarily worked with continuous QoS dimensions, and assumed that the utility gained by improvements along a QoS dimension were always representable by concave functions. In this paper, we relax both assumptions. One, we support discrete QoS operating points. Two, we make no assumptions about the concavity of the utility functions. Using these as the basis, we tackle the problem of maximizing system utility by allocating a single finite resource to satisfy the QoS requirements of multiple applications along multiple QoS dimensions. We present two near-optimal algorithms to solve this problem. The first yields an allocation within a known bounded distance from the optimal solution, and the second yields an allocation whose distance from the optimal solution can be explicitly controlled by the QoS manager. We compare the run-times of these near-optimal algorithms and their solution quality relative to the optimal allocation, which in turn is computed using dynamic programming. These detailed evaluations provide practical insight into which of these algorithms can be used online in real-time systems.\nTitle:\nOn quality of service optimization with discrete QoS options\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.689, mean value for all the writers is 8.648, which makes it 0.035 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDynamic resource management is generating growing interest as a way to simplify systems deployment, react to changing operational conditions and improve efficiency in using system resources. Buttazzo et al proposed the elastic task model (ETM) that dynamically moves the tasks instantiation period around a nominal value, increasing or decreasing the bandwidth that each task uses from the resource. However such computation requires a number of iteration steps. This paper identifies the QoS-based resource distribution problem and proposes a generic framework that encompasses the ETM. An algorithm is presented for the resource distribution that computes the results in minimum time, outperforming the original ETM scheme in terms of computational complexity.\nTitle:", "model_inf_time": 1.42}, {"id": "41151", "output": "Overlay Topology Optimization in P2P-TV Systems: An Experimental Evaluation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nChunk and peer scheduling is among the main driver of performance in P2P streaming systems. While previous work has analytically proved that optimal scheduling algorithms exist, such strategies are based on a large number of strong assumptions about the knowledge that a single peer has of the rest of the system. This short paper presents a protocol for turning these theoretical results into practical ones, by taking into account practical aspects like the diffusion time of signaling messages and a partial knowledge of the participating peers.\nTitle:\nScheduling in P2P Streaming: From Algorithms to Protocols\n\nAbstract:\nUnstructured, chunk-based P2P streaming (TV and Video) systems are becoming popular and are subject of intense research. Chunk and peer selection strategies (or scheduling) are among the main driver of performance. This work presents the formal proof that there exist a distributed scheduling strategy which is able to distribute every chunk to all N peers in exactly ***log2 (N ) *** + 1 steps. Since this is the minimum number of steps needed to distribute a chunk, the proposed strategy is optimal. Such a strategy is implementable and an entire class of deadline-based schedulers realize it. We show that at least one of the deadline-based schedulers is resilient to the reduction of the neighborhood size down to values as small as log2 (N ). Selected simulation results highlighting the properties of the algorithms in realistic scenarios complete the paper.\nTitle:\nOn the Optimal Scheduling of Streaming Applications in Unstructured Meshes\n\nAbstract:\nSplitting a P2P video distribution in multiple media flows with different priorities is an interesting approach for developing flexible and adaptive streaming systems, ranging from VoD to TV. Such an approach can both yield satisfactory quality to all end users and be light in network resources usage, because low-priority flows can be discarded a-priori when target peers do not have enough resources to receive them. This paper focuses on chunk-based video distribution in unstructured meshes, adopting a push strategy (the sender takes the scheduling decision) based on buffer map exchange to avoid sending duplicated chunks. A deadline-based scheduling algorithm is proposed, where different flows of chunks are prioritized using different deadline postponing parameters for each flow. Some experimental results show good differentiation properties and streaming performance much better than with strict priority enforcement. Also, PSNR measures on real video streams show improvements compared to both strict priority and single stream distribution.\nTitle:\nDeadline-Based Differentiation in P2P Streaming\n\nAbstract:\nWe propose an adaptive scheduling technique to schedule highly dynamic multimedia tasks on a CPU. We use a combination of two techniques: the first one is a feedback mechanism to track the resource requirements of the tasks based on \u201clocal\u201d observations. The second one is a mechanism that operates with a \u201cglobal\u201d visibility, reclaiming unused bandwidth. The combination proves very effective: resource reclaiming increases the robustness of the feedback, while the identification of the correct bandwidth made by the feedback increases the effectiveness of the reclamation. We offer both theoretical results and an extensive experimental validation of the approach.\nTitle:\nA Robust Mechanism for Adaptive Scheduling of Multimedia Applications\n\nAbstract:\nPeer-to-peer (P2P) and cloud computing, two of the Internet trends of the last decade, hold similar promises: the (virtually) infinite availability of computing and storage resources. But there are important differences: the cloud provides highly-available resources, but at a cost; P2P resources are for free, but their availability is shaky. Several academic and commercial projects have explored the possibility of mixing the two, creating a large number of peer-assisted applications, particularly in the field of content distribution, where the cloud provides a highly-available and persistent service, while P2P resources are exploited for free whenever possible to reduce the economic cost. While executing active servers on elastic computing facilities like Amazon EC2 and pairing them with user-provided peers is definitely one way to go, this paper proposes a novel approach that further reduces the economic cost. Here, a passive storage service like Amazon S3 is exploited not only to distribute content to clients, but also to build and manage the P2P network linking them. An effort is made to guarantee that the read/write load imposed on the storage remains constant, regardless of the number of peers/clients. These two choices allows us to keep the monetary cost of the cloud always under control, in the presence of just one peer or with a million of them. We show the feasibility of our approach by discussing two cases studies for content distribution: the Dilbert's comic strips and the hourly News Update podcast from CNN.\nTitle:\nCloudy Weather For P2p, With A Chance Of Gossip\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.66, mean value for all the writers is 8.648, which makes it 0.843 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nPeer-to-peer live-streaming (P2P-TV) systems\u2019 goal is disseminating real-time video content using peer-to-peer technology. Their performance is driven by the overlay topology, i.e., the virtual topology that peers use to exchange video chunks. Several proposals have been made in the past to optimize it, yet few experimental studies have corroborated results. The aim of this paper is to provide a comprehensive experimental comparison based on PeerStreamer in order to benchmark different strategies for the construction and maintenance of the overlay topology in P2P-TV systems. We present only experimental results in which fully distributed strategies are evaluated in both controlled experiments and the Internet using thousands of peers. Results confirm that the topological properties of the overlay have a deep impact on both user quality of experience and network load. Strategies based solely on random peer selection are greatly outperformed by smart yet simple and actually implementable strategies. The most performing strategy we devise guarantees to deliver almost all chunks to all peers with a playout delay as low as 6 s even when system load approaches 1, and in almost adversarial network scenarios. PeerStreamer is open-source to make results reproducible and allow further research by the community.\nTitle:", "model_inf_time": 1.58}, {"id": "41152", "output": "Hybrid Feature Selection for Lightweight and Efficient Intrusion Detection Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCurrent intrusion detection systems (IDS) examine all data features to detect intrusion or misuse patterns. Some of the features may be redundant or contribute little (if anything) to the detection process. The purpose of this study is to identify important input features in building an IDS that is computationally efficient and effective. We investigated the performance of two feature selection algorithms involving Bayesian networks (BN) and Classification and Regression Trees (CART) and an ensemble of BN and CART. Empirical results indicate that significant input feature selection is important to design an IDS that is lightweight, efficient and effective for real world detection systems. Finally, we propose an hybrid architecture for combining different feature selection algorithms for real world intrusion detection.\nTitle:\nFeature deduction and ensemble design of intrusion detection systems\n\nAbstract:\nCurrent Intrusion Detection Systems (IDS) examine all data features to detect intrusion or misuse patterns. Some of the features may be redundant or contribute little (if anything) to the detection process. The purpose of this research is to identify important input features in building an IDS that is computationally efficient and effective. This paper propose a novel matrix factorization approach for feature deduction and design of intrusion detection systems. Experiment results indicate that the proposed method is efficient.\nTitle:\nMatrix Factorization Approach for Feature Deduction and Design of Intrusion Detection Systems\n\nAbstract:\nCurrent Intrusion Detection Systems (IDS) examine all data features to detect intrusion or misuse patterns. Some of the features may be redundant or contribute little (if anything) to the detection process. The purpose of this study is to identify important input features in building an IDS that is computationally efficient and effective. This paper proposes an IDS model based on general and enhanced Flexible Neural Tree (FNT). Based on the pre-defined instruction/operator sets, a flexible neural tree model can be created and evolved. This framework allows input variables selection, over-layer connections and different activation functions for the various nodes involved. The FNT structure is developed using an evolutionary algorithm and the parameters are optimized by particle swarm optimization algorithm. Empirical results indicate that the proposed method is efficient.\nTitle:\nFeature selection and intrusion detection using hybrid flexible neural tree\n\nAbstract:\nAn intrusion is defined as a violation of the security policy of the system, and, hence, intrusion detection mainly refers to the mechanisms that are developed to detect violations of system security policy. Current intrusion detection systems (IDS) examine all data features to detect intrusion or misuse patterns. Some of the features may be redundant or contribute little (if anything) to the detection process. The purpose of this study is to identify important input features in building an IDS that is computationally efficient and effective. This article proposes an IDS model based on a general and enhanced flexible neural tree (FNT). Based on the predefined instruction/operator sets, a flexible neural tree model can be created and evolved. This framework allows input variables selection, overlayer connections, and different activation functions for the various nodes involved. The FNT structure is developed using an evolutionary algorithm, and the parameters are optimized by a particle swarm optimization algorithm. Empirical results indicate that the proposed method is efficient. (c) 2007 Wiley Periodicals, Inc.\nTitle:\nHybrid Flexible Neural-Tree-Based Intrusion Detection Systems\n\nAbstract:\nIntrusion Detection System (IDS) is an important and necessary component in ensuring network security and protecting network resources and infrastructures. In this paper, we effectively introduced intrusion detection system by using Principal Component Analysis (PCA) with Support Vector Machines (SVMs) as an approach to select the optimum feature subset. We verify the effectiveness and the feasibility of the proposed IDS system by several experiments on NSL-KDD dataset. A reduction process has been used to reduce the number of features in order to decrease the complexity of the system. The experimental results show that the proposed system is able to speed up the process of intrusion detection and to minimize the memory space and CPU time cost.\nTitle:\nPrinciple components analysis and Support Vector Machine based Intrusion Detection System\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.821, mean value for all the writers is 8.648, which makes it 1.001 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMost of the current Intrusion Detection Systems (IDS) examine all data features to detect intrusion or misuse patterns. Some of the features may be redundant or contribute little (if anything) to the detection process. We investigated the performance of two feature selection algorithms involving Bayesian Networks (BN) and Classification and Regression Trees (CART) and an ensemble of BN and CART. An hybrid architecture is further proposed by combining different feature selection algorithms. Empirical results indicate that significant input feature selection is important to design an IDS that is lightweight, efficient and effective for real world detection systems.\nTitle:", "model_inf_time": 1.31}, {"id": "41153", "output": "Hybrid Intrusion Detection Systems Using Decision Trees and Support Vector Machines", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAn Intrusion Detection System (IDS) is a program that analyzes what happens or has happened during an execution and tries to find indications that the computer has been misused. An IDS does not eliminate the use of preventive mechanism but it works as the last defensive mechanism in securing the system. This paper evaluates the performances of Multi-Expression Programming (MEP) to detect intrusions in a network. Results are then compared with Linear Genetic Programming (LGP) approach. Empirical results clearly show that genetic programming could play an important role in designing light weight, real time intrusion detection systems.\nTitle:\nMEPIDS: multi-expression programming for intrusion detection system\n\nAbstract:\nIntrusion detection is the process of monitoring the events occurring in a computer system or network and analyz- ing them for signs of intrusions, defined as attempts to compromise the confidentiality, integrity, availability, or to bypass the security mechanisms of a computer or net- work. This paper proposes the development of an Intru- sion Detection Program (IDP) which could detect known attack patterns. An IDP does not eliminate the use of any preventive mechanism but it works as the last defen- sive mechanism in securing the system. Three variants of genetic programming techniques namely Linear Ge- netic Programming (LGP), Multi-Expression Program- ming (MEP) and Gene Expression Programming (GEP) were evaluated to design IDP. Several indices are used for comparisons and a detailed analysis of MEP technique is provided. Empirical results reveal that genetic program- ming technique could play a major role in develop- ing IDP, which are light weight and accurate when compared to some of the conventional intrusion detection systems based on machine learning paradigms.\nTitle:\nEvolutionary Design of Intrusion Detection Programs\n\nAbstract:\nRisk assessment is often done by human experts, because there is no exact and mathematical solution to the problem. Usually the human reasoning and perception process cannot be expressed precisely. This paper propose a light weight risk assessment system based on an Hierarchical Takagi-Sugeno model designed using evolutionary algorithms. Performance comparison is done with neuro-fuzzy and genetic programming methods. Empirical results indicate that the techniques are robust and suitable for developing light weight risk assessment models, which could be integrated with intrusion detection and prevention systems.\nTitle:\nHierarchical Takagi-Sugeno Models for Online Security Evaluation Systems\n\nAbstract:\nDue to the wide deployment of sensor networks recently security in sensor networks has become a hot research topic. Popular ways to secure a sensor network are by including cryptographic techniques or by safeguarding sensitive information from unauthorized access/manipulation and by implementing efficient intrusion detection mechanisms. This paper proposes a novel ant colony based intrusion detection mechanism which could also keep track of the intruder trials. The IDEAS technique could work in conjunction with the conventional machine learning based intrusion detection techniques to secure the sensor networks. The algorithm is presented and illustrated by simulating a sensor network.\nTitle:\nIDEAS: Intrusion Detection based on Emotional Ants for Sensors\n\nAbstract:\nThis paper proposes a decision support system for tactical air combat environment using a combination of unsupervised learning for clustering the data and an ensemble of three well-known genetic programming techniques to classify the different decision regions accurately. The genetic programming techniques used are: Linear Genetic programming (LGP), Multi-Expression Programming (MEP) and Gene Expression Programming (GEP). The clustered data are used as the inputs to the genetic programming algorithms. Some simulation results demonstrating the difference of these techniques are also performed. Test results reveal that the proposed ensemble method performed better than the individual GP approaches and that the method is efficient.\nTitle:\nDecision Support Systems Using Ensemble Genetic Programming\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.148, mean value for all the writers is 8.648, which makes it 0.427 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe process of monitoring the events occurring in a computer system or network and analyzing them for sign of intrusions is known as intrusion detection system (IDS). This paper presents two hybrid approaches for modeling IDS. Decision trees (DT) and support vector machines (SVM) are combined as a hierarchical hybrid intelligent system model (DT-SVM) and an ensemble approach combining the base classifiers. The hybrid intrusion detection model combines the individual base classifiers and other hybrid machine learning paradigms to maximize detection accuracy and minimize computational complexity. Empirical results illustrate that the proposed hybrid systems provide more accurate intrusion detection systems.\nTitle:", "model_inf_time": 1.35}, {"id": "41154", "output": "Sybil Attack Mitigation in Decentralized Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn Ad hoc On Demand Vector (AODV) routing protocol for Mobile Ad hoc Networks (MANET), malicious nodes can easily disrupt the communication. A malicious node that is not part of any route may launch Denial of Service (DoS) Attack. Also once a route is formed, any node in the route may turn malicious and may refrain from forwarding packets, modify them before forwarding or may even forward to an incorrect intermediate node. Such malicious activities by a misbehaving node cannot be checked for in pure AODV protocol. In this paper, a proactive scheme is proposed to detect the above-mentioned malicious activities.\nTitle:\nSecurity scheme for malicious node detection in mobile ad hoc networks\n\nAbstract:\n  A number of systems in recent times suffer from attacks like DDoS and Ping of Death. Such attacks result in loss of critical system resources and CPU cycles, as these compromised systems behave in an abnormal manner. The effect of such abnormalities is worse in case of compromised systems handling financial transaction, since it leads to severe monetary losses. In this paper we propose a system that uses the Replicated State Machine approach to detect abnormality in system usage. The suggested system is based on PAXOS algorithm, an algorithm for solving the consensus problem in a network of unreliable processors. \nTitle:\nA PAXOS based State Machine Replication System for Anomaly Detection\n\nAbstract:\nIn Mobile Ad hoc Networks (MANET), various types of Denial of Service Attacks (DoS) are possible because of the inherent limitations of its routing protocols. Considering the Ad hoc On Demand Vector (AODV) routing protocol as the base protocol it is possible to find a suitable solution to overcome the attack of initiating / forwarding fake Route Requests (RREQs) that lead to hogging of network resources and hence denial of service to genuine nodes. In this paper, a proactive scheme is proposed that can prevent a specific kind of DoS attack and identify the misbehaving node. Since the proposed scheme is distributed in nature it has the capability to prevent Distributed DoS (DDoS) as well. The performance of the proposed algorithm in a series of simulations reveal that the proposed scheme provides a better solution than existing approaches with no extra overhead.\nTitle:\nSecurity Scheme for Distributed DoS in Mobile Ad Hoc Networks\n\nAbstract:\n  A number of works in the field of intrusion detection have been based on Artificial Immune System and Soft Computing. Artificial Immune System based approaches attempt to leverage the adaptability, error tolerance, self- monitoring and distributed nature of Human Immune Systems. Whereas Soft Computing based approaches are instrumental in developing fuzzy rule based systems for detecting intrusions. They are computationally intensive and apply machine learning (both supervised and unsupervised) techniques to detect intrusions in a given system. A combination of these two approaches could provide significant advantages for intrusion detection. In this paper we attempt to leverage the adaptability of Artificial Immune System and the computation intensive nature of Soft Computing to develop a system that can effectively detect intrusions in a given network. \nTitle:\nA Hybrid Approach Towards Intrusion Detection Based on Artificial Immune System and Soft Computing\n\nAbstract:\n  The introduction of the social networking platform has drastically affected the way individuals interact. Even though most of the effects have been positive, there exist some serious threats associated with the interactions on a social networking website. A considerable proportion of the crimes that occur are initiated through a social networking platform [5]. Almost 33% of the crimes on the internet are initiated through a social networking website [5]. Moreover activities like spam messages create unnecessary traffic and might affect the user base of a social networking platform. As a result preventing interactions with malicious intent and spam activities becomes crucial. This work attempts to detect the same in a social networking platform by considering a social network as a weighted graph wherein each node, which represents an individual in the social network, stores activities of other nodes with respect to itself in an optimized format which is referred to as localized data-set. The weights associated with the edges in the graph represent the trust relationship between profiles. The weights of the edges along with the localized data-set is used to infer whether nodes in the social network are compromised and are performing spam or malicious activities. \nTitle:\nA Heuristic Reputation Based System to Detect Spam activities in a Social Networking Platform, HRSSSNP\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.087, mean value for all the writers is 8.648, which makes it 0.375 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAny decentralised distributed network is particularly vulnerable to the Sybil attack wherein a malicious node masquerades as several different nodes, called Sybil nodes, simultaneously in an attempt to disrupt the proper functioning of the network. Such attacks may cause damage on a fairly large scale especially since they are difficult to detect and there has been no universally accepted scheme to counter them as yet. In this paper, we discuss the different kinds of Sybil attacks including those occurring in peer-to-peer reputation systems, self-organising networks and even social network systems. In addition, various methods that have been suggested over time to decrease or eliminate their risk completely are also analysed along with their modus operandi.\nTitle:", "model_inf_time": 1.27}, {"id": "41155", "output": "Challenge-Based Speaker Recognition for Mobile Device Authentication", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nOnline banking and electronic payment systems on the Internet are becoming increasingly advanced. On the machine level, transactions take place between client and server hosts through a secure channel protected with SSL/TLS. User authentication is typically based on two or more factors. Nevertheless, the development of various malwares and social engineering attacks transform the user's PC in an untrusted device and thereby making user authentication vulnerable. This paper investigates how user authentication with biometrics can be made more robust in the online banking context by using a specific device called OffPAD. This context requires that authentication is realized by the bank and not only by the user (or by the personal device) contrary to standard banking systems. More precisely, a new protocol for the generation of one- time passwords from biometric data is presented, ensuring the security and privacy of the entire transaction. Experimental results show an excellent performance considering with regard to false positives. The security analysis of our protocol also illustrates the benefits in terms of strengthened security.\nTitle:\nOne-Time Biometrics for Online Banking and Electronic Payment Authentication.\n\nAbstract:\nThe storage of fingerprints is an important issue as this biometric modality is more and more deployed for real applications. The a prori impossibility to revoke a biometric template (like a password) in case of theft, is a major concern for privacy reasons. We propose in this paper a new method to secure fingerprint minutiae templates by storing a bio code while keeping good recognition results. We show the efficiency of the method in comparison to some published methods for different scenarios.\nTitle:\nBiohashing for Securing Minutiae Template\n\nAbstract:\nWe present in this paper a study on the ability and the benefits of using a keystroke dynamics authentication method for collaborative systems. Authentication is a challenging issue in order to guarantee the security of use of collaborative systems during the access control step. Many solutions exist in the state of the art such as the use of one time passwords or smart-cards We focus in this paper on biometric based solutions that do not necessitate any additional sensor. Keystroke dynamics is an interesting solution as it uses only the keyboard and is invisible for users. Many methods have been published in this field. We make a comparative study of many of them considering the operational constraints of use for collaborative systems.\nTitle:\nKeystroke dynamics authentication for collaborative systems\n\nAbstract:\nTemplate protection is a crucial issue in biometrics. Many algorithms have been proposed in the literature among secure computing approaches, crypto-biometric algorithm and feature transformation schemes. The BioHashing algorithm belongs to this last category and has very interesting properties. Among them, we can cite its genericity since it could be applied on any biometric modality, the possible cancelability of the generated BioCode and its efficiency when the secret is not stolen by an impostor. Its main drawback is its weakness face to a combined attack (zero effort with the stolen secret scenario). In this paper, we propose a transformation-based biometric template protection scheme as an improvement of the BioHashing algorithm where the projection matrix is generated by combining the secret and the biometric data. Experimental results on two biometric modalities, namely digital fingerprint and finger knuckle print images, show the benefits of the proposed method face to attacks while keeping a good efficiency.\nTitle:\nEnhancing the Security of Transformation Based Biometric Template Protection Schemes\n\nAbstract:\nBiometrics is an emerging technology more and more present in our daily life. However, building biometric systems requires a large amount of data that may be difficult to collect. Collecting such sensitive data is also very time consuming and constrained, s.a. GDPR legislation. In the case of keystroke dynamics, existing databases have less than 200 users. For these reasons, we aim at generating a keystroke dynamics synthetic dataset. This paper presents the generation of keystroke data from known users as a first step towards the generation of synthetic datasets, and could also be used to impersonate users' identity.\nTitle:\nAnalysis of Keystroke Dynamics for the Generation of Synthetic Datasets\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.015, mean value for all the writers is 8.648, which makes it 0.313 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nUser authentication is a major trend to guarantee the security of electronic transactions when using mobile devices such as tablets or mobile phones. Biometrics is for us the only real user authentication method. In this article, we propose to realize a speaker recognition approach to achieve this goal. We use a challenge-based method to avoid the replay attack (especially if the impostor has recorded the user's voice). In this case, free text recognition is realized. Experimental results on the CMU database show very good results, while providing low computation times.\nTitle:", "model_inf_time": 1.21}, {"id": "41156", "output": "A General Method for Backward Gradient Computation in Dynamic Systems Using Signal-Flow-Graphs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA large class of nonlinear dynamic adaptive systems such as dynamic recurrent neural networks can be effectively represented by signal flow graphs (SFGs). By this method, complex systems are described as a general connection of many simple components, each of them implementing a simple one-input, one-output transformation, as in an electrical circuit. Even if graph representations are popular in the neural network community, they are often used for qualitative description rather than for rigorous representation and computational purposes. In this article, a method for both on-line and batch-backward gradient computation of a system output or cost function with respect to system parameters is derived by the SFG representation theory and its known properties. The system can be any causal, in general nonlinear and time-variant, dynamic system represented by an SFG, in particular any feedforward, time-delay, or recurrent neural network. In this work, we use discrete-time notation, but the same theory holds for the continuous-time case. The gradient is obtained in a straightforward way by the analysis of two SFGs, the original one and its adjoint (obtained from the first by simple transformations), without the complex chain rule expansions of derivatives usually employed. This method can be used for sensitivity analysis and for learning both off-line and on-line. On-line learning is particularly important since it is required by many real applications, such as digital signal processing, system identification and control, channel equalization, and predistortion.\nTitle:\nA signal-flow-graph approach to on-line gradient calculation.\n\nAbstract:\nA large class of non-linear dynamic adaptive systems such as dynamic recurrent neural networks can be very effectively represented by Signal-Flow-Graphs (SFGs). By this method, complex systems are described as a general connection of many simple components, each of them implementing a simple one-input one-output transformation, as in an electrical circuit. Even if graph representations are popular in the neural network community, they are often use d for qualitative description rather than for rigorous representation and computational purposes. Following an approach originally developed by A.Y. Lee for continuous-time systems based on the concept of adjoint graph, a new algorithm to estimate the derivative of the output with respect to an internal parameter was recently proposed by some of the authors for discrete-time systems. This paper extends further this approach to multirate digital systems, which are nowadays widely used. The new method can be employed for gradient-based learning of general multirate circuits, such as new \u9a74multirate\u9a74 neural networks.\nTitle:\nA General Approach to Gradient Based Learning in Multirate Systems and Neural Networks\n\nAbstract:\nThis paper is focused on the learning algorithms for dynamic multilayer perceptron neural networks where each neuron synapsis is modelled by an infinite impulse response (IIR) filter (IIR MLP). In particular, the Backpropagation Through Time (BPTT) algorithm and its less demanding approximated on-line versions are considered. In fact it is known that the BPTT algorithm is not causal and therefore can be implemented only in batch mode, while many real problems require on-line adaptation. In this paper we give the complete BPTT formulation for the IIR MLP, derive an already known on-line learning algorithm as a particular approximation of the BPTT, and propose a new approximated algorithm. Several computer simulations of identification of dynamical systems will also be presented to assess the performance of the approximated algorithms and to compare the IIR MLP with more traditional dynamic networks.\nTitle:\nOn-Line Learning Algorithms For Neural Networks With Iir Synapses\n\nAbstract:\nIn this paper, the problem of calculation of first and second derivatives in general non-linear dynamical systems is addressed and an attempt of solution by means of signal flow graph (SFG) techniques is proposed. First and full second derivatives of an output of the initial system respect with the node variables of the starting SFG are delivered through an adjoint graph derived without using Lee's theorem. Mixed second derivatives are deduced by quantities attained in adjoint graphs of the original graph or graphs related to it. A detailed theoretical demonstration of these formulations is given. Even though no adjoint graph has been derived in case of mixed derivatives, the ability of the proposed method to determine all Hessian matrix entries in a complete automatic way is highlighted.\nTitle:\nA novel signal flow graph based solution for calculation of first and second derivatives of dynamical nonlinear systems\n\nAbstract:\nIn this paper, we study the properties of a new kind of complex domain artificial neural networks called complex adaptive spline neural networks (CASNN), which are able to adapt their activation functions by varying the control points of a Catmull- Rom cubic spline. This new kind of neural network can be implemented as a very simple structure being able to improve the generalization capabilities using few training epochs. Due to its low architectural complexity this network can be used to cope with several nonlinear DSP problem at high throughput rate.\nTitle:\nA class of fast complex domain neural networks for signal processing applications\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.442, mean value for all the writers is 8.648, which makes it 1.531 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, making use of the Signal-Flow-Graph (SFG) representation and its known properties, we derive a new general method for backward gradient computation of a system output or cost function with respect to past (or present) system parameters. The system can be any causal, in general non-linear and time-variant, dynamic system represented by a SFG, in particular any feed forward or recurrent neural network. In this work we use discrete time notation, but the same theory holds for the continuos time case. The gradient is obtained by the analysis of two SFGs, the original one and its adjoint. This method can be used both for on-line and off-line learning. In the latter case using the Mean Square Error cost function, our approach particularises to E. Wan's method that is not suited for online training of recurrent networks. Computer simulations of non-linear dynamic systems identification will also be presented to assess the performance of the algorithm resulting from the application of the proposed method in the case of locally recurrent neural networks.\nTitle:", "model_inf_time": 1.68}, {"id": "41157", "output": "Supporting Requirements Validation of Function-Centered Designs Against Evolving Stakeholder Intentions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn model-based engineering of embedded systems, manual validation activities such as reviews and inspections are needed to ensure that the system under development satisfies the stakeholder intentions. During the engineering process, changes in the stakeholder intentions typically trigger revisions of already developed and documented engineering artifacts including requirements and design specifications. In practice, changes in stakeholder intentions are often not immediately perceived and not properly documented. Moreover, they are quite often not consistently incorporated into all relevant engineering artifacts. In industry, typically manual reviews are executed to ensure that the relevant stakeholder intentions are adequately considered in the engineering artifacts. In this article, we introduce a dedicated review model to aid the reviewer in conducting manual reviews of behavioral requirements and functional design specification\u2014two core artifacts in function-centered engineering of embedded software. To investigate whether the proposed solution is beneficial we conducted controlled experiments showing that the use of the dedicated review model can significantly increase the effectiveness and efficiency of manual reviews. Additionally, the use of the dedicated review model leads to significantly more confident decisions of the reviewers and is perceived by the reviewers as significantly more supportive compared with reviews without the dedicated review model.\nTitle:\nImproving manual reviews in function-centered engineering of embedded systems using a dedicated review model\n\nAbstract:\nThe need to co-develop requirements and architectural artefacts, especially for innovative solutions, is widely recognised and accepted. Surprisingly, no comprehensive approach exists to structure the co-design process and to support the stakeholders, requirements engineers, and system architects in co-developing innovative requirements and architectural artefacts. In this paper, we propose a method for the co-design of requirements and architectural artefacts based on two viewpoints, the system usage viewpoint and the system architecture viewpoint. Initially, the two viewpoints are nearly decoupled. The method consists of five sub-processes that support the development of each viewpoint, the comparison of the two viewpoints, the consolidation of the viewpoints, and the definition of detailed system requirements based on the two viewpoints. The consolidation of system usage and coarse-grained system architecture is driven by the refinement of system interaction scenarios into architectural scenarios and the refinement of the associated usage goals. Preliminary results of applying our method in industry are reported.\nTitle:\nStructuring the co-design of requirements and architecture\n\nAbstract:\nResearch in process-centered environments (PCEs) has focused on project management support and has neglected method guidance for the engineers performing the (software) engineering process. It has been dominated by the search for suitable process-modeling languages and enactment mechanisms. The consequences of process orientation on the computer-based engineering environments, i.e., the interactive tools used during process performance, have been studied much less. In this article, we present the PRIME (Process Integrated Modeling Environments) framework which empowers method guidance through process-integrated tools. In contrast to the tools of PCEs, the process-integrated tools of PRIME adjust their behavior according to the current process situation and the method definitions. Process integration of PRIME tools is achieved through (1) the definition of tool models; (2) the integration of the tool models and the method definitions; (3) the interpretation of the integrated environment model by the tools, the process-aware control integration mechanism, and the enactment mechanism; and (4) the synchronization of the tools and the enactment mechanism based on a comprehensive interaction protocol. We sketch the implementation of PRIME as a reusable implementation framework which facilitates the realization of process-integrated tools as well as the process integration of external tools. We define a six-step procedure for building a PRIME-based process-integrated environment (PIE) and illustrate how PRIME facilitates change integration on an easy-to-adapt modeling level.\nTitle:\nPRIME\u2014toward process-integrated modeling environments: 1\n\nAbstract:\nOver the past decade, a dramatic increase of functionality, quantity, size, and complexity of software-intensive embedded systems in the automotive industry can be observed. In particular, the growing complexity drives current requirements engineering practices to the limits. In close cooperation between partners from industry and academia, the recently completed REMsES (Requirements Engineering and Management for software-intensive Embedded Systems) project has developed a guideline to support requirements engineering processes in the automotive industry. The guideline enables the requirements engineers to cope with the challenges that arise due to quantity, size and complexity of software-intensive systems. This article presents the major results of the project, namely, the fundamental principles of the approach, the guideline itself, the tool support, and the major findings obtained during the evaluation of the approach.\nTitle:\nGuiding requirements engineering for software-intensive embedded systems in the automotive industry\n\nAbstract:\nTrends in society and technology force requirements engineering to expand its role from a one-shot activity in the development process to a virtual image that accompanies the changing reality of a system. A maturing software market also requires a better understanding of the differentiation in market segments for requirements engineering and standardisation of methodologies within these segments. On the research side, this requires a coherent perspective of hitherto parallel research directions towards a comprehensive understanding of requirements processes, as well as the optimal exploitation of new technologies that support the main role of requirements engineering; mutual learning of all stakeholders concerned\nTitle:\nRequirements engineering in 2001: (virtually) managing a changing reality\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.757, mean value for all the writers is 8.648, which makes it 0.093 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the embedded systems industry, function-centered engineering is commonly applied to address the increasing number and complexity of system functions. During function-centered engineering, the functional design that is created based on the defined requirements for the system is the main artifact that serves as a basis for subsequent development activities. If stakeholder intentions change and modifications become necessary, they are frequently incorporated directly into the functional design without updating the behavioral requirements accordingly. As a consequence, the correctness of the interplay of system functions as defined in the functional design cannot be assessed by checking it against the defined requirements (since they are outdated) but needs to be checked against the current stakeholder intentions. More precisely, the requirements engineer has to validate the functional design against the stakeholder intentions because he is the expert concerning the stakeholder intentions and can communicate with the stakeholders regarding them, if necessary. However, the requirements engineer is typically not familiar with the functional design and its notation on the one hand, and, on the other hand, the overall behavior of the system is spread across various diagrams in the functional design. Therefore, the requirements engineer needs a more abstract and consolidated view of the functional design in order to be able to validate its correctness with regard to the current stakeholder intentions. In this paper, we present an approach which is based on a specific kind of review model that is automatically generated from the functional design and supports the requirements engineer in her task. The approach that is presented in this paper is subject of ongoing research.\nTitle:", "model_inf_time": 1.69}, {"id": "41158", "output": "Extensible Message ID Assignment for Automotive CAN", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWhen designing a distributed computing-system, the communication networks are a key determining factor for system's performance. A common approach is to minimize bandwidth-consumption, while other important objectives -- maintainability, extensibility, robustness -- get less attention in the literature. In this work we provide a design-methodology how to efficiently balance these conflicting objectives. We build an initial network configuration by applying heuristics. Then, we refine this configuration by using optimization strategies which address the multi-objective optimization problem. By doing so, the network configuration not only satisfies the requirements of the current communication-demand, but it is also prepared to handle additional future communication-demand. Experimental results from an automotive case-study show that extensibility can be significantly improved (up to 44%) while trading only a little bandwidth-efficency (1% deteriation).\nTitle:\nOn Extensible Networks for Embedded Systems\n\nAbstract:\nMany of the processors used in automotive Electronic Control Units (ECUs) are resource constrained due to the cost pressures of volume production; they have relatively low clock speeds and limited memory. Controller Area Network (CAN) is used to connect the various ECUs; however, the broadcast nature of CAN means that every message transmitted on the network can potentially cause additional processing load on the receiving nodes, whether the message is relevant to that ECU or not. Hardware filters can reduce or even eliminate this unnecessary load by filtering out messages that are not needed by the ECU. Filtering is done on the message IDs which are primarily used to identify the contents of the message and its priority. In this paper, we consider the problem of selecting filter configurations to minimize the load due to undesired messages. We show that the general problem is NP-complete. We therefore propose and evaluate an approach based on Simulated Annealing. We show that this approach finds near-optimal filter configurations for the interesting case where there are more desired messages than available filters.\nTitle:\nAnalysis And Optimization Of Message Acceptance Filter Configurations For Controller Area Network (Can)\n\nAbstract:\nTo move mixed criticality research into industrial practice requires models whose run-time behaviour is acceptable to systems engineers. Certain aspects of current models, such as abandoning lower criticality tasks when certain situations arise, do not give the robustness required in application domains such as the automotive and aerospace industries. In this paper a new bailout protocol is developed that still guarantees high criticality tasks but minimises the negative impact on lower criticality tasks via a timely return to normal operation. We show how the bailout protocol can be integrated with existing techniques, utilising offline slack to further improve performance. Static analysis is provided for the strong schedulability guarantees, while scenario based evaluation via simulation is used to explore the effectiveness of the protocol.\nTitle:\nA Bailout Protocol for Mixed Criticality Systems\n\nAbstract:\nThe Worst-Case Execution Time (WCET) is an important execution metric for real-time systems, and an accurate estimate for this increases the reliability of subsequent schedulability analysis. Performance enhancing features on modern processors, such as pipelines and caches, however, make it difficult to accurately predict the WCET. One technique for finding the WCET is to use test data generated using search algorithms. Existing work on search-based approaches has been successfully used in both industry and academia based on a single criterion function, the WCET, but only for simple processors. This paper investigates how effective this strategy is for more complex processors and to what extent other criteria help guide the search, e.g. the number of cache misses. Not unexpectedly the work shows no single choice of criteria work best across all problems. Based on the findings recommendations are proposed on which criteria are useful in particular situations.\nTitle:\nWCET analysis of modern processors using multi-criteria optimisation\n\nAbstract:\nDuring system synthesis (i.e., task allocation) the transmission of messages between tasks is usually addressed in a simplistic way. If a message is exchanged via an external bus, it is assumed each message is packed in an individual frame. This assumption leads to an overestimation of bus bandwidth demand and frame response time. For some systems (i.e., automotive), this pessimism is not acceptable and therefore frame packing is often performed where multiple messages are packed into a single frame. In this paper, an improved frame packing approach is provided.\nTitle:\nOptimized Frame Packing for Embedded Systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.269, mean value for all the writers is 8.648, which makes it 0.53 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nController Area Network (CAN) is widely used in automotive applications. Policies for message ID and thus priority assignment have a significant impact on schedulability. In addition, they also determine extensibility; the scope to add messages required by future upgrades without compromising schedulability. In this paper we address message ID assignment, such that the system is extensible. First, we provide an assessment metric that provides an in-depth view of the extensibility of a given ID-assignment, tailored for use in automotive applications. Second, we develop a practical ID-assignment policy which maximizes extensibility. This policy provides an upgrade pathway: it is used to provide the initial ID-assignment, and also used for ID-assignments during subsequent upgrades. The policy optimizes extensibility by maintaining Deadline minus Jitter Monotonic Priority Ordering, which ensures that it does not compromise either schedulability or robustness to errors on the bus. Evaluation using a simple automotive benchmark shows the effectiveness of the policy over multiple upgrades.\nTitle:", "model_inf_time": 1.28}, {"id": "41159", "output": "Haptic Interface for Sketching Assistance", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA new portable device for haptic interaction with virtual environments is presented. It is a lightweight interface for the fingertips, designed for providing cutaneous feedback and displaying the contact - non contact transition in highly immersive virtual environments. In this paper the kinematics, the mechanical design and the control system are described. The device has been mounted on a kinesthetic haptic interface which tracks its position: the overall system can provide both cutaneous and kinesthetic feedback. Finally the performance of the portable device in a simple task of shape exploration has been evaluated and compared to a kinesthetic haptic interface.\nTitle:\nDesign of a novel finger haptic interface for contact and orientation display.\n\nAbstract:\nThe present work deals with the design, implementation and assessment of a new haptic system specifically conceived for manipulative tasks in virtual environments. Such a system was designed by taking into account specific issues related to fine manipulation, such as multipoint haptics, coherence, transparency and physical representation. The haptic system described herein is integrated with a virtual environment engine for the simulation of multifinger manipulation. A preliminary evaluation of the system was conducted by comparing human performance in the manipulation of virtual objects with respect to real objects, according to the data available in the literature. The experiments confirm how the most relevant relationships among physiological and physical parameters involved in manipulation are also preserved during virtual manipulation. However, an in-depth analysis of the results shows that simulation parameters affect the level of force control during virtual manipulation and the quality of the perceived force feedback.\nTitle:\nDesign and validation of a complete haptic system for manipulative tasks.\n\nAbstract:\nThis paper is concerned with the reactive robot system (RRS) which has been introduced as a novel way of approaching human-robot interactions by exploiting the capabilities of haptic interfaces to transfer skills (from the robot to unskilled persons). The RRS was implemented based on two levels of interaction. The first level, which implements the first two stages of the learning process, represents the conventional control way of interchanging a set of forces in response to a static read of the contact position of some pre-defined dynamic rules (passive interaction). The second level, which implements the last stage of the learning process, represents an enhanced way of interaction between haptic interfaces and humans. This level adds to robotic system a degree of intelligence which enables the robot to dynamically adapt its behavior depending on user wishes (active interaction). In particular, in this paper, the implementation of the second level of the RRS is described in detail. A set of experiments was performed, applied to Japanese handwriting, to verify if second level of the RRS can interact with humans during the autonomous stage of the learning process. The results demonstrated that our system can still provide assistance to users on the autonomous stage while mostly respecting their intentions without significantly affecting their performance.\nTitle:\nReactive robot system using a haptic interface: an active interaction to transfer skills from the robot to unskilled persons\n\nAbstract:\nIn this paper we present a novel haptic device capable of providing both kinesthetic and local haptic cues at the level of the fingerpad. The system is composed of a supporting haptic interface and a fingertip haptic display. The augmentation of locally displayed haptic information may improve the performance in tasks such as shape recognition by haptic exploration. In this paper the overall device is presented, and some preliminary experiments are reported investigating the role of haptic local cues in haptic perception of curvature.\nTitle:\nA fingertip haptic display for improving local perception of shape cues\n\nAbstract:\nThis paper presents a haptic system that is conceived to support the design process of a class of products or services in order to make them more accessible to people affected by hand tremor diseases. The main aim is to foster the designer empathy allowing her/him to directly feel the effect of the impairment in first person. Specifically, a desktop haptic device is employed to induce a programmable hand-tremor, that is typically observed in people affected by some kind of neurological diseases, on healthy subjects (i.e., the designers). The developed tool is based on a wrist-attached haptic interface with a workspace that is comparable to that of the arm of the user. Such device is able to exert controlled forces on the useru0027s wrist and induces a hand-tremor whose frequency and amplitude are correlated with those measured on impaired people. The control of the device is based on a custom trajectory-tracking algorithm that takes as input tremor signals that are acquired on patients using an optical motion tracking system. In this paper, we present the employed haptic system, the structure of the control system and the experimental validation of the controller done through the acquisition of data on six patients affected by Parkinsonu0027s disease.\nTitle:\nDesktop Haptic Interface for Simulation of Hand-Tremor.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.242, mean value for all the writers is 8.648, which makes it 1.36 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis work presents an study of performance improvement of unskilled people to drawn simple sketches. We have assisted the unskilled people when drawn using a haptic interface which acts as a virtual guide taking advantage of its force feedback capabilities. In the first part of the study, an application has been developed to extract fixed templates from image files; the application extract the principal edges in the images to build output trajectories (templates) that are used by haptic interface controller, after that, the user can \"fill\" the virtual templates with the assistance of the force feedback capabilities of the interface. Based on the obtained results for fixed templates, a second application was developed; the user can generate interactive templates indicating where he/she desires to put a geometrical template (circle, tine or arc) inside the haptic interface's workplace; once that the position of the template is defined, the interface shows its position graphically and then user can fill it assisted by the haptic interface force feedback.\nTitle:", "model_inf_time": 1.3}, {"id": "41160", "output": "Business Models for Anonymous Web Surfing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present a novel probabilistic model for user interaction in image retrieval applications which accounts for consistency among the retrieved images and considers the distribution of images in the database which is searched for. Common models for relevance feedback do not consider this and thus do not incorporate all available information. The proposed method is evaluated on two publicly available benchmark databases and clearly outperforms recent competitive methods.\nTitle:\nA Probabilistic Model for User Relevance Feedback on Image Retrieval\n\nAbstract:\nWe present a method to classify images into different categories of pornographic content to create a system for filtering pornographic images from network traffic. Although different systems for this application were presented in the past, most of these systems are based on simple skin colour features and have rather poor performance. Recent advances in the image recognition field in particular for the classification of objects have shown that bag-of-visual-words-approaches are a good method for many image classification problems. The system we present here, is based on this approach, uses a task-specific visual vocabulary and is trained and evaluated on an image database of 8500 images front different categories. It is shown that it clearly outperforms earlier systems on this dataset and further evaluation on two novel web-traffic collections shows the good performance of the proposed system.\nTitle:\nBag-Of-Visual-Words Models For Adult Image Classification And Filtering\n\nAbstract:\nWe present and discuss our participation in the four tasks of the ImageCLEF 2006 Evaluation. In particular, we present a novel approach to learn feature weights in our content-based image retrieval system FIRE. Given a set of training images with known relevance among each other, the retrieval task is reformulated as a classification task and then the weights to combine a set of features are trained discriminatively using the maximum entropy framework. Experimental results for the medical retrieval task show large improvements over heuristically chosen weights. Furthermore the maximum entropy approach is used for the automatic image annotation tasks in combination with a part-based object model. Using our object classification methods, we obtained the best results in the medical and in the object annotation task.\nTitle:\nImage Retrieval and Annotation Using Maximum Entropy.\n\nAbstract:\nIn this paper we present a novel transliteration technique which is based on deep belief networks. Common approaches use finite state machines or other methods similar to conventional machine translation. Instead of using conventional NLP techniques, the approach presented here builds on deep belief networks, a technique which was shown to work well for other machine learning problems. We show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an Arabic-English transliteration task.\nTitle:\nA deep learning approach to machine transliteration\n\nAbstract:\nA major problem in the field of content-based image retrieval is the lack of a common performance measure which allows the researcher to compare different image retrieval systems in a quantitative and objective manner. We analyze different proposed performance evaluation measures, select an appropriate one, and give quantitative results for four different, freely available image retrieval tasks using combinations of features. This work gives a concrete starting point for the comparison of content-based image retrieval systems. An appropriate performance measure and a set of databases are proposed and results for different retrieval methods are given.\nTitle:\nClassification error rate for quantitative evaluation of content-based image retrieval systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.411, mean value for all the writers is 8.648, which makes it 0.202 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe technology for anonymous communication has been thoroughly researched. But despite the existence of several protection services, a business model for anonymous web surfing has not emerged as of today. One possibility to stimulate adoption is to facilitate it in a specific subnet. The idea is to identify a promising target group which has a substantial benefit from adopting the technology and to facilitate the adoption within that target group. We examine the feasibility of this approach for anonymity services. We identify a potential target group --- consumers of pornographic online material --- and empirically validate their suitability by conducting a traffic analysis. We also discuss several business models for anonymity services. We argue that providers of anonymity services should try to generate revenue from content providers like adult entertainment distributors. The latter could benefit from offering anonymous access to their products by differentiating against competitors or by selling their products at a higher price over the anonymous channel.\nTitle:", "model_inf_time": 1.06}, {"id": "41161", "output": "Fluid Annotation: Efficient Human-Machine Collaboration for Image Annotation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDeep neural networks deliver state-of-the-art visual recognition, but they rely on large datasets, which are time-consuming to annotate. These datasets are typically annotated in two stages: (1) determining the presence of object classes at the image level and (2) marking the spatial extent for all objects of these classes. In this work we use speech, together with mouse inputs, to speed up this process. We first improve stage one, by letting annotators indicate object class presence via speech. We then combine the two stages: annotators draw an object bounding box via the mouse and simultaneously provide its class label via speech. Using speech has distinct advantages over relying on mouse inputs alone. First, it is fast and allows for direct access to the class name, by simply saying it. Second, annotators can simultaneously speak and mark an object location. Finally, speech-based interfaces can be kept extremely simple, hence using them requires less mouse movement compared to existing approaches. Through extensive experiments on the COCO and ILSVRC datasets we show that our approach yields high-quality annotations at significant speed gains. Stage one takes $$2.3{\\times }-14.9{\\times }$$ less annotation time than existing methods based on a hierarchical organization of the classes to be annotated. Moreover, when combining the two stages, we find that object class labels come for free: annotating them at the same time as bounding boxes has zero additional cost. On COCO, this makes the overall process $$1.9\\times $$ faster than the two-stage approach.\nTitle:\nEfficient Object Annotation via Speaking and Pointing.\n\nAbstract:\nImageNet is a large-scale database of object classes with millions of images. Unfortunately only a small fraction of them is manually annotated with bounding-boxes. This prevents useful developments, such as learning reliable object detectors for thousands of classes. In this paper we propose to automatically populate ImageNet with many more bounding-boxes, by leveraging existing manual annotations. The key idea is to localize objects of a target class for which annotations are not available, by transferring knowledge from related source classes with available annotations. We distinguish two kinds of source classes: ancestors and siblings. Each source provides knowledge about the plausible location, appearance and context of the target objects, which induces a probability distribution over windows in images of the target class. We learn to combine these distributions so as to maximize the location accuracy of the most probable window. Finally, we employ the combined distribution in a procedure to jointly localize objects in all images of the target class. Through experiments on 0.5 million images from 219 classes we show that our technique (i) annotates a wide range of classes with bounding-boxes; (ii) effectively exploits the hierarchical structure of ImageNet, since all sources and types of knowledge we propose contribute to the results; (iii) scales efficiently.\nTitle:\nLarge-scale knowledge transfer for object localization in ImageNet\n\nAbstract:\nTraining object class detectors typically requires a large set of images with objects annotated by bounding boxes. However, manually drawing bounding boxes is very time consuming. In this paper we greatly reduce annotation time by proposing center-click annotations: we ask annotators to click on the center of an imaginary bounding box which tightly encloses the object instance. We then incorporate these clicks into existing Multiple Instance Learning techniques for weakly supervised object localization, to jointly localize object bounding boxes over all training images. Extensive experiments on PASCAL VOC 2007 and MS COCO show that: (1) our scheme delivers high-quality detectors, performing substantially better than those produced by weakly supervised techniques, with a modest extra annotation effort, (2) these detectors in fact perform in a range close to those trained from manually drawn bounding boxes, (3) as the center-click task is very fast, our scheme reduces total annotation time by 9x to 18x.\nTitle:\nTraining Object Class Detectors with Click Supervision\n\nAbstract:\nWe introduce a weakly supervised approach for learning human actions modeled as interactions between humans and objects. Our approach is human-centric: We first localize a human in the image and then determine the object relevant for the action and its spatial relation with the human. The model is learned automatically from a set of still images annotated only with the action label. Our approach relies on a human detector to initialize the model learning. For robustness to various degrees of visibility, we build a detector that learns to combine a set of existing part detectors. Starting from humans detected in a set of images depicting the action, our approach determines the action object and its spatial relation to the human. Its final output is a probabilistic model of the human-object interaction, i.e., the spatial relation between the human and the object. We present an extensive experimental evaluation on the sports action data set from [1], the PASCAL Action 2010 data set [2], and a new human-object interaction data set.\nTitle:\nWeakly supervised learning of interactions between humans and objects.\n\nAbstract:\nFor the task of assigning labels to an image to summarize its contents, many early attempts use segment-level information and try to determine which parts of the images correspond to which labels. Best performing methods use global image similarity and nearest neighbor techniques to transfer labels from training images to test images. However, global methods cannot localize the labels in the images, unlike segment-level methods. Also, they cannot take advantage of training images that are only locally similar to a test image. We propose several ways to combine recent image-level and segment-level techniques to predict both image and segment labels jointly. We cast our experimental study in an unified framework for both image-level and segment-level annotation tasks. On three challenging datasets, our joint prediction of image and segment labels outperforms either prediction alone on both tasks. This confirms that the two levels offer complementary information.\nTitle:\nCombining image-level and segment-level models for automatic annotation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.053, mean value for all the writers is 8.648, which makes it 0.508 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe introduce Fluid Annotation, an intuitive human-machine collaboration interface for annotating the class label and outline of every object and background region in an image. Fluid annotation is based on three principles:(I) Strong Machine-Learning aid. We start from the output of a strong neural network model, which the annotator can edit by correcting the labels of existing regions, adding new regions to cover missing objects, and removing incorrect regions.The edit operations are also assisted by the model.(II) Full image annotation in a single pass. As opposed to performing a series of small annotation tasks in isolation [51,68], we propose a unified interface for full image annotation in a single pass.(III) Empower the annotator.We empower the annotator to choose what to annotate and in which order. This enables concentrating on what the ma-chine does not already know, i.e. putting human effort only on the errors it made. This helps using the annotation budget effectively.\n\nThrough extensive experiments on the COCO+Stuff dataset [11,51], we demonstrate that Fluid Annotation leads to accurate an-notations very efficiently, taking 3x less annotation time than the popular LabelMe interface [70].\nTitle:", "model_inf_time": 1.59}, {"id": "41162", "output": "QoS-based Software Configuration Management (QSCM) for High Availability", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nServer consolidation in Cloud Computing makes it possible for multiple servers or desktops to run on one physical server to get high resource utilization, low cost and less energy consumption. However, the scheduler in virtual machine monitor (VMM) is agnostic about the communication behavior between the guest operating systems. It leads to inefficient network communication in consolidated environment. In particular, the CPU resource management has a critical impact on the network latency between co-resident virtual machines (VMs) when there are CPU-bound and I/O-bound workloads existing simultaneously. It brings a negative impact on latency-sensitive VMs. In this paper, we present the design and implementation of CIVSched scheduling to make the VMM aware of the communication behavior between two inter-VMs running on the same virtual platform. CIVSched inspects the network packets transmitted between local domains and find the destination VM and the target process inside that will receive the packets. Then, the destination VM and the target process are preferentially scheduled by VMM scheduler and guest OS scheduler respectively. The cooperation of these two schedulers makes the network packets received by the target application timely. Experimental results show that the CIVSched scheduling can reduce the average response time of network traffic by up to 18% for the highly consolidated environment while keeping the fairness of the VMM scheduler.\nTitle:\nCIVSched: Communication-aware Inter-VM Scheduling in Virtual Machine Monitor Based on the Process\n\nAbstract:\nCurrent online banking scheme built on ordinary software stack, which comprises of the operating system and its applications running on it, is facing attacks including Phishing, Pharming, Malicious Software Attacks (MSW), Man in the Middle Attacks (MITM) and Key logger. Today's countermeasures either prevent only part of these attacks or have high cost on performance and usability. In this paper, we introduce the Domain Online Banking (DOBank), a novel security scheme for online banking that combines the virtual machine (VM) technology with web services. Firstly, DOBank encapsulates the banking service into a lightweight domain and protects it from any attacks caused by virus from the user's host. Secondly, the domain can access certain hardware devices exclusively against Key logger and gains nearly native performance using the pass through technology. Finally, we use the virtual Trusted Platform Module (vTPM) for the online banking domain's integrity verification as well as the SSL/TLS (Security Sockets Layer/Transport Layer Security) protocol for the confidentiality of data transaction over the internet. We show that this scheme is secure enough to prevent typical viruses that threaten the online banking. The experiments on the network throughput and the time consumed of integrity measurement show it adds little overhead to the overall system.\nTitle:\nA Novel Security Scheme for Online Banking Based on Virtual Machine\n\nAbstract:\nHardware/software (HW/SW) partitioning and scheduling are essential to embedded systems. In this paper, a hybrid algorithm derived from Tabu Search (TS) and Simulated Annealing (SA) is proposed for solving the HW/SW partitioning problem. The annealing procedure of SA is employed to accelerate the updating of Tabu tables for task scheduling, and the virtual hardware resource is also set to implement the customized TS. The Earliest-Deadline-First (EDF) strategy is introduced to describe the reconfiguration of FPGA. Moreover, an algorithm combining the Breadth-First-Search (BFS) with Depth-First-Search (DFS) is proposed for HW/SW task scheduling to fit the features of reconfigurable systems. Experimental results show that the improvement over the latest efficient combinatorial algorithm is up to 50%. The proposed virtual hardware expanding technique makes the performance increase up to 97.51% on random graphs. The improvement on task scheduling is by 50% in comparison to popular algorithms cited in this paper.\nTitle:\nHybrid algorithms for hardware/software partitioning and scheduling on reconfigurable devices.\n\nAbstract:\nHigh-level process management is quantitative management. The Process Performance Baseline (PPB) of process or subprocess under statistical management is the most important concept. It is the basis of process control and improvement. The existing methods for establishing process baseline are too coarse-grained or have some limitation, which lead to inaccurate or ineffective quantitative management. In this paper, we propose an approach called BSR (Baseline-Statistic-Refinement) for establishing and refining software process performance baseline, and present the experience result to validate its effectiveness for quantitative process management.\nTitle:\nBSR: a statistic-based approach for establishing and refining software process performance baseline\n\nAbstract:\nPurpose - The purpose of this paper is to find a practical active sliding mode control approach for synchronization of two uncertain chaotic systems. Design/methodology/approach - Sliding mode control approach is known to be an efficient alternative way to implement synchronization for uncertain chaotic systems. However, design of traditional sliding mode controller usually needs complex state transformation. Owing to a novel idea of virtual state feedback, a control strategy for synchronization of uncertain chaotic systems is presented, which does not need any complex state transformation. Furthermore, based on Lyapunov stability theory, a sufficient condition is drawn for the robust stability of the error dynamics of synchronization for uncertain chaotic systems. Findings - A novel active sliding mode control approach is proposed to achieve the synchronization of two uncertain chaotic systems. Research limitations/implications - The main limitation is that uncertainties must meet matched conditions. Practical implications The paper presents a useful control approach for synchronization of two uncertain chaotic systems. Originality/value - The proposed sliding mode control approach based on novel virtual state feedback does not need any complex state transformation, unlike the traditional sliding mode control.\nTitle:\nA novel active sliding mode control for synchronization of uncertain chaotic systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.823, mean value for all the writers is 8.648, which makes it 1.856 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe conventional Software Configuration Management (SCM) tools have shown their weakness in large and complex software systems with the development of modern software. This paper provides a better SCM tool with high availability to facilitate software development process. A novel approach named Quality of Service (QoS)-based SCM (QSCM) is presented, which introduces QoS guarantee techniques at the application level to web-based SCM system in order to improve SCM system's performance. QSCM classifies users together with SCM activities and treats with requests according to their priority. When the server is overloaded, it stops treatment with new requests so that to avoid from crash. The work described here is a first step towards this goal.\nTitle:", "model_inf_time": 1.63}, {"id": "41163", "output": "Gamer-Created Profiles: Capturing and Storing Contextual Gaming Information", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper reviews the current status of Learning Analytics with special focus on their application in Serious Games. After presenting the advantages of incorporating Learning Analytics into game-based learning applications, different aspects regarding the integration process including modeling, tracing, aggregation, visualisation, analysis and employment of gameplay data are discussed. Associated challenges in this field as well as examples of best practices are also examined.\nTitle:\nLearning Analytics and Serious Games: Trends and Considerations\n\nAbstract:\nA wide variety of virtual worlds exists today. They embed the user into a virtual reality allowing interaction with the virtual environment and, in case of multiplayer worlds, other users. Our goal is to support these interactions. Therefore we are using virtual parameters to describe the user's virtual context and provide additional services depending on this context. These services can either be included into the virtual world or can be realized using a community portal. In this paper we will introduce the concept of virtual context based services and its fundamentals like the classification of interaction in virtual worlds, a generic interaction model and an extended model including additional services.\nTitle:\nVirtual context based services for support of interaction in virtual worlds\n\nAbstract:\nIn the future of Massively Multiuser Online Gaming new improvements and innovations are needed. With demands of gamers for support and involvement, increasing new approaches are required in order to enhance community participation and increase the gaming experience. Using the concept of Virtual Context Based Services we have developed a novel approach to solve these challenges. Our concept enables information exchange game and genre independent by using the VCBS middleware. With the eXtensible game description language (xgdl) we propose a standardized way to describe the virtual context of a gamer. VCBS is an interface between games and other internet applications, which involves game communities and supports gamers.\nTitle:\nVirtual context based services for multiplayer online games to facilitate community participation\n\nAbstract:\n\n Multiplayer Online Games (MOGs) are a thriving market leading to a multiplicity of game related internet applications. Enabling\n information exchange between games and these applications is essential, but still an unsolved challenge. Our Virtual Context\n Based Service (VCBS) middleware enables such an information exchange. The VCBS middleware is an interface between games and\n other internet applications, which also supports community activities. In this paper we describe the architecture of the VCBS\n middleware and its components, and introduce our concept for generic MOG interfaces.\n \n \n \nTitle:\nA Middleware for the Controlled Information Exchange Between Online Games and Internet Applications\n\nAbstract:\nIn this paper, we describe a set of personalized exergames which combine methods and concepts of serious games, adaptation and personalization, authoring and sensor technologies. Compared to existing systems, the set of games does not only keep track of the user's vital state, but also directly integrates vital parameters into the gameplay and supports the training and motivation for sustainable physical activity in a playful manner.\nTitle:\nSerious games for health: personalized exergames\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.459, mean value for all the writers is 8.648, which makes it 0.692 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents methods for capturing and storing gaming related context information into online gamer created profiles. Captured information may not be manipulated while being processed and stored within user created profiles and therefore is reliable. We propose a concept of client side capturing of profile information within the working environment of a gamer playing online games and a profile server storing generic community-extendable gaming profile data models that can be individualized. This addresses the creative power of gaming communities and faces the challenge to define machine processable data structures that can be updated with context information. A gamer created profile, enriched with detailed usage and reliable context information, improves the expressiveness and keeps it more up-to-date.\nTitle:", "model_inf_time": 1.35}, {"id": "41164", "output": "Optimizing Design Pattern Detection Through a Filtering Phase", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAs part of the reengineering process, the identification of design patterns offers important information to the designer. In fact, the identification of implemented design patterns could be useful for the comprehension of an existing design and provides the grounds for further code/design improvements. However, existing pattern detection approaches generally have problems in detecting patterns in an optimal manner. They either detect exact pattern instantiations or have no guidelines in deciding which pattern to look for first amongst the various patterns. To overcome these two limitations, we propose to optimize any pattern detection approach by preceding it by a preliminary \"sniffing\" step that detects the potential existence of patterns and orders the candidate patterns in terms of their degree of resemblance to design fragments. Our approach uses design metrics to characterize the structure and semantics of the various design patterns.\nTitle:\nPredicting The Existence Of Design Patterns Based On Semantics And Metrics\n\nAbstract:\nDesign patterns are good design solutions to recurring problems. Many works were interested in design patterns identification either for reverse engineering purposes, or for design improvement purposes. All existing approaches considered that a pattern is detected through both its structure and behavior, but no one considers the semantic aspect conveyed by the class and method names. In this paper, we propose a technique that exploits the semantic aspect to identify occurrences of a pattern in a design. In addition to the structural and behavioral analyses, the semantic analysis is very useful, specifically when there is a doubt between two design patterns having similar structures. By resolving a non deterministic identification, the identification precision increases.\nTitle:\nA Design Pattern Detection Approach Based on Semantics.\n\nAbstract:\nDesign patterns capitalize the knowledge of expert designers and offer reuse that provides for higher design quality and overall faster development. To attain these advantages, a designer must, however, overcome the difficulties in understanding design patterns and determining those appropriate for his/her particular application. On the other hand, one way to benefit from design patterns is to assist inexperienced designers in pattern detection during the design elaboration. Such detection should tolerate variations between the design and the pattern since the exact instantiation of a pattern is infrequent in a design. However, not all variations of a pattern are tolerated. In particular, some structural variations may result in non-optimal instantiations where the requirements are respected but the structure is different; such variations are called spoiled patterns and should also be detected and transformed into acceptable pattern instantiations.This paper first presents an improvement of our design/spoiled pattern detection approach, named MAPeD (Multi-phase Approach for Pattern Discovery). The latter uses an XML information retrieval technique to identify design/spoiled pattern occurrences in a design using, first, static and semantic information and, secondly, dynamic information. This multi-phase detection approach tolerates structural differences between the examined design and the identified design pattern. Furthermore, thanks to the matching information it collects, our identification technique can offer assistance for the improvement of a design. In its second contribution, this paper evaluates MAPeD by comparing its recall and precision rates for five open source systems: JHotDraw, JUnit, JRefactory, MapperXML, QuickUML. The latter were used by other approaches in experimental evaluations. Our evaluation shows that our design pattern identification approach has an average improvement of 9.98% in terms of precision over the best known approach.\nTitle:\nEvaluation Of An Automated Multi-Phase Approach For Patterns Discovery\n\nAbstract:\nDespite their advantages in design quality improvement and rapid software development, design patterns remain difficult to reuse for inexperienced designers. The main difficulty consists in how to recognize the applicability of an appropriate design pattern for a particular application. Design problems can be found in a design with different shapes, unfortunately, often through poor solutions. To deal with this situation, we propose an approach that recognizes pattern problems in a design and that assists in transforming them into their corresponding design patterns. Our approach adapts an XML document retrieval technique to detect the situation necessitating a pattern usage. Unlike current approaches, ours accounts for both the structural and semantic aspects of a pattern problem. In addition, it tolerates design alterations of pattern problems.\nTitle:\nA new approach for pattern problem detection\n\nAbstract:\nRecommendation systems provide suggestions for items that are potentially interesting for a user in a given context. The provided recommendations are extracted generally from a huge amount of data collected from several sources of information. Thus a recommendation system requires firstly a pre-treatment step to prepare the data and secondly the application of some techniques such as data mining techniques to handle and extract the knowledge to be recommended to the user from the data. Our contribution consists on proposing a Recommendation System for Software Engineering (RSSE). This system recommends UML classes in the design phase of UML classes diagrams. Our RSSE is composed by two main phases: an off-line phase in which we use a clustering algorithm to partition UML classes collected from several UML classes diagrams based on the semantic relations existing between their characteristics. We have defined a metric that measures the similarity between UML classes. The second is an online phase in which we use the obtained clusters of UML classes to propose suggestions to the user based on elements added to his UML classes diagram under construction. The proposed system is then experimentally evaluated by using a UML classes corpus collected from several UML classes diagrams. The experimental evaluation shows very encouraging ratio of useful recommendations.\nTitle:\nAn UML class recommender system for software design\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.463, mean value for all the writers is 8.648, which makes it 0.695 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDesign patterns represent high-level recurring abstractions that reflect the know-how of experts. Their detection is a key task in the context of software engineering; it is very useful in program comprehension, design recovery and also for re-documenting object-oriented systems. Despite their usefulness, current design pattern detection approaches have a high time complexity that hinders their application. This complexity is further aggravated with the absence of guiding principles in determining which pattern to look for first among the various patterns. To overcome this practical limit, we propose to optimize any pattern detection approach by foregoing it by a filtering phase that detects possible existence of patterns and that orders the candidate patterns in terms of their degree of resemblance to the analyzed design fragments. The herein proposed filtering approach exploits semantic and structural design metrics to look for the semantic and structural symptoms of design pattern instances. Its performance is experimentally demonstrated through our design pattern detection method MAPeD (Multi-phase Approach for Pattern Discovery) applied on the open source system JUnit.\nTitle:", "model_inf_time": 1.39}, {"id": "41165", "output": "Usability in Medicine and Healthcare: A Challenge for HCI", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nRecently HCI researchers have taken an interest in the concept of cool and how it can be harnessed in the design process. This is due to cool's potential positive impact on adoption and the wider user experience. In this paper we explore the concept of cool and the challenges faced in the study of cool in design. We highlight the lack of rigorous research and causal understanding of cool, the social nature of the concept, and the likely need of heavy marketing strategy to achieve such status as challenges to the setting of cool as a design goal. We propose that more needs to be understood about the behaviour of cool before it can be considered an appropriate design aim.\nTitle:\nShould cool be a design goal?\n\nAbstract:\nIn this paper we look at four different interpretations of the term 'Ubiquitous Computing' - many computers; people using them much of the time; embedded computers, and 'invisible' systems - and consider how the two more specialist interpretations are being undermined by the other two. We explain how the increased manifestation of computers in our environment alters the ways in which we should consider how to design ubiquitous systems. There are some specific implications for design of interfaces to artefacts containing embedded computers and these are discussed in the context of recent work on Projected Cognition.\nTitle:\nDisembedding computers: interfacing ubiquitous computers\n\nAbstract:\nIn this paper, we introduce the notion of Projected Cognition as an extension to Distributed Cognition. Distributed Cognition is a conceptual framework which can be useful in studying human interactions with artefacts; the idea is that of cognition not bounded by the cranium but instead perfusing artefacts in ways that are recoverable. We argue that this analysis has not been fully understood in relation to the behaviour of humans with artefacts in that the intentionality in behaviour has been ignored. We argue that we need to view the human as sometimes projecting their intention in behaviour onto the artefacts they use, and suggest that this conception permits greater clarity in the study of user behaviour with artefacts such as computers. We illustrate the development with case studies of two users of complex configurations of computers as well as examples drawn from the published literature. We conclude with consideration of some design implications and discussion of related domains in HCI where Projected Cognition could be influential.\nTitle:\nProjected Cognition - extending Distributed Cognition for the study of human interaction with computers\n\nAbstract:\nDigital behaviour change interventions, particularly those using pervasive computing technology, hold great promise in supporting users to change their behaviour. However, most interventions fail to take habitual behaviour into account, limiting their potential impact. This failure is partly driven by a plethora of overlapping behaviour change theories and related strategies that do not consider the role of habits. We critically review the main theories and models used in the research to analyse their application to designing effective habitual behaviour change interventions. We highlight the potential for Dual Process Theory, modern habit theory, and Goal Setting Theory, which together model how users form and break habits, to drive effective digital interventions. We synthesise these theories into an explanatory framework, the Habit Alteration Model, and use it to outline the state of the art. We identify the opportunities and challenges of habit-focused interventions.\n\n\nTitle:\nDigital Behaviour Change Interventions to Break and Form Habits.\n\nAbstract:\nThis study presents interview based case studies of users who work with multiple computers as well as multiple displays. Such users have not been studied before. The behaviour is discussed in terms of both technical and cognitive dimensions, and we identify the importance of having multiple carets and the complexity of multi-tasking and how it can be supported across multiple machines in a way not possible on a single system.\nTitle:\nMultiple carets, multiple screens and multi-tasking: new behaviours with multiple computers\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.959, mean value for all the writers is 8.648, which makes it 0.588 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nEnsuring good usability can be seen as the key success factor in our whole digital world: technology must support people. In particular, Medicine and Healthcare are currently subject to exceedingly rapid technological change. Vital areas for the economy include health of nations; medicine and healthcare entangles everybody, accordingly the role of usability is of increasing importance. Consequently, Medicine and Healthcare are a great challenge for Human-Computer Interaction (HCI) research; however, it is of vital importance that the findings are integrated into engineering at a systemic level. Information Processing, in particular its potential effectiveness in modern Health Services and the optimization of processes and operational sequences, is of increasing interest, but we need to ensure that we engineer effective solutions as well as understanding the stakeholders and the issues they can and do encounter. It is particularly important for Medical Information Systems (e.g. Hospital Information Systems and Decision Support Systems) to be designed from the perspective of the end users, especially given that this is a diverse set of people.\nTitle:", "model_inf_time": 1.31}, {"id": "41166", "output": "Locally Optimal Source Routing for Energy-Efficient Geographic Routing in Wireless Sensor Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nGeographic routing is one of the most widely-accepted techniques to route information in large-scale wireless sensor networks. It is based on a greedy forwarding strategy by which a sensor node selects as next hop relay the most promising neighbor (according to some metric) among those being closer to the destination than itself. This decision is based solely on the position of its neighbors and the destination. Given that sensor nodes are usually operated by batteries, energy-efficiency is a very important metric to be considered by the routing protocol. In this paper we present Locally-Optimal Source Routing (LOSR), a new localized and energy-efficient geographic routing algorithm for wireless sensor networks. Unlike existing energy-efficient geographic routing algorithms, in which current node routing the packet only considers nodes closer to destination than itself, LOSR uses all nodes in the neighborhood to compute a local energy-optimal path formed only by neighbors of the current node towards the selected next hop. Then, source routing is used to force data packets to follow that locally optimal path until next hop is reached. Our simulation results show that the proposed algorithm outperforms the best existing solution, over a variety of network densities and scenarios.\nTitle:\nExploiting Local Knowledge to Enhance Energy-Efficient Geographic Routing\n\nAbstract:\nGeographic Routing (GR) algorithms, require nodes to periodically transmit HELLO messages to allow neighbors know their positions (beaconing mechanism). Beacon-less routing algorithms have recently been proposed to reduce the control overhead due to these messages. However, existing beacon-less algorithms have not considered realistic physical layers. Therefore, those algorithms cannot work properly in realistic scenarios. In this paper we present a new beacon-less routing protocol called BOSS. Its design is based on the conclusions of our open-field experiments using Tmote-sky sensors. BOSS is adapted to error-prone networks and incorporates a new mechanism to reduce collisions and duplicate messages produced during the selection of the next forwarder node. We compare BOSS with Beacon-Less Routing (BLR) and Contention-Based Forwarding (CBF) algorithms through extensive simulations. The results show that our scheme is able to achieve almost perfect packet delivery ratio (like BLR) while having a low bandwidth consumption (even lower than CBF). Additionally, we carried out an empirical evaluation in a real testbed that shows the correctness of our simulation results.\nTitle:\nBeacon-Less Geographic Routing in Real Wireless Sensor Networks.\n\nAbstract:\nGeographic routing protocols are one of the most common routing schemes for sensor networks. These protocols consist of two different modes of operation: greedy routing to forward data to the destination using neighbors which are closer to the destination than current node and face routing to avoid voids in the network. Face routing requires the graph to be planar, which usually means that some crossing links of the original network cannot be considered when routing in face mode. In this paper we introduce a new localized scheme to build a virtual spanner which is planar by construction and is guaranteed to be connected if the underlying network is connected as well. Unlike previous works, by performing face routing over this spanner we can reduce energy consumption in face mode because the elimination of any of the original links in the network is not required. Thus, the most energy-efficient paths can be selected when the protocol enters face mode. The virtual spanner is easy-to-build and uses only local information, making it scalable to large-scale networks. Routing is always performed in real nodes; virtual nodes are used only as routing anchors when the agent is in face mode. In addition, our simulation results show that the proposed scheme outperforms the best energy-efficient geographic routing protocol for different network densities and energy models.\nTitle:\nEnergy-Efficient face routing on the virtual spanner\n\nAbstract:\nMost usage scenarios for ad hoc and wireless sensor networks (WSN) require some degree of one-to-many or many- to-many interactions. In particular, for the case of WSN there is a number of scenarios in which a node has to send the same data to multiple destinations. Given that sensor networks have very limited resources, multicasting is a very interesting approach to deliver the same data packet to multiple destinations while reducing the amount of bandwidth and power consumption. Furthermore, recent studies have shown that it is of paramount importance to take into account the error prune nature of the wireless links when designing energy-efficient routing protocols. In this paper, we extend our previously proposed protocol LEMA (Localized Energy-Efficient Multicast Algorithm), to deal with the problems of the error prone WSN. Our simulation results show that for networks with enough density the protocol is able to outperform even well-known centralized heuristics such as Minimum Incremental Power (MIP) and Shortest Path Tree (SPT) based on energy.\nTitle:\nEnergy-efficient geographic multicast routing for error-prone wireless sensor networks\n\nAbstract:\nWe study the problem of geographic multicast routing in a wireless sensor network. In particular, we are interested in geographic routing solutions with a very limited control overhead and overall bandwidth consumption. Existing geographic multicast routing protocols require nodes to periodically exchange beacon messages to gather information about the position of their neighbors. These beacons represent a waste of resources, specially in areas of the network with no active communications. Beacons also induce significant problems in real deployments such as interferences and collisions that cause inconsistencies in neighboring tables. In this paper we propose a new beacon-less geographic multicast routing protocol called BRUMA. Unlike previous solutions, BRUMA uses the propagation of data packets to opportunistically select next hops among those that are reachable from the sending node. This allows the protocol to overcome most of the issues of beacon. based solutions in real deployments. Our simulations show that BRUMA achieves a higher packet delivery ratio and a lower overall bandwidth consumption than GMR, which is the protocol performing best among existing geographic multicast solutions.\nTitle:\nBruma: Beacon-Less Geographic Routing For Multicast Applications\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.657, mean value for all the writers is 8.648, which makes it 1.714 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe analyze the problem of finding an energy-efficient path from a source node to a destination using geographic routing. Existing schemes have neglected the fact that neighbors which are not closer to the destination than the current node can still reduce energy consumption by taking part in the selected path. Moreover, recent works have confirmed that the generally used Unit Disk Graph to model Wireless Sensor Networks does not represent accurately the behavior of real links. We propose a new scheme called Locally Optimal Source Routing (LOSR) that is able to use neighbors which do not provide advance toward the destination to reduce the overall energy consumption while still avoiding routing loops. Using an Automatic Repeat reQuest (ARQ) mechanism hop by hop we overcome the problems caused by errors in radio transmissions and we introduce a novel routing metric, which accounts for those errors in the energy consumption. Our simulation results show that the proposed scheme outperforms existing solutions over a variety of scenarios and network densities.\nTitle:", "model_inf_time": 1.72}, {"id": "41167", "output": "Delay-Dependent and Delay-Independent Stability Criteria for Cellular Neural Networks with Single and Multiple Delays", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSome sufficient conditions for the asymptotic stability of cellular neural networks with time delay are derived using the Lyapunov-Krasovskii stability theory for functional differential equations as well as the linear matrix inequality (LMI) approach. The analysis shows how some well-known results can be refined and generalized in a straightforward manner. Moreover, the stability criteria obtained are delay-independent. They are less conservative and restrictive than those reported so far in the literature, and provide a more general set of criteria for determining the stability of delayed cellular neural networks.\nTitle:\nStability Analysis For Delayed Cellular Neural Networks Based On Linear Matrix Inequality Approach\n\nAbstract:\nIn this paper, the global asymptotic stability of cellular neural networks with time delay is discussed using some novel Lyapunov functionals. Novel sufficient conditions for this type of stability are derived. They are less restrictive and more practical than those currently used. As a result, the design of cellular neural networks with time delay is refined. Our work can also be generalized to cellular neural networks with time-varying delay, a topic on which little research work has been done. By means of several different Lyapunov functionals, some sufficient conditions related to the global asymptotic stability for cellular neural networks with perturbations of time-varying delays are derived.\nTitle:\nNovel Stability Conditions For Cellular Neural Networks With Time Delay\n\nAbstract:\nWe present new sufficient conditions for the asymptotic stability of a two-neuron network with different time delays. These conditions lead to delay-dependent and delay-independent asymptotic stabilities, respectively. They are shown to be less conservative and restrictive than those reported in the literature. Some examples are included to illustrate our results\nTitle:\nNew Asymptotic Stability Criteria for a Two-Neuron Network With Different Time Delays\n\nAbstract:\nIn this paper, the asymptotic stability of a two-neuron system with different time delays has been investigated. Some criteria for determining the global asymptotically stability of equilibrium are derived from the theory of monotonic dynamical system and the approach of Lyapunov functional. For local asymptotic stability, some elegant criteria are also obtained by the Nyquist criteria. We find that one of them depends on the length of delays while the other ones do not. In the latter case, the delays are sometimes called harmless delays. The results obtained have leading significance in the study of neural networks composed of a large number of neurons with different time delays.\nTitle:\nAsymptotic stability criteria for a two-neuron network with different time delays.\n\nAbstract:\nIn this paper, the conventional bidirectional associative memory (BAM) neural network with signal transmission delay is intervalized in order to study the bounded effect of deviations in network parameters and external perturbations. The resultant model is referred to as a novel interval dynamic BAM (IDBAM) model. By combining a number of different Lyapunov functionals with the Razumikhin technique, some sufficient conditions for the existence of unique equilibrium and robust stability are derived. These results are fairly general and can be verified easily. To go further, we extend our investigation to the time-varying delay case. Some robust stability criteria for BAM with perturbations of time-varying delays are derived. Besides, our approach for the analysis allows us to consider several different types of activation functions, including piecewise linear sigmoids with bounded activations as well as the usual C1-smooth sigmoids. We believe that the results obtained have leading significance in the design and application of BAM neural networks.\nTitle:\nRobust stability of interval bidirectional associative memory neural network with time delays.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.967, mean value for all the writers is 8.648, which makes it 1.125 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe stability issues of the equilibrium points of the cellular neural networks (CNN) with single and multiple delays are further investigated. Several novel delay-dependent and delay-independent asymptotical/exponential stability criteria are established by employing parameterized first-order model transformation, Lyapunov-Krasovskii stability theorem and LMI technique in virtue of the linearization of considered model. The stability regions with respect to the delay parameters are formulated by applying the proposed results. To the best of the authors' knowledge, few (if any) reports about such \"linearization\" approach to stability analysis for delayed neural network models have been presented in the open literatures. Some numerical examples are also given to illustrate the effectiveness of our results and to compare with the recent results.\nTitle:", "model_inf_time": 1.65}, {"id": "41168", "output": "HCI Challenges of Lifelogging for Older Users", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nToday we are witnessing diverse forms and styles of interactive platforms and devices quickly penetrating to people's everyday lives. New applications and services for smartphones, tablets, game consoles connected to TVs, and other embedded appliances are constantly appearing and diversifying the way we interact with technology. Thus when we design visualization and interaction strategies for the emerging lifelogging activity, it is important to consider affordances and contexts for these emerging interactive devices: by the time the lifelogging activity becomes truly ubiquitous, we will be interacting with even more diverse set of devices to support the activity. In this paper, we describe an early stage of our on-going project where we sketched a series of interactive visualization and their corresponding usage scenarios for three different interactive platforms: (1) smartphone, (2) tablet, and (3) desktop. Our sketch was rendered on these corresponding devices in such a way as to maximize the special interaction characteristics of each device and provides three very different lifelog data usage scenarios.\nTitle:\nVisualizing lifelog data for different interaction platforms\n\nAbstract:\nMuch effort has focused in recent years on developing more life-like robots. In this paper we propose a model of memory for robots, based on human digital memories, though our model incorporates an element of forgetting to ensure that the robotic memory appears more human and therefore can address some of the challenges for human-robot interaction.\nTitle:\niForgot: a model of forgetting in robotic memories\n\nAbstract:\nThe SenseCam is a small wearable personal device which automatically captures up to 2,500 images per day. This yields a very large personal collection of images, or in a sense a large visual diary of a person's day. Intelligent techniques are necessary for effective structuring, searching and browsing of this image collection for locating important or significant events in a person's life. In this paper we identify three stages in the process of capturing and structuring SenseCam images and then displaying them to an end user to review. These stages are expressed in terms of the Canonical process stages to which they correlate.\nTitle:\nConstructing a SenseCam visual diary as a media process\n\nAbstract:\nThe field of Human-Computer Interaction provides a number of useful tools and methods for obtaining information on end-users and their usage context to inform the design of computer systems, yet relatively little is known on how to go about designing for a completely novel application where there is no user base, no existing practice of use available at the start. The success of the currently available HCI methodology that focuses on understanding users' needs and establishing requirements is well-deserved in making computing applications usable in terms of fitting them to end-users' usage contexts. However, too much emphasis on identifying user needs tends to stifle other more exploratory design activities where new types of applications are invented in order to discover or create new activities currently not practiced. In this paper, we argue that a great starting point of novel application design is not the problem space (trying to rigorously define the user requirements) but the solution space (trying to leverage emerging computational technologies and growing design knowledge for various interaction platforms), and we build a foundation for a pragmatic design methodology supported by the authors' extensive experience in designing novel applications inspired by emerging media technologies.\nTitle:\nDesigning novel applications inspired by emerging media technologies\n\nAbstract:\nIn this paper, we present prototype interactive TV software that incorporates visual content analysis tools and social networking in the home TV. We present the challenges of working with the living room TV environment and outline how we have utilized visual processing and search technologies to address these challenges and create a novel prototype interactive TV system.\nTitle:\nSocial recommendation and visual analysis on the TV\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.909, mean value for all the writers is 8.648, which makes it 0.223 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we describe the HCI challenges associated with the novel domain of lifelogging for older users. The SenseCam is a passively capturing wearable camera, worn via a lanyard around the neck and used to create a personal lifelog or visual recording of the wearer's life, which generates information that may be very helpful as a human memory aid. Indeed, given that memory defects are more marked in the elderly, we believe that lifelogging browsing techniques which are considerate of the elderly are imperative. Thus, the challenge tackled in this work was to design and integrate the lifelogging activity supported by new technologies in such a way that can easily be learned and used by older people, enabling them to enhance and enrich their lives with the new technologies. This work provides design practitioners of future lifelogging interfaces early sight of the lessons we have learned in making lifelogging technologies accessible to elderly non-computing literate participants.\nTitle:", "model_inf_time": 1.21}, {"id": "41169", "output": "Temporal Reasoning with Temporal Expressions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.\nTitle:\nJoint inference for event timeline construction\n\nAbstract:\nWe present a system for the semantic role labeling task. The system combines a machine learning technique with an inference procedure based on integer linear programming that supports the incorporation of linguistic and structural constraints into the decision process. The system is tested on the data provided in CoNLL-2004 shared task on semantic role labeling and achieves very competitive results.\nTitle:\nSemantic role labeling via integer linear programming inference\n\nAbstract:\n  This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers gain from the use of {\\em quantity schemas} that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems. \nTitle:\nSolving General Arithmetic Word Problems\n\nAbstract:\n  Identifying temporal relations between events is an essential step towards natural language understanding. However, the temporal relation between two events in a story depends on, and is often dictated by, relations among other events. Consequently, effectively identifying temporal relations between events is a challenging problem even for human annotators. This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods. As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem. \nTitle:\nA Structured Learning Approach to Temporal Relation Extraction.\n\nAbstract:\nThis paper presents a novel sequence labeling model based on the latent-variable semi-Markov conditional random fields for jointly extracting argument roles of events from texts. The model takes in coarse mention and type information and predicts argument roles for a given event template. This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. Our key contribution is a novel learning framework called structured preference modeling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. We establish and discuss connections between this framework and other existing works. We show empirically that the structured preferences are crucial to the success of our task. Our model, trained without annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised approaches.\nTitle:\nAutomatic event extraction with structured preference modeling\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.118, mean value for all the writers is 8.648, which makes it 1.305 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents a demonstration of a temporal reasoning system that addresses three fundamental tasks related to temporal expressions in text: extraction, normalization to time intervals and comparison. Our system makes use of an existing state-of-the-art temporal extraction system, on top of which we add several important novel contributions. In addition, we demonstrate that our system can perform temporal reasoning by comparing normalized temporal expressions with respect to several temporal relations. Experimental study shows that the system achieves excellent performance on all the tasks we address.\nTitle:", "model_inf_time": 0.97}, {"id": "41170", "output": "A visually grounded spoken language generation system", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nRecent psycholinguistic experiments show that acoustic and syntactic aspects of online speech processing are influenced by visual context through cross-modal influences. During inter- pretation of speech, visual context seems to steer speech pro- cessing and vice versa. We present a real-time multimodal sys- tem motivated by these findings that performs early integration of visual contextual information to recognize the most likely word sequences in spoken language utterances. The system first acquires a grammar and a visually grounded lexicon from a \"show-and-tell\" procedure where the training input consists of camera images consisting of sets of objects paired with ver- bal object descriptions. Given a new scene, the system gener- ates a dynamic visually-grounded language model and drives a dynamic model of visual attention to steer speech recognition search paths towards more likely word sequences.\nTitle:\nA visual context-aware multimodal system for spoken language processing\n\nAbstract:\nWe present a visually-grounded language understanding model based on a study of how people verbally describe objects in scenes. The emphasis of the model is on the combination of individual word meanings to produce meanings for complex referring expressions. The model has been implemented, and it is able to understand a broad range of spatial referring expressions. We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework. The implemented system selects the correct referents in response to natural language expressions for a large percentage of test cases. In an analysis of the system's successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account.\nTitle:\nGrounded semantic composition for visual scenes\n\nAbstract:\nSpeaking using unconstrained natural language is an intuitive and flexible way for humans to interact with robots. Understanding this kind of linguistic input is challenging because diverse words and phrases must be mapped into structures that the robot can understand, and elements in those structures must be grounded in an uncertain environment. We present a system that follows natural language directions by extracting a sequence of spatial description clauses from the linguistic input and then infers the most probable path through the environment given only information about the environmental geometry and detected visible objects. We use a probabilistic graphical model that factors into three key components. The first component grounds landmark phrases such as \u00c2\u00bfthe computers\u00c2\u00bf in the perceptual frame of the robot by exploiting co-occurrence statistics from a database of tagged images such as Flickr. Second, a spatial reasoning component judges how well spatial relations such as \u00c2\u00bfpast the computers\u00c2\u00bf describe a path. Finally, verb phrases such as \u00c2\u00bfturn right\u00c2\u00bf are modeled according to the amount of change in orientation in the path. Our system follows 60% of the directions in our corpus to within 15 meters of the true destination, significantly outperforming other approaches.\nTitle:\nToward understanding natural language directions\n\nAbstract:\nFuse is a situated spoken language understanding system that uses visual context to steer the interpretation of speech. Given a visual scene and a spoken description, the system finds the object in the scene that best fits the meaning of the description. To solve this task, Fuse performs speech recognition and visually-grounded language understanding. Rather than treat these two problems separately, knowledge of the visual semantics of language and the specific contents of the visual scene are fused during speech processing. As a result, the system anticipates various ways a person might describe any object in the scene, and uses these predictions to bias the speech recognizer towards likely sequences of words. A dynamic visual attention mechanism is used to focus processing on likely objects within the scene as spoken utterances are processed. Visual attention and language prediction reinforce one another and converge on interpretations of incoming speech signals which are most consistent with visual context. In evaluations, the introduction of visual context into the speech recognition process results in significantly improved speech recognition and understanding accuracy. The underlying principles of this model may be applied to a wide range of speech understanding problems including mobile and assistive technologies in which contextual information can be sensed and semantically interpreted to bias processing.\nTitle:\nTowards situated speech understanding: visual context priming of language models\n\nAbstract:\nSituated models of meaning ground words in the non-linguistic context, or situation, to which they refer. Applying such models to sports video retrieval requires learning appropriate representations for complex events. We propose a method that uses data mining to discover temporal patterns in video, and pair these patterns with associated closed captioning text. This paired corpus is used to train a situated model of meaning that significantly improves video retrieval performance.\nTitle:\nSituated models of meaning for sports video retrieval\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.531, mean value for all the writers is 8.648, which makes it 0.1 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA spoken language generation system has been developed that learns to describe objects in computer-generated visual scenes. The system is trained by a \u2018show-and-tell\u2019 procedure in which visual scenes are paired with natural language descriptions. Learning algorithms acquire probabilistic structures which encode the visual semantics of phrase structure, word classes, and individual words. Using these structures, a planning algorithm integrates syntactic, semantic, and contextual constraints to generate natural and unambiguous descriptions of objects in novel scenes. The system generates syntactically well-formed compound adjective noun phrases, as well as relative spatial clauses. The acquired linguistic structures generalize from training data, enabling the production of novel word sequences which were never observed during training. The output of the generation system is synthesized using word-based concatenative synthesis drawing from the original training speech corpus. In evaluations of semantic comprehension by human judges, the performance of automatically generated spoken descriptions was comparable to human-generated descriptions. This work is motivated by our long-term goal of developing spoken language processing systems which grounds semantics in machine perception and action.\nTitle:", "model_inf_time": 1.25}, {"id": "41171", "output": "Conceptual Modeling of Secure Data Warehouses", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nData Warehouses (DW), Multidimensional (MD) databases, and On-Line Analytical Processing (OLAP) applications provide companies with many years of historical information for the decision-making process. Owing to the relevant information managed by these systems, they should provide strong security and confidentiality measures from the early stages of a DW project in the MD modeling and enforce them. In the last years, there have been some proposals to accomplish the MD modeling at the conceptual level. Nevertheless, none of them considers security measures as an important element in their models, and therefore, they do not allow us to specify confidentiality constraints to be enforced by the applications that will use these MD models. In this paper, we present an Access Control and Audit (ACA) model for the conceptual MD modeling. Then, we extend the Unified Modeling Language (UML) with this ACA model, representing the security information (gathered in the ACA model) in the conceptual MD modeling, thereby allowing us to obtain secure MD models. Moreover, we use the OSCL (Object Security Constraint Language) to specify our ACA model constraints, avoiding in this way an arbitrary use of them. Furthermore, we align our approach with the Model-Driven Architecture, the Model-Driven Security and the Model-Driven Data Warehouse, offering a proposal highly compatible with the more recent technologies.\nTitle:\nModel-driven multidimensional modeling of secure data warehouses\n\nAbstract:\nDue to the sensitive data contained in Data Warehouses (DW), it is essential to specify security measures from the early stages of the DW design and enforce them. Traditional access control models for transactional (relational) databases, based on tables, columns and rows, are not appropriate for DWs. Instead, security and audit rules defined for DWs must be specified based on the multidimensional (MD) modeling used to design data warehouses. Current approaches for the conceptual modeling of DWs do not allow us to specify security and confidentiality constraints in the conceptual modeling phase. In this paper, we propose an Access Control and Audit (ACA) model for DWs by specifying security rules in the conceptual MD modeling. Thus, we define authorization rules for users and objects and we assign sensitive information rules and authorization roles to the main elements of a MD model (e.g., facts or dimensions). Moreover, we also specify certain audit rules allowing us to analyze user behaviors. To be able to include and use our ACA model in the conceptual MD modeling, we extend the Unified Modeling Language (UML) with our ACA model, thereby allowing us to design secure MD models. Finally, to show the benefit of our approach, we apply our approach to a health care case study.\nTitle:\nAccess control and audit model for the multidimensional modeling of data warehouses\n\nAbstract:\nDue to the sensitive data contained in Data Warehouses (DWs), it is essential to specify security measures from the early stages of the DWs design and enforce them. In this paper, we will present a UML profile to represent multidimensional and security aspects of our conceptual modeling. Our approach proposes the use of UML packages in order to group classes together into higher level units creating different levels of abstraction, and therefore, simplifying the final model. Furthermore, we present an extension of the relational model to consider security and audit measures represented in the conceptual modeling. To accomplish this, we based on the Relational Package of the Common Warehouse Metamodel (CWM) and extend it to properly represent all security and audit rules defined in the conceptual modeling of DWs. Finally, we will show an example to illustrate the applicability of our proposal.\nTitle:\nUsing UML packages for designing secure data warehouses\n\nAbstract:\nData Warehouses (DWs) are currently considered to be the cornerstone of Business Intelligence (BI) systems. Security is a key issue in DWs since the business information that they manage is crucial and highly sensitive, and should be carefully protected. However, the increasing amount of data available on the Web signifies that more and more DW systems are considering the Web as the primary data source through which to populate their DWs. XML is therefore widely accepted as being the principal means through which to provide easier data and metadata interchange among heterogeneous data sources from the Web and the DW systems. Although security issues have been considered during the whole development process of traditional DWs, current research lacks approaches with which to consider security when the target platform is based on the Web and XML technologies. The idiosyncrasy of the unstructured and semi-structured data available on the Web definitely requires particular security rules that are specifically tailored to these systems in order to permit their particularities to be captured correctly. In order to tackle this situation, in this paper, we propose a methodological approach based on the Model Driven Architecture (MDA) for the development of Secure XML DWs. We therefore specify a set of transformation rules that are able to automatically generate not only the corresponding XML structure of the DW from secure conceptual DW models, but also the security rules specified within the DW XML structure, thus allowing us to implement both aspects simultaneously. A case study is provided at the end of the paper to show the benefits of our approach.\nTitle:\nModel driven development of secure XML data warehouses: a case study\n\nAbstract:\nData Warehouses (DWs) are widely accepted as the core of current decision support systems. Therefore, it is vital to incorporate security requirements from the early stages of the DWs projects and enforce them in the further design phases. Very few approaches specify security and audit measures in the conceptual modeling of DWs. Furthermore, these security measures are specified in the final implementation on top of commercial systems as there is not a standard relational representation of security measures for DWs (i.e. the well-known star schema does not allow us to specify security and audit measures on its multidimensional representation of data; instead, they must be specified on top of the implemented relational tables). On the other hand, the Common Warehouse Metamodel (CWM) has been accepted as the standard for the exchange and the interoperability of metadata. Nevertheless, it does not allow us to specify security measures for DWs. In this paper, we make use of the own extension mechanisms provided by the CWM to extend the relational package in order to build a star schema that represents the security and audit rules captured during the conceptual modeling phase of DWs. Finally, in order to show the benefits of our extension, we apply it to a case study related to the management of the pharmacy consortium business.\nTitle:\nBuilding a secure star schema in data warehouses by an extension of the relational package from CWM\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.784, mean value for all the writers is 8.648, which makes it 0.969 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nData Warehouses (DWs), Multidimensional (MD) Databases, and On-Line Analytical Processing Applications are used as a very powerful mechanism for discovering crucial business information. Considering the extreme importance of the information managed by these kinds of applications, it is essential to specify security measures from the early stages of the DW design in the MD modeling process, and enforce them. In the past years, some proposals for representing main MD modeling properties at the conceptual level have been stated. Nevertheless, none of these proposals considers security issues as an important element in its model, so they do not allow us to specify confidentiality constraints to be enforced by the applications that will use these MD models. In this paper, we will discuss the specific confidentiality problems regarding DWs as well as present an extension of the Unified Modeling Language for specifying security constraints in the conceptual MD modeling, thereby allowing us to design secure DWs. One key advantage of our approach is that we accomplish the conceptual modeling of secure DWs independently of the target platform where the DW has to be implemented, allowing the implementation of the corresponding DWs on any secure commercial database management system. Finally, we will present a case study to show how a conceptual model designed with our approach can be directly implemented on top of Oracle 10g.\nTitle:", "model_inf_time": 1.43}, {"id": "41172", "output": "A Framework for Information Quality Management", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nInadequate levels of Data Quality (DQ) in Information Systems (IS) suppose a very important problem for organizations. In any case, they look for to assure data quality from earlier stages on information system developments. This paper proposes to incorporate mechanisms into software development methodologies, in order to integrate users DQ requirements aimed at assuring the data quality from the beginning of development. It brings a framework consisting of processes, activities and tasks, well defined, which would be incorporated in existent software development methodology, as METRICA V3; and therefore, to assure software product data quality created according to this methodology. The extension presented, is a guideline, and this can be extended and applied to other development methodologies like Unified Development Process.\nTitle:\nDeveloping Data Quality Aware Applications\n\nAbstract:\nTraditionally, data quality management has mainly focused on both data source and data target. Increasingly, data processing to get a data product need raw data typically distributed among different data sources. However, if data quality is not preserved when transmitted, resulting data product and consequent information will not be of much value. It is necessary to improve exchange methods and means to get a better information process. This paper focus on that issue, proposing a new approach for assuring and transmitting data quality in the interchange. Using XML and related technologies, a document structure that considers data quality as a main topic is defined. The resulting schema is verified using several measures and comparing it to the data source.\nTitle:\nDQXSD: An XML schema for data quality - an XSD for supporting data quality in XML\n\nAbstract:\nSoftware process improvement is an important aspect in achieving capable processes, and so organizations are obviously concerned about it. However, to improve software process it is necessary to assess it in order to check its weaknesses and strengths. The assessment can be performed according to a given assessment process or any other and the processes of the organization can also use one particular process model or any other. The goal of this work is to provide an environment that allows us to carry out assessments that are in accord with various different process assessment models, on several process reference models. We have developed an environment composed of two components; one of these generates the database schema for storing the process reference model and assessment information and the other one assesses the process with reference to this information, generating results in several formats, to make it possible to interpret data. With this environment, assessment of software process is an easy task, whichever assessment process is used, and regardless of the process model used in the organization.\nTitle:\nEvaltool - A Flexible Environment For The Capability Assessment Of Software Processes\n\nAbstract:\nIn the past years, both industrial and research communities in Software Engineering have shown special interest in Software Process Improvement--SPI. This is evidenced by the growing number of publications on the topic. The literature offers numerous quality frameworks for addressing SPI practices, which may be classified into two groups: ones that describe \"what\" should be done (ISO 9001, CMMI) and ones that describe \"how\" it should be done (Six Sigma, Goal Question Metrics-GQM). When organizations decide to adopt improvement initiatives, many models may be implied, each leveraging the best practices provided, in the quest to address the improvement challenges as well as possible. This may at the same time, however, generate confusion and overlapping activities, as well as extra effort and cost. That, in turn, risks generating a series of inefficiencies and redundancies that end up leading to losses rather than to effective process improvement. Consequently, it is important to move toward a harmonization of quality frameworks, aiming to identify intersections and overlapping parts, as well as to create a multi-model improvement solution. Our aim in this work is twofold: first of all, we propose a theoretical harmonization process that supports organizations interested in introducing quality management and software development practices or concerned about improving those they already have. This is done with specific reference to CMMI-DEV and ISO 9001 models in the direction \"ISO to CMMI-DEV\", showing how GQM is used to define operational goals that address ISO 9001 statements, reusable in CMMI appraisals. Secondly, we apply the theoretical comparison process to a real case, i.e., a Small Enterprise certified ISO 9001.\nTitle:\nHarmonization of ISO/IEC 9001: 2000 and CMMI-DEV: from a theoretical comparison to a real case application\n\nAbstract:\nBusiness process improvement is a key aspect for organizational improvement. We focus the business process improvement in the first stage of process lifecycle, design stage, because it is a means to avoid the propagation of errors to later stages, in which their detection and correction may be more difficult. Since business process improvement is centered in business process models, a proposal of certain steps based on measurement activities on conceptual models (measurement, evaluation and redesign) is described. The application of these steps in business process models produces an increase of the quality of them. Quality is defined as the level of understandability and modifiability, subcharacteristics of the usability and maintainability in ISO 9126. The steps for model improvement have been applied to a real hospital business process model. The model was modified by following expert opinions and modeling guidelines, thus leading to the attainment of a higher-quality model. Our findings clearly support the practical utility of measurement activities for business process model improvement.\nTitle:\nImproving Quality of Business Process Models\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.524, mean value for all the writers is 8.648, which makes it 0.747 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn order to enhance their global business performance, organizations must be careful with the quality of their information since it is one of their main assets. Analogies to quality management of classical products demonstrate that Information Quality is also preferably attainable through management by integrating some corresponding Information Quality management activities into the organizational processes. To achieve this goal we have developed an Information Quality Management Framework (IQMF). It is articulated on the concept of Information Management Process (IMP), based on the idea of Software Process. An IMP is a combination of two sub-processes: the first, a production process, aimed to manufacture information from raw data, and the second to adequately manage the required Information Quality level of the first. IQMF consists of two main components: an Information Quality Management Maturity Model (IQM3), and a Methodology for the Assessment and Improvement of Information Quality Management (MAIMIQ), which uses IQM3 as a reference model for the assessment and for the improvement goal of an IMP. Therefore, as a result of an assessment with MAIMIQ, an IMP can be said to have raised one of the maturity levels described in IQM3, and as improvement goal, it would be desirable to achieve a higher maturity level. Since an Information System can be seen as a set of several IMPs sharing several resources, it is possible to improve the Information Quality level of the entire Information System by improving the most critical IMPs. This paper is focused only on describing the foundations and structure of IQM3, which is based on staged CMMI.\nTitle:", "model_inf_time": 1.34}, {"id": "41173", "output": "Improving Contextual Advertising Matching with Selective Wikipedia Matching", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\na prevalent type of Web advertising, contextual advertising refers to the placement of the most relevant commercial ads within the content of a Web page, to provide a better user experience and as a result increase the user's ad-click rate. However, due to the intrinsic problems of homonymy and polysemy, the low intersection of keywords, and a lack of sufficient semantics, traditional keyword matching techniques are not able to effectively handle contextual matching and retrieve relevant ads for the user, resulting in an unsatisfactory performance in ad selection. In this paper, we introduce a new contextual advertising approach to overcome these problems, which uses Wikipedia thesaurus knowledge to enrich the semantic expression of a target page (or an ad). First, we map each page into a keyword vector, upon which two additional feature vectors, the Wikipedia concept and category vector derived from the Wikipedia thesaurus structure, are then constructed. Second, to determine the relevant ads for a given page, we propose a linear similarity fusion mechanism, which combines the above three feature vectors in a unified manner. Last, we validate our approach using a set of real ads, real pages along with the external Wikipedia thesaurus. The experimental results show that our approach outperforms the conventional contextual advertising matching approaches and can substantially improve the performance of ad selection.\nTitle:\nImproving contextual advertising matching by using Wikipedia thesaurus knowledge\n\nAbstract:\nWeb advertising, a form of online advertising, which uses the Internet as a medium to post product or service information and attract customers, has become one of the most important marketing channels. As one prevalent type of web advertising, contextual advertising refers to the placement of the most relevant ads at appropriate positions of a web page, so as to provide a better user experience and increase the user's ad-click rate. However, most existing contextual advertising techniques only take into account how to select as relevant ads for a given page as possible, without considering the positional effect of the ad placement on the page, resulting in an unsatisfactory performance in ad local context relevance. In this paper, we address the novel problem of position-wise contextual advertising, i.e., how to select and place relevant ads properly for a target web page. In our proposed approach, the relevant ads are selected based on not only global context relevance but also local context relevance, so that the embedded ads yield contextual relevance to both the whole target page and the insertion positions where the ads are placed. In addition, to improve the accuracy of global and local context relevance measure, the rich wikipedia knowledge is used to enhance the semantic feature representation of pages and ad candidates. Last, we evaluate our approach using a set of ads and pages downloaded from the Internet, and demonstrate the effectiveness of our approach.\nTitle:\nPosition-wise contextual advertising: Placing relevant ads at appropriate positions of a web page.\n\nAbstract:\nA traditional classification approach based on keyword matching represents each text document as a set of keywords, without considering the semantic information, thereby, reducing the accuracy of classification. To solve this problem, a new classification approach based on Wikipedia matching was proposed, which represents each document as a concept vector in the Wikipedia semantic space so as to understand the text semantics, and has been demonstrated to improve the accuracy of classification. However, the immense Wikipedia semantic space greatly reduces the generation efficiency of a concept vector, resulting in a negative impact on the availability of the approach in an online environment. In this paper, we propose an efficient Wikipedia semantic matching approach to document classification. First, we define several heuristic selection rules to quickly pick out related concepts for a document from the Wikipedia semantic space, making it no longer necessary to match all the concepts in the semantic space, thus greatly improving the generation efficiency of the concept vector. Second, based on the semantic representation of each text document, we compute the similarity between documents so as to accurately classify the documents. Finally, evaluation experiments demonstrate the effectiveness of our approach, i.e., which can improve the classification efficiency of the Wikipedia matching under the precondition of not compromising the classification accuracy.\nTitle:\nAn efficient Wikipedia semantic matching approach to text document classification.\n\nAbstract:\nCurrently, commercial search engines have implemented methods to suggest alternative Web queries to users, which helps them specify alternative related queries in pursuit of finding needed Web pages. In this paper, we address the Web search problem on related queries to improve retrieval quality by devising a novel search rank aggregation mechanism. Given an initial query and the suggested related queries, our search system concurrently processes their search result lists from an existing search engine and then forms a single list aggregated by all the retrieved lists. In particular we propose a generic rank aggregation framework which considers not only the number of wins that an item won in a competition, but also the quality of its competitor items in calculating the ranking of Web items. The framework combines the traditional and random walk based rank aggregation methods to produce a more reasonable list to users. Experimental results show that the proposed approach can clearly improve the retrieval quality in a parallel manner over the traditional search strategy that serially returns result lists. Moreover, we also empirically investigate how different rank aggregation methods affect the retrieval performance.\nTitle:\nEnhancing Web Search by Aggregating Results of Related Web Queries\n\nAbstract:\nThe locality of Web pages within a Web site is initially determined by the designer's expectation. Web usage mining can discover the patterns in the navigational behaviour of Web visitors, in turn, improve Web site functionality and service designing by considering users' actual opinion. Conventional Web page clustering technique is often utilized to reveal the functional similarity of Web pages. However, high-dimensional computation problem will be incurred due to taking user transaction as dimension. In this paper, we propose a new Web page grouping approach based on a probabilistic latent semantic analysis (PLSA) model. An iterative algorithm based on maximum likelihood principle is employed to overcome the aforementioned computational shortcoming. The Web pages are classified into various groups according to user access patterns. Meanwhile, the semantic latent factors or tasks are characterized by extracting the content of \"dominant\" pages related to the factors. We demonstrate the effectiveness of our approach by conducting experiments on real world data sets.\nTitle:\nUsing probabilistic latent semantic analysis for Web page grouping\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.726, mean value for all the writers is 8.648, which makes it 0.067 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe current boom of the Web is associated with the revenues originated from Web advertising. As one prevalent type of Web advertising, contextual advertising refers to the placement of the most relevant commercial textual ads within the content of a Web page, so as to provide a better user experience and thereby increase the revenues of Web site owners and an advertising platform. Therefore, in contextual advertising, the relevance of selected ads with a Web page is essential. However, some problems, such as homonymy and polysemy, low intersection of keywords and context mismatch, can lead to the selection of irrelevant textual ads for a Web page, making that a simple keyword matching technique generally gives poor accuracy. To overcome these problems and thus to improve the relevance of contextual ads, in this paper we propose a novel Wikipedia-based matching technique which, using selective matching strategies, selects a certain amount of relevant articles from Wikipedia as an intermediate semantic reference model for matching Web pages and textual ads. We call this technique SIWI: Selective Wikipedia Matching, which, instead of using the whole Wikipedia articles, only matches the most relevant articles for a page (or a textual ad), resulting in the effective improvement of the overall matching performance. An experimental evaluation is conducted, which runs over a set of real textual ads, a set of Web pages from the Internet and a dataset of more than 260\u8117\u807d000 articles from Wikipedia. The experimental results show that our method performs better than existing matching strategies, which can deal with the matching over the large dataset of Wikipedia articles efficiently, and achieve a satisfactory contextual advertising effect.\nTitle:", "model_inf_time": 1.52}, {"id": "41174", "output": "Efficient Approximation Schemes for Non-Preemptive Scheduling with Lp-Norm Completion Times", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe study an online scheduling problem with rejection on $$m\\ge 2$$ identical machines, in which we deal with unit size jobs. Each arriving job has a rejection value (a rejection cost or penalty for minimization problems, and a rejection profit for maximization problems) associated with it. A buffer of size $$K$$ is available to store $$K$$ jobs. A job which is not stored in the buffer must be either assigned to a machine or rejected. Upon the arrival of a new job, the job can be stored in the buffer if there is a free slot (possibly created by evicting other jobs and assigning or rejecting every evicted job). At termination, the buffer must be emptied. We study four variants of the problem, as follows. We study the makespan minimization problem, where the goal is to minimize the sum of the makespan and the penalty of rejected jobs, and the $$\\ell _p$$ norm minimization problem, where the goal is to minimize the sum of the $$\\ell _p$$ norm of the vector of machine completion times and the penalty of rejected jobs. We also study two maximization problems, where the goal in the first version is to maximize the sum of the minimum machine load (the cover value of the machines) and the total rejection profit, and in the second version the goal is to maximize a function of the machine completion times (which measures the balance of machine loads) and the total rejection profit. We show that an optimal solution (an exact solution for the offline problem) can always be obtained in this environment, and determine the required buffer size. Specifically, for all four variants we present optimal algorithms with $$K=m-1$$ and prove that in each case, using a buffer of size at most $$m-2$$ does not allow the design of an optimal algorithm, which makes our algorithms optimal in this respect as well. The lower bounds hold even for the special case where the rejection value is equal for all input jobs.\nTitle:\nOnline scheduling with rejection and reordering: exact algorithms for unit size jobs\n\nAbstract:\nIn this paper, we consider an ordinal on-line scheduling problem. A sequence of n independent jobs has to be assigned non-preemptively to two uniformly related machines. We study two objectives which are maximizing the minimum machine completion time, and minimizing the lp, norm of the completion times. It is assumed that the values of the processing times of jobs are unknown at the time of assignment. However it is known in advance that the processing times of arriving jobs are sorted in a non-increasing order. We are asked to construct an assignment of all jobs to the machines at time zero, by utilizing only ordinal data rather than actual magnitudes of jobs. For the problem of maximizing the minimum completion time we first present a comprehensive lower bound on the competitive ratio, which is a piecewise function of machine speed ratio s. Then, we propose an algorithm which is optimal for any s \u2265 1. For minimizing the lp norm, we study the case of identical machines (s = 1) and present tight bounds as a function of p.\nTitle:\nOptimal on-line algorithms for the uniform machine scheduling problem with ordinal data\n\nAbstract:\n  We study classic scheduling problems on uniformly related machines. Efficient polynomial time approximation schemes (EPTAS's) are fast and practical approximation schemes. New methods and techniques are essential in developing such improved approximation schemes, and their design is a primary goal of this research agenda. We present EPTAS's for the scheduling problem of a set of jobs on uniformly related machines so as to minimize the total weighted completion time, both for the case with release dates and its special case without release dates. These problems are NP-hard in the strong sense, and therefore EPTAS's are the best possible approximation schemes unless P=NP. Previously, only PTAS's were known for these two problems, while an EPTAS was known only for the special case of identical machines without release dates. \nTitle:\nMinimum total weighted completion time: Faster approximation schemes.\n\nAbstract:\nThe problem of scheduling jobs that arrive over time on a single machine is well studied. We study the preemptive model and the model with restarts. We provide lower bounds for deterministic and randomized algorithms for several optimality criteria: weighted and unweighted total completion time, and weighted and unweighted total flow time. By using new techniques, we provide the first lower bounds for several of these problems, and we significantly improve the bounds that were known.\nTitle:\nLower bounds for on-line single-machine scheduling\n\nAbstract:\nWe consider the problem of on-line scheduling of jobs arriving one by one on uniformly related machines, with or without preemption. We prove a lower bound of 2, both with and without preemption, for randomized algorithms working for an arbitrary number of machines. For a constant number of machines we give new lower bounds for the preemptive case.\nTitle:\nA lower bound for on-line scheduling on uniformly related machines\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.416, mean value for all the writers is 8.648, which makes it 1.051 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe consider basic problems of non-preemptive scheduling on uniformly related machines. For a given schedule, defined by a partition of the jobs into m subsets corresponding to the m machines, $$C_i$$ C i denotes the completion time of machine i . Our goal is to find a schedule that minimizes or maximizes $$\\sum _{i=1}^m C_i^p$$ i = 1 m C i p for a fixed value of p such that $$0 0 < p < . For $$p>1$$ p > 1 the minimization problem is equivalent to the well-known problem of minimizing the $$\\ell _p$$ \u2113 p norm of the vector of the completion times of the machines, and for $$0 0 < p < 1 , the maximization problem is of interest. Our main result is an efficient polynomial time approximation scheme (EPTAS) for each one of these problems. Our schemes use a non-standard application of the so-called shifting technique. We focus on the work (total size of jobs) assigned to each machine and introduce intervals of work that are forbidden. These intervals are defined so that the resulting effect on the goal function is sufficiently small. This allows the partition of the problem into sub-problems (with subsets of machines and jobs) whose solutions are combined into the final solution using dynamic programming. Our results are the first EPTAS's for this natural class of load balancing problems.\nTitle:", "model_inf_time": 1.73}, {"id": "41175", "output": "The High Stakes Race for Expired Domain Names:  A Look at Drop-Catch Practices and Security Implications", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nInternet domain names expire when not renewed and may be claimed by a new owner. To date, despite existing work on abuses of residual trust after domain ownership changes, it is not well understood how often and how fast re-registrations occur, and the underlying processes are often over-simplified in scientific literature, leading to a potential bias in those studies. While in principle registration data is available in Whois databases, scalability issues and data ambiguities make re-registrations a challenging subject of study in practice. By focusing on domains about to be deleted, we were able to track 7.4 M com, net, org, biz and name domains over up to ten months to gather data for a survival analysis of re-registrations. Our results show that expiration processes may vary, and that many re-registrations happen soon after deletion, especially for older domains. We also discuss intricacies of Whois data to aid in avoiding potential pitfalls, as fast domain ownership changes combined with hidden domain states may pose challenges to operational and research communities.\nTitle:\nWHOIS Lost in Translation: (Mis)Understanding Domain Name Expiration and Re-Registration.\n\nAbstract:\nNumerous surveys have shown that Web users are concerned about the loss of privacy associated with online tracking. Alarmingly, these surveys also reveal that people are also unaware of the amount of data sharing that occurs between ad exchanges, and thus underestimate the privacy risks associated with online tracking.In reality, the modern ad ecosystem is fueled by a flow of user data between trackers and ad exchanges. Although recent work has shown that ad exchanges routinely perform cookie matching with other exchanges, these studies are based on brittle heuristics that cannot detect all forms of information sharing, especially under adversarial conditions.In this study, we develop a methodology that is able to detect client-and server-side flows of information between arbitrary ad exchanges. Our key insight is to leverage retargeted ads as a tool for identifying information flows. Intuitively, our methodology works because it relies on the semantics of how exchanges serve ads, rather than focusing on specific cookie matching mechanisms. Using crawled data on 35,448 ad impressions, we show that our methodology can successfully categorize four different kinds of information sharing behavior between ad exchanges, including cases where existing heuristic methods fail.We conclude with a discussion of how our findings and methodologies can be leveraged to give users more control over what kind of ads they see and how their information is shared between ad exchanges.\nTitle:\nTracing Information Flows Between Ad Exchanges Using Retargeted Ads\n\nAbstract:\nAccording to copyright holders, One-Click Hosters (OCHs) such as Megaupload are frequently used to host and distribute copyright infringing content. This has spurred numerous initiatives by legislators, law enforcement and content producers. Due to a lack of representative data sets that properly capture private uses of OCHs (such as sharing holiday pictures among friends), to date, there are no reliable estimates of the proportion of legitimate and infringing files being uploaded to OCHs. This situation leaves the field to the partisan arguments brought forward by copyright owners and OCHs. In this paper, we provide empirical data about the uses and misuses of OCHs by analysing six large data sets containing file metadata that we extracted from a range of popular OCHs. We assess the status of these files with regard to copyright infringement and show that at least 26% to 79% of them are potentially infringing. Perhaps surprising after the shutdown by the FBI for alleged copyright infringement, we found Megaupload to have the second highest proportion of legitimate files in our study.\nTitle:\nHoliday Pictures Or Blockbuster Movies? Insights Into Copyright Infringement In User Uploads To One-Click File Hosters\n\nAbstract:\nModern web applications are increasingly moving program code to the client in the form of JavaScript. With the growing adoption of HTML5 APIs such as postMessage, client-side validation (CSV) vulnerabilities are consequently becoming increasingly important to address as well. However, while detecting and preventing attacks against web applications is a well-studied topic on the server, considerably less work has been performed for the client. Exacerbating this issue is the problem that defenses against CSVs must, in the general case, fundamentally exist in the browser, rendering current server-side defenses inadequate. In this paper, we present ZigZag, a system for hardening JavaScript-based web applications against clientside validation attacks. ZigZag transparently instruments client-side code to perform dynamic invariant detection on security-sensitive code, generating models that describe how - and with whom - client-side components interact. ZigZag is capable of handling templated JavaScript, avoiding full re-instrumentation when JavaScript programs are structurally similar. Learned invariants are then enforced through a subsequent instrumentation step. Our evaluation demonstrates that ZigZag is capable of automatically hardening client-side code against both known and previously-unknown vulnerabilities. Finally, we show that ZigZag introduces acceptable overhead in many cases, and is compatible with popular websites drawn from the Alexa Top 20 without developer or user intervention.\nTitle:\nZigZag: Automatically Hardening Web Applications Against Client-side Validation Vulnerabilities\n\nAbstract:\nWeb developers routinely rely on third-party Java-Script libraries such as jQuery to enhance the functionality of their sites. However, if not properly maintained, such dependencies can create attack vectors allowing a site to be compromised.  this paper, we conduct the first comprehensive study of client-side JavaScript library usage and the resulting security implications across the Web. Using data from over 133 k websites, we show that 37% of them include at least one library with a known vulnerability; the time lag behind the newest release of a library is measured in the order of years. In order to better understand why websites use so many vulnerable or outdated libraries, we track causal inclusion relationships and quantify different scenarios. We observe sites including libraries in ad hoc and often transitive ways, which can lead to different versions of the same library being loaded into the same document at the same time. Furthermore, we find that libraries included transitively, or via ad and tracking code, are more likely to be vulnerable. This demonstrates that not only website administrators, but also the dynamic architecture and developers of third-party services are to blame for the Webu0027s poor state of library management. The results of our work underline the need for more thorough approaches to dependency management, code maintenance and third-party code inclusion on the Web.\nTitle:\nThou Shalt Not Depend on Me: Analysing the Use of Outdated JavaScript Libraries on the Web.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.612, mean value for all the writers is 8.648, which makes it 0.822 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nEvery day, hundreds of thousands of Internet domain names are abandoned by their owners and become available for re-registration. Yet, there appears to be enough residual value and demand from domain speculators to give rise to a highly competitive ecosystem of drop-catch services that race to be the first to re-register potentially desirable domain names in the very instant the old registration is deleted. To pre-empt the competitive (and uncertain) race to re-registration, some registrars sell their own customers' expired domains pre-release, that is, even before the names are returned to general availability.These practices are not without controversy, and can have serious security consequences. In this paper, we present an empirical analysis of these two kinds of post expiration domain ownership changes. We find that 10 % of all com domains are re-registered on the same day as their old registration is deleted. In the case of org, over 50 % of re-registrations on the deletion day occur during only 30 s. Furthermore, drop-catch services control over 75 % of accredited domain registrars and cause more than 80 % of domain creation attempts, but represent at most 9.5 % of successful domain creations. These findings highlight a significant demand for expired domains, and hint at highly competitive re-registrations.Our work sheds light on various questionable practices in an opaque ecosystem. The implications go beyond the annoyance of websites turned into \"Internet graffiti\" [26], as domain ownership changes have the potential to circumvent established security mechanisms.\nTitle:", "model_inf_time": 2.21}, {"id": "41176", "output": "A Comprehensive Analysis of Linux Malware", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nOver the last decade, there has been a significant increase in the number and sophistication of malware-related attacks and infections. Many detection techniques have been proposed to mitigate the malware threat. A running theme among existing detection techniques is the similar promises of high detection rates, in spite of the wildly different models (or specification classes) of malicious activity used. In addition, the lack of a common testing methodology and the limited datasets used in the experiments make difficult to compare these models in order to determine which ones yield the best detection accuracy. In this paper, we present a systematic approach to measure how the choice of behavioral models influences the quality of a malware detector. We tackle this problem by executing a large number of testing experiments, in which we explored the parameter space of over 200 different models, corresponding to more than 220 million of signatures. Our results suggest that commonly held beliefs about simple models are incorrect in how they relate changes in complexity to changes in detection accuracy. This implies that accuracy is non-linear across the model space, and that analytical reasoning is insufficient for finding an optimal model, and has to be supplemented by testing and empirical measurements.\nTitle:\nA quantitative study of accuracy in system call-based malware detection\n\nAbstract:\nAnubis is a dynamic malware analysis platform that executes submitted binaries in a controlled environment. To perform the analysis, the system monitors the invocation of important Windows API calls and system services, it records the network traffic, and it tracks data flows. For each submission, reports are generated that provide comprehensive reports about the activities of the binary under analysis. Anubis receives malware samples through a public web interface and a number of feeds from security organizations and anti-malware companies. Because the samples are collected from a wide range of users, the collected samples represent a comprehensive and diverse mix of malware found in the wild. In this paper, we aim to shed light on common malware behaviors. To this end, we evaluate the Anubis analysis results for almost one million malware samples, study trends and evolution of malicious behaviors over a period of almost two years, and examine the influence of code polymorphism on malware statistics.\nTitle:\nA view on current malware behaviors\n\nAbstract:\nModels based on system calls are a popular and common approach to characterize the run-time behavior of programs. For example, system calls are used by intrusion detection systems to detect software exploits. As another example, policies based on system calls are used to sandbox applications or to enforce access control. Given that malware represents a significant security threat for today's computing infrastructure, it is not surprising that system calls were also proposed to distinguish between benign processes and malicious code. Most proposed malware detectors that use system calls follows program-centric analysis approach. That is, they build models based on specific behaviors of individual applications. Unfortunately, it is not clear how well these models generalize, especially when exposed to a diverse set of previously-unseen, real-world applications that operate on realistic inputs. This is particularly problematic as most previous work has used only a small set of programs to measure their technique's false positive rate. Moreover, these programs were run for a short time, often by the authors themselves. In this paper, we study the diversity of system calls by performing a large-scale collection (compared to previous efforts) of system calls on hosts that run applications for regular users on actual inputs. Our analysis of the data demonstrates that simple malware detectors, such as those based on system call sequences, face significant challenges in such environments. To address the limitations of program-centric approaches, we propose an alternative detection model that characterizes the general interactions between benign programs and the operating system (OS). More precisely, our system-centric approach models the way in which benign programs access OS resources (such as files and registry entries). Our experiments demonstrate that this approach captures well the behavior of benign programs and raises very few (even zero) false positives while being able to detect a significant fraction of today's malware.\nTitle:\nAccessMiner: using system-centric models for malware protection\n\nAbstract:\nBoth the operational and academic security communities have used dynamic analysis sandboxes to execute malware samples for roughly a decade. Network information derived from dynamic analysis is frequently used for threat detection, network policy, and incident response. Despite these common and important use cases, the efficacy of the network detection signal derived from such analysis has yet to be studied in depth. This paper seeks to address this gap by analyzing the network communications of 26.8 million samples that were collected over a period of five years. Using several malware and network datasets, our large scale study makes three core contributions. (1) We show that dynamic analysis traces should be carefully curated and provide a rigorous methodology that analysts can use to remove potential noise from such traces. (2) We show that Internet miscreants are increasingly using potentially unwanted programs (PUPs) that rely on a surprisingly stable DNS and IP infrastructure. This indicates that the security community is in need of better protections against such threats, and network policies may provide a solid foundation for such protections. (3) Finally, we see that, for the vast majority of malware samples, network traffic provides the earliest indicator of infection - several weeks and often months before the malware sample is discovered. Therefore, network defenders should rely on automated malware analysis to extract indicators of compromise and not to build early detection systems.\nTitle:\nA Lustrum of Malware Network Communication: Evolution and Insights\n\nAbstract:\nAs embedded systems are more than ever present in our society, their security is becoming an increasingly important issue. However, based on the results of many recent analyses of individual firmware images, embedded systems acquired a reputation of being insecure. Despite these facts, we still lack a global understanding of embedded systems' security as well as the tools and techniques needed to support such general claims. In this paper we present the first public, large-scale analysis of firmware images. In particular, we unpacked 32 thousand firmware images into 1.7 million individual files, which we then statically analyzed. We leverage this large-scale analysis to bring new insights on the security of embedded devices and to underline and detail several important challenges that need to be addressed in future research. We also show the main benefits of looking at many different devices at the same time and of linking our results with other large-scale datasets such as the ZMap's HTTPS survey. In summary, without performing sophisticated static analysis, we discovered a total of 38 previously unknown vulnerabilities in over 693 firmware images. Moreover, by correlating similar files inside apparently unrelated firmware images, we were able to extend some of those vulnerabilities to over 123 different products. We also confirmed that some of these vulnerabilities altogether are affecting at least 140K devices accessible over the Internet. It would not have been possible to achieve these results without an analysis at such wide scale. We believe that this project, which we plan to provide as a firmware unpacking and analysis web service, will help shed some light on the security of embedded devices.\nTitle:\nA large-scale analysis of the security of embedded firmwares\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.103, mean value for all the writers is 8.648, which makes it 1.241 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nFor the past two decades, the security community has been fighting malicious programs for Windows-based operating systems. However, the recent surge in adoption of embedded devices and the IoT revolution are rapidly changing the malware landscape. Embedded devices are profoundly different than traditional personal computers. In fact, while personal computers run predominantly on x86-flavored architectures, embedded systems rely on a variety of different architectures. In turn, this aspect causes a large number of these systems to run some variants of the Linux operating system, pushing malicious actors to give birth to \"Linux malware.\" To the best of our knowledge, there is currently no comprehensive study attempting to characterize, analyze, and understand Linux malware. The majority of resources on the topic are available as sparse reports often published as blog posts, while the few systematic studies focused on the analysis of specific families of malware (e.g., the Mirai botnet) mainly by looking at their network-level behavior, thus leaving the main challenges of analyzing Linux malware unaddressed. This work constitutes the first step towards filling this gap. After a systematic exploration of the challenges involved in the process, we present the design and implementation details of the first malware analysis pipeline specifically tailored for Linux malware. We then present the results of the first large-scale measurement study conducted on 10,548 malware samples (collected over a time frame of one year) documenting detailed statistics and insights that can help directing future work in the area.\nTitle:", "model_inf_time": 1.51}, {"id": "41177", "output": "Static Analysis for Detecting Taint-Style Vulnerabilities in Web Applications", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe number and the importance of web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable web applications by means of static source code analysis. More precisely, we use flow-sensitive, interprocedural and context-sensitive data flow analysis to discover vulnerable points in a program. In addition to the taint analysis at the core of our engine, we employ a precise alias analysis targeted at the unique reference semantics commonly found in scripting languages. Moreover, we enhance the quality and quantity of the generated vulnerability reports by employing an iterative two-phase algorithm for fast and precise resolution of file inclusions. The presented concepts are targeted at the general class of taint-style vulnerabilities and can be easily applied to the detection of vulnerability types such as SQL injection, cross-site scripting (XSS), and command injection. We implemented the presented concepts in Pixy, a high-precision static analysis tool aimed at detecting cross-site scripting and SQL injection vulnerabilities in PHP programs. To demonstrate the effectiveness of our techniques, we analyzed a number of popular, open-source web applications and discovered hundreds of previously unknown vulnerabilities. Both the high analysis speed as well as the low number of generated false positives show that our techniques can be used for conducting effective security audits.\nTitle:\nStatic analysis for detecting taint-style vulnerabilities in web applications\n\nAbstract:\nThe number and the importance of web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable web applications by means of static source code analysis. To this end, we present a novel, precise alias analysis targeted at the unique reference semantics commonly found in scripting languages. Moreover, we enhance the quality and quantity of the generated vulnerability reports by employing a novel, iterative two-phase algorithm for fast and precise resolution of file inclusions.We integrated the presented concepts into Pixy~\\cite{jovanovic06:pixy_short}, a high-precision static analysis tool aimed at detecting cross-site scripting vulnerabilities in PHP scripts. To demonstrate the effectiveness of our techniques, we analyzed three web applications and discovered 106 vulnerabilities. Both the high analysis speed as well as the low number of generated false positives show that our techniques can be used for conducting effective security audits.\nTitle:\nPrecise alias analysis for static detection of web application vulnerabilities\n\nAbstract:\nAs the popularity of the web increases and web applications become tools of everyday use, the role of web security has been gaining importance as well. The last years have shown a significant increase in the number of web-based attacks. For example, there has been extensive press coverage of recent security incidences involving the loss of sensitive credit card information belonging to millions of customers.Many web application security vulnerabilities result from generic input validation problems. Examples of such vulnerabilities are SQL injection and Cross-Site Scripting (XSS). Although the majority of web vulnerabilities are easy to understand and to avoid, many web developers are, unfortunately, not security-aware. As a result, there exist many web sites on the Internet that are vulnerable.This paper demonstrates how easy it is for attackers to automatically discover and exploit application-level vulnerabilities in a large number of web applications. To this end, we developed SecuBat, a generic and modular web vulnerability scanner that, similar to a port scanner, automatically analyzes web sites with the aim of finding exploitable SQL injection and XSS vulnerabilities. Using SecuBat, we were able to find many potentially vulnerable web sites. To verify the accuracy of SecuBat, we picked one hundred interesting web sites from the potential victim list for further analysis and confirmed exploitable flaws in the identified web pages. Among our victims were well-known global companies and a finance ministry. Of course, we notified the administrators of vulnerable sites about potential security problems. More than fifty responded to request additional information or to report that the security hole was closed.\nTitle:\nSecuBat: a web vulnerability scanner\n\nAbstract:\nWeb applications have become an integral part of the daily lives of millions of users. Unfortunately, web applications are also frequently targeted by attackers, and attacks such as XSS and SQL injection are still common. In this paper, we present an empirical study of more than 7000 input validation vulnerabilities with the aim of gaining deeper insights into how these common web vulnerabilities can be prevented. In particular, we focus on the relationship between the specific programming language used to develop web applications and the vulnerabilities that are commonly reported. Our findings suggest that most SQL injection and a significant number of XSS vulnerabilities can be prevented using straight-forward validation mechanisms based on common data types. We elaborate on these common data types, and discuss how support could be provided in web application frameworks.\nTitle:\nAn empirical analysis of input validation mechanisms in web applications and languages\n\nAbstract:\nWeb applications have become an integral part of the daily lives of millions of users. Unfortunately, web applications are also frequently targeted by attackers, and criticial vulnerabilities such as cross-site scripting and SQL injection are still common. As a consequence, much effort in the past decade has been spent on mitigating web application vulnerabilities. Current techniques focus mainly on sanitization: either on automated sanitization, the detection of missing sanitizers, the correctness of sanitizers, or the correct placement of sanitizers. However, these techniques are either not able to prevent new forms of input validation vulnerabilities such as HTTP Parameter Pollution, come with large runtime overhead, lack precision, or require significant modifications to the client and/or server infrastructure. In this paper, we present IPAAS, a novel technique for preventing the exploitation of cross-site scripting and SQL injection vulnerabilities based on automated data type detection of input parameters. IPAAS automatically and transparently augments otherwise insecure web application development environments with input validators that result in significant and tangible security improvements for real systems. We implemented IPAAS for PHP and evaluated it on five real-world web applications with known cross-site scripting and SQL injection vulnerabilities. Our evaluation demonstrates that IPAAS would have prevented 83% of SQL injection vulnerabilities and 65% of cross-site scripting vulnerabilities while incurring no developer burden.\nTitle:\nPreventing Input Validation Vulnerabilities in Web Applications through Automated Type Analysis\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.63, mean value for all the writers is 8.648, which makes it 0.015 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe number and the importance of Web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable Web applications by means of static source code analysis. More precisely, we use flow-sensitive, interprocedural and context-sensitive data flow analysis to discover vulnerable points in a program. In addition, alias and literal analysis are employed to improve the correctness and precision of the results. The presented concepts are targeted at the general class of taint-style vulnerabilities and can be applied to the detection of vulnerability types such as SQL injection, cross-site scripting, or command injection. Pixy, the open source prototype implementation of our concepts, is targeted at detecting cross-site scripting vulnerabilities in PHP scripts. Using our tool, we discovered and reported 15 previously unknown vulnerabilities in three web applications, and reconstructed 36 known vulnerabilities in three other web applications. The observed false positive rate is at around 50% (i.e., one false positive for each vulnerability) and therefore, low enough to permit effective security audits.\nTitle:", "model_inf_time": 1.82}, {"id": "41178", "output": "Temporal Sequences for Passage-Level Music Similarity", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present new methods for computing inter-song similari- ties using intersections between multiple audio pieces. The intersection contains portions that are similar, when one s ong is a derivative work of the other for example, in two differ- ent musical recordings. To scale our search to large song databases we have developed an algorithm based on locality- sensitive hashing (LSH) of sequences of audio features called audio shingles. LSH provides an efficient means to identify approximate nearest neighbors in a high-dimensional fea- ture space. We combine these nearest neighbor estimates, each a match from a very large database of audio to a small portion of the query song, to form a measure of the approx- imate similarity. We demonstrate the utility of our methods on a derivative works retrieval experiment using both ex- act and approximate (LSH) methods. The results show that LSH is at least an order of magnitude faster than the exact nearest neighbor method and that accuracy is not impacted by the approximate method.\nTitle:\nSong Intersection by Approximate Nearest Neighbor Search\n\nAbstract:\nWe propose an automatic method for measuring content-based music similarity, enhancing the current generation of music search engines and recommended systems. Many previous approaches to track similarity require brute-force, pair-wise processing between all audio features in a database and therefore are not practical for large collections. However, in an Internet-connected world, where users have access to millions of musical tracks, efficiency is crucial. Our approach uses features extracted from unlabeled audio data and near-neigbor retrieval using a distance threshold, determined by analysis, to solve a range of retrieval tasks. The tasks require temporal features-analogous to the technique of shingling used for text retrieval. To measure similarity, we count pairs of audio shingles, between a query and target track, that are below a distance threshold. The distribution of between-shingle distances is different for each database; therefore, we present an analysis of the distribution of minimum distances between shingles and a method for estimating a distance threshold for optimal retrieval performance. The method is compatible with locality-sensitive hashing (LSH)-allowing implementation with retrieval times several orders of magnitude faster than those using exhaustive distance computations. We evaluate the performance of our proposed method on three contrasting music similarity tasks: retrieval of mis-attributed recordings (fingerprint), retrieval of the same work performed by different artists (cover songs), and retrieval of edited and sampled versions of a query track by remix artists (remixes). Our method achieves near-perfect performance in the first two tasks and 75% precision at 70% recall in the third task. Each task was performed on a test database comprising 4.5 million audio shingles.\nTitle:\nAnalysis of Minimum Distances in High-Dimensional Musical Spaces\n\nAbstract:\nIn this paper, we propose a novel method for obtaining la- beled training data to estimate the parameters in a super- vised learning model for automatic chord recognition. To this end, we perform harmonic analysis on symbolic data to generate label files. In parallel, we generate audio data from the same symbolic data, which are then provided to a machine learning algorithm along with label files to esti- mate model parameters. Experimental results show higher performance in frame-level chord recognition than the pre- vious approaches.\nTitle:\nAutomatic Chord Recognition from Audio Using a HMM with Supervised Learning\n\nAbstract:\nWe report on the construction of a real-time computer system capable of distinguishing speech signals from music signals over a wide range of digital audio input. We have examined 13 features intended to measure conceptually distinct properties of speech and/or music signals, and combined them in several multidimensional classification frameworks. We provide extensive data on system performance and the cross-validated training/test setup used to evaluate the system. For the datasets currently in use, the best classifier classifies with 5.8% error on a frame-by-frame basis, and 1.4% error when integrating long (2.4 second) segments of sound\nTitle:\nConstruction and evaluation of a robust multifeature speech/music discriminator\n\nAbstract:\nThis paper describes a system for connecting non-speech sounds and words using linked multi-dimensional vector spaces. An approach based on mixture of experts learns the mapping between one space and the other. This paper describes the con-version of audio and semantic data into their respective vector spaces. Two different mixture-of-probability-expert models are trained to learn the association between acoustic queries and the corresponding semantic explanation, and visa versa. Test results are presented based on commercial sound effects CDs. \nTitle:\nMixtures of probability experts for audio retrieval and indexing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.481, mean value for all the writers is 8.648, which makes it 0.996 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper demonstrates the importance of temporal sequences for passage-level music information retrieval. A number of audio analysis problems are solved successfully by using models that throw away the temporal sequence data. This paper suggests that we do not have this luxury when we consider a more difficult problem: that is finding musically similar passages within a narrow range of musical styles or within a single musical piece. Our results demonstrate a significant improvement in performance for audio similarity measures using temporal sequences of features, and we show that quantizing the features to string-based representations also performs well, thus admitting efficient implementations based on string matching.\nTitle:", "model_inf_time": 1.28}, {"id": "41179", "output": "A Perceptually Guided Subspace Method for Sinusoidal Audio Modeling", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe problem of modeling a signal segment as a sum of exponentially damped sinusoidal components is of interest in it wide range of fields, including speech and audio processing. Often, model parameters are estimated using subspace based techniques that exploit the so-called shift-invariance property. A drawback of these estimation techniques in relation to speech and audio processing is that the perceptual relevance of the model components is not taken into account. In this paper we show how to combine well-known subspace based estimation techniques with a recently developed perceptual distortion measure, to obtain an algorithm for extracting perceptually relevant model components. In analysis-synthesis experiments with wideband audio signals, objective and subjective evaluations show that the proposed algorithm improves perceived signal quality considerable over traditional subspace based analysis methods.\nTitle:\nA Perceptual Subspace Method For Sinusoidal Speech And Audio Modeling\n\nAbstract:\nIn this paper we propose a rate-distortion optimal algorithm for sinusoidal modeling of audio and speech. The algorithm determines for a pre-specified target bit-rate the optimal (variable-length) time segmentation, the optimal distribution of sinusoidal components over the segments and the optimal (scalar) quantizers for quantizing the sinusoid parameters. The optimization is done by jointly optimizing the segment lengths, number of sinusoids and quantizers using high-resolution quantization theory and dynamic programming techniques, which makes it possible to solve the algorithm in polynomial time. A particular advantage of the proposed method is that it solves the problem of, given a target bit-rate, finding the optimal balance between total number of sinusoids and number of bits per sinusoid.\nTitle:\nJointly Optimal Time Segmentation, Component Selection And Quantization For Sinusoidal Coding Of Audio And Speech\n\nAbstract:\nIn this paper we present a numerically robust method for modeling audio signals which is based on a exponential data model. This model is a generalization of the classical sinusoidal model in the sense that it allows the amplitude of the sinusoids to evolve exponentially. We show that, using this model, so-called attacks can be represented very efficiently and we propose an algorithm for finding the exponentials in a robust way. Moreover, we show that by using a proper segmentation of the input data into variable length segments the signal-to-noise ratio can be drastically improved as compared to a fixed-length analysis.\nTitle:\nRobust Exponential Modeling Of Audio Signals\n\nAbstract:\nSinusoidal coding is a key technique for low rate audio coding. In sinusoidal coding, the target signal is represented by perceptually relevant sinusoids; however, often the sinusoids are estimated with- out taking into account that the sinusoidal parameters are going to be differentially encoded. In this paper we present an algorithm for joint extraction and time-differential encoding of sinusoidal model parameters. For a pre-specified target bit rate, the algorithm extracts the set of sinusoids for a sequence of signal segments which lead to minimum distortion in the reconstructed signal. Furthermore, it determines which sinusoids in a given segment should be encoded time-differentially relative to which in the previous segment. Sim- ulation experiments show that the proposed algorithm leads to a reduction of 3-5% in bit rate compared to a state-of-the-art time- differential sinusoidal coding system.\nTitle:\nTIME-DIFFERENTIAL ENCODING OF SINUSOIDAL MODEL PARAMETERS FOR MULTIPLE SUCCESSIVE SEGMENTS\n\nAbstract:\nPsychoacoustical models have been used extensively within audio coding applications over the past decades. Recently, parametric coding techniques have been applied to general audio and this has created the need for a psychoacoustical model that is specifically suited for sinusoidal modelling of audio signals. In this paper, we present a new perceptual model that predicts masked thresholds for sinusoidal distortions. The model relies on signal detection theory and incorporates more recent insights about spectral and temporal integration in auditory masking. As a consequence, the model is able to predict the distortion detectability. In fact, the distortion detectability defines a (perceptually relevant) norm on the underlying signal space which is beneficial for optimisation algorithms such as rate-distortion optimisation or linear predictive coding. We evaluate the merits of the model by combining it with a sinusoidal extraction method and compare the results with those obtained with the ISO MPEG-1 Layer I-II recommended model. Listening tests show a clear preference for the new model. More specifically, the model presented here leads to a reduction of more than 20% in terms of number of sinusoids needed to represent signals at a given quality level.\nTitle:\nA perceptual model for sinusoidal audio coding based on spectral integration\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.653, mean value for all the writers is 8.648, which makes it 0.857 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe problem of modeling a signal segment as a sum of exponentially damped sinusoidal components arises in many different application areas, including speech and audio processing. Often, model parameters are estimated using subspace based techniques which arrange the input signal in a structured matrix and exploit the so-called shift-invariance property related to certain vector spaces of the input matrix. A problem with this class of estimation algorithms, when used for speech and audio processing, is that the perceptual importance of the sinusoidal components is not taken into account. In this work we propose a solution to this problem. In particular, we show how to combine well-known subspace based estimation techniques with a recently developed perceptual distortion measure, in order to obtain,an algorithm for extracting perceptually relevant model components. In analysis-synthesis experiments with wideband audio signals, objective and subjective evaluations show that the proposed Algorithm improves perceived signal quality considerable over traditional subspace based analysis methods.\nTitle:", "model_inf_time": 1.53}, {"id": "41180", "output": "Single-Tone Signaling for Effective Intercell Interference Management", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, a distributed cooperative multicell beamforming algorithm is proposed, and a detail analysis and solving method for instantaneous and statistical channel state information (CSI) are presented. Firstly, an improved distributed iterative beamforming algorithm is proposed for the multiple-input single-output interference channel (MISO IC) scenario which chooses virtual signal-to-interference-and-noise (SINR) as decision criterion to initialize and then iteratively solves the constrained optimization problem of maximizing the virtual SINR for a given level of generated interference to other users. Then, the algorithm is generalized to the multicell date sharing scenario with a heuristics power allocation scheme based on a viewpoint of the layered channel. Finally, the performance is illustrated through numerical simulations.\nTitle:\nDistributed Cooperative Multicell Beamforming Based On A Viewpoint Of Layered Channel\n\nAbstract:\nIn this paper, we propose a novel opportunistic scheduling algorithm in multi-cell cooperation scenario, i.e., joint multi-user diversity (MUD) exploitation and inter-cell interference (ICI) mitigation based scheduling (MEIMS). Our algorithm jointly considers the intended channel condition of the scheduled user from its serving cell and the orthogonality between that and the corresponding interference channels to concurrently scheduled users in neighboring cells so as to exploit MUD and mitigate ICI simultaneously. The performance of our algorithm is evaluated through simulation. Results show that, our scheme can significantly enhance the received signal to interference plus noise ratio (SINR) with relatively better fairness guarantee, thus achieves the largest throughput and utility comparing to several well-known scheduling algorithms.\nTitle:\nJoint MUD exploitation and ICI mitigation based scheduling with limited base station cooperation\n\nAbstract:\nIn this paper, we propose a novel opportunistic scheduling algorithm, joint useful and interference channel based scheduling (JUICS), in coordinated multi-point (CoMP) transmission scenario. Our scheme jointly considers the useful channel condition of the scheduled user from its serving cell and the orthogonality between that and the corresponding interference channels to concurrently scheduled users in neighboring CoMP cells, thus to exploit multi-user diversity (MUD) and mitigate inter-cell interference (ICI) simultaneously. The performance of the proposed algorithm is evaluated through simulation in terms of the cumulative distribution performance of the received signal to interference plus noise ratio (SINR) and that of the scheduled times of all CoMP users. Results show that, with limited complexity and overhead, our scheme can significantly enhance the received SINR with relatively better fairness guarantee, thus to achieve the largest throughput and utility comparing to several well-known scheduling algorithms.\nTitle:\nA Novel Opportunistic Scheduling Algorithm in Coordinated Multi-Point Transmission Scenario\n\nAbstract:\nThe combination of the large-scale multiple-input-multiple-output (MIMO) and orthogonal frequency-division multiplexing (OFDM) transmissions is a promising candidate for future wireless communication systems. We investigate a quality of service (QoS) guaranteed user-scheduling algorithm for large-scale MIMO-OFDM systems, where pilots are assigned to served users. In general, a wireless channel con...\nTitle:\nQoS-Guaranteed User Scheduling and Pilot Assignment for Large-Scale MIMO-OFDM Systems.\n\nAbstract:\nIn this letter, we propose a low-complexity sparse channel estimation method for orthogonal frequency division multiplexing (OFDM) systems. The proposed method uses a discrete Fourier transform (DFT)-based technique for channel estimation and a novel sorted noise space discrimination technique to estimate the channel length and tap positions. Simulation results demonstrate that the reduction in signal space improves the channel estimation performance.\nTitle:\nA Low-Complexity Sparse Channel Estimation Method For Ofdm Systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.128, mean value for all the writers is 8.648, which makes it 1.263 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOver-the-air intercell signaling is one of the most effective means for fast, distributed, and dynamic intercell interference management (ICIM), which is the key to maintaining the quality of service (QoS) and fairness in most interference-dominant networks, such as femtocell networks. However, the design of this type of signaling is challenging. In this paper, we address these challenges and propose an effective solution, which we refer to as single-tone signaling (STS). The proposed STS possesses many highly desirable and crucial properties for ICIM, such as strong resistance to interference, deep coverage, low overhead, and most of all, the lack of the need for dedicated resources. In addition, STS provides an effective means of estimating interference channels for advanced interference management. The effectiveness of STS is demonstrated through applications to a femtocell network.\nTitle:", "model_inf_time": 1.26}, {"id": "41181", "output": "A Geometric Jaccard Similarity Measure for General Type-2 Fuzzy Sets", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSubsethood and similarity measures are important concepts in fuzzy set (FS) theory. There are many different definitions of them, for both type-1 (T1) FSs and interval type-2 (IT2) FSs. In this paper, Rickard et al.'s definition of IT2 FS subsethood measure, extended from Kosko's T1 FS subsethood measure using the Representation Theorem, and Nguyen and Kreinovich's IT2 FS similarity measure, extended from the Jaccard similarity measure for T1 FSs, are introduced. Efficient algorithms for computing them are also proposed. Simulations demonstrate that our proposed algorithms outperform existing algorithms in the literature.\nTitle:\nEfficient algorithms for computing a class of subsethood and similarity measures for interval type-2 fuzzy sets\n\nAbstract:\nComparing the similarity between two fuzzy sets (FSs) is needed in many applications. The focus herein is linguistic approximation using type-1 (T1) FSs, i.e. associating a T1 FS Awith a linguistic label from a vocabulary. Because each label is represented by an T1 FS Bi, there is a need to compare the similarity of Aand Bito find the Bimost similar to A. In this paper, a vector similarity measure (VSM) is proposed for T1 FSs, whose two elements measure the similarity in shape and proximity, respectively. A comparative study shows that the VSM gives best results. Additionally, the VSM can be easily extended to interval type-2 FSs.\nTitle:\nA Vector Similarity Measure for Type-1 Fuzzy Sets\n\nAbstract:\nFuzzy logic is frequently used in computing with words (CWW). When input words to a CWW engine are modeled by interval type-2 fuzzy sets (IT2 FSs), the CWW engine's output can also be an IT2 FS, A~, which needs to be mapped to a linguistic label so that it can be understood. Because each linguistic label is represented by an IT2 FS B~\"i, there is a need to compare the similarity of A~ and B~\"i to find the B~\"i most similar to A~. In this paper, a vector similarity measure (VSM) is proposed for IT2 FSs, whose two elements measure the similarity in shape and proximity, respectively. A comparative study shows that the VSM gives more reasonable results than all other existing similarity measures for IT2 FSs for the linguistic approximation problem. Additionally, the VSM can also be used for type-1 FSs, which are special cases of IT2 FSs when all uncertainty disappears.\nTitle:\nA vector similarity measure for linguistic approximation: Interval type-2 and type-1 fuzzy sets\n\nAbstract:\nFuzzy logic is frequently used in computing with words (CWW). When input words to a CWW engine are modeled by interval type-2 fuzzy sets (IT2 FSs), the CWW engine's output can also be an IT2 FS, A tilde, which needs to be mapped to a linguistic label so that it can be understood. Because each linguistic label is represented by an IT2 FS Bi, there is a need to compare the similarity of A tilde and B tildei to find the B tildei most similar to A tilde. In this paper, a vector similarity measure (VSM) is proposed for IT2 FSs, whose two elements measure the similarity in shape and proximity, respectively. A comparative study shows that the VSM gives more reasonable results than all other existing similarity measures for IT2 FSs.\nTitle:\nA Vector Similarity Measure for Interval Type-2 Fuzzy Sets\n\nAbstract:\nRanking methods, similarity measures and uncertainty measures are very important concepts for interval type-2 fuzzy sets (IT2 FSs). So far, there is only one ranking method for such sets, whereas there are many similarity and uncertainty measures. A new ranking method and a new similarity measure for IT2 FSs are proposed in this paper. All these ranking methods, similarity measures and uncertainty measures are compared based on real survey data and then the most suitable ranking method, similarity measure and uncertainty measure that can be used in the computing with words paradigm are suggested. The results are useful in understanding the uncertainties associated with linguistic terms and hence how to use them effectively in survey design and linguistic information processing.\nTitle:\nA comparative study of ranking methods, similarity measures and uncertainty measures for interval type-2 fuzzy sets\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.672, mean value for all the writers is 8.648, which makes it 1.727 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe similarity between two fuzzy sets (FSs) is an important concept in fuzzy logic. As the research interest on general type-2 (GT2) FSs has increased recently, many similarity measures for them have also been proposed. This paper gives a comprehensive overview of existing similarity measures for GT2 FSs, points out their limitations, and, by using an intuitive geometric explanation, proposes a Jaccard similarity measure for GT2 FSs that is an extension of the popular Jaccard similarity measure for type-1 and interval type-2 FSs. The fundamental difference between the proposed Jaccard similarity measure for GT2 FSs and all existing similarity measures is that the Jaccard similarity measure considers the overall geometries of two GT2 FSs and does not depend on a specific representation of the GT2 FSs, whereas all existing similarity measures for GT2 FSs depend either on the vertical slice representation or the \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\alpha$</tex-math></inline-formula>\n-plane representation. We show that the Jaccard similarity measure for GT2 FSs satisfies four properties of a similarity measure and demonstrate its reasonableness using two examples.\nTitle:", "model_inf_time": 1.53}, {"id": "41182", "output": "Novel Weighted Averages and Interactive Addition of Fuzzy Constraints", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose and demonstrate an effective methodology for implementing the generalized extension principle to solve Advanced Computing with Words (ACWW) problems. Such problems involve implicit assignments of linguistic truth, probability, and possibility. To begin, we establish the vocabularies of the words involved in the problems, and then collect data from subjects about the words after which fuzzy set models for the words are obtained by using the Interval Approach (IA) or the Enhanced Interval Approach (EIA). Next, the solutions of the ACWW problems, which involve the fuzzy set models of the words, are formulated using the Generalized Extension Principle. Because the solutions to those problems involve complicated functional optimization problems that cannot be solved analytically, we then develop a numerical method for their solution. Finally, the resulting fuzzy set solutions are decoded into natural language words using Jaccard's similarity measure. We explain how ACWW problems can solve some potential prototype engineering problems and connect the methodology of this paper with Perceptual Computing.\nTitle:\nOn Advanced Computing With Words Using the Generalized Extension Principle for Type-1 Fuzzy Sets\n\nAbstract:\nThis paper explains how to compute normalized interval type-2 fuzzy sets in closed form and explains how the results reduce to well-known results for type-1 fuzzy sets and interval sets. Such normalized interval type-2 fuzzy sets may be needed in linguistic probability computations or multiple criteria decision analysis under uncertainty.\nTitle:\nOn Computing Normalized Interval Type-2 Fuzzy Sets\n\nAbstract:\nIn this paper, we synthesize interval type-2 fuzzy set models of linguistic probability words and linguistic quantifiers by applying the Enhanced Interval Approach to data collected from subjects about those words. We establish some user friendly sub-vocabularies of linguistic probabilities and linguistic modifiers, so that they can be used in advanced computing with words applications. The user friendly vocabularies are based on axioms of fuzzy probabilities.\nTitle:\nModeling Linguistic Probabilities And Linguistic Quantifiers Using Interval Type-2 Fuzzy Sets\n\nAbstract:\nIn this paper, we present solutions to an Advanced Computing with Words problem that is equivalent to one of Zadeh's challenge problems on linguistic probabilities. We use a syllogism based on the entailment principle to interpret the problem so that it yields two linguistic belief structures. Then we perform an addition of those linguistic belief structures to obtain a belief structure on the variable about which linguistic probabilities have to be inferred. We show that pessimistic (lower) and optimistic (upper) probabilities can be inferred from such a belief structure using Linguistic Weighted Averages and pessimistic and optimistic compatibility measures. Then, we choose vocabularies for linguistic attributes (lifetimes of products) and linguistic probabilities that are involved in the problem statement. The vocabularies are modeled using interval type-2 fuzzy sets. We calculate optimistic (upper) and pessimistic (lower) probabilities, and map them into words present in the vocabulary of linguistic probabilities, so that the results can be comprehended by a human.\nTitle:\nAdvanced computing with words using syllogistic reasoning and arithmetic operations on linguistic belief structures\n\nAbstract:\nThis paper provides a new methodology for determining a word's interval type-2 fuzzy set model using only one subject, a Person FOU. It uses interval end-point uncertainty intervals instead of only the end-point intervals. Such uncertainty intervals are relatively easy to collect and they do not introduce methodological uncertainties during the data-collection process. This new method is applied to ten probability words. Person FOUs are obtained for these words, and the robustness of this new method to the choice of the probability distribution that is assigned to the interval end-point uncertainty intervals is examined and demonstrated.\nTitle:\nDetermining interval type-2 fuzzy set models for words using data collected from one subject: Person FOUs\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.863, mean value for all the writers is 8.648, which makes it 1.89 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, some properties of Novel Weighted Averages that are related to the concepts of possibility theory are examined. It is shown that Novel Weighted Averages have certain interpretations in terms of addition of interactive interval or fuzzy constraints. To do this, alternative forms of Novel Weighted Averages are provided. In particular, an alternative form of Novel Weighted Averages represented by the Extension Principle is determined. It is shown that, when fuzzy set models of words are obtained by collecting data from subjects in a Computing with Words setting, interactive addition of fuzzy sets is not a well-defined method, and the optimization problems related to it may have no solutions, although interactive addition is recommended in the literature for solving multicriteria decision making problems and for dealing with uncertain probabilities. On the other hand, Novel Weighted Averages perform a specific normalization that guarantees that they always exist.\nTitle:", "model_inf_time": 1.26}, {"id": "41183", "output": "Performance Evaluation of WMN-GA System for Node Placement in WMNs Considering Normal Distribution of Mesh Clients and Different Selection and Mutation Operators", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we evaluate the performance of WMN-GA system for node placement problem in WMNs. For evaluation, we consider Exponential and Uniform Distribution of mesh clients and different selection and mutation operators. The population size is considered 64 and the number of generation 200. For evaluation, we consider the giant component and the number of covered users metrics. The simulation results shows that the WMN-GA system performs better for Exponential distribution of mesh clients.\nTitle:\nPerformance Evaluation of WMN-GA System for Node Placement in WMNs Considering Exponential and Weibull Distribution of Mesh Clients and Different Selection and Mutation Operators\n\nAbstract:\nIn this paper, we evaluate the performance of WMN-GA system for node placement problem in WMNs. For evaluation, we consider Weibull Distribution of mesh clients and different selection operators. The population size is considered 64 and the number of generation 200. For evaluation, we consider the giant component and the number of covered users metrics. The simulation results show that for small and medium grid sizes and big density of mesh clients WMN-GA performs better for Linear Ranking. For big grid sizes and low density of mesh clients WMN-GA performs better for Exponential Ranking.\nTitle:\nEffects of Selection Operators for Mesh Router Placement in Wireless Mesh Networks Considering Weibull Distribution of Mesh Clients\n\nAbstract:\nWireless Mesh Networks (WMNs) have become an important networking infrastructure for providing cost-efficient broadband wireless connectivity. WMNs are showing their applicability in deployment of medical, transport and surveillance applications in urban areas, metropolitan, neighboring communities and municipal area networks. In this paper, we deal with connectivity and coverage problem of WMN. Because these problems are known to be NP-Hard, we propose and implement a system based on Genetic Algorithms (GAs) called WMN-GA. We evaluate the performance of WMN-GA for different mutation operators and show that single mutation operator has better behavior considering size of giant component and the number of covered users.\nTitle:\nPerformance Evaluation of WMN Using WMN-GA System for Different Mutation Operators\n\nAbstract:\nWireless Mesh Networks (WMNs) are currently attracting a lot of attention from wireless research and technology community due to their importance as means for providing cost-efficient broadband wireless connectivity. WMNs are based on mesh topology, in which every node is connected to one or more nodes, enabling thus the information transmission in more than one path. In this paper, we deal with the effects of mutation and crossover operators in GA for node placement problem. We evaluate the performance of the proposed system using different genetic operators and different distributions of router nodes considering giant component parameter. The simulation results show that for Exponential and Linear Ranking methods, the system has a good performance.\nTitle:\nEvaluation Of Wmn-Ga For Different Mutation And Crossover Rates Considering Giant Component Parameter\n\nAbstract:\nWireless Mesh Networks (WMNs) are currently attracting a lot of attention from wireless research and technology community due to their importance as means for providing cost-efficient broadband wireless connectivity. WMNs are based on mesh topology, in which every node is connected to one or more nodes, enabling thus the information transmission in more than one path. In this paper, we deal with the effects of mutation and crossover operators in GA for node placement problem. We evaluate the performance of the proposed system using different genetic operators and different distributions of router nodes considering giant component parameter. The simulation results show that for Exponential and Linear Ranking methods, the system has a good performance.\nTitle:\nEffects of Mutation and Crossover in Genetic Algorithms for Node Placement in WMNs Considering Giant Component Parameter\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 12.018, mean value for all the writers is 8.648, which makes it 2.875 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWireless Mesh Networks (WMNs) are currently attracting a lot of attention from wireless research and technology community due to their importance for providing cost-efficient broadband wireless connectivity. Issues for achieving the network connectivity and user coverage are related with the node placement problem. In this paper, we evaluate the performance of WMN-GA system for node placement problem in WMNs. For evaluation, we consider Normal Distribution of mesh clients and different selection and mutation operators. The population size is considered 64 and the number of generation 200. For evaluation, we consider the giant component and the number of covered users metrics. The simulation results shows that the WMN-GA system performs better for Single Mutation and Linear Ranking.\nTitle:", "model_inf_time": 2.09}, {"id": "41184", "output": "A Fuzzy Logic Approach for Affective Behavior Labeling in Educational Discourse", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nEmotion awareness is becoming a key aspect in collaborative work at academia, enterprises and organizations that use collaborative group work in their activity. Due to pervasiveness of ICT's, most of collaboration can be performed through communication media channels such as discussion forums, social networks, etc. The emotive state of the users while they carry out their activity such as collaborative learning at Universities or project work at enterprises and organizations influences very much their performance and can actually determine the final learning or project outcome. Therefore, monitoring the users' emotive states and using that information for providing feedback and scaffolding is crucial. To this end, automated analysis over data collected from communication channels is a useful source. In this paper, we propose an approach to process such collected data in order to classify and assess emotional states of involved users and provide them feedback accordingly to their emotive states. In order to achieve this, a fuzzy approach is used to build the emotive classification system, which is fed with data from ANEW dictionary, whose words are bound to emotional weights and these, in turn, are used to map Fuzzy sets in our proposal. The proposed fuzzy-based system has been evaluated using real data from collaborative learning courses in an academic context.\nTitle:\nA Fuzzy-Based Approach for Classifying Students' Emotional States in Online Collaborative Work\n\nAbstract:\nIn this work we investigate the importance of emotion awareness in e-learning environments, with emphasis to Computer Supported Collaborative Learning (CSCL) activities. Our presented solution involves a new conceptual model of emotions of interest in e-learning context. In the basis of this model, a computational model has been implemented employing self-report of emotions, affective feedback and effective emotion visualisations. Both models have been tested in real education settings, contributing to the research agenda.\nTitle:\nEmbedding Emotion Awareness Into E-Learning Environments\n\nAbstract:\nAffective or Emotion oriented computing constitutes an emerging research field that is still in its early stages. The lack of empirical results together with the complexity that attributes emotions, subjects research to a diversity of theories, models and tools. In the current paper we present a critical review of the state of the art on emotion measurement models, methods and tools and we suggest some informal rules towards their realistic use in education settings.\nTitle:\nEmotion Measurement in Intelligent Tutoring Systems: What, When and How to Measure\n\nAbstract:\nThe enrichment of Computer Supported Collaborative (CSCL) Systems with emotion awareness features (detect emotion patterns and respond effectively) opens a window to the future of learner-to-computer interaction. In the current paper we present a system's design that attempts to evaluate the learner's affective state while he/she is taking part into collaborative tasks, in three different time-points: (i) before the task, (ii) in real-time (while collaborating) and (iii) retrospective (after the task). For the emotion assessment process, self-reporting and sentiment analysis are explored. Based on effective production rules that correlate emotions or emotion sequences to affective feedback techniques, the system will provide emotion scaffolds corresponding to user needs and feelings.\nTitle:\nA Multi-fold Time Approach to Address Emotions in Live and Virtualized Collaborative Learning\n\nAbstract:\nEndowing learning systems with emotion awareness features (capture user's affective state and provide affective feedback), seems quite promising. This paper describes a system implementation that provides emotion awareness, both explicitly, by self-reporting of emotions through a usable web tool, and implicitly, via sentiment analysis. Prominent theories, models and techniques of emotion, emotion learning, emotion detection and affective feedback are reviewed. We also present findings from our experiment with university students, validating the explicit mechanism in real education settings. Finally, we set open issues for future experimentation, contributing to the research agenda.\nTitle:\nA Dual-Modal System that Evaluates User's Emotions in Virtual Learning Environments and Responds Affectively.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.074, mean value for all the writers is 8.648, which makes it 2.07 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMonitoring users\u2019 emotive states and using that information for providing feedback and scaffolding is crucial. In the learning context, emotions can be used to increase students\u2019 attention as well as to improve memory and reasoning. In this context, tutors should be prepared to create affective learning situations and encourage collaborative knowledge construction as well as identify those students\u2019 feelings which hinder learning process. In this paper, we propose a novel approach to label affective behavior in educational discourse based on fuzzy logic, which enables a human or virtual tutor to capture students\u2019 emotions, make students aware of their own emotions, assess these emotions and provide appropriate affective feedback. To that end, we propose a fuzzy classifier that provides a priori qualitative assessment and fuzzy qualifiers bound to the amounts such as few, regular and many assigned by an affective dictionary to every word. The advantage of the statistical approach is to reduce the classical pollution problem of training and analyzing the scenario using the same dataset. Our approach has been tested in a real online learning environment and proved to have a very positive influence on students\u2019 learning performance.\nTitle:", "model_inf_time": 1.47}, {"id": "41185", "output": "A Fuzzy-Based Cluster-Head Selection System for WSNs Considering Node Movement", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting the cluster head is not easy in different environments which may have different characteristics. In order to deal with this problem, we have proposed a power reduction algorithm for sensor networks based on fuzzy logic and node movement. The proposed system different from our previous systems uses 4 input linguistic parameters: Remaining Power of Sensor (RPS), Degree of Number of Neighbor Nodes (D3N), Distance from Cluster Centroid (DCC) and Sensor Speed (SS) for cluster-head decision. By considering the moving speed of the sensor we are able to predict if the node will leave the cluster. We evaluate the proposed system by simulations and show that it has a good cluster-head selection.\nTitle:\nA Fuzzy-Based Cluster-Head Selection System forWSNs Considering Different Parameters\n\nAbstract:\nCluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting of the cluster head is not easy in different environments which may have different characteristics. In order to deal with this problem, we have proposed a power reduction algorithm for sensor networks based on fuzzy logic and number of neighbor nodes. In this paper, we propose a new fuzzy-based cluster-head selection system to improve the performance of the previous system. The proposed system in different from previous system uses 4 input linguistic parameter for cluster-head decision. We evaluate the proposed system by simulations and show that the proposed system has a good cluster-head selection.\nTitle:\nA New Fuzzy-based Cluster-Head Selection System for WSNs\n\nAbstract:\nCluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting the cluster head is not easy in different environments which may have different characteristics. In order to deal with this problem, we have proposed a power reduction algorithm for sensor networks based on fuzzy logic and node movement. The proposed system uses 4 input linguistic parameters: Remaining Power of Sensor (RPS), Degree of Number of Neighbor Nodes (D3N), Distance from Cluster Centroid (DCC) and Sensor Speed (SS) for cluster-head decision. We consider for evaluation two cases: when sensors are static and when sensors move. By considering the moving speed of the sensor we are able to predict if the node will leave the cluster. We evaluate the proposed system by simulations and show that it has a good cluster-head selection.\nTitle:\nA Fuzzy-Based Cluster-Head Selection System for WSNs: A Comparison Study for Static and Mobile Sensors\n\nAbstract:\nCluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting the cluster head is not easy in different environments which may have different characteristics. In order to deal with this problem, we have proposed a system for controlling sensor speed in Wireless Sensor Networks (WSNs). The proposed system is based on fuzzy logic. We use 4 input linguistic parameters: Remaining Power of Sensor (RPS), Degree of Number of Neighbor Nodes (D3N), Distance from Cluster Centroid (DCC) and Sensor Speed (SS) for selection of the cluster-head and the control of sensor speed. By controlling the sensor speed, we are able to predict whether the node will leave or stay in the cluster. We evaluate the proposed system by simulations and show that the system has a good behavior.\nTitle:\nA Fuzzy-Based Simulation System for Cluster-Head Selection and Sensor Speed Control in Wireless Sensor Networks\n\nAbstract:\nCluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting of the cluster head is not easy in different environments which may have different characteristics. In our previous work, in order to deal with this problem, we proposed a power reduction algorithm for sensor networks based on fuzzy logic and number of neighbour nodes. We call this algorithm F3N. In this paper, we evaluate F3N and LEACH by many simulation results. We evaluate the performance of our proposed system for tree different parameters: Remaining Battery Power of Sensor (RPS), Degree of Number of Neighbour Nodes (D3N), and Distance from Cluster Centroid (DCC). From the simulation results, we found that the probability of a sensor node to be a cluster head is increased with increase of number of neighbour nodes and remained battery power and is decreased with the increase of distance from the cluster centroid.\nTitle:\nEvaluation of an Intelligent Fuzzy-Based Cluster Head Selection System for WSNs Using Different Parameters\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 12.522, mean value for all the writers is 8.648, which makes it 3.305 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting of the cluster head is not easy in different environments which may have different characteristics. In order to deal with this problem, we propose a power reduction algorithm for sensor networks based on fuzzy logic and node movement. The proposed system uses 3 input linguistic parameters: remaining battery power of sensor, degree of number of neighbor nodes and sensor speed for cluster-head decision. By considering the moving speed of the sensor node, we are able to find the nodes which leave the cluster. We evaluate the proposed system by simulations and show that the proposed system has a good cluster-head selection.\nTitle:", "model_inf_time": 1.68}, {"id": "41186", "output": "A Model-Driven Toolsuite for Component-Based Distributed Real-Time and Embedded Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nComponent middleware provides dependable and efficient platforms that support key functional, and quality of service (QoS) needs of distributed real-time embedded (DRE) systems. Component middleware, however, also introduces challenges for DRE system developers, such as evaluating the predictability of DRE system behavior, and choosing the right design alternatives before committing to a specific platform or platform configuration. Model-based technologies help address these issues by enabling design-time analysis, and providing the means to automate the development, deployment, configuration, and integration of component-based DRE systems. To this end, this paper applies model checking techniques to DRE design models using model transformations to verify key QoS properties of component-based DRE systems developed using Real-time CORBA. We introduce a formal semantic domain for a general class of DRE systems that enables the verification of distributed non-preemptive real-time scheduling. Our results show that model-based techniques enable design-time analysis of timed properties and can be applied to effectively predict, simulate, and verify the event-driven behavior of component-based DRE systems.\nTitle:\nVerifying distributed real-time properties of embedded systems via graph transformations and model checking\n\nAbstract:\nThis paper provides two contributions to R&D on model-driven development (MDD) techniques that help codify the impact of middleware configurations on end-to-end distributed real-time and embedded (DRE) system quality of service (QoS). First, we describe how MDD techniques can help select middleware configuration parameters that satisfy key functional and QoS requirements of DRE systems. Second, we apply our MDD techniques to empirically evaluate the end-to-end QoS of representative DRE systems in the avionics and industrial manufacturing domains. Our results show how MDD techniques significantly enhance conventional ad hoc processes used by developers to configure middleware that meets the QoS needs of DRE systems.\nTitle:\nModel-driven techniques for evaluating the QoS of middleware configurations for DRE systems\n\nAbstract:\nThis paper provides two contributions to the study of developing and applying domain-specific modeling languages (DSMLS) to distributed real-time and embedded (DRE)systems - particularly those systems using standards-based QoS-enabled component middleware. First, it describes the Platform-Independent Component Modeling Language (PICML), which is a DSML that enables developers to define component interfaces, QoS parameters and software building rules, and also generates descriptor files that facilitatesystem deployment. Second, it applies PICML to an unmanned air vehicle (UAV) application portion of an emergency response system to show how PICML resolves keycomponent-based DRE system development challenges. Our results show that the capabilities provided by PICML - combined with its design-and deployment-time validationcapabilities - eliminates many common errors associated with conventional techniques, thereby increasing the effectiveness of applying QoS-enabled component middleware technologies to the DRE system domain.\nTitle:\nA Platform-Independent Component Modeling Language for Distributed Real-Time and Embedded Systems\n\nAbstract:\nDespite advances in standards-based commercial-off-the-shelf (COTS) technologies, key challenges must be addressed before mission-critical distributed real-time and embedded (DRE) systems can be developed effectively and productively using COTS component-based software. For example, developers of DRE systems con- tinue to use ad hoc means to select and compose their applications and middleware due to the lack of formally analyzable and verifiable building block components. This chapter shows how Model-Driven Development (MDD) techniques and tools can be used to specify, analyze, optimize, synthesize, validate, and deploy standards- compliant component middleware platforms that can be customized for the needs of next-generation DRE systems. Our results show how MDD techniques and tools have been integrated successfully with standards-based QoS-enabled component middleware to significantly improve the quality and productivity associated with developing mission-critical DRE systems.\nTitle:\nModel Driven Development for Distributed Real-Time and Embedded Systems\n\nAbstract:\nModel-driven development (MDD) is gaining importance as an approach to resolving lifecycle challenges of large-scale distributed real-time and embedded (DRE) systems (e.g., avionics mission computing). DRE systems are characterized by their stringent requirements for quality of service (QoS), such as predictable end-to-end latencies, timeliness and scalability. Delivering the QoS needs of DRE systems entails the need to configure correctly, fine tune and provision the infrastructure used to host the DRE systems, which crosscuts different layers of middleware, operating systems and networks. Addressing these tangled deployment and configuration concerns of DRE systems requires integrating the principles of Aspect-Oriented Software Development (AOSD) with MDD. This demo showcases a set of software tools that resolve both the inherently and accidental complexities arising due to the configuration and deployment crosscutting concerns of component middleware-based DRE systems.\nTitle:\nAddressing crosscutting deployment and configuration concerns of distributed real-time and embedded systems via aspect-oriented & model-driven software development\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.875, mean value for all the writers is 8.648, which makes it 1.047 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nModel-Driven Development (MDD) is an emerging paradigm that uses Domain-Specific Modelling Languages (DSMLs) to provide 'correct-by-construction' capabilities for many software development activities. This paper describes a MDD toolsuite called Component Synthesis using Model-Integrated Computing (CoSMIC), a collection of DSMLs that support the development, configuration, deployment, and validation of component-based DRE systems. We also describe how we have applied CoSMIC to an avionics mission computing application to resolve key component-based DRE system development challenges. Our results show that the design-, deployment- and Quality Assurance (QA)-time capabilities provided by CoSMIC help to eliminate key complexities associated with development of QoS-enabled component middleware applications.\nTitle:", "model_inf_time": 1.73}, {"id": "41187", "output": "SAT Solving for Recursive Path Orders", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper introduces a propositional encoding for lexicographic path orders in connection with dependency pairs. This facilitates the application of SAT solvers for termination analysis of term rewrite systems based on the dependency pair method. We address two main inter-related issues and encode them as satisfiability problems of propositional formulas that can be efficiently handled by SAT solving: (1) the combined search for a lexicographic path order together with an argument filtering to orient a set of inequalities; and (2) how the choice of the argument filtering influences the set of inequalities that have to be oriented. We have implemented our contributions in the termination prover AProVE. Extensive experiments show that by our encoding and the application of SAT solvers one obtains speedups in orders of magnitude as well as increased termination proving power.\nTitle:\nSAT solving for argument filterings\n\nAbstract:\nWe introduce a propositional encoding of the recursive path order with status (RPO). RPO is a combination of a multiset path order and a lexicographic path order which considers permutations of the arguments in the lexicographic comparison. Our encoding allows us to apply SAT solvers in order to determine whether a given term rewrite system is RPO-terminating. Furthermore, to apply RPO within the dependency pair framework, we combined our novel encoding for RPO with an existing encoding for argument filters. We implemented our contributions in the termination prover AProVE. Our experiments show that due to our encoding, combining termination provers with SAT solvers improves the performance of RPO-implementations by orders of magnitude.\nTitle:\nProving Termination Using Recursive Path Orders and SAT Solving\n\nAbstract:\nThis paper presents a Prolog interface to the MiniSat satisfiability solver. Logic programming with satisfiability combines the strengths of the two paradigms: logic programming for encoding search problems into satisfiability on the one hand and efficient SAT solving on the other. This synergy between these two exposes a programming paradigm that we propose here as a logic programming pearl. To illustrate logic programming with SAT solving, we give an example Prolog program that solves instances of Partial MAXSAT.\nTitle:\nLogic programming with satisfiability\n\nAbstract:\n\n We introduce pairwise cardinality networks, networks of comparators, derived from pairwise sorting networks, which express\n cardinality constraints. We show that pairwise cardinality networks are superior to the cardinality networks introduced in\n previous work which are derived from odd-even sorting networks. Our presentation identifies the precise relationship between\n odd-even and pairwise sorting networks. This relationship also clarifies why pairwise sorting networks have significantly\n better propagation properties for the application of cardinality constraints.\n \n \nTitle:\nPairwise Cardinality Networks\n\nAbstract:\nWe describe an algorithm for proving termination of programs abstracted to systems of monotonicity constraints in the integer domain. Monotonicity constraints are a nontrivial extension of the well-known size-change termination method. While deciding termination for systems of monotonicity constraints is PSPACE complete, we focus on a well-defined and significant subset, which we call MCNP (for \"monotonicity constraints in NP\"), designed to be amenable to a SAT-based solution. Our technique is based on the search for a special type of ranking function defined in terms of bounded differences between multisets of integer values. We describe the application of our approach as the back end for the termination analysis of Java Bytecode. At the front end, systems of monotonicity constraints are obtained by abstracting information, using two different termination analyzers: AProVE and COSTA. Preliminary results reveal that our approach provides a good trade-off between precision and cost of analysis.\nTitle:\nSAT-based termination analysis using monotonicity constraints over the integers.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.62, mean value for all the writers is 8.648, which makes it 0.877 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper introduces a propositional encoding for recursive path orders (RPO), in connection with dependency pairs. Hence, we capture in a uniform setting all common instances of RPO, i.e., lexicographic path orders (LPO), multiset path orders (MPO), and lexicographic path orders with status (LPOS). This facilitates the application of SAT solvers for termination analysis of term rewrite systems (TRSs). We address four main inter-related issues and show how to encode them as satisfiability problems of propositional formulas that can be efficiently handled by SAT solving: (A) the lexicographic comparison w.r.t. a permutation of the arguments; (B) the multiset extension of a base order; (C) the combined search for a path order together with an argument filter to orient a set of inequalities; and (D) how the choice of the argument filter influences the set of inequalities that have to be oriented (so-called usable rules). We have implemented our contributions in the termination prover AProVE. Extensive experiments show that by our encoding and the application of SAT solvers one obtains speedups in orders of magnitude as well as increased termination proving power.\nTitle:", "model_inf_time": 1.17}, {"id": "41188", "output": "Cooperative-Tx/Rx FDE with Incremental Relaying for Broadband Single-Carrier Transmission", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe propose an incremental relaying scheme using joint Tx/Rx frequency-domain equalization (FDE) for single-carrier (SC) transmission. If a packet sent by a source node (S) has been correctly decoded at a relay node (R), but not at the destination node (D), retransmission is cooperatively done by S and R. Assuming that the channel state information (CSI) is shared by S, R, and D, joint Tx/Rx FDE is performed. We derive a set of optimal/suboptimal Tx/Rx FDE weights among S, R, and D, based on the minimum mean square error (MMSE) criterion under total transmit power constraint of S and R. Computer simulation verifies the effectiveness of the proposed scheme.\nTitle:\nSingle-Carrier Incremental Relaying with Joint Tx/Rx FDE\n\nAbstract:\nIn this paper, joint transmit/receive frequency-domain equalization (FDE) is proposed for analog network coded (ANC) single-carrier (SC) bi-directional multi-antenna relay. In the proposed scheme, diversity transmission using transmit FDE is performed at relay station (RS) equipped with multiple antennas while receive FDE is carried out at base station (BS) and mobile terminal (MT) both equipped with single antenna. The transmit and receive FDE weights are jointly optimized so as to minimize the end-to-end mean square error (MSE). We evaluate, by computer simulation, the throughput performance and show that the joint transmit/receive FDE obtains the spatial and frequency diversity gains and accordingly achieve better throughput performance compared to either the transmit FDE only or the receive FDE only. It is also shown that ANC SC bi-directional multi-antenna relay can extend the communication coverage area for the given required throughput compared to conventional direct transmission\nTitle:\nJoint Transmit/Receive Mmse-Fde For Analog Network Coded Single-Carrier Bi-Directional Multi-Antenna Relay\n\nAbstract:\nIn this paper, we propose a new joint transmit and receive spatial/frequency-domain filtering for single-carrier (SC) multiple-input multiple-output (MIMO) eigenmode transmission using iterative interference cancellation (IC). Iterative IC is introduced to a previously proposed joint transmit and receive spatial/frequency-domain filtering based on minimum mean square error criterion (called joint Tx/Rx MMSE filtering) to reduce the residual inter-symbol interference (ISI) after the Rx filtering. The optimal Tx/Rx filters are derived based on the MMSE criterion taking into account the iterative IC. The superiority of our proposed technique is confirmed by computer simulation.\nTitle:\nJoint Tx/Rx Mmse Filtering For Single-Carrier Mimo Eigenmode Transmission Using Iterative Interference Cancellation\n\nAbstract:\nIn this paper, we propose a joint transmit/receive frequency-domain equalization (FDE) based on minimum mean square error (MMSE) criterion for multi-input multi-output (MIMO) analog network coding (ANC) in single-carrier (SC) bi-directional relay communications. In the proposed scheme, the relay station (RS) equipped with multiple antennas carries out antenna diversity and one-tap transmit FDE, and the base station (BS) and mobile terminal (MT) receivers, both equipped with single antenna, carry out one-tap receive FDE only. The FDE weights at RS, BS and MT are jointly optimized so as to minimize the end-to-end mean square error (MSE). We evaluate, by computer simulations, the bit error rate (BER) performance when using the proposed scheme, and discuss how the transmit FDE at RS operates and affects the BER performance.\nTitle:\nJoint Transmit/Receive MMSE-FDE for MIMO Analog Network Coding in Single-Carrier Bi-Directional Relay Communications.\n\nAbstract:\nIn this paper, we propose a joint transmit and receive linear filtering based on minimum mean square error criterion (joint Tx/Rx MMSE filtering) for single-carrier (SC) multiple-input multiple-output (MIMO) transmission. Joint Tx/Rx MMSE filtering transforms the MIMO channel to the orthogonal eigenmodes to avoid the inter-antenna interference (IAI) and performs MMSE based transmit power allocation to sufficiently suppress the inter-symbol interference (ISI) resulting from the severe frequency-selectivity of the channel. Rank adaptation and adaptive modulation are jointly introduced to narrow the gap of received signal-to-interference plus noise power ratio (SINR) among eigenmodes. The superiority of the SC-MIMO transmission with joint Tx/Rx MMSE filtering and joint rank adaptation/adaptive modulation is confirmed by computer simulation.\nTitle:\nJoint Tx/Rx Mmse Filtering For Single-Carrier Mimo Transmission\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.916, mean value for all the writers is 8.648, which makes it 1.935 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we propose joint cooperative-transmit/receive frequency-domain equalization (cooperative-Tx/Rx FDE) with incremental relaying (IR) for broadband single-carrier (SC) transmission under the total and individual transmit power constraint at a source node ( S ) and a relay node ( R ). We derive the optimum cooperative-Tx/Rx FDE weights to minimize the mean square error (MSE) after packet combining at a destination node (\\ssr D ). We show that the optimum cooperative-Tx FDE weights allocate the transmit power in the minimum MSE (MMSE) sense in the frequency domain and in the maximal ratio transmission (MRT) sense in the spatial domain. To relax the condition that the complete channel state information (CSI) needs to be shared among all nodes, selection-based suboptimum cooperative-Tx FDE weights are derived. Computer simulation verifies the effectiveness of the proposed schemes and shows that the 5%-outage throughput of the system can be increased by around 30% over the conventional IR with only Rx FDE.\nTitle:", "model_inf_time": 1.79}, {"id": "41189", "output": "Modeling Imprecise User Relationships in Cooperative Information Retrieval", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nOntology-based information seeking is one of the most promising approaches to enhance existing search interfaces with features enabling users to better express their information needs or to improve exploratory search styles. This entails the interaction of users with concepts and relations embodied in ontologies in a dialogue process that can be interpreted as a query or used as a sign to suggest other paths that could lead to casual encounters. In this paper, we describe a number of ontology-enabled search tactics that have been experienced in prototype experiments, along with other possible techniques that would eventually be useful, as pointed out by existing research on information seeking models.\nTitle:\nUser Interface Tactics in Ontology-Based Information Seeking\n\nAbstract:\nRelationship marketing strategies focus on the construction and maintenance of tailored relationship with customers. Consequently, electronic commerce systems following the relationship approach may benefit from Web personalization techniques in tailoring the interaction with its users according to an evolving customer model. In this context, relationship-value market segmentation becomes a central customer modeling activity. But value segmentation categories are inherently vague due to the use of imprecise linguistic categories, combined with a degree of uncertainty about customer behavior, and the difficulty inherent to estimating intangible variables. In this paper, a fuzzy approach to value segmentation is described, allowing more flexible customer segments. Fuzzy models of value estimations are represented by fuzzy triangular numbers, and two segmentation approaches, directed and discovery-oriented are briefly described. The usefulness of the approach is then illustrated through concrete personalization techniques based on those fuzzy categories.\nTitle:\nOn fuzziness in relationship value segmentation: applications to personalized e-commerce\n\nAbstract:\nThe integration of learning objects with Semantic Web technologies requires the representation of learning object metadata in ontological databases. In this paper, some of the issues regarding the expression of learning object specifications as part of the OpenCyc terminological knowledge base are discussed, illustrating some of the advanced behaviors that are enabled by such integration.\nTitle:\nOn Integrating Learning Object Metadata inside the OpenCyc Knowledge Base\n\nAbstract:\nCurrent Semantic Web technologies provide a logic-based framework for the development of advanced, adaptive applications based on ontologies. But the experience in using them has shown that, in some cases, it would be convenient to extend its logic support to handle vague- ness and imprecision in some way. In this paper, the role of vagueness in the description of Web user interface characteristics is addressed, from the viewpoint of the design of adaptive behaviors that are connected to such descriptions. Concretely, vague descriptions combined with quanti- fied fuzzy rules and flexible connectors are described, and their usefulness is illustrated through preference modeling, filtering and adaptive linking scenarios.\nTitle:\nThe Role of Vague Categories in Semantic and Adaptive Web Interfaces.\n\nAbstract:\nClassical collaborative filtering algorithms generate recommendations on the basis of ratings provided by users that express their subjective preference on concrete items. The correlation of ratings is used in such schemes as an implicit measure of common interest between users, that is used to predict ratings, so that these ratings determine recommendations. The common formulae used for the computation of predicted ratings use standard weighted averaging schemes as the fixed aggregation mechanism that determines the result of the prediction. Nonetheless, the surrounding context of these rating systems suggest that an approach considering a degree of group consensus in the aggregation process may better capture the essence of the \u201cword\u2013of\u2013mouth\u201d philosophy of such systems. This paper reports on the empirical evaluation of such an alternative approach in which OWA operators with different properties are tested against a dataset to search for the better empirical adjustment. The resulting algorithm can be considered as a generalization of the original Pearson formula based algorithm that allows for the fitting of the aggregation behavior to concrete databases of ratings. The results show that for the particular context studied, higher orness degrees reduce overall error measures, especially for high ratings, which are more relevant in recommendation settings. The adjustment procedure can be used as a general-purpose method for the empirical fit of the behavior of collaborative filtering systems. \u00a9 2008 Wiley Periodicals, Inc.\nTitle:\nEmpirical assessment of a collaborative filtering algorithm based on OWA operators\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.688, mean value for all the writers is 8.648, which makes it 1.74 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCooperation in information retrieval contexts can be used to share query results inside groups of individuals with common objectives, provided that all of them are aware of each other. The strength of the social relationships between group members is in most cases a matter of comparative degree, and thus relationships can be modelled through fuzzy conceptual associations. These associations can then be used to implement personalized features, aimed at improving the interaction of the user with the query tool. In this paper, an approach to modelling imprecise relationships between users in the context of information retrieval is described, along with a concrete case study implemented as a wrapper of a conventional search engine, using a fuzzy database to store the model of the group members.\nTitle:", "model_inf_time": 1.26}, {"id": "41190", "output": "The ACL Anthology Network: A Networked Database of Computational Linguistics", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe ACL Anthology is a digital archive of conference and journal papers in natural language processing and computational linguistics. Its primary purpose is to serve as a reference repository of research results, but we believe that it can also be an object of study and a platform for research in its own right. We describe an enriched and standardized reference corpus derived from the ACL Anthology that can be used for research in scholarly document processing. This corpus, which we call the ACL Anthology Reference Corpus (ACL ARC), brings together the recent activities of a number of research groups around the world. Our goal is to make the corpus widely available, and to encourage other researchers to use it as a standard testbed for experiments in both bibliographic and bibliometric research.\nTitle:\nThe ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics\n\nAbstract:\nThis introduction to AI Magazine's,special issue on networks and AI summarizes the seven articles in the special issue by characterizing the nature of the networks that are the focus of each of the articles. A short tutorial on graph theory and network structures is included for those less familiar with the topic.\nTitle:\nIntroduction to the Special Issue on AI and Networks\n\nAbstract:\nWe investigate the task of generating coherent survey articles for scientific topics. We introduce an extractive summarization algorithm that combines a content model with a discourse model to generate coherent and readable summaries of scientific topics using text from scientific articles relevant to the topic. Human evaluation on 15 topics in computational linguistics shows that our system produces significantly more coherent summaries than previous systems. Specifically, our system improves the ratings for coherence by 36% in human evaluation compared to C-Lexrank, a state of the art system for scientific article summarization.\nTitle:\nSurveyor: A System For Generating Coherent Survey Articles For Scientific Topics\n\nAbstract:\nMulti-document summaries produced via sentence extraction often suffer from a number of cohesion problems, including dangling anaphora, sudden shifts in topic and incorrect or awkward chronological ordering. Therefore, the development of an automated revision process to correct such problems is a research area of current interest. We present the RevisionBank, a corpus of 240 extractive, multi- document summaries that have been manually revised to promote cohesion. The summaries were revised by six linguistic students using a constrained set of revision operations that we previously developed. In the current paper, we describe the process of developing a taxonomy of cohesion problems and corrective revision operators that address such problems, as well as an annotation schema for our corpus. Finally, we discuss how our taxonomy and corpus can be used for the study of revision-based multi-document summarization as well as for summary evaluation.\nTitle:\nRevisionBank: A Resource for Revision-based Multi-document Summarization and Evaluation\n\nAbstract:\nGraphs are a powerful representation formalism that can be applied to a variety of aspects related to language processing. We provide an overview of how Natural Language Processing problems have been projected into the graph framework, focusing in particular on graph construction - a crucial step in modeling the data to emphasize the phenomena targeted.\nTitle:\nA survey of graphs in natural language processing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.63, mean value for all the writers is 8.648, which makes it 0.015 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe introduce the ACL Anthology Network (AAN), a manually curated networked database of citations, collaborations, and summaries in the field of Computational Linguistics. We also present a number of statistics about the network including the most cited authors, the most central collaborators, as well as network statistics about the paper citation, author citation, and author collaboration networks.\nTitle:", "model_inf_time": 1.22}, {"id": "41191", "output": "Assessment of Grasp Quality by Robotic Hands", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe problem of grasping objects with a robotic hand is considered. The possibility that some or all of the fingers are not able to arbitrarily control interactions with the grasped objects is taken into account. Such defective manipulation systems can still be cooperatively coordinated so as to perform usefully. In fact, having defective arms is the norm rather than an exception in many manipulation operations, such as power grasps with a hand, or whole arm manipulation of large objects. The paper attempts to solve the problem of optimizing contact forces in the power grasp of an object. To do so, a basis of the subspace of grasp forces that are available for grasp optimization is firstly established. This analysis is instrumental for subsequent optimization strategies, incorporating the quest for an extremum of some quality criterion. A control algorithm is presented that guarantees the asymptotic convergence to the grasp force configuration that minimizes the risk of slippage while maintaining bounded contact forces between the fingers and the object\nTitle:\nAnalysis and control of power grasping\n\nAbstract:\nThe authors discuss the application of intrinsic tactile sensing (ITS) to grasp and manipulation control. A brief description of ITS, i.e., contact sensing based on force/torque measurements at fingertips, is provided. A method for using sensory feedback in the control of grasp forces to augment grasp robustness against slippage is discussed with respect to a simple grasp type; simulation and experimental data are provided. The possible generalization of this sensor-driven approach to the control of optimal grasp force in complex grasp configurations is addressed\nTitle:\nAugmentation of grasp robustness using intrinsic tactile sensing\n\nAbstract:\nIn this paper some applications of dexterous robotic hands to fine manipulation operations are discussed. Common to this kind of operations is the important role played by the frictional characteristics of the fingers and of manipulated objects. The paper discusses a procedure for measuring apparent coefficients of friction between the fingertips and manipulated objects. To take into account real contact phoenomena, the distinction is introduced between translational and rotational friction limits. Reported experiments rely on force-based (intrinsic) contact sensing devices, implemented in the phalanges of an articulated robot hand (Salisbury Hand). Data collected during these procedures can be subsequently used for tasks such as recognizing the superficial features of objects, controlling the internal grasp forces exerted by the hand on delicate objects, and following the contours of the surface of unknown objects.\nTitle:\nExperimental Evaluation of Friction Characteristics with an Articulated Robotic Hand\n\nAbstract:\nRecent work on the analysis of natural and robotic hands has introduced the notion of postural synergies as a principled organization of their complexity, based on the physical characteristics of the hand itself. Such characteristics include the mechanical arrangements of joints and fingers, their couplings, and the low-level control reflexes, that determine the specific way the concept of \"hand\" is embodied in a human being or a robot. While the focus of work done so far with postural synergies has been on motion planning for grasp acquisition, in this paper we set out to investigate the role that different embodiments have on the choice of grasping forces, and on the ultimate quality of the grasp. Numerical results are presented showing quantitatively the role played by different synergies (from the most fundamental to those of higher-order) in making a number of different grasps possible. The effect of number and types of engaged synergies on the distribution of optimal grasp forces is considered. Moreover, robustness of results is investigated with respect to variation in uncertain parameters such as contact and joint stiffness.\nTitle:\nOn the role of hand synergies in the optimal choice of grasping forces\n\nAbstract:\nSince the introduction of the first prototypes of robotic end-effectors showing manipulation capabilities, much research focused on the design and control of robot hand and grippers. While many studies focus on enhancing the sensing capabilities and motion agility, a less explored topic is the engineering of the surfaces that enable the hand to contact the object.In this paper we present the prototype of the Velvet Fingers smart gripper, a novel concept of end-effector combining the simple mechanics and control of under-actuated devices together with high manipulation possibilities, usually offered only by dexterous robotic hands. This enhancement is obtained thanks to active surfaces, i.e. engineered contact surfaces able to emulate different levels of friction and to apply tangential thrusts to the contacted object. Through the paper particular attention is dedicated to the mechanical implementation, sense drive and control electronics of the device; some analysis on the control algorithms are reported. Finally, the capabilities of the prototype are showed through preliminary grasps and manipulation experiments.\nTitle:\nImplementation And Control Of The Velvet Fingers: A Dexterous Gripper With Active Surfaces\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.615, mean value for all the writers is 8.648, which makes it 0.825 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper we discuss some aspects related to the practical assessment of the quality of a grasp by a robotic hand on objects of unknown shape, based on sensorial feedback from tactile and force sensors on the hand. We briey discuss the concept of contact and grasp robustness, pointing out that the former is an easily computable but overconservative sucien t condition for the latter. Some experimental results on a simple gripper, the so{called \\Instrumented Talon\", are reported as an illustration. This paper presents procedures for the assessment of the quality of grasps by robotic hands. The interest of having a good measure of the quality of the grasp is twofold: during planning of a manipulation sequence, it allows the optimization of the posi- tioning of the hand with respect to the object to be grasped, and the grasping forces; during the execution of a grasping task, such measure can be used as a performance index according to which local optimization techniques can be used in order to react, at least sub{optimally, to external disturbances and modeling errors.\nTitle:", "model_inf_time": 1.29}, {"id": "41192", "output": "A Realistic and Efficient Differential Fault Attack on the Grain Family of Stream Ciphers", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDifferential Fault Attack (DFA) considers injection of faults and the most general set-up should take care of faults at random location and random time. Then one should be able to identify the exact location as well as the exact timing of the fault (including the multi bit ones) with the help of fault signatures. In this paper we solve the problem of DFA under a general frame-work, introducing the idea of probabilistic signatures. The method considers the Maximum Likelihood approach related to probability distributions. Our techniques subsume all the existing DFAs against the Grain family, MICKEY 2.0 and Trivium. In the process we provide improved fault attacks for all the versions of Grain family and also for MICKEY 2.0. Our generalized method successfully takes care of the cases where certain parts of the keystream bits are missing (this situation may arise for authentication purpose). In particular, we show that the unsolved problem of identifying the faults in random time for Grain 128a can be solved in this manner. Moreover, for MICKEY 2.0, our method not only provides improvement in fault identification probability but also reduces the required faults by 60 %, compared to the best known result.\nTitle:\nProbabilistic signature based generalized framework for differential fault analysis of stream ciphers.\n\nAbstract:\nThe 32-bit MAC of Grain-128a is a linear combination of the first 64 and then the alternative keystream bits. In this paper we describe a successful differential fault attack on Grain-128a, in which we recover the Secret Key by observing the correct and faulty MACs of certain chosen messages. The attack works due to certain properties of the Boolean functions and corresponding choices of the taps from the LFSR. We present methods to identify the fault locations and then construct a set of linear equations to obtain the contents of the LFSR and the NFSR. Our attack requires less than 211 fault injections and invocations of less than 212 MAC generation routines.\nTitle:\nA differential fault attack on grain-128a using MACs\n\nAbstract:\nIn this paper we describe several ideas related to differential fault attack (DFA) on MICKEY 2.0, a stream cipher from eStream hardware profile. Using the standard assumptions for fault attacks, we first show that if the adversary can induce random single bit faults in the internal state of the cipher, then by injecting around \\(2^{16.7}\\) faults and performing \\(2^{32.5}\\) computations on an average, it is possible to recover the entire internal state of MICKEY at the beginning of the key-stream generation phase. We further consider the scenario where the fault may affect more than one (at most three) neighboring bits and in that case we require around \\(2^{18.4}\\) faults on an average to mount the DFA. We further show that if the attacker can solve multivariate equations (say, using SAT solvers) then the attack can be carried out using around \\(2^{14.7}\\) faults in the single-bit fault model and \\(2^{16.06}\\) faults for the multiple-bit scenario\nTitle:\nImproved differential fault attack on MICKEY 2.0.\n\nAbstract:\nIn this paper we study a differential fault attack against the Grain family of stream ciphers. The attack works due to certain properties of the Boolean functions and corresponding choices of the taps from the LFSR. The existing works, by Berzati et al. (2009) and Karmakar et al. (2011), are applicable only on Grain-128 exploiting certain properties of the combining Boolean function h. That idea could not easily be extended to the corresponding Boolean function used in Grain v1. Here we show that the differential fault attack can indeed be efficiently mounted for the Boolean function used in Grain v1. In this case we exploit the idea that there exists certain suitable \u03b1 such that $h(\\mathbf{x}) + h({\\mathbf x} + \\mathbf{\\alpha})$ is linear. In our technique, we present methods to identify the fault locations and then construct set of linear equations to obtain the contents of the LFSR and the NFSR. As a countermeasure to such fault attack, we provide exact design criteria for Boolean functions to be used in Grain like structure.\nTitle:\nA differential fault attack on the grain family of stream ciphers\n\nAbstract:\nIn this paper we present a differential fault attack on the stream cipher MICKEY 2.0 which is in eStream's hardware portfolio. While fault attacks have already been reported against the other two eStream hardware candidates Trivium and Grain, no such analysis is known for MICKEY. Using the standard assumptions for fault attacks, we show that if the adversary can induce random single bit faults in the internal state of the cipher, then by injecting around 216.7 faults and performing 232.5 computations on an average, it is possible to recover the entire internal state of MICKEY at the beginning of the key-stream generation phase. We further consider the scenario where the fault may affect at most three neighbouring bits and in that case we require around 218.4 faults on an average.\nTitle:\nA differential fault attack on MICKEY 2.0\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.504, mean value for all the writers is 8.648, which makes it 0.73 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe series of published works, related to differential fault attack (DFA) against the Grain family, require quite a large number (hundreds) of faults and also several assumptions on the locations and the timings of the faults injected. In this paper, we present a significantly improved scenario from the adversarial point of view for DFA against the Grain family of stream ciphers. Our model is the most realistic one so far as it considers that the cipher has to be re-keyed only a few times and faults can be injected at any random location and at any random point of time, i.e., no precise control is needed over the location and timing of fault injections. We construct equations based on the algebraic description of the cipher by introducing new variables so that the degrees of the equations do not increase. In line of algebraic cryptanalysis, we accumulate such equations based on the fault-free and faulty key-stream bits and solve them using the SAT Solver Cryptominisat-2.9.5 installed with SAGE 5.7. In a few minutes we can recover the state of Grain v1, Grain-128 and Grain-128a with as little as 10, 4 and 10 faults respectively.\nTitle:", "model_inf_time": 1.73}, {"id": "41193", "output": "A Hybrid Approach for Analyzing Customer Satisfaction Profiles", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper aims to demonstrate that established rank correlation measures are not ideally Suited for measuring rank correlation for numerical data that are perturbed by noise. We propose a. robust rank correlation measure on the basis of fuzzy orderings. The superiority of the new measure is demonstrated by means of illustrative examples.\nTitle:\nTowards a Robust Rank Correlation Measure for Numerical Observations on the Basis of Fuzzy Orderings\n\nAbstract:\nIn this paper we introduce a new approach to automatic attribute and granularity selection for building optimum regression trees. The method is based on the minimum description length principle (MDL) and aspects of granular computing. The approach is verified by giving an example using a data set which is extracted and preprocessed from an operational information system of the Components Toolshop of Volkswagen AG.\nTitle:\nAttribute value selection considering the minimum description length approach and feature granularity\n\nAbstract:\nIn this work, we revisit the curse of dimensionality, especially the concentration of the norm phenomenon which is the inability of distance functions to separate points well in high dimensions. We study the influence of the different properties of a distance measure, viz., triangle inequality, boundedness and translation invariance and on this phenomenon. Our studies indicate that unbounded distance measures whose expectations do not exist are to be preferred. We propose some new distance measures based on our studies and present many experimental results which seem to confirm our analysis. In particular, we study these distance measures w.r.t. indices like relative variance and relative contrast and further compare and contrast these measures in the setting of nearest neighbour/proximity searches and hierarchical clustering.\nTitle:\nCan Unbounded Distance Measures Mitigate The Curse Of Dimensionality?\n\nAbstract:\nDetermining the number of clusters is a crucial problem in cluster analysis. Cluster validity measures are one way to try to find the optimum number of clusters, especially for prototype-based clustering. However, no validity measure turns out to work well in all cases. In this paper, we propose an approach to determine the number of cluster based on the minimum description length principle which does not need high computational costs and is also applicable in the context of fuzzy clustering.\nTitle:\nCluster validity measures based on the minimum description length principle\n\nAbstract:\nWe introduce an objective function-based fuzzy clustering technique that assigns one influence parameter to each single data variable for each cluster. Our method is not only suited to detect structures or groups of data that are not uniformly distributed over the structure's single domains, but gives also information about the influence of individual variables on the detected groups. In addition, our approach can be seen as a generalization of the well-known fuzzy c-means clustering algorithm.\nTitle:\nFuzzy clustering with weighting of data variables\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.209, mean value for all the writers is 8.648, which makes it 0.479 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nQuestionnaires are a common tool to gain insight to customer satisfaction. The data available from such questionnaires is an important source of information for a company to judge and improve its performance in order to achieve maximum customer satisfaction. Here, we are interested in finding out, how much individual customer segments are similar or differ w.r.t. to their satisfaction profiles. We propose a hybrid approach using measures for the similarity of satisfaction profiles based on principles from statistics in combination with visualization techniques. The applicability and benefit of our approach is demonstrated on the basis of real-world customer data.\nTitle:", "model_inf_time": 1.02}, {"id": "41194", "output": "Numerical Stability Analysis of Fuzzy Control Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAbstract Fuzzy systems are currently being used in a wide field of industrial and scientific applications. Since the design and especially the optimization process of fuzzy systems can be very time consuming, it is convenient to have algorithms which construct and optimize them automatically. One popular approach is to combine fuzzy systems with learning techniques derived from neural networks. Such approaches are usually called neuro-fuzzy systems. In this paper we present our view of neuro-fuzzy systems and an implementation,in the area of control theory: the NEFCON-Model. This model is able to learn and optimize the rule base of a Mamdani like fuzzy controller online by a reinforcement learning algorithm that uses a fuzzy error measure. Therefore, we also describe some methods to determine a fuzzy error measure for a dynamic,system. In addition we present some implementations,of the model and an application example. The presented implementations,are available free of charge for non-commercial purposes. Key words Hybrid methods; Neuro-fuzzy system; System\nTitle:\nNeuro-fuzzy control based on the NEFCON-model: recent developments\n\nAbstract:\nThis paper provides an overview of fuzzy systems from the viewpoint of similarity relations. Similarity relations turn out to be an appealing framework in which typical concepts and techniques applied in fuzzy systems and fuzzy control can be better understood and interpreted. They can also be used to describe the indistinguishability inherent in any fuzzy system that cannot be avoided.\nTitle:\nThe Inherent Indistinguishability in Fuzzy Systems\n\nAbstract:\nA kind of neural network architecture designed for control tasks is presented. It is called the fuzzy neural network. The structure of the network can be interpreted in terms of a fuzzy controller. It has a three-layered architecture and uses fuzzy sets as its weights. The fuzzy error backpropagation algorithm, a special learning algorithm inspired by the standard BP-procedure for multivariable neural networks, is able to learn the fuzzy sets. The extended version that is presented is also able to learn fuzzy-if-then rules by reducing the number of nodes in the hidden layer of the network. The network does not learn from examples, but by evaluating a special fuzzy error measure\nTitle:\nA fuzzy neural network learning fuzzy control rules and membership functions by fuzzy error backpropagation\n\nAbstract:\n . Fuzzy controllers are designed to work with knowledge inthe form of linguistic control rules. But the translation of these linguisticrules into the framework of fuzzy set theory depends on the choiceof certain parameters, for which no formal method is known. The optimizationof these parameters can be carried out by neural networks,which are designed to learn from training data, but which are in generalnot able to profit from structural knowledge. In this paper we discuss... \nTitle:\nCombining Neural Networks and Fuzzy Controllers\n\nAbstract:\nWe present a neuro-fuzzy architecture for function approximation based on supervised learning. The learning algorithm is able to determine the structure and the parameters of a fuzzy system. The approach is an extension to our already published NEFCON and NEFCLASS models which are used for control or classification purposes. The proposed extended model, which we call NEFPROX, is more general and can be used for any application based on function approximation.\nTitle:\nNeuro-fuzzy systems for function approximation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.036, mean value for all the writers is 8.648, which makes it 0.522 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAn important field of application for fuzzy control is plants of which no parametrical model exists or which include important nonlinear elements. In such systems, it is difficult to describe the behavior of a fuzzy controller by analytical equations. As a result, it is hard to deal with the stability of a closed-loop system containing a fuzzy controller. As a possibility to solve this problem, we introduce a numerical algorithm that does not need analytical descriptions of the plant or of the controller. The algorithm computes the trajectories in a discretized state space to examine stability in the Lyapunov sense.\nTitle:", "model_inf_time": 1.06}, {"id": "41195", "output": "A Multi-Agent SLA Negotiation Protocol for Cloud Federations", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this work, we propose an algorithm, called E-Learning Ontology Enrichment (ELOE), to derive a global representation from the personal ontologies of different agents present in an e-Learning MAS. Using ELOE, each agent of a MAS-based e-Learning system can autonomously enrich its own ontology by using semantic negotiation and, at the same time, it can access to the global ontology to have a view of the terms used by all the other agents. Each term of the global ontology is associated with a set of meanings, and each meaning is associated with the set of agents that have used it. This way an agent, having to send a message to an interlocutor, is able of choosing from the global ontology the most suitable term with the most appropriate meaning. Only if the agent does not find in the global ontology the appropriate term, it will use a personal term that probably will lead to a new semantic negotiation process. This way, the use of the onerous task of the semantic negotiation will be limited to only the strictly necessary situations, and consequently the whole communication cost is decreased.\nTitle:\nUsing Semantic Negotiation for Ontology Enrichment in e-Learning Multi-agent Systems\n\nAbstract:\nCloud Computing is the reference model for IT specialists offering high quality services in a pay-per-use fashion. Architectures and protocols for Cloud Federations have been designed to meet emerging requirements of Cloud infrastructure. In such a context IT providers can mutually rent virtual resources and exchange metadata in order to get more opportunity. In this paper we define VST - Virtual machine Software Tracking - as the process of extracting detailed information about software running in virtual machines in a transparent manner. Coupling VST data with those of resource consumption can help IaaS providers to be more precise when assigning resources to VMs. Moreover, sharing VST information across the cloud federation can help IaaS providers to get decision about VM placement in time. For this aim we provide an agent based architecture for collecting and sharing VST data in a Cloud Federation, together with a skeleton implementation by which the architecture has been tested.\nTitle:\nAn Agent Based Architecture for VM Software Tracking in Cloud Federations\n\nAbstract:\nCloud Computing is a versatile computing paradigm capable of attracting a wide variety of applications. However, the necessity to provide a wide range of complex services led providers to establish mutual agreements to provide large-scale distributed multi-cloud environments. Providers gain the opportunity to compose service workflows that are effective and efficient, taking resources of their own competitors, and the capability to satisfy unexpected workload peaks. In this paper, we propose a reputation-based model aiming at supporting the service composition by considering measures of QoS collected by the measuring systems, and reputation measures collected with the customers by means of users feedback.\nTitle:\nA Reputation-Based Approach to Improve QoS in Cloud Service Composition\n\nAbstract:\nIn order to use remote cloud-based services, existing legacy applications need to be adapted, hence partially rewritten to include calls to such services and take decisions on the basis of the dynamic state of these services. This knowledge on cloud services makes applications run smoothly, however it also makes applications more complex. In this paper we propose a general software architecture, making use of aspect-orientation, that allows storage-related code within applications to be redefined in order to integrate applications with appropriate services available on several clouds. Our provided components allow integration while keeping cloud-and application-related concerns well separated, hence obtaining modularity. Additional components provide applications with enhanced features, such as monitoring the behaviour of cloud resources, connecting with several cloud providers, and generating replicas of most accessed contents. As a result, the availability of contents will be increased, i.e., in a given moment the most unloaded cloud service will be automatically selected and accessed.\nTitle:\nEnhancing applications with cloud services by means of aspects\n\nAbstract:\nIn this paper we analyse the typical inter-cloud scenario, with particular reference to the negotiation of a CSA (Cloud Service Agreement). Thereafter, the roles of the main players - customers, providers, brokers, and auditors - are taken into account along with reciprocal trust relationships. On this basis, we propose a hybrid model for ranking cloud services. By means of the proposed model we take into account QoS measurements performed by independent Brokers plus certifications made by Auditors. Furthermore, in addition to objective QoS measures, the described model represents an attempt to integrate a reputation system which involves the community of Cloud users in a typical inter-cloud scenario. Indeed, they are supposed to interact with brokers in order to release feedbacks based on their direct experience, and send opinion (recommendations) to potential Cloud customers.\nTitle:\nA Hybrid Model for Ranking Cloud Services.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.38, mean value for all the writers is 8.648, which makes it 0.625 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAbstractThe emergence of complex cloud services and the growing number of QoS metrics entail the need of automation to transform business requirements into QoS constraints. Furthermore, in the emergent context of Cloud Federations providers need to share knowledge to improve interoperability when negotiating SLA Service Level Agreements. Intelligent software agents are capable of supporting activities related to SLA negotiation, as well as sharing knowledge into large-scale distributed environments. Since they are usually provided with different ontologies, understanding the meaning of the messages requires the availability of common, global ontologies among customers and providers. In order to deal with this issue, we discuss a multi-agent SLA negotiation protocol which does not need a common global agent ontology. The protocol takes into account different concerns related to the understanding of semantic and technical terms, and the knowledge about the agent abilities on semantic and technical terms are shared among the Cloud Federation.\nTitle:", "model_inf_time": 1.43}, {"id": "41196", "output": "Fault-Tolerant Self-Organizing Flocking for UAV Monitoring Missions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we analyze the problem of self-organization for a flock of multirotor UAVs out on a monitoring mission. Such a mission consists in, basically, acquiring data relevant to a certain area of the terrain and transmitting them to a Base Station.\nTitle:\nA fault-tolerant self-organizing flocking approach for UAV aerial survey.\n\nAbstract:\nThis paper presents a new approach for resource allocation in a grid based on \"spatial computing\" concepts. We model a grid using a flat architecture consisting of nodes connected by an overlay network. The 2D spatial distribution of the nodes of the grid, together with the quantity of resource available in each node, forms a 3D surface, where valleys correspond to nodes with a large quantity of available resource. We propose an algorithm for resource allocation that is based on surfing such a 3D surface, in search for the deepest valley (global minimum). The algorithm, which aims at fairly distributing among nodes the quantity of leased resource, is based on some heuristics that mimic the laws of kinematics. Experimental results show the effectiveness of the algorithm.\nTitle:\nA decentralized strategy for resource allocation\n\nAbstract:\nThis paper presents some experimental results on the evaluation of a resource finding algorithm for a peer-to- peer Grid. The algorithm, which is based on \"spatial com- puting\" concepts, models a Grid as a P2P overlay network, on which resources are obtained by a suitable navigation strategy that forwards a request from node to node, un- til one is found offering the desired amount of resources. The algorithm is experimentally evaluated by means of a software simulator, and fully characterised by assessing the performances afforded by different navigation strategies and the influence of several parameters, such as the number of nodes, their workload, the origin (node) of the resource request, etc.\nTitle:\nEvaluating Strategies for Resource Finding in a Peer-to-Peer Grid\n\nAbstract:\nThis paper presents a resource finding and allocation scheme for Grid-/Cloud-computing systems, called SW-HYGRA (Small-World HYperspace-based Grid Resource Allocation). It models the overall system using a decentralized peer-to-peer approach organizing the nodes in an overlay network featuring specific convenient properties. SWHYGRA intends to exploit the small-world effect, by employing an algorithm aimed at building an overlay network whose structure mimics the small-world model. Such networks feature a high clustering degree coupled with a very low average path length, both being key characteristics for the SW-HYGRA approach. The former allows nodes with similar resource availability to be clustered; together with the latter, it permits a resource finding algorithm to reach a target node in a fast and efficient way.\nTitle:\nExploiting the Small-World Effect for Resource Finding in P2P Grids/Clouds\n\nAbstract:\nFinding the position of a mobile agent in a wide distributed system still represents an open research issue. The various existing mobile agent platforms implement ad hoc naming and location policies, studied to address the requirements of the design choices made. This paper proposes a naming scheme and a location protocol of general validity for mobile agents able to effectively meet all the typical requirements of mobile agent environments and, thus, easy to integrate into different platforms. The paper identifies the main characteristics which an agent naming scheme and a location protocol of general validity should have, and suggests some properties and parameters to be taken into account to evaluate the effectiveness of naming schemes and location protocols. Then, we propose a \u9a74human readable\u9a74 agent naming scheme based on the distributed environment outlined in MASIF, and a suitable location finding protocol called the Search-By-Path-Chase. Both of them are compared with some of the solutions already provided, using the properties and the parameters suggested. The performances are finally evaluated by means of a set of measurements.\nTitle:\nLocating mobile agents in a wide distributed environment\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.612, mean value for all the writers is 8.648, which makes it 0.031 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper describes an algorithm aiming at coordinating of a set of multirotor UAVs to self-organise in order to create a flock performing a monitoring mission. The aim of the flock is to acquire data relevant to a certain area of the terrain and transmit them to a Base Station. Two basic aspects are taken into account: (i) to plan a proper path which minimises mission time, and (ii) to identify possible faults in one or more multirotors, taking care of performing a re-scouting of terrain regions in order to avoid loss of data. The algorithm uses same rules to ensure flocking formation and area coverage. It is validated using a software simulator to measure and report some parameters useful to understand the performances and effectiveness of the approach.\nTitle:", "model_inf_time": 1.41}, {"id": "41197", "output": "Enhancing Web Server QoS Through Aspect-Oriented Design", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe handling of Quality of Service (QoS) parameters on a web server is a concern that generally spreads across the several modules responsible for fetching and delivering contents as per incoming requests. In order to avoid tangling between QoS and communication-related concerns, this paper proposes the use of aspect-orientation to handle QoS on top of an existing web server, Jigsaw. We show how an aspect-based architecture can be effective for providing the web server with several improvements when processing incoming requests. QoS parameters have been handled by associating requests with priorities and by introducing into the web server checks on resource-usage and tasks to be executed. Suggested aspects are connected at compile-time to existing classes, hence keeping the QoS-enforcing code separated from web server modules.\nTitle:\nAugmenting a Web Server with QoS by Means of an Aspect-Oriented Architecture\n\nAbstract:\nAn internet application typically consists of a business logic complemented by additional concerns, such as communication, security and higher level features such as dependability. We advocate the use of aspect orientation as an advanced concern separation technology, ensuring higher modularity and reusability of code. In this paper we show how such an approach can be put into practice, by enhancing an existing web application with a dependability layer, addressing availability, reliability and data integrity concerns. Specifically, we have produced a set of aspects and other suitable components which can be smoothly integrated into an existing web server, endowing it with the ability to redirect service requests under excessive load. Redirection may target either cloud-powered lightweight replicas of the main web server, or publicly exposed browsing caches from clients. Experiments were conducted on the W3C Jigsaw web server.\nTitle:\nKaqudai: A Dependable Web Infrastructure Made Out of Existing Components\n\nAbstract:\nIn order to use remote cloud-based services, existing legacy applications need to be adapted, hence partially rewritten to include calls to such services and take decisions on the basis of the dynamic state of these services. This knowledge on cloud services makes applications run smoothly, however it also makes applications more complex. In this paper we propose a general software architecture, making use of aspect-orientation, that allows storage-related code within applications to be redefined in order to integrate applications with appropriate services available on several clouds. Our provided components allow integration while keeping cloud-and application-related concerns well separated, hence obtaining modularity. Additional components provide applications with enhanced features, such as monitoring the behaviour of cloud resources, connecting with several cloud providers, and generating replicas of most accessed contents. As a result, the availability of contents will be increased, i.e., in a given moment the most unloaded cloud service will be automatically selected and accessed.\nTitle:\nEnhancing applications with cloud services by means of aspects\n\nAbstract:\nThe server side of business software systems is commonly implemented today by an ensemble of Java classes distributed over several hosts. In this scenario, it is often necessary, for performance tuning or bug fixing, to update the code or change the location of some classes. Since business systems must typically stay on-line 24 hours a day, changes and updates should be made without stopping system execution.This paper proposes a distributed software architecture which clearly separates the functionalities of the server-side application from its on-line adaptation capabilities. As a result, developers are freed from considering adaptation concerns, which are instead provided by separate, application-independent, transparently integrated components. The latter analyse data related to the operational conditions of the application, and, based on available statistics and expected behaviour, trigger changes on the application classes.The bytecode of classes expected to need on-line updating is modified at load time, so as to insert hooks that will support run-time changes. No tampering with class files is required. Particular care has been taken to ensure the type-compatibility of classes thus manipulated.\nTitle:\nHandling run-time updates in distributed applications\n\nAbstract:\nTypically, real-time applications are developed by introducing real time concerns into functionality concerns. This leads to difficulties in the reuse of applications, and to limited capabilities of the underlying runtime system to adapt itself for the sake of meeting the desired deadline. In this paper we explore an approach that supports the automated development of real-time aware applications. This is made possible by a tool that performs a suitable analysis and transformation, aiming to automatically incorporate real-time concerns into an unaware application. The analysis serves to identify, within the application, subtasks and (possibly) subsubtasks, together with their minimum and maximum execution times and partial deadlines. The transformation has the twofold aim of monitoring critical timing aspects of the execution, and adapting the application on-the-fly to help meeting the deadlines. Adaptation measures involve not only task scheduling, but also the substitution of application parts, as well as the activation of additional (possibly distributed) hardware and software resources.\nTitle:\nSeparating Soft Real-Time from Functionality Concerns in Complex Object-Oriented Applications\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.042, mean value for all the writers is 8.648, which makes it 1.189 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe main responsibilities of a web server are to listen from the communication channel and to prepare replies to requests. Additional responsibilities include adapting processing activities, for example, through scheduling or request filtering, so as to satisfy Quality of Service QoS requirements. Typical QoS-related concerns address behavioural constraints e.g. response time bounds, satisfiable by scheduling the most urgent requests first and resource monitoring, for optimal use. Although such concerns are spread across several web server components, they should be handled separately from communication-related ones, for the sake of modularity.\nTitle:", "model_inf_time": 1.38}, {"id": "41198", "output": "Reconstructive Explanation: Explanation as Complex Problem Solving", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nExisting explanation facilities are typically far more appropriate for knowledge engineers engaged in system maintenance than for end-users of the system. This is because the explanation is little more than a trace of the detailed problem-solving steps. An alternative approach recognizes that an effective explanation often needs to substantially reorganize the line of reasoning and bring to bear additional information to support the result. Explanation itself becomes a complex problem-solving process that depends not only on the line of reasoning, but also on additional knowledge of the domain. We present a new computational model of explanation and argue that it results in significant improvements over traditional approaches.\nTitle:\nReconstructive explanation: explanation as complex problem solving\n\nAbstract:\nExpertise in fault diagnosis often depends on recog\u00ad nizing particular patterns in observed data correspond\u00ad ing to situations that have previously been 6een and correctly interpreted This approach can result in significant efficiencies by avoiding a costly and detailed analysis based on the causal relationships between faults and observable data Recognition-based reason\u00ad ing requires highly focused search strategies in order to extract relevant information from a knowledge base that contains a large number of prototypes for various possible faults This paper describes a model of recognition-based reasoning and describes an imple\u00ad mentation of the model. designed to solve diagnostic problems in pediatric cardiology The model first analyzes portions of the patient data to hypothesize a small set of potential diseases Knowledge specific to each selected hypothesis is then used to refine these initial choices The model has been validated against actual hospital cases and performs in a manner compar\u00ad able to expert physicians\nTitle:\nRecognition-Based Diagnostic Reasoning\n\nAbstract:\nFault localization in program debugging is the process of identifying program statements which cause anomalous behavior. We have developed a prototype, knowledge-based model of the fault localization process. Novel features of the model include the integration of prototypic and causal reasoning about faults and a recognition-based mechanism for program abstraction. An explicit division of knowledge from the applications, programming, and language domains facilitate model tuning within as well as across applications domains. We describe model structure and performance for a class of faults associated with master file update programs.\nTitle:\nKnowledge-based fault localization in debugging\n\nAbstract:\nWe present a diagnostic model of software fault localization. A diagnoatic. approach to fault localization has proven effective In the domains of medicine and digital hardware. Applying this approach to the software domain requires two extensions: a heuristic abstraction mechanism which infers program function from structure using recognition and transformation tactics; and a search mechanism which integrates both prototypic and causal reasoning about faults.\nTitle:\nDiagnostic reasoning in software fault localization\n\nAbstract:\nProblems which Induce performance that has the false appearance of success (garden path problems) may be an Inevitable consequence of the need (In both human and computer problem solving systems) to create abstract knowledge representations In order to make problem solving efficient. An example is presented from a domain of physics problem solving tasks in which a hierarchical organisation of lines of reasoning leads to errors of the garden path type. Several aspects of a possible model of the problem solving process In these tasks are briefly outlined.\nTitle:\nStrolling down the garden path: error prone tasks in expert problem solving\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.789, mean value for all the writers is 8.648, which makes it 0.12 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nExisting explanation facilities are typically far more appropriatefor knowledge engineers engaged in system maintenance than for end-usersof the system. This is because the explanation is little more than atrace of the detailed problem-solving steps. An alternative approachrecognizes that an effective explanation often needs to substantiallyreorganize the actual line of reasoning and bring to bear additionalinformation to support the result. Explanation itself becomes a complexproblem-solving process that depends not only on the actual line ofreasoning, but also on additional knowledge of the domain. This paperpresents a new computational model of explanation and argues that itresults in significant improvements over traditional approaches.\u2014Authors' Abstract\nTitle:", "model_inf_time": 1.21}, {"id": "41199", "output": "Lower Bounds on the Global Linear Complexity of Nonlinearly Filtered PN-Sequences", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAn efficient algorithm for computing lower bounds on the global linear complexity of nonlinearly filtered PN-sequences is presented. The technique here developed is based exclusively on the realisation of bitwise logic operations, which makes it appropriate for both software simulation and hardware implementation. The algorithm can be applied to any arbitrary nonlinear function with a unique term of maximum order. Thus, the extent of its application for different types of filter generators is quite broad. Furthermore. emphasis is on the large lower bounds obtained that confirm the exponential growth of the global linear complexity for the class of nonlinearly filtered sequences.\nTitle:\nGlobal Linear Complexity Analysis Of Filter Keystream Generators\n\nAbstract:\nA method of analysis for the linear complexity of nonlinearly filtered PN-sequences is presented. The procedure provides a general lower bound for the linear complexity and an algorithm to improve it. The results obtained are valid for any nonlinear function with a unique term of maximum order and for any maximal-length LFSR. This work, which has as starting point the root presence test by Rueppel, is based on the handling of binary strings instead of determinants in a finite field.\nTitle:\nOn the Linear Complexity of Nonlinear Filtered PN-sequences\n\nAbstract:\nThe application of a nonlinear filtering function to a Linear Feedback Shift Register (LFSR) is a general technique for designing pseudorandom sequence generators with cryptographic application. In this paper, we investigate the equivalence between different nonlinear filtering functions applied to distinct LFSRs. It is a well known fact that given a binary sequence generated from a pair (nonlinear filtering function, LFSR), the same sequence can be generated from any other LFSR of the same length by using another filtering function. However, until now no solution has been found for the problem of computing such an equivalent. This paper analyzes the specific case in which the reciprocal LFSR of a given register is used to generate an equivalent of the original nonlinear filtering function. The main advantage of the contribution is that weaker equivalents can be computed for any nonlinear filter, in the sense that such equivalents could be used to cryptanalyze apparently secure generators. Consequently, to evaluate the cryptographic resistance of a sequence generator, the weakest equivalent cipher should be determined and not only a particular instance.\nTitle:\nWeak Equivalents for Nonlinear Filtering Functions\n\nAbstract:\nThis work shows that sequences generated by a class of linear cellular automata equal output sequences of certain nonlinear sequence generators A simple modelling process for obtaining the automata from a partial description of such generators is here described Furthermore, a method that uses the linearity of these cellular models for reconstructing some deterministic bits of the keystream sequence is presented.\nTitle:\nConcatenated automata in cryptanalysis of stream ciphers\n\nAbstract:\nA new class of linear sequence generators based on cellular automata is here introduced in order to model several nonlinear keystream generators with practical applications in symmetric cryptography. The output sequences are written as solutions of linear difference equations, and three basic properties (period, linear complexity and number of different output sequences) are analyzed.\nTitle:\nUsing Linear Difference Equations To Model Nonlinear Cryptographic Sequences\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.4, mean value for all the writers is 8.648, which makes it 0.212 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAn algorithm for computing lower bounds on the global linear complexity of nonlinearly filtered PN-sequences is presented. Unlike the existing methods, the algorithm here presented is based on the realization of bit wise logic operations. The numerical results obtained are valid for any nonlinear function with a unique term of maximum order and for any maximal-length LFSR. To illustrate the power of this technique, we give some high lower bounds that confirm Rueppel's conclusion about the exponential growth of the linear complexity in filter generators.\nTitle:", "model_inf_time": 1.4}, {"id": "41200", "output": "Only Knowing with Noisy Sensors and Actions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nUncertainty seems to be inherent in most robotic applications. This is because a robot's sensors and actuators are in general imprecise and prone to error. The logic-based action language GOLOG was introduced for the purpose of high-level robot control, but its usefulness was limited because it did not address uncertainty. In this paper we show how this deficiency can be overcome.\nTitle:\nProbabilistic complex actions in GOLOG\n\nAbstract:\nAbstract: In traditional formal approaches to knowledge representation, agents are assumed to believe all the logical consequences of their knowledge bases. As a result, reasoning in the first-order case becomes undecidable. Since real agents are constrained by resource limitations, it seems appropriate to look for weaker forms of reasoning with better computational properties. One way to approach the problem is by modeling belief. Reasoning can then be understood as the question whether a belief follows ...\nTitle:\nLimited reasoning in first-order knowledge bases\n\nAbstract:\nThe idea of \\\"only knowing\\\" a collection of sentences, as proposed by Levesque, has been previously shown to be very useful in characterizing knowledge-based agents: in terms of a specification, a precise and perspicuous account of the beliefs and non-beliefs is obtained in a monotonic setting. Levesque's logic is based on a first-order modal language with quantifying-in, thus allowing for de re versus de dicto distinctions, among other things. However, the logic and its recent dynamic extension only deal with the case of a single agent. In this work, we propose a first-order multiagent framework with knowledge, actions, sensing and only knowing, that is shown to inherit all the features of the single agent version. Most significantly, we prove reduction theorems by means of which reasoning about knowledge and actions in the framework simplifies to non-epistemic, non-dynamic reasoning about the initial situation.\nTitle:\nMultiagent only knowing in dynamic systems\n\nAbstract:\nActions depend crucially on what an agent knows and does not know. For example, an action may have a precondition that requires knowing the referent of a term, which is generally referred to as knowing who or knowing what. Alternatively, executing a sense action may be the result of realizing that the referent of a term is not known yet. The latter requires an agent to reason about all it knows about the world. This concept, also called only knowing, has been studied using possible-world semantics, yet only in the static case. One of the best understood action formalisms is the situation calculus. Moreover, it also comes equipped with a possible-world model of knowledge, which has led to deep insights into the relationship between action and knowledge. Adding only knowing to the situation calculus, which is the topic of this paper, turns out to be much more problematic than in the case of adding knowledge. It requires a reconstruction of the situation calculus itself by first developing a possible-world model of action and then interpreting situations explicitly as possible worlds. The properties of the situation calculus, which normally need to be stipulated axiomatically, are shown to be valid formulas in our model. More importantly, only knowing is fully integrated into the action formalism.\nTitle:\nOnly Knowing in the Situation Calculus\n\nAbstract:\nLevesque introduced the notion of \\\"only knowing\\\" to precisely capture the beliefs of a knowledge base. He also showed how only knowing can be used to formalize nonmonotonic behavior within a monotonic logic. Levesque's logic only deals with a single agent, and therefore, a number of attempts have been made to generalize only knowing to the many agent case. However, all these attempts have some undesirable features. Most significantly, these attempts are propositional and it is not clear how they are to be extended to the first-order case. In this work, we propose a new semantical account of multiagent only knowing which, for the first time, has a natural possible-world semantics for a quantified language with equality. Among other things, properties about Levesque's logic generalize faithfully to the many agent case with this account. For the propositional fragment, we also provide a sound and complete axiomatization. Finally, we obtain a multiagent first-order version of the nonmonotonicity exhibited by the logic of only knowing.\nTitle:\nSemantical considerations on multiagent only knowing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.614, mean value for all the writers is 8.648, which makes it 0.029 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWhen reasoning about actions and sensors in realistic domains, the ability to cope with uncertainty often plays an essential role. Among the approaches dealing with uncertainty, the one by Bacchus, Halpern and Levesque, which uses the situation calculus, is perhaps the most expressive. However, there are still some open issues. For example, it remains unclear what an agent's knowledge base would actually look like. The formalism also requires second-order logic to represent uncertain beliefs, yet a first-order representation clearly seems preferable. In this paper we show how these issues can be addressed by incorporating noisy sensors and actions into an existing logic of only-knowing.\nTitle:", "model_inf_time": 1.27}, {"id": "41201", "output": "On Relationships Between Attributes With Uncertain Values", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe briefly introduce the fuzzy measure and then discuss its use in representing information about uncertain variables. A relationship between the fuzzy measure and the Dempster-Shafer belief structure is discussed and a method for generating the family of fuzzy measures associated with a belief structure is described. We discuss the use of the Shapley index as a means for introducing an extension of the concept of entropy to fuzzy measures called the Shapley entropy. It is shown that all fuzzy measures generated from a given Dempster-Shafer belief structure have the same value for their Shapley entropy. We introduce the cardinality index of a fuzzy measure and use it to define the attitudinal character of a fuzzy measure. A semantics for this attitudinal character in the framework of using fuzzy measures to represent information about uncertain variables is suggested.\nTitle:\nOn The Cardinality Index And Attitudinal Character Of Fuzzy Measures\n\nAbstract:\nWe introduce the concept of a fuzzy measure and describe the process of combining fuzzy measures to form new measures. We discuss the role of fuzzy measures in modeling uncertain information and its use in modeling granular uncertain information with the aid of measure based belief structures. We turn to the problem of fusing multiple measure based belief structures. First we look at the case when the belief structures being fused have the same focal elements. Then we turn to case where the structures being fused have different focal elements. Finally we compare measure-based fusion with Dempster' s rule.\nTitle:\nOn The Fusion Of Multiple Measure Based Belief Structures\n\nAbstract:\nWe discuss the need for tools for representing various types of uncertain information in decision-making. We introduce the Dempster-Shafer belief structure and discuss how it provides a formal mathematical framework for representing various types of uncertain information. We provide some fundamental ideas and mechanisms related to these structures. We then investigate their role in the important task of decision-making under uncertainty.\nTitle:\nDempster-Shafer belief structures for decision making under uncertainty\n\nAbstract:\nWe first describe the basics of fuzzy systems modeling. Fundamental to this is a collection of rules, a rule base, in which the rule antecedents are fuzzy subsets. We first look at issue of the determination of the firing level of a rule for fuzzy set inputs and the subsequent rule base output. We next consider the situation where the system input is uncertain and modeled by a Dempster\u2013Shafer belief structure. Here our input is a collection of fuzzy subsets and the true input fuzzy set is selected based on a probability distribution over these potential input fuzzy sets. We next consider the situation where our input is modeled via a generalized belief structure where the determination of applicable input fuzzy set is modeled via a measure over these potential input fuzzy sets.\nTitle:\nFuzzy rule bases with generalized belief structure inputs.\n\nAbstract:\nWe look at the transformation of information via mappings under various types of uncertainties associated with the input and output variables. We first look at mappings in which the input uniquely determines the output and investigate how various types of uncertain input information reflect into the output variable. Here, we use a measure representation for our input variable uncertainty. We next consider the case where we have an additional uncertainty in the output of the mapping, instead of it being a unique value it is a set of possible values. The special case we see here is the Dempster-Shafer belief structure. We further generalize the situation by allowing our knowledge of the output of the mapping to be expressed in terms of a measure theoretic formulation. Here, we provide a very general formulation for second-order uncertainties.\nTitle:\nDempster-Shafer Structures With General Measures\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.966, mean value for all the writers is 8.648, which makes it 0.582 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nRelationships between attribute values can involve concepts such as similar or greater then. We first discuss the mathematical structure of relationships. We consider both crisp and fuzzy relationships. We introduce some important examples of mathematical relationships and describe their formal properties. We then turn to the problem of calculating the degree of relationship between attributes whose values are uncertain. We first consider the case when the uncertainties in the attribute values are expressed in terms of possibility distributions. We next look at the case when the uncertainties are in the form of a standard Dempster\u2013Shafer belief structure. We finally consider the situation when the uncertain information associated with the attribute values are expressed in terms of Dempster-Shafer belief structures whose focal elements are fuzzy sets.\nTitle:", "model_inf_time": 1.11}, {"id": "41202", "output": "Selective Encryption of 3D Object Binary Formats for Visual Confidentiality", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCryptographic techniques are used to secure confidential data from unauthorized access but these techniques are very sensitive to noise. A single bit change in encrypted data can have catastrophic impact over the decrypted data. This paper addresses the problem of removing bit error in visual data which are encrypted using AES algorithm in the CBC mode. In order to remove the noise, a method is proposed which is based on the statistical analysis of each block during the decryption. The proposed method exploits local statistics of the visual data and confusion/diffusion properties of the encryption algorithm to remove the errors. Experimental results show that the proposed method can be used at the receiving end for the possible solution for noise removing in visual data in encrypted domain.\nTitle:\nNoise removing in encrypted color images by statistical analysis\n\nAbstract:\nCryptographic techniques are used to secure confidential data from unauthorized access, but these techniques are very sensitive to noise. A single bit change in encrypted data may have catastrophic impact over the decrypted data. This paper addresses the problem of removing bit error in visual data which are encrypted using AES algorithm by block. In order to remove the noise, three statistical analyses are proposed which are based on Global variance, Mean local variance and Sum of squared derivative. These methods exploit local statistics of the visual data and confusion/diffusion properties of the encryption algorithm to correct the errors. Experimental results show that the proposed approaches can be used at the receiving end for the possible solution for error correction in visual data in encrypted domain.\nTitle:\nDecryption of noisy encrypted images by statistical analysis\n\nAbstract:\nThis paper presents a novel method for the real-time protection of new emerging High Efficiency Video Coding (HEVC) standard. Structure preserving selective encryption is being performed in CABAC entropy coding module of HEVC, which is significantly different from CABAC entropy coding of H.264/AVC. In CABAC of HEVC, exponential Golomb coding is replaced by truncated Rice (TR) up to a specific value for binarization of transform coefficients. Selective encryption is performed using AES cipher in cipher feedback mode on a plaintext of binstrings in a context aware manner. The encrypted bitstream has exactly the same bit-rate and is format complaint. Experimental evaluation and security analysis of the proposed algorithm is performed on several benchmark video sequences containing different combinations of motion, texture and objects.\nTitle:\nInvestigating The Structure Preserving Encryption Of High Efficiency Video Coding (Hevc)\n\nAbstract:\nIn this paper we propose a new approach to protect video sequences while using selective encryption (SE) and reducing the encryption ratio (ER). Several methods of SE have been applied to video codec H.264/AVC in CAVLC mode in order to perform confidentiality, bit-rate, and data-size of protected video-sequences. In our scheme, SE-CAVLC is used but ER is decreased while preserving the confidentiality of the videos. Prediction error of H.264/AVC is used to spread a selective encryption through each frame, which allows the selection of just a part of the macro-blocks to encrypt.\nTitle:\nFast protection of H.264/AVC by reduced selective encryption of CAVLC\n\nAbstract:\nIn this paper, a novel method for fast protection of AVS video coding standard alongwith compression is presented. Here the problems of compression and selective encryption (SE) have been simultaneously addressed for AVS part-2 Jizhun profile. It is performed in the context-based 2D variable length coding (C2DVLC) module of video codec. SE is performed by using the Advanced Encryption Standard (AES) algorithm with the Cipher Feedback (CFB) mode on a subset of codewords. C2DVLC serves the purpose of encryption step without affecting the coding efficiency of AVS by keeping the bitrate unchanged, generating completely compliant bitstream and utilizing negligible computational power. Nine different benchmark video sequences containing different combinations of motion, texture and objects are used for experimental evaluation of the proposed algorithm.\nTitle:\nSelective Encryption Of C2dvlc Of Avs Video Coding Standard For I & P Frames\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.152, mean value for all the writers is 8.648, which makes it 1.283 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nUnlike full encryption methods, for which the encryption does not preserve the internal structure of files, this paper presents an efficient format compliant selective encryption method for 3D object binary formats. The method encrypts selected bits of the 3D object geometry to visually protect the content without increasing the file size. Geometrical distortions are created in order to partially or fully protect the content, but they do not corrupt 3D scenes for preview purposes. As a function of the degradation level, we can obtain security from visual confidentiality to transparent encryption passing through sufficient encryption providing solutions for different application scenarios. Experimental results and analysis show the efficiency of the proposed method.\nTitle:", "model_inf_time": 1.37}, {"id": "41203", "output": "Efficient Algorithms for Geometric Problems on Planar Polygons", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present an $\\Theta (n)$ time worst-case deterministic algorithm for finding the constrained Delaunay triangulation and constrained Voronoi diagram of a simple n-sided polygon in the plane. Up to now, only an O(n log n) worst-case deterministic and an O(n) expected time bound have been shown, leaving an O(n) deterministic solution open to conjecture.\nTitle:\nFinding the Constrained Delaunay Triangulation and Constrained Voronoi Diagram of a Simple Polygon in Linear Time\n\nAbstract:\nIn this paper, we investigate the maximum weight triangulation of a polygon inscribed in a circle (simply inscribed polygon). A complete characterization of maximum weight triangulation of such polygons has been obtained. As a consequence of this characterization, an O(n2) algorithm for finding the maximum weight triangulation of an inscribed n-gon is designed. In case of a regular polygon, the complexity of this algorithm can be reduced to O(n). We also show that a tree admits a maximum weight drawing if its internal node connects at most 2 nonleaf nodes. The drawing can be done in O(n) time. Furthermore, we prove a property of maximum planar graphs which do not admit a maximum weight drawing on any set of convex points.\nTitle:\nMaximum Weight Triangulation and Its Application on Graph Drawing\n\nAbstract:\nTwo planar geometric problems relating to a convex n-gon P and a simple nonconvex m-gon Q are considered.\nTitle:\nOptimal Algorithms for the Intersection and the Minimum Distance Problems Between Planar Polygons\n\nAbstract:\nIn this paper, we present an O(1/\u03b1log n) (for any constant 0 \u2264 \u03b1 \u2264 1) time parallel algorithm for constructing the constrained Voronoi diagram of a set L of n non-crossing line segments in E2, using O(n1+\u03b1) processors on a CREW PRAM model. This parallel algorithm also constructs the constrained Delaunay triangulation of L in the same time and processor bound by the duality. Our method established the conversions from finding the constrained Voronoi diagram L to finding the Voronoi diagram of S, the endpoint set of L. We further showed that this conversion can be done in O(log n) time using n processors in CREW PRAM model. The complexity of the conversion implies that any improvement of the complexity for finding the Voronoi diagram of a point set will automatically bring the improvement of the one in question.\nTitle:\nA Parallel Algorithm for Finding the Constrained Voronoi Diagram of Line Segments in the Plane\n\nAbstract:\nIn a rectangular grid, given two sets of nodes, S (sources) and T (sinks), of size N2 each, the disjoint paths (DP) problem is to connect as many nodes in S to the nodes in T using a set of \u201cdisjoint\u201d paths. (Both edge-disjoint and vertex-disjoint cases are considered in this paper.) Note that in this DP problem, a node in S can be connected to any node in T. Although in general the sizes of S and T do not have to be the same, algorithms presented in this paper can also find the maximum number of disjoint paths pairing nodes in S and T. We use the network flow approach to solve this DP problem. By exploiting all the properties of the network, such as planarity and regularity of a grid, integral flow, and unit capacity source/sink/flow, we can optimally compress the size of the working grid (to be defined) from O(N2) to O(N1.5) and solve the problem in O(N2.5) time for both the edge-disjoint and vertex-disjoint cases, an improvement over the straightforward approach which takes O(N3) time.\nTitle:\nEfficient algorithms for finding the maximum number of disjoint paths in grids\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.0, mean value for all the writers is 8.648, which makes it 0.3 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA generalized problem is defined in terms of functions on sets and illustrated in terms of the computational geometry of simple\n planar polygons. Although its apparent time complexity is O(n\n 2), the problem is shown to be solvable for several cases of interest (maximum and minimum distance, intersection detection\n and rerporting) in O(n logn), O(n) or O(logn) time, depending on the nature of a specialized \u201cselection\u201d function. (Some of the cases can also be solved by the Voronoi\n diagram method; but time complexity increases with that approach.) A new use of monotonicity and a new concept of \u201clocality\u201d\n in set mappings contribute significantly to the derivation of the results.\nTitle:", "model_inf_time": 1.26}, {"id": "41204", "output": "Agreement and Disagreement in Argument Diagrams", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe development of graphical argument models is an active and growing area of research in Articial Intelligence and Law. The aim is to develop models which may be readily used by legal professionals and novices to produce and parse arguments. If this goal is to be realized it is important to develop models that human reasoners can manipulate and assess consistently. We report on an ongoing study of graph agreement in the context of the LARGO system.\nTitle:\nToward assessing law students' argument diagrams\n\nAbstract:\nThis paper describes a study in which student-created diagrams about arguments in an ill-defined domain were manually graded by two independent human graders. Findings include that the graders overall agreed with each other on their grades, but their agreement was lower than one would expect in well-defined domains, and higher for solutions of extreme quality.\nTitle:\nAssessing Argument Diagrams in an Ill-defined Domain\n\nAbstract:\nFormal and computational models of argument are ideally suited for education in ill-defined domains such as law, pub- lic policy, and science. Open-ended arguments play a central role in these areas but students of the domains may not have been taught an explicit model of argument. Computational models of argument may be ideally suited to act as argument tutors guiding students in the formation of arguments and ar- gument analysis according to an explicit model. In order to achieve this it is important to establish that the models can be understood and evaluated reliably, an empirical question. In this paper we report ongoing work on the diagnostic utility of argument diagrams produced in the LARGO tutoring system.\nTitle:\nComputational Argument as a Diagnostic Tool: The role of reliability.\n\nAbstract:\nPrevious research has highlighted the advantages of graphical argument representations. A number of tutoring systems have been built that support students in rendering arguments graphically, as they learn argumentation skills. The relative tutoring benefits of graphical argument representations have not been reliably shown, however. In this paper we present an evaluation of the LARGO system which enables law students graphically to represent examples of legal interpretation with hypotheticals they observe while reading texts of U.S. Supreme Court oral arguments. We hypothesized that LARGO's graphical representations and advice would help students to identify important elements of the arguments (i.e., proposed hypotheses, hypothetical challenges, and responses) and to reflect on their significance to the argument's merits better than a purely text-based alternative. In an experiment, we found some empirical support for this hypothesis.\nTitle:\nEvaluating Legal Argument Instruction with Graphical Representations Using LARGO\n\nAbstract:\nDiagrams appear to be a convenient vehicle for teaching argumentation skills in ill-defined domains, but can an ITS provide useful feedback on students' argument diagrams without assuming a well-defined procedure for objectively evaluating argument? LARGO is an ITS for legal argumentation that supports students as they diagram transcripts of US Supreme Court oral argument. It provides on-demand advice by identifying small, interesting or incomplete patterns within students' graphs. We conducted a study in which LARGO was used as mandatory part of a first-year law school class. In contrast to prior findings in lab studies with voluntary participants, the use of LARGO did not lead to superior learning as compared to a text-based note-taking tool. These results can be partially attributed to low use of the graphical tools and advice by the students as well as (and possibly due to) a different motivational focus. Some evidence was found that higher engagement with the system led to better learning, leaving open the tantalizing possibility of helping especially lower-aptitude students through use of LARGO.\nTitle:\nRe-evaluating LARGO in the Classroom: Are Diagrams Better Than Text for Teaching Argumentation Skills?\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.938, mean value for all the writers is 8.648, which makes it 0.247 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDiagrammatic models of argument are increasingly prominent in AI and Law. Unlike everyday language these models formalize many of the the components and relationships present in arguments and permit a more formal analysis of an arguments' structural weaknesses. Formalization, however, can raise problems of agreement. In order for argument diagramming to be widely accepted as a communications tool, individual authors and readers must be able to agree on the quality and meaning of a diagram as well as the role that key components play. This is especially problematic when arguers seek to map their diagrams to or from more conventional prose. In this paper we present results from a grader agreement study that we have conducted using LARGO diagrams. We then describe a detailed example of disagreement and highlight its implications for both our diagram model and modeling argument diagrams in general.\nTitle:", "model_inf_time": 1.12}, {"id": "41205", "output": "Parcours: A Cooperative Educational Game for Interactive Tabletops", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nFor many practical learning scenarios, the integrated use of more than one learning tool is educationally beneficial. In these cases, interoperability between learning tools - getting the pieces to talk to one another in a coherent, well-founded manner - is a crucial requirement that is often hard to achieve. This paper describes a re-usable software design that aims at the integration of independent learning tools into one collaborative learning scenario. We motivate the usefulness and expressiveness of combining several learning tools into one integrated learning experience. Based on this we sketch software design principles that integrate several existing components into a joint technical framework. The feasibility of the approach, which we name the \"Scalable Adapter\" design pattern, is shown with several implementation examples from different educational technology domains, including Intelligent Tutoring Systems and collaborative learning environments.\nTitle:\nThe Scalable Adapter Design Pattern: Enabling Interoperability Between Educational Software Tools\n\nAbstract:\nField activities and collaborative learning are prominent educational approaches. Various devices have been used to implement these approaches. For example, mobile devices have been excellently employed to facilitate outdoor learning, as desktops and laptops have been for indoor collaborative learning. But the use of laptops and desktops for group activities has some limitations like support for limited number of learners who can use a device at a time. Recently, interactive multi-touch tables with shareable interfaces and large displays have reached the market. We propose to integrate all these technologies in a common framework to provide a seamless learning environment providing assistance for learning both indoors and outdoors.\nTitle:\nSupporting Field and In-class Collaborative Learning: Towards a Generalized Framework\n\nAbstract:\nFor many practical learning scenarios, the integrated use of more than one learning tool is educationally beneficial. In these cases, interoperability between learning tools --- getting the pieces to talk --- is a crucial requirement that is often hard to achieve. This paper describes an architecture that aims at the integration of independent learning tools into one collaborative learning scenario.\nTitle:\nHow Do We Get the Pieces to Talk? An Architecture to Support Interoperability between Educational Tools\n\nAbstract:\nRecently, researchers from multiple disciplines have been showing their common interest in automatic question generation for educational purposes. In this paper, we review the state of the art of approaches to developing educational applications of question generation. We conclude that although a great variety of techniques on automatic question generation exists, just a small amount of educational systems exploiting question generation has been developed and deployed in real classroom settings. We also propose research directions for deploying the question technology in computer-supported educational systems.\nTitle:\nAutomatic Question Generation for Educational Applications - The State of Art.\n\nAbstract:\nThis paper reports on an exploratory pilot study that has been conducted to investigate which collaboration technologies are suitable (and which are not) to support collaborative writing. The study confirmed known requirements covered by existing tools, but also revealed some requirements that are not met by available technologies.\nTitle:\nTowards supporting phases in collaborative writing processes\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.043, mean value for all the writers is 8.648, which makes it 1.19 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nToday, typical classrooms are still equipped with blackboards, chalk and sometimes overhead projectors. Technology-enriched rooms can often only be found in school libraries or computer pools where students can research topics on the WWW or use other specific computer applications. In this paper, we present an educational game called \"Parcours\", developed for the interactive SMART table. This cooperative design game, installed on a tabletop that is located within a classroom, is intended to teach primary school children collaboration and coordination skills as well as logical thinking.\nTitle:", "model_inf_time": 1.12}, {"id": "41206", "output": "An Intelligent Tutoring System for Legal Argumentation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe intelligent tutoring system LARGO allows law students to annotate a transcript of an oral argu- ment using diagrams which can be linked to text portions. LARGO analyzes diagrams and gives feed- back to support students' reflection. The feedback mechanisms rely on certain hypotheses about the interaction between the student and the system. In particular, a linear working mode (from beginning to end of the transcript) and a consistent and correct linking of diagram elements to the text are assumed. Based on an empirical study, this paper argues that the design of LARGO is functional and that the central interaction hypotheses are confirmed.\nTitle:\nStudent's Usage of Multiple Linked Argument Representations in LARGO\n\nAbstract:\nThe development of graphical argument models is an active and growing area of research in Articial Intelligence and Law. The aim is to develop models which may be readily used by legal professionals and novices to produce and parse arguments. If this goal is to be realized it is important to develop models that human reasoners can manipulate and assess consistently. We report on an ongoing study of graph agreement in the context of the LARGO system.\nTitle:\nToward assessing law students' argument diagrams\n\nAbstract:\nThis paper describes a study in which student-created diagrams about arguments in an ill-defined domain were manually graded by two independent human graders. Findings include that the graders overall agreed with each other on their grades, but their agreement was lower than one would expect in well-defined domains, and higher for solutions of extreme quality.\nTitle:\nAssessing Argument Diagrams in an Ill-defined Domain\n\nAbstract:\nThis paper presents an approach for intelligent tutoring in the field of legal argumentation. In this approach, students study transcripts of US Supreme Court oral argument and create a graphical representation of argument flow as tests offered by attorneys being challenged by hypotheticals posed by Justices. The proposed system, which is based on the collaborative modeling framework Cool Modes, is capable of detecting three types of weaknesses in arguments; when it does, it presents the student with a self explanation prompt. This kind of feedback seems more appropriate than the \u201cstrong connective feedback\u201d typically offered by model-tracing or constraint-based tutors. Structural and context weaknesses in arguments are handled by graph grammars, and the critical problem of detecting and dealing with content weaknesses in student contributions is addressed through a collaborative filtering approach, thereby avoiding the critical problem of natural language processing in legal argumentation. An early version of the system was pilot tested with two students.\nTitle:\nToward legal argument instruction with graph grammars and collaborative filtering techniques\n\nAbstract:\nPrecise Natural Language Understanding is needed in Geometry Tutoring to accurately determine the semantic content of students' explanations. The paper presents an NLU system developed in the context of the Geometry Explanation Tutor. The system combines unification-based syntactic processing with description logics based semantics to achieve the necessary accuracy level. Solutions to specific semantic problems dealing with equivalence of semantic representations are described. Experimental results on classification accuracy are also presented.\nTitle:\nUnderstanding students' explanations in geometry tutoring\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.902, mean value for all the writers is 8.648, which makes it 1.923 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDieser Artikel stellt ein intelligentes Tutorensystem f\u00fcr die Rechtswissenschaften vor. Das System, welches bisher im Rahmen von einigen Pilotstudien getestet wurde, soll Jurastudenten helfen, vor Gericht notwendige Argumentationsstrategien zu lernen. Im verwendeten Ansatz werden Gerichtsprotokolle als Lernmaterialien verwendet: Studenten annotieren diese und erstellen graphische Repr\u00e4sentationen des Argumentationsverlaufs. Das System kann dabei zur Reflexion der von Anw\u00e4lten vorgebrachten Argumente anregen und Lernende auf m\u00f6gliche Schw\u00e4chen in ihrer Analyse des Disputs hinweisen. Dies geschieht gr\u00f6 \u03b2tenteils in Form von Aufforderungen zur Selbsterkl\u00e4rung - eine f\u00fcr pr\u00e4zisere R\u00fcckmeldungen notwendige exakte Definition von Korrektheit ist in der betrachteten Dom\u00e4ne oft nicht m\u00f6glich. Zur Erkennung von Schw\u00e4chen verwendet das System Graphgrammatiken und kollaborative Filtermechanismen. Die Priorit\u00e4tsbestimmung f\u00fcr R\u00fcckmeldungen, welche in diesem Artikel genauer dargestellt wird, ber\u00fccksichtigt mehrere Faktoren des Benutzungskontextes.\nTitle:", "model_inf_time": 1.17}, {"id": "41207", "output": "Topological Insights into Burer's Reformulation of Mixed-Binary QPs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nBy now, many copositive reformulations of mixed-binary QPs have been discussed, triggered by Burer\u2019s seminal characterization from 2009. In conic optimization, it is very common to use approximation hierarchies based on positive-semidefinite (psd) matrices where the order increases with the level of the approximation. Our purpose is to keep the psd matrix orders relatively small to avoid memory size problems in interior point solvers. Based upon on a recent discussion on various variants of completely positive reformulations and their relaxations (Bomze et al. in Math Program 166(1\u20132):159\u2013184, 2017), we here present a small study of the notoriously hard multidimensional quadratic knapsack problem and quadratic assignment problem. Our observations add some empirical evidence on performance differences among the above mentioned variants. We also propose an alternative approach using penalization of various classes of (aggregated) constraints, along with some theoretical convergence analysis. This approach is in some sense similar in spirit to the alternating projection method proposed in\u00a0Burer (Math Program Comput 2:1\u201319, 2010) which completely avoids SDPs, but for which no convergence proof is available yet.\nTitle:\nNotoriously hard (mixed-)binary QPs: empirical evidence on new completely positive approaches\n\nAbstract:\nTriggered by Burer\u2019s seminal characterization from 2009, many copositive reformulations of mixed-binary QPs have been discussed by now. Most of them can be used as proper relaxations, if the intractable co(mpletely)positive cones are replaced by tractable approximations. While the widely used approximation hierarchies have the disadvantage to use positive-semidefinite (psd) matrices of orders which rapidly increase with the level of approximation, alternatives focus on the problem of keeping psd matrix orders small, with the aim to avoid memory problems in the interior point algorithms. This work continues this approach, proposing new reformulations and relaxations. Moreover, we provide a thorough comparison of the respective duals and establish a monotonicity relation among their duality gaps. We also identify sufficient conditions for strong duality/zero duality gap in some of these formulations and generalize some of our observations to general conic problems.\nTitle:\nA fresh CP look at mixed-binary QPs: new formulations and relaxations.\n\nAbstract:\nCopositive optimization problems are particular conic programs: optimize linear forms over the copositive cone subject to\n linear constraints. Every quadratic program with linear constraints can be formulated as a copositive program, even if some\n of the variables are binary. So this is an NP-hard problem class. While most methods try to approximate the copositive cone\n from within, we propose a method which approximates this cone from outside. This is achieved by passing to the dual problem,\n where the feasible set is an affine subspace intersected with the cone of completely positive matrices, and this cone is approximated\n from within. We consider feasible descent directions in the completely positive cone, and regularized strictly convex subproblems.\n In essence, we replace the intractable completely positive cone with a nonnegative cone, at the cost of a series of nonconvex\n quadratic subproblems. Proper adjustment of the regularization parameter results in short steps for the nonconvex quadratic\n programs. This suggests to approximate their solution by standard linearization techniques. Preliminary numerical results\n on three different classes of test problems are quite promising.\nTitle:\nQuadratic factorization heuristics for copositive programming\n\nAbstract:\nThe famous Frank--Wolfe theorem ensures attainability of the optimal value for quadratic objective functions over a (possibly unbounded) polyhedron if the feasible values are bounded. This theorem does not hold in general for conic programs where linear constraints are replaced by more general convex constraints like positive semidefiniteness or copositivity conditions, despite the fact that the objective can be even linear. This paper studies exact penalizations of (classical) quadratic programs, i.e., optimization of quadratic functions over a polyhedron, and applies the results to establish a Frank--Wolfe-type theorem for the primal-dual pair of a class of conic programs that frequently arises in applications. One result is that uniqueness of the solution of the primal ensures dual attainability, i.e., existence of the solution of the dual.\nTitle:\nA Conic Duality Frank--Wolfe-Type Theorem via Exact Penalization in Quadratic Optimization\n\nAbstract:\nCopositive optimization is a quickly expanding scientific research domain with wide-spread applications ranging from global nonconvex problems in engineering to NP-hard combinatorial optimization. It falls into the category of conic programming (optimizing a linear functional over a convex cone subject to linear constraints), namely the cone $${\\mathcal{C}}$$ of all completely positive symmetric n \u8133 n matrices (which can be factorized into $${FF^\\top}$$ , where F is a rectangular matrix with no negative entry), and its dual cone $${\\mathcal{C}^*}$$ , which coincides with the cone of all copositive matrices (those which generate a quadratic form taking no negative value over the positive orthant). We provide structural algebraic properties of these cones, and numerous (counter-)examples which demonstrate that many relations familiar from semidefinite optimization may fail in the copositive context, illustrating the transition from polynomial-time to NP-hard worst-case behaviour. In course of this development we also present a systematic construction principle for non-attainability phenomena, which apparently has not been noted before in an explicit way. Last but not least, also seemingly for the first time, a somehow systematic clustering of the vast and scattered literature is attempted in this paper.\nTitle:\nThink co(mpletely)positive ! Matrix properties, examples and a clustered bibliography on copositive optimization\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.442, mean value for all the writers is 8.648, which makes it 0.677 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn an important paper, Burer (Math. Program Ser. A 120:479\u2013495, 2009) recently showed how to reformulate general mixed-binary\n quadratic optimization problems (QPs) into copositive programs where a linear functional is minimized over a linearly constrained\n subset of the cone of completely positive matrices. In this note we interpret the implication from a topological point of\n view, showing that the Minkowski sum of the lifted feasible set and the lifted recession cone gives exactly the closure of\n the former. We also discuss why feasibility of the copositive program implies feasibility of the original mixed-binary QP,\n which can be derived from the arguments in Burer (Math. Program Ser. A 120:479\u2013495, 2009) without any further condition.\nTitle:", "model_inf_time": 1.82}, {"id": "41208", "output": "Fast Population Game Dynamics for Quadratic Optimization Problems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe propose a fast population game dynamics, motivated by the analogy with infection and immunization processes within a population of \"players,\" for finding dominant sets, a powerful graph-theoretical notion of a cluster. Each step of the proposed dynamics is shown to have a linear time/space complexity and we show that, under the assumption of symmetric affinities, the average population payoff is strictly increasing along any non-constant trajectory, thereby allowing us to prove that dominant sets are asymptotically stable (i.e., attractive) points for the proposed dynamics. The approach is general and can be applied to a large class of quadratic optimization problems arising in computer vision. Experimentally, the proposed dynamics is found to be orders of magnitude faster than and as accurate as standard algorithms.\nTitle:\nFast population game dynamics for dominant sets and other quadratic optimization problems\n\nAbstract:\nEvolutionary game-theoretic models and, in particular, the so-called replicator equations have recently proven to be remarkably effective at approximately solving the maximum clique and related problems. The approach is centered around a classic result from graph theory that formulates the maximum clique problem as a standard (continuous) quadratic program and exploits the dynamical properties of these models, which, under a certain symmetry assumption, possess a Lyapunov function. In this letter, we generalize previous work along these lines in several respects. We introduce a wide family of game-dynamic equations known as payoff-monotonic dynamics, of which replicator dynamics are a special instance, and show that they enjoy precisely the same dynamical properties as standard replicator equations. These properties make any member of this family a potential heuristic for solving standard quadratic programs and, in particular, the maximum clique problem. Extensive simulations, performed on random as well as DIMACS benchmark graphs, show that this class contains dynamics that are considerably faster than and at least as accurate as replicator equations. One problem associated with these models, however, relates to their inability to escape from poor local solutions. To overcome this drawback, we focus on a particular subclass of payoff-monotonic dynamics used to model the evolution of behavior via imitation processes and study the stability of their equilibria when a regularization parameter is allowed to take on negative values. A detailed analysis of these properties suggests a whole class of annealed imitation heuristics for the maximum clique problem, which are based on the idea of varying the parameter during the imitation optimization process in a principled way, so as to avoid unwanted inefficient solutions. Experiments show that the proposed annealing procedure does help to avoid poor local optima by initially driving the dynamics toward promising regions in state space. Furthermore, the models outperform state-of-the-art neural network algorithms for maximum clique, such as mean field annealing, and compare well with powerful continuous-based heuristics.\nTitle:\nPayoff-monotonic game dynamics and the maximum clique problem.\n\nAbstract:\nWith this paper we offer a game-theoretic perspective for the all-pervasive matching problem in computer vision. Specifically, we formulate the matching problem as a (population) non-cooperative game where the potential associations between the items to be matched correspond to (pure) strategies, while payoffs reflect the degree of compatibility between competing hypotheses. Within this formulation, the solutions of the matching problem correspond to evolutionary stable states (ESS's), a robust population-based generalization of the notion of a Nash equilibrium. In order to find ESS's of our matching game, we propose using a novel, fast evolutionary game dynamics motivated by Darwinian selection processes, which let the pure strategies play against each other until an equilibrium is reached. A distinguishing feature of the proposed framework is that it allows one to naturally deal with general many-to-many matching problems even in the presence of asymmetric compatibilities. The potential of the proposed approach is demonstrated via two sets of image matching experiments, both of which show that our results outperform those obtained using well-known domain-specific algorithms.\nTitle:\nMatching as a non-cooperative game\n\nAbstract:\nWe develop a framework for the image segmentation problem based on a new graph-theoretic formulation of clustering. The approach is motivated by the analogies between the intuitive concept of a cluster and that of a dominant set of vertices, a novel notion that generalizes that of a maximal complete subgraph to edge-weighted graphs. We also establish a correspondence between dominant sets and the extrema of a quadratic form over the standard simplex, thereby allowing us the use of continuous optimization techniques such as replicator dynamics from evolutionary game theory. Such systems are attractive as can be coded in a few lines of any high-level programming language, can easily be implemented in a parallel network of locally interacting units, and offer the advantage of biological plausibility. We present experimental results on real-world images which show the effectiveness of the proposed approach.\nTitle:\nA new graph-theoretic approach to clustering and segmentation\n\nAbstract:\nWe develop a framework for the image segmentation problem based on a new graph-theoretic formulation of clustering. The approach is motivated by the analogies between the intuitive concept of a cluster and that of a dominant set of vertices, a novel notion that generalizes that of a maximal complete subgraph to edge-weighted graphs. We also establish a correspondence between dominant sets and the extrema of a quadratic form over the standard simplex, thereby allowing us the use of continuous optimization techniques such as replicator dynamics from evolutionary game theory. Such systems are attractive as can easily be implemented in a parallel network of locally interacting computational units, and offer the advantage of biological plausibility. We present experimental results on real-world images which show the effectiveness of the approach.\nTitle:\nImage Segmentation by Dominant Sets\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.415, mean value for all the writers is 8.648, which makes it 1.052 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nQuadratic optimization lies at the very heart of many structural pattern recognition and computer vision problems, such as graph matching, object recognition, image segmentation, etc., and it is therefore of crucial importance to devise algorithmic solutions that are both efficient and effective. As it turns out, a large class of quadratic optimization problems can be formulated in terms of so-called ''standard quadratic programs'' (StQPs), which ask for finding the extrema of a quadratic polynomial over the standard simplex. Computationally, the standard approach for attacking this class of problems is to use replicator dynamics, a well-known family of algorithms from evolutionary game theory inspired by Darwinian selection processes. Despite their effectiveness in finding good solutions in a variety of applications, however, replicator dynamics suffer from being computationally expensive, as they require a number of operations per step which grows quadratically with the dimensionality of the problem being solved. In order to avoid this drawback, in this paper we propose a new population game dynamics (InImDyn) which is motivated by the analogy with infection and immunization processes within a population of ''players.'' We prove that the evolution of our dynamics is governed by a quadratic Lyapunov function, representing the average population payoff, which strictly increases along non-constant trajectories and that local solutions of StQPs are asymptotically stable (i.e., attractive) points. Each step of InImDyn is shown to have a linear time/space complexity, thereby allowing us to use it as a more efficient alternative to standard approaches for solving StQPs and related optimization problems. Indeed, we demonstrate experimentally that InImDyn is orders of magnitude faster than, and as accurate as, replicator dynamics on various applications ranging from tree matching to image registration, matching and segmentation.\nTitle:", "model_inf_time": 1.47}, {"id": "41209", "output": "Kernel-Level Scheduling in the NANOS Environment", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe nano-threads programming model was proposed to effectively integrate multiprogramming on shared-memory multiprocessors, with the exploitation of fine-grain parallelism from standard applications. A prerequisite for the applicability of the nano-threads programming model is the ability of the runtime environment to manage parallelism at any level of granularity with minimal overheads. In this paper, we introduce runtime techniques for efficient memory management and user-level scheduling in an experimental runtime system designed to support the nano-threads programming model. We evaluate the exploitation of processor affinity for the management of nano-thread contexts, and the use of hierarchical queues to implement user-level scheduling strategies for applications with inherent multilevel parallelism. The proposed mechanisms attempt to obtain maximum benefits from data locality on cache-coherent NUMA multiprocessors. Through the use of synthetic benchmarks, we find that our mechanism for memory management in the runtime system reduces overheads by 52% on average, compared to other known mechanisms. The use of hierarchical queues gives significant performance improvements between 17% and 40%, compared to scheduling strategies that use local queues.\nTitle:\nEfficient Runtime Thread Management for the Nano-Threads Programming Model\n\nAbstract:\nThe bus that connects processors to memory is known to be a major architectural bottleneck in SMPs. However, both software and scheduling policies for these systems generally focus on memory hierarchy optimizations and do not address the bus bandwidth limitations directly. In this paper we first present experimental results which indicate that bus saturation can cause an up to almost three-fold slow-down to applications. Motivated by these results, we introduce two scheduling policies that take into account the bus bandwidth consumption of applications. The necessary information is provided by performance monitoring counters which are present in all modern processors. Our algorithms organize jobs so that processes with high-bandwidth and low-bandwidth demands are co-scheduled to improve bus bandwidth utilization without saturating the bus. We found that our scheduler is effective with applications of varying bandwidth requirements, from very low to close to the limit of saturation. We also tuned our scheduler for robustness in the presence of bursts of high bus bandwidth consumption from individual jobs. The new scheduling policies improve system throughput by up to 68% (26% in average) in comparison with the standard Linux scheduler.\nTitle:\nScheduling Algorithms With Bus Bandwidth Considerations For Smps\n\nAbstract:\nIn this work, we present an OpenMP implementation suitable for multiprogrammed environments on Intel-based SMP systems. This implementation consists of a runtime system and a resource manager, while we use the NanosCompiler to transform OpenMP-coded applications into code with calls to our runtime system. The resource manager acts as the operating system scheduler for the applications built with our runtime system. It executes a custom made scheduling policy to distribute the available physical processors to the active applications. The runtime system cooperates with the resource manager in order to adapt each application's generated parallelism to the number of processors allocated to it, according to the resource manager scheduling policy. We use the OpenMP version of the NAS Parallel Benchmark suite in order to evaluate the performance of our implementation. In our experiments we compare the performance of our implementation with that of a commercial OpenMP implementation. The comparison proves that our approach performs better both on a dedicated and on a heavily multiprogrammed environment.\nTitle:\nA Multiprogramming Aware OpenMP Implementation\n\nAbstract:\nIn this paper we reformulate the thread scheduling problem on multiprogrammed SMPs Scheduling algorithms usually attempt to maximize performance of memory intensive applications by optimally exploiting the cache hierarchy We present experimental results indicating that \u2013 contrary to the common belief \u2013 the extent of performance loss of memory-intensive, multiprogrammed workloads is disproportionate to the deterioration of cache performance caused by interference between threads In previous work [1] we found that memory bandwidth saturation is often the actual bottleneck that determines the performance of multiprogrammed workloads Therefore, we present and evaluate two realistic scheduling policies which treat memory bandwidth as a first-class resource Their design methodology is general enough and can be applied to introduce bus bandwidth-awareness to conventional scheduling policies Experimental results substantiate the advantages of our approach.\nTitle:\nRealistic workload scheduling policies for taming the memory bandwidth bottleneck of SMPs\n\nAbstract:\nMultiprocessor systems are increasingly becoming the sys- tems of choice for low and high-end servers, running such diverse tasks as number crunching, large-scale simulation s, data base engines and world wide web server applications. With such diverse workloads, system utilization and through- put, as well as execution time become important performance metrics. In this paper we present efficient kernel schedul- ing policies and propose a new kernel-user interface aiming at supporting efficient parallel execution in diverse workl oad environments. Our approach relies on support for user level threads which are used to exploit parallelism within applic a- tions, and a two-level scheduling policy which coordinates the number of resources allocated by the kernel with the num- ber of threads generated by each application. We compare our scheduling policies with the native gang scheduling policy of the IRIX 6.4 operating system on a Silicon Graphics Ori- gin2000. Our experimental results show substantial perfor- mance gains in terms of overall workload execution times, in- dividual application execution times, and cache performance. Current shared-memory multiprocessor architectures, inc lud- ing small symmetric multiprocessors and scalable distribu ted shared memory systems, are widely adopted as a viable and powerful platform for high-performance computing. Among other attractive properties, these machines offer a standa rd, transparent multiuser/multiprogramming environment to sup- port large numbers of simultaneously running parallel and s e- quential applications. Parallel applications that execute on a multiprogramming system often suffer from serious performance degradation, due to the interferences between simultaneously executing programs which contend for system resources. Events like blocking system calls, paging, starting and termination of pro- cesses introduce a high variability in the system workload, and can have significant impact in execution time. A major draw- back of the vast majority of parallel applications is that th eir implementation assumes that the programs are executed in a dedicated system, so they setup the desired number of proces-\nTitle:\nKernel-level scheduling for the nano-threads programming model\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.412, mean value for all the writers is 8.648, which makes it 1.505 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nScheduling parallel applications on shared--memory multiprocessors is a difficult task that requires a lot of tuning from application programmers, as well as operating system developers and system managers. In this paper, we present the characteristics related to kernel--level scheduling of the NANOS environment and the results we are achieving. The NANOS environment is designed and tuned specifically to achieve high performance in current shared--memory multiprocessors. Taking advantage of the wide and efficient dialog established between applications and the NANOS environment, we are designing powerful scheduling policies. The information exchanged ranges from simply communicating the number of requested processors to providing information of the current speedup achieved by the applications. We have devised several scheduling policies that use this interface, such as Equipartition, Variable Time Quantum DSS and Dynamic Performance Analysis. The results we have obtained with these policies indicate that there is a lot of work to do in the search for a \"good\" scheduling policy, which can include characteristics like sustainable execution times, fairness and throughput. For instance, we show through several experiments that benefits in execution time range from 15% to 100%, depending on the policy used and the characteristics of the workload.\nTitle:", "model_inf_time": 1.51}, {"id": "41210", "output": "On the Hardness of Approximating the Longest Common Rigid Subsequence Problem", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe longest common subsequence problem (LCS) and the closest substring problem (CSP) are two models for the finding of common patterns in strings. The two problem have been studied extensively. The former was previously proved to be not polynomial-time approximable within ratio n\u03b4 for a constant \u03b4. The latter was previously proved to be NP-hard and have a PTAS. In this paper, the longest common rigid subsequence problem (LCRS) is studied. LCRS shares similarity with LCS and CSP and has an important application in motif finding in biological sequences. LCRS is proved to be Max-SNP hard in this paper. An exact algorithm with quasi-polynomial average running time is also provided.\nTitle:\nOn the longest common rigid subsequence problem\n\nAbstract:\nWe consider an approximate pattern matching problem for undirected acyclic graphs. Specifically, let P be a pattern graph, D a data graph and t an integer. We present an algorithm to locate a subgraph in D whose distance from P is at most t. The distance measure used here is the degree-2 metric published previously. The time complexity of our algorithm is O(NPNDddlogd) where NP and ND are the number of nodes in P and D, respectively; d=min{dP,dD}; dP and dD are the maximum degree of P and D, respectively. Central to our algorithm is a procedure for finding approximate patterns in rooted unordered trees and freely allowing cuts. We discuss two applications of the algorithms in chemical information search and website management on the Internet.\nTitle:\nFinding approximate patterns in undirected acyclic graphs\n\nAbstract:\nGiven two rooted, labeled, and unordered trees, we consider the problem of finding the largest common sub-tree and the problem of finding the edit distance between them. We show that both problems are MAX SNP-hard. This means that neither problem has a polynomial time approximation scheme (PTAS) unless P = NP.\nTitle:\nSome MAX SNP-hard results concerning unordered labeled trees\n\nAbstract:\nThis paper considers the problem of computing the editing distance between unordered, labeled trees. We give efficient polynomial-time algorithms for the case when one tree is a string or has a bounded number of leaves. By contrast, we show that the problem is NP-complete even for binary trees having a label alphabet of size two.\nTitle:\nOn the editing distance between unordered labeled trees\n\nAbstract:\nThe problem of finding this similar consensus (also known as the largest approximately common substructures) of two trees arises in many pattern recognition applications. This paper presents a dynamic programming algorithm to solve the problem based on the distance measure originated from Tanaka and Tanaka. The algorithm runs as fast as the best-known algorithm for comparing two trees using Tanaka's distance measure when the allowed distance between the common substructures is a constant independent of the input trees. In addition, we establish a hierarchy among Tanaka's distance measure and three other edit-based distance measures published in the literature.\nTitle:\nFinding similar consensus between trees: an algorithm and a distance hierarchy\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.913, mean value for all the writers is 8.648, which makes it 0.226 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe longest common subsequence problem (LCS) and the closest substring problem (CSP) are two models for finding common patterns in strings, and have been studied extensively. Though both LCS and CSP are NP-Hard, they exhibit very different behavior with respect to polynomial time approximation algorithms. While LCS is hard to approximate within n \u03b4 for some \u03b40, CSP admits a polynomial time approximation scheme. In this paper, we study the longest common rigid subsequence problem (LCRS). This problem shares similarity with both LCS and CSP and has an important application in motif finding in biological sequences. We show that it is NP-hard to approximate LCRS within ratio n \u03b4 , for some constant \u03b40, where n is the maximum string length. We also show that it is NP-Hard to approximate LCRS within ratio \u03a9(m), where m is the number of strings.\nTitle:", "model_inf_time": 1.41}, {"id": "41211", "output": "A Granularity-Based Spatiotemporal Conceptual Model for Capturing Spatial and Temporal Semantics", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe describe DISTIL (DIstributed design of SpaTIo-temporaL data), a web-based conceptual modeling prototype system that can help capture the semantics of spatio-temporal data. Via DISTIL, we describe an annotation-based approach that divides spatio-temporal conceptual design into two steps: first capture the current reality of an application using a conventional conceptual model without considering the spatial aspects, and only then annotate the schema with the spatio-temporal semantics of the application. A database development team can use DISTIL to capture and validate their spatio-temporal data requirements. Using DISTIL we demonstrate that the annotation-based approach for capturing spatio-temporal requirements is straightforward to implement, satisfies ontology-based and cognition-based requirements, and integrates seamlessly into the existing database design methodologies.\nTitle:\nDISTIL: A Design Support Environment for Conceptual Modeling of Spatio-temporal Requirements\n\nAbstract:\nA database design-support environment supports a data analyst in eliciting, articulating, specifying and validating data-related requirements. Extant design-support environments--based on conventional conceptual models--do not adequately support applications that need to organize data based on time (e.g., accounting, portfolio management, personnel management) and/or space (e.g., facility management, transportation, logistics). For geo-spatio-temporal applications, it is left to database designers to discover, design and implement--on an ad-hoc basis--the temporal and geospatial concepts that they need to represent the miniworld. To elicit the geo-spatio-temporal data semantics, we characterize guiding principles for augmenting the conventional conceptual database design approach, present our annotation-based approach, and illustrate how our proposed approach can be instantiated via a proof-of-concept prototype. Via a proof-of-concept database design-support environment, we exemplify our annotation-based approach, and show how segregating \"what\" from \"when/where\" via annotations satisfies ontologic- and cognition-based requirements, dovetails with existing database design methodologies, results in upward-compatible conceptual as well as XML schemas, and provides a straightforward mechanism to extend extant design-support environments.\nTitle:\nOn augmenting database design-support environments to capture the geo-spatio-temporal data semantics\n\nAbstract:\nWhile many real-world applications need to organize data based on space (e.g., geology, geomarketing, environmental modeling) and/or time (e.g., accounting, inventory management, personnel management), existing conventional conceptual models do not provide a straightforward mechanism to explicitly capture the associated spatial and temporal semantics. As a result, it is left to database designers to discover, design, and implement\u9a74on an ad hoc basis\u9a74the temporal and spatial concepts that they need. We propose an annotation-based approach that allows a database designer to focus first on nontemporal and nongeospatial aspects (i.e., \"what\u9a74) of the application and, subsequently, augment the conceptual schema with geospatiotemporal annotations (i.e., \"when\u9a74 and \"where\u9a74). Via annotations, we enable a supplementary level of abstraction that succinctly encapsulates the geospatiotemporal data semantics and naturally extends the semantics of a conventional conceptual model. An overarching assumption in conceptual modeling has always been that expressiveness and formality need to be balanced with simplicity. We posit that our formally defined annotation-based approach is not only expressive, but also straightforward to understand and implement.\nTitle:\nAugmenting a Conceptual Model with Geospatiotemporal Annotations\n\nAbstract:\nTime provides context for all our experiences, cognition, and coordinated collective action. Prior research in linguistics, artificial intelligence, and temporal databases suggests the need to differentiate between temporal facts with goal-related semantics (i.e., telic) from those are intrinsically devoid of culmination (i.e., atelic). To differentiate between telic and atelic data semantics in conceptual database design, we propose an annotation-based temporal conceptual model that generalizes the semantics of a conventional conceptual model. Our temporal conceptual design approach involves: 1) capturing \"what\" semantics using a conventional conceptual model; 2) employing annotations to differentiate between telic and atelic data semantics that help capture \"when\" semantics; 3) specifying temporal constraints, specifically nonsequenced semantics, in the temporal data dictionary as metadata. Our proposed approach provides a mechanism to represent telic/atelic temporal semantics using temporal annotations. We also show how these semantics can be formally defined using constructs of the conventional conceptual model and axioms in first-order logic. Via what we refer to as the \"semantics of composition,\" i.e., semantics implied by the interaction of annotations, we illustrate the logical consequences of representing telic/atelic data semantics during temporal conceptual design.\nTitle:\nCapturing Telic/Atelic Temporal Data Semantics: Generalizing Conventional Conceptual Models\n\nAbstract:\nBusiness rules are the basis of any organization. From an information systems perspective, these business rules function as constraints on a database helping ensure that the structure and content of the real world sometimes referred to as miniworld--is accurately incorporated into the database. It is important to elicit these rules during the analysis and design stage, since the captured rules are the basis for subsequent development of a business constraints repository. We present a taxonomy for set-based business rules, and describe an overarching framework for modeling rules that constrain the cardinality of sets. The proposed framework results in various types constraints, i.e.. attribute, class, participation, projection, co-occurrence, appearance and overlappinq, on a semantic model that supports abstractions like classification, generalization/specialization, aggregation and association. We formally define the syntax of our proposed framework in Backus-Naur Form and explicate the semantics using first-order logic. We describe partial ordering in the constraints and define the concept of metaconstraints, which can be used for automatic constraint consistency checking during the design stage itself. We demonstrate the practicality of our approach with a case study and show how our approach to modeling business rules seamlessly integrates into existing database design methodology. Via our proposed framework, we show how explicitly capturing data semantics will help bridge the semantic gap between the real world and its representation in an information system.\nTitle:\nA comprehensive framework for modeling set-based business rules during conceptual database design\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.934, mean value for all the writers is 8.648, which makes it 1.097 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nGranularities are integral to spatial and temporal data. A large number of applications require storage of facts along with their temporal and spatial context, which needs to be expressed in terms of appropriate granularities. For many real-world applications, a single granularity in the database is insufficient. In order to support any type of spatial or temporal reasoning, the semantics related to granularities needs to be embedded in the database. Specifying granularities related to facts is an important part of conceptual database design because under-specifying the granularity can restrict an application, affect the relative ordering of events and impact the topological relationships. Closely related to granularities is indeterminacy, i.e., an occurrence time or location associated with a fact that is not known exactly. In this paper, we present an ontology for spatial granularities that is a natural analog of temporal granularities. We propose an upward-compatible, annotation-based spatiotemporal conceptual model that can comprehensively capture the semantics related to spatial and temporal granularities, and indeterminacy without requiring new spatiotemporal constructs. We specify the formal semantics of this spatiotemporal conceptual model via translation to a conventional conceptual model. To underscore the practical focus of our approach, we describe an on-going case study. We apply our approach to a hydrogeologic application at the United States Geologic Survey and demonstrate that our proposed granularity-based spatiotemporal conceptual model is straightforward to use and is comprehensive.\nTitle:", "model_inf_time": 1.92}, {"id": "41212", "output": "Specifying an Oscilloscope with Z", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper shows how formal specification techniques can be beneficially applied in the develop ment of electronic instrumentation. As an example of our approach we present a specification of a simple electronic instrument, written in the Z specification language. We argue that such specifications can be used to gain insight into software/hardware systems and to clarify the resulting design. A consequence is that formal specifications can assume a pivotal role in system design as non-executable prototypes and lead to a cost-effect application of formal techniq,ues in industrial settings.\nTitle:\nFormally specifying electronic instruments\n\nAbstract:\nWe use our experience in applying formal methods to the development of electronic instrumentation systems to argue the value of developing formal, domain-specific models that serve as reusable frameworks for a family of software products. To illustrate what we mean by framework we present a non-trivial specification for a family of instruments, and show how certain properties of that specification lead to its potential for reusability. Finally, we outline the important research issues that are raised by this approach. In particular, we examine the suitability of existing formal specification notations for explicitly characterizing and instantiating such frameworks.\nTitle:\nFormal Specifications as Reusable Frameworks\n\nAbstract:\nIn this paper, we argue that the reality of today's software systems requires us to consider uncertainty as a first-class concern in the design, implementation, and deployment of those systems. We further argue that this induces a paradigm shift, and a number of research challenges that must be addressed.\nTitle:\nSoftware engineering in an uncertain world\n\nAbstract:\nCorrect assembly of software components is an important issue in Component Based Software Engineering. Composing a system from reusable components often introduces a set of problems related to communication and compatibility. In particular, one of the main problems in component assembly is that components may have incompatible interaction behavior. In this paper, we address this problem using an architecture-based approach that can detect integration mismatches, and semi-automatically synthesize a suitable adaptor, or glue code, to bridge them.\nTitle:\nAdaptor Synthesis for Protocol-Enhanced Component Based Architectures\n\nAbstract:\nA critical issue in the design of a professional software engineering degree program is the way in which formal methods are integrated into the curriculum. The approach taken by most programs is to teach formal techniques for software development in a separate course on formal methods. In this paper we detail some of the problems with that approach and describe an alternative in which formal methods are integrated across the curriculum. We illustrate the strengths and weaknesses of this alternative in terms of our experience of using it in the Master of Software Engineering Program at Carnegie Mellon University.\nTitle:\nMaking formal methods education effective for professional software engineers\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.03, mean value for all the writers is 8.648, which makes it 1.38 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis case study presents the development of an abstract oscilloscope specification, using Z notation. A description is given of the problem and its context. An abstract model of an oscilloscope that clarifies its user-accessible functions is described. Issues that must be addressed to scale up this specification to deal with more complicated, realistic oscilloscopes are discussed. The use of formal models and formal reasoning in this specification is examined.<>\nTitle:", "model_inf_time": 0.97}, {"id": "41213", "output": "Efficient rule matching in a relational database: The DBCond approach", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIssues pertinent to the concurrent execution of rules in a database management system (DBMS) are studied. Rules are modeled as database transactions. As such, they should follow serializability as their correctness criterion for execution. Rule execution has the additional constraint that the rules, conditions must be true in the database for the actions that execute, and rules must fail when their conditions are not true any longer. Based on this observation, two locking-based protocols are discussed. Information on the possible conflicts between conditions and actions of rules is used to provide greater concurrent access to the relations, based on a new lock paradigm. A simulation testbed was developed in order to study the rule features and database characteristics that play an important role in the performance of concurrent production rule execution\nTitle:\nExperiments on the concurrent rule execution in database systems\n\nAbstract:\nIn this paper we highlight the basic approach taken in the design of the DIPS system, and briefly present the main contributions. These include the use of special data structures to store rule definitions; they are implemented using relations. A matching algorithm uses these structures to efficiently identify when the antecedents of productions are satisfied, making them applicable for execution. Partial match information stored in the data structures is used by the matching algorithm. We also describe a proposed concurrent execution strategy for applicable productions, which surpasses in performance, the traditional sequential OPS5 production execution algorithm. The requirements for a correct, serializable execution, based on locking, is described. An advantage of the matching technique in DIPS is that it is fully parallelizable, which makes it attractive for implementation in parallel computing environments.\nTitle:\nData intensive production systems: the DIPS approach\n\nAbstract:\nIt has been widely recognized that many future database applications, including engineering processes, manufacturing and communications, will require some kind of rule based reasoning. In this paper we study methods for storing and manipulating large rule bases using relational database management systems. First, we provide a matching algorithm which can be used to efficiently identify applicable rules. The second contribution of this paper, is our proposal for concurrent execution strategies which surpass, in terms of performance, the sequential OPS5 execution algorithm. The proposed method is fully parallelizable, which makes its use even more attractive, as it can be used in parallel computing environments.\nTitle:\nImplementing large production systems in a DBMS environment: concepts and algorithms\n\nAbstract:\nWe present our research on the concurrent execution of rules in a database environment. Traditionally, the serializability criterion of correctness is defined on the basis of read/write conflicts. With rules, however, the conditions must be true for the actions to execute, and rules must fail when their conditions are no longer true. A different correctness criterion is thus defined on the basis of conflicts between conditions and actions. We develop a locking based protocol and discuss extensions to a conventional transaction manager. One extension is a new lock compatibility matrix which provides greater concurrent access. The second extension is to allow concurrent execution within a transaction. A simulation-based performance study is described. We identify characteristic features of the rules and study their impact on performance. The impact of using the new lock compatibility matrix, the effect of varying the database size and the number of rules executing concurrently, and the effect of skew are studied.\nTitle:\nA simulation-based study on the concurrent execution of rules in a database environment\n\nAbstract:\n In this paper we present our research on defining a correct semantics for a class of update rule (UR) programs,and discuss implementing these programs in a DBMS environment. Update rules execute by updating relations in adatabase which may cause the further execution of rules. A correct semantics must guarantee that the execution ofthe rules will terminate and that it will produce a minimal updated database. The class of UR programs is syntacticallyidentified, based upon a concept that is... \nTitle:\nSemantics for update rule programs and implementation in a relational database management system\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.657, mean value for all the writers is 8.648, which makes it 0.861 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMethods for storing and manipulating large rule bases using a relational database management systems (DBMS) are discussed. An approach to decomposing and storing the condition elements in the antecedents of rules such as those used in production rule-based systems is presented. A set-oriented approach, DBCond, which uses a special data structure that is implemented using relations is proposed. A matching algorithm for DBCond uses the relational structures to efficiently identify rules whose antecedents are satisfied. The performance of DBCond is compared with that of DBRete, a DBMS implementation of the Rete match algorithm developed for use with the production rule language OPS5. DBCond is also compared with DBQuery, a method that is based on evaluating queries corresponding to the conditions in the antecedents of the rules. Improvements to the data structure and the algorithms of the DBCond method are described. An advantage of DBCond is that it is fully parallelizable, thus making it attractive for parallel computing environments.\nTitle:", "model_inf_time": 1.47}, {"id": "41214", "output": "Approximation Algorithms for L(h, k)-Labeling", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe L(h, k)-labeling is an assignment of non negative integer labels to the nodes of a graph such that 'close' nodes have labels which differ by at least k, and 'very close' nodes have labels which differ by at least h. The span of an L(h, k)-labeling is the difference between the largest and the smallest assigned label. We study L(h, k)-labelings of cellular, squared and hexagonal grids, seeking those with minimum span for each value of k and h >= k. The L(h, k)-labeling problem has been intensively studied in some special cases, i.e. when k = 0 (vertex coloring), h = k (vertex coloring the square of the graph) and h = 2k (radio- or lambda - coloring) but no results are known in the general case for regular grids. In this paper, we completely solve the L(h, k)-labeling problem on cellular grids, finding exact values of the span for each value of h and k; only in a small interval we provide different upper and lower bounds. For the sake of completeness, we study also hexagonal and squared grids.\nTitle:\nOptimal L(H, K)-Labeling Of Regular Grids\n\nAbstract:\nGiven two non negative integers h and k, an L(h, k)-labeling of a graph G = (V, E) is a map from V to a set of labels such that adjacent vertices receive labels at least h apart, while vertices at distance at most 2 receive labels at least k apart. The goal of the L(h, k)-labeling problem is to produce a legal labeling that minimizes the largest label used. Since the decision version of the L(h, k)-labeling problem is NP-complete, it is important to investigate classes of graphs for which the problem can be solved efficiently. Along this line of though, in this paper we deal with co-comparability graphs and two of its subclasses: interval graphs and unit-interval graphs. Specifically, we provide, in a constructive way, the first upper bounds on the L(h, k)-number of co-comparability graphs and interval graphs. To the best of our knowledge, ours is the first reported result concerning the L(h, k)-labeling of co-comparability graphs. In the special case where k = 1, our result improves on the best previously-known approximation ratio for interval graphs.\nTitle:\nOn the L(h, k)-labeling of co-comparability graphs\n\nAbstract:\nGiven a graph G = and two positive integers j and k, 1)-edge-labeling is a function f assigning to edges of E colors from a set {0,1,..., k(f)} such that vertical bar f(e) - f (e')vertical bar >= J if e and el are adjacent, i.e. they share a common endpoint, If (e) - f (e')vertical bar >= k if e and et are not adjac;ent and there exists an edge adjacent to both e and el. The aim of the L(j, k)- edge labeling problem consists of :finding a coloring fund iou eu Ii I ha the value of kf is minimum. This minimum value is called lambda(n,k)' (p).This problem has already been studied on hexagonal, squared and triangular grids, but most not coinciding upper and lower bounds on lambda(j,k)' have been proposed. In this paper we close some of these gaps or 'find better bounds on lambda(j,k)' in the special cases j = 1,2 and k = 1. Moreover, we propose tight L(j, k)-edge-labelings for eight-regular grids.\nTitle:\nOptimal L(J, K)-Edge-Labeling Of Regular Grids\n\nAbstract:\nGiven two nonnegative integers h and k, an L(h, k)-labeling of a graph G = (V, E) is a map from V to a set of integer labels such that adjacent vertices receive labels at least h apart, while vertices at distance at most 2 receive labels at least k apart. The goal of the L(h, k)-labeling problem is to produce a legal labeling that minimizes the largest label used. Since the decision version of the L(h, k)-labeling problem is NP-complete, it is important to investigate classes of graphs for which the problem can be solved efficiently. Along this line of thought, in this article we deal with co-comparability graphs, its subclass of interval graphs, and circular-arc graphs. To the best of our knowledge, ours is the first reported result concerning the L(h, k)-labeling of co-comparability and circular-arc graphs. In particular, we provide the first algorithm to L(h, k)-label co-comparability, interval, and circular-arc graphs with a bounded number of colors. Finally, in the special case where k = 1 and G is an interval graph, our algorithm improves on the best previously-known ones using a number of colors that is at most twice the optimum. \u00a9 2008 Wiley Periodicals, Inc. NETWORKS, 2009\nTitle:\nOn the L(h, k)-labeling of co-comparability graphs and circular-arc graphs\n\nAbstract:\nAn L(h,k)-labeling of a graph G is an integer labeling of vertices of G, such that adjacent vertices have labels which differ by at least h, and vertices at distance two have labels which differ by at least k. The span of an L(h,k)-labeling is the difference between the largest and the smallest label. We investigate L(h,k)-labelings of trees of maximum degree \u0394, seeking those with small span. Given \u0394, h and k, span \u03bb is optimal for the class of trees of maximum degree \u0394, if \u03bb is the smallest integer such that every tree of maximum degree \u0394 has an L(h,k)-labeling with span at most \u03bb. For all parameters \u0394,h,k, such that h<k, we construct L(h,k)-labelings with optimal span. We also establish optimal span of L(h,k)-labelings for stars of arbitrary degree and all values of h and k.\nTitle:\nLabeling trees with a condition at distance two\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.755, mean value for all the writers is 8.648, which makes it 0.091 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nGiven an undirected graph G, an L(h, k)-labelling of G assigns colors to vertices from the integer set {0,.. lambda(h,k)}, such that any two vertices v(i) and v(j) receive colors c(v(i)) and c(v(j)) satisfying the following conditions: i) if v(i) and v(j) are adjacent then vertical bar c(v(i)) - c(v(j))vertical bar >= h; ii) if v(i) and v(j) are at distance two then vertical bar c(v(i)) - c(v(j))vertical bar >= k. The aim of the L(h, k)-labelling problem is to minimize lambda(h,k)- In this paper we study the approximability of the L(h,k)-labelling problem on bipartite graphs and extend the results to s-partite and general graphs. Indeed, the decision version of this problem is known to be DIP-complete in general and, to our knowledge, there are no polynomial solutions, either exact or approximate, for bipartite graphs. Here, we state some results concerning the approximability of the L(h,k)-labelling problem for bipartite graphs, exploiting a novel technique, consisting in computing approximate vertex- and edge-colorings of auxiliary graphs to deduce an L(h, k)-labelling for the input bipartite graph. We derive an approximation algorithm with performance ratio bounded by (4)/D-3(2), where, D is equal to the minimum even value bounding the minimum of the maximum degrees of the two partitions. One of the above coloring algorithms is in fact an approximating edge-coloring algorithm for hypergraphs of maximum dimension d, i.e. the maximum edge cardinality, with performance ratio d. Furthermore, we consider a different approximation technique based on the reduction of the L(h, k)-labelling problem to the vertex-coloring of the square of a graph. Using this approach we derive an approximation algorithm with performance ratio bounded by min(h, 2k)root n + o(k root n), assuming h >= k. Hence, the first technique is competitive when D O(n(1/4)) These algorithms match with a result in [2] stating that L(1,1) labelling n-vertex bipartite graphs is hard to approximate within(n1/2-)epsilon, for any epsilon > 0, unless NP=ZPP. We then extend the latter approximation strategy to s-partite graphs, obtaining a (min(h, sk)root n + o(sk root n))-approximation ratio, and to general graphs deriving an (h root n + o(h root n))-approximation algorithm, assuming h >= k. Finally, we prove that the L(h, k)-labelling problem is not easier than coloring the square of a graph.\nTitle:", "model_inf_time": 1.85}, {"id": "41215", "output": "Contingent Evaluation of Information Systems Structure and Business Performance", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe disruption of operations due to IS failure becomes more important as IS has become an increasingly essential component of the organization's operations and can affect its strategic objectives. Nevertheless, traditional IS risk analysis methods do not adequately reflect the loss from disruption of operations in determining the value of IS assets. Quantitative methods do not measure the loss from disruption of operations. Qualitative methods consider the loss, but their results are subjective and not suitable for cost-benefit decision support. There is a lack of systematic methods to measure the value of IS assets from the viewpoint of operational continuity.This study presents an IS risk analysis method based on a business model. The method uses a systematic quantitative approach dealing with operational continuity: the importance of various business functions and the necessity level of various assets are first determined. The value of each asset is then determined based on these two levels.The proposed method adds the first stage, organizational investigation, to traditional risk analysis. The process of the method utilizes various methodologies such as paired comparison, asset-function assignment tables, and asset dependency diagrams.\nTitle:\nThe IS risk analysis based on a business model\n\nAbstract:\nIn response to the call for more research on impactful green IS, this paper examines one country-level environmental information disclosure system (EIDs), the U.S. Toxic Release Inventory (TRI), as a strategic tool or infrastructure of green IS and empirically tests the impact of environmental performance on financial performance in the chemical industry using both real financial and forward-looking measures. In particular, the study explores EDIs-inspired environmental managerial efforts and their influence on firms' financial performance. The study's finding that firms with better environmental performance also show greater cost competitiveness challenges the view that environmental managerial activities constitute an additional burden irrespective of firms' business operations. Instead, it suggests, these can be innovative activities that improve a firm's production efficiency, material resource management, and financial performance and sustainability.\nTitle:\nThe Role of Environmental Information Disclosure Systems and Their Impact on Firm Performance\n\nAbstract:\nIntellectual capital (IC) has prevailed as a measure of core competency and competitive advantage which explains the gap between the market value and book value of an organization at a time of decreasing usefulness of current financial reporting. In spite of the importance of IC management (ICM), few applicable ICM methodologies have been addressed. There has been no basis model for IC statements, nor bottom-line indicators of the value of IC. The selection of effective IC indicators is a major task of the companies that are preparing IC reports. This paper has proposed a decision model based on the analysis of the conceptual framework of the qualitative characteristics of financial information and an examination of information quality of the information system. The application of the analytic hierarchy process makes it possible to extract weights for setting the priority among criteria in the mobile telecommunications industry. During the last decade, this industry has experienced a dramatic growth in its intellectual value, and the value of IC seems to have had a major impact on the value of the mobile telecommunications companies. Based on specified criteria and weighting, this paper presents the results of a case study illustrating the results of selected indicators from candidate indicators in the mobile telecommunications company.\nTitle:\nPrioritization and selection of intellectual capital measurement indicators using analytic hierarchy process for the mobile telecommunications industry\n\nAbstract:\nThis paper investigates the asymmetric costs of false positive and negative errors to enhance the IDS performance. The proposed method utilizes the neural network model to consider the cost ratio of false negative errors to false positive errors. Compared with false positive errors, false negative errors incur a greater loss to organizations which are connected to the systems by networks. This method is designed to accomplish both security and system performance objectives. The results of our empirical experiment show that the neural network model provides high accuracy in intrusion detection. In addition, the simulation results show that the effectiveness of intrusion detection can be enhanced by considering the asymmetric costs of false negative and false positive errors.\nTitle:\nThe neural network models for IDS based on the asymmetric costs of false negative errors and false positive errors\n\nAbstract:\nAttracting new users is critical for the success of new information and communication technologies (ICT) such as mobile data services (MDS). Given the rapid growth and large investments in ICT, it is important to understand the formation processes of user behaviors in the ICT environment. This study develops a theoretical framework to examine the role of utilitarian and hedonic values in an MDS adoption phenomenon. This study also presents an investigation of the key antecedents of utilitarian and hedonic values to understand the mechanism of enhancing these values. This study posits information quality, system quality, and perceived fee as the key antecedents of utilitarian and hedonic values. The proposed research model is empirically evaluated by using survey data collected from 120 potential adopters. The results of this study show that the adoption intention is solely determined by utilitarian value. The findings also indicate that the information quality and perceived fee play a significant role in the formation of adoption intention of MDS. Theoretical and practical implications of the findings are discussed.\nTitle:\nThe role of utilitarian and hedonic values and their antecedents in a mobile data service environment\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 12.526, mean value for all the writers is 8.648, which makes it 3.309 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper argues that the contingent evaluation approach can help an organization to evaluate the impact of information systems (IS) on business performance. The structure of IS is one of the most important factors in applying the contingent approach, because it reflects the evolution of the computing environment and is also aligned with business objectives and strategies. We empirically derived four taxonomics of IS structure and tested the relationships between the derived typologies and the prioritized or adopted measures of their performance. Firms with different IS structure were found to consider some measures to be more appropriate than others. The empirical results support the idea that the contingent measurement approach has the potential to be an even more useful framework if it incorporates more contingency factors, such as environmental, organizational contexts, and other characteristics.\nTitle:", "model_inf_time": 1.5}, {"id": "41216", "output": "Improved Iterative Recoding Algorithms Using Preprocessing Rules", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nOrder-w reprocessing is a suboptimal soft-decision decoding approach for binary linear block codes in which up to w bits are systematically flipped on the so-called most reliable (information) basis (MRB). This correspondence first incorporates two preprocessing rules into order-w reprocessing and shows that, with appropriate choice of parameters, the proposed order-w reprocessing with preprocessing requires comparable complexity to order-w reprocessing but achieves asymptotically the performance of order-(w+2) reprocessing. To complement the MRB, a second basis is employed for practical SNRs and this approach is systematically extended to a multibasis order-w reprocessing scheme for high signal-to-noise ratios (SNRs). It is shown that the proposed multibasis scheme significantly enlarges the error-correction radius, a commonly used measure of performance at high SNRs, over the original (single-basis) order-w reprocessing. As a by-product, this approach also precisely characterizes the asymptotic performance of the well-known Chase and generalized minimum distance (GMD) decoding algorithms. The proposed algorithm successfully decodes the (192,96) Reed-Solomon concatenated code and the (256,147) extended BCH code in near optimal manner (within 0.01 dB at a block-error rate of 10-5) with affordable computational cost\nTitle:\nSoft-Decision Decoding of Linear Block Codes Using Preprocessing and Diversification\n\nAbstract:\nThis paper proposes a sub-optimal maximum-likelihood decoding algorithm for linear block codes. Given reliability information about each bit in the received word, our approach initially finds the set of most reliable \"basis\" information bits; it then searches for the most likely codeword by iteratively flipping bits in this basis set, constructing the corresponding codeword and determining its likelihood. The search of \"test error patterns\" on the set of basis information bits is conducted so that at each iteration the new set of basis information bits differs from a previously studied (and stored) one in a single bit position. Thus, the new codeword can be obtained from a stored codeword by modifying only the redundant bits that depend on the modified bit. The space complexity of the proposed decoding scheme is polynomial in the code length. We also describe an heuristic criterion to approximate the optimal set of test error patterns subject to a limit on the maximum computational effort. Simulation results with codes of relatively large length show that the proposed approach is very efficient, especially under low SNR\nTitle:\nSoft-decision decoding of linear block codes using efficient iterative G-space encodings\n\nAbstract:\nThis correspondence investigates soft-decision decoding of binary linear block codes using ordered recodings of test error patterns on the so-called \"most reliable basis.\" The analysis demonstrates the optimality of the most reliable basis by showing that, among all possible bases, the most reliable basis minimizes the list error probability for a very general (and well-defined) class of orderings for recoding operations. The correspondence then proposes a suboptimal algorithm which utilizes reprocessing ordering and incorporates two techniques that render it computationally very efficient: 1) an iterative reference recoding technique which simplifies the recoding operation required for each test error pattern, and 2) an adaptive skipping rule which significantly reduces the average number of recodings. Simulation results with codes of relatively large length show that the proposed algorithm is computationally very efficient in comparison to existing algorithms in the literature\nTitle:\nSoft-Decision Decoding Using Ordered Recodings on the Most Reliable Basis\n\nAbstract:\nIn this paper we consider a deterministic worst-case framework for perfect reconstruction of discrete data transmission through a dispersive communication channel. More specifically, we extend our previous work to capture time-varying transmission dynamics that also include the case of linear time-varying preprocessing of the data without increasing the power of the transmitted signal. We present necessary and sufficient conditions for perfect reconstructability and formulate a framework for the synthesis of an optimal preprocessor. In the case of periodic preprocessing followed by (periodic) DFE at the receiving end, we provide a design procedure based on l1-optimization. The application of this procedure to representative communication channels suggests that careful choice of linear time-varying preprocessing can significantly enhance our ability for perfect reconstruction without increasing the power of the transmitted signal\nTitle:\nTime-varying power-limited preprocessing for perfect reconstruction of binary signals\n\nAbstract:\nIn this paper, we study the application of decoding algorithms to the multiple fault diagnosis (MFD) problem. Prompted by the resemblance between graphical representations for MFD problems and parity check codes, we develop a suboptimal iterative belief propagation algorithm (BPA) that is based on the graphical inference method for low density parity check codes. Our simulation results suggest that the algorithm performance strongly depends on the connection density and the reliability of the alarm network. In particular, when the connection density is low and when the alarms and/or connections are unreliable, the algorithm performs almost optimally, i.e., it converges to the solution with the highest posterior probability most of the times. We also provide analytical bounds on the performance of the algorithm for special classes of systems in our framework\nTitle:\nGraphical Inference Methods for Fault Diagnosis based on Information from Unreliable Sensors\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.206, mean value for all the writers is 8.648, which makes it 1.329 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, two universally applicable preprocessing rules are developed to improve iterative receding algorithms that utilize the most reliable basis (MRB), while focusing on order-w reprocessing. Simulation results show that the proposed algorithm successfully decodes the Reed-Solomon concatenated code in near optimal manner at a block-error-rate with the maximum number of candidate codewords.\nTitle:", "model_inf_time": 1.39}, {"id": "41217", "output": "Vector Representation of Stochastic Signals for Pattern Recognition", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis letter demonstrates hidden Markov model (HMM), multilayer perceptron (MLP), and time-delay recursive neural network (TDRNN) architectures for the purpose of recognizing pitch accents given observation of the F0 and energy trajectories. At an insertion error rate of 25%, the deletion error rates of the MLP, TDRNN, and HMM are 13.2%, 7.9%, and 32.7%, respectively, despite the fact that both MLP...\nTitle:\nAutomatic Recognition of Pitch Movements Using Multilayer Perceptron and Time-Delay Recursive Neural Network\n\nAbstract:\nIn this paper, we propose a novel saliency-based algorithm to detect foreground regions in highly dynamic scenes. We first convert input video frames to multiple patch-based feature maps. Then, we apply temporal saliency analysis to the pixels of each feature map. For each temporal set of co-located pixels, the feature distance of a point from its kth nearest neighbor is used to compute the temporal saliency. By computing and combining temporal saliency maps of different features, we obtain foreground likelihood maps. A simple segmentation method based on adaptive thresholding is applied to detect the foreground objects. We test our algorithm on images sequences of dynamic scenes, including public datasets and a new challenging wildlife dataset we constructed. The experimental results demonstrate the proposed algorithm achieves state-of-the-art results.\nTitle:\nForeground object detection in highly dynamic scenes using saliency\n\nAbstract:\nIn this paper, we address the problem of object class recognition via observations from actively selected views/modalities/features under limited resource budgets. A Partially Observable Markov Decision Process (POMDP) is employed to find optimal sensing and recognition actions with the goal of long-term classification accuracy. Heterogeneous resource constraints--such as motion, number of measurements and bandwidth--are explicitly modeled in the state variable, and a prohibitively high penalty is used to prevent the violation of any resource constraint. To improve recognition performance, we further incorporate discriminative classification models with POMDP, and customize the reward function and observation model correspondingly. The proposed model is validated on several data sets for multi-view, multi-modal vehicle classification and multi-view face recognition, and demonstrates improvement in both recognition and resource management over greedy methods and previous POMDP formulations.\nTitle:\nActive Planning, Sensing, and Recognition Using a Resource-Constrained Discriminant POMDP\n\nAbstract:\nOne important class of state emission densities of the hiddenMarkov model (HMM) is the Gaussian mixture densities. The classical Baum-Welch algorithm often fails to reliably learn the Gaussian mixture densities when there is insufficient training data, due to the large number of free parameters present in the model. In this paper, we propose a novel strategy for robustly and accurately learning the Gaussian mixture state emission densities of the HMM. The strategy is based on an ensemble framework for probability density estimation in which the learning of the Gaussian mixture densities is formulated as a gradient descent search in a function space. The resulting learning algorithm is called \u201cthe boosting Baum-Welch algorithm.\u201d Our preliminary experiment results on emotion recognition from speech show that the proposed algorithm outperforms the original Baum-Welch algorithm on this task.\nTitle:\nToward robust learning of the Gaussian mixture state emission densities for hidden Markov models\n\nAbstract:\nSpeech perceptual features, such as Mel-frequency Cepstral Coefficients (MFCC), have been widely used in acoustic event detection. However, the different spectral structures between speech and acoustic events degrade the performance of the speech feature sets. We propose quantifying the discriminative capability of each feature component according to the approximated Bayesian accuracy and deriving a discriminative feature set for acoustic event detection. Compared to MFCC, feature sets derived using the proposed approaches achieve about 30% relative accuracy improvement in acoustic event detection.\nTitle:\nFeature analysis and selection for acoustic event detection\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.726, mean value for all the writers is 8.648, which makes it 0.92 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this letter, we propose a novel vector representation of stochastic signals for pattern recognition (PR) based on adapted ergodic hidden Markov models (HMMs). This vector representation is generic in nature and may be used with various types of stochastic signals (e.g., image, speech, etc.) and applied to a broad range of PR tasks (e.g., classification, regression, etc.). More importantly, by combining the vector representation with optimal distance metric learning (e.g., linear discriminant analysis) directly from the data, the performance of a PR system may be significantly improved. Our experiments on an image-based recognition task clearly demonstrate the effectiveness of the proposed vector representation of stochastic signals for potential use in many PR systems.\nTitle:", "model_inf_time": 1.16}, {"id": "41218", "output": "Coding-Oriented Clustering with Gray Code for Noise-Robust and Shape-Adaptive Clustering", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present in this paper a new clustering algorithm for multivariate data. This algorithm, called BOOL (Binary coding Oriented clustering), can detect arbitrarily shaped clusters and is noise tolerant. BOOL handles data using a two-step procedure: data points are first discretized and represented as binary words, clusters are then iteratively constructed by agglomerating smaller clusters using this representation. This latter step is carried out with linear complexity by sorting such binary representations, which results in dramatic speedups when compared with other techniques. Experiments show that BOOL is faster than K-means, and about two to three orders of magnitude faster than two state-of-the-art algorithms that can detect non-convex clusters of arbitrary shapes. We also show that BOOL's results are robust to changes in parameters, whereas most algorithms for arbitrarily shaped clusters are known to be overly sensitive to such changes. The key to the robustness of BOOL is the hierarchical structure of clusters that is introduced automatically by increasing the accuracy of the discretization.\nTitle:\nA Fast and Flexible Clustering Algorithm Using Binary Discretization\n\nAbstract:\nOnly few machine learning methods; e.g., the decision tree-based classification method, can handle mixed-type data sets containing both of discrete (binary and nominal) and continuous (real-valued) variables and, moreover, no semi-supervised learning method can treat such data sets directly. Here we propose a novel semi-supervised learning method, called SELF (SEmi-supervised Learning via FCA), for mixed-type data sets using Formal Concept Analysis (FCA). SELF extracts a lattice structure via FCA together with discretizing continuous variables and learns classification rules using the structure effectively. Incomplete data sets including missing values can be handled directly in our method. We experimentally demonstrate competitive performance of SELF compared to other supervised and semi-supervised learning methods. Our contribution is not only giving a novel semi-supervised learning method, but also bridging two fields of conceptual analysis and knowledge discovery.\nTitle:\nSemi-supervised learning for mixed-type data via formal concept analysis\n\nAbstract:\nWe propose a new approach for semi-supervised learning using closed set lattices, which have been recently used for frequent pattern mining within the framework of the data analysis technique of Formal Concept Analysis FCA. We present a learning algorithm, called SELF SEmi-supervised Learning via FCA, which performs as a multiclass classifier and a label ranker for mixed-type data containing both discrete and continuous variables, while only few learning algorithms such as the decision tree-based classifier can directly handle mixed-type data. From both labeled and unlabeled data, SELF constructs a closed set lattice, which is a partially ordered set of data clusters with respect to subset inclusion, via FCA together with discretizing continuous variables, followed by learning classification rules through finding maximal clusters on the lattice. Moreover, it can weight each classification rule using the lattice, which gives a partial order of preference over class labels. We illustrate experimentally the competitive performance of SELF in classification and ranking compared to other learning algorithms using UCI datasets.\nTitle:\nSemi-supervised learning on closed set lattices\n\nAbstract:\nIn this paper, we study a learning procedure from positive data for bounded unions of certain class of languages. Our key tools are the notion of characteristic sets and hypergraphs. We generate hypergraphs from given positive data and exploit them in order to find characteristic sets.\nTitle:\nLearning Bounded Unions of Noetherian Closed Set Systems Via Characteristic Sets\n\nAbstract:\nIn this paper, we propose a multi-class classification algorithm to apply it to data sets increasing frequently. The algorithm performs lazy learning based on formal concept analysis. We designed it so that it obtains localness in predicting classes of test data and feature selection simultaneously. From a given data set that consists of a set of training data and a set of test data, the algorithm generates a single formal concept lattice. Every formal concept in the lattice represents a cluster of data that are generated by various feature selections. In order to classify each test datum, plausible clusters are selected and combined into a set of neighbors for the test datum. Our algorithm can construct sets of neighbors for test data that are never generated by other algorithms, e.g., the k-nearest neighbor algorithm and decision tree classifiers. We compare our algorithm with other algorithms by experiments using UCI datasets and show that ours is comparable to the others at the viewpoint of correctness.\nTitle:\nLocal Feature Selection by Formal Concept Analysis for Multi-class Classification.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.184, mean value for all the writers is 8.648, which makes it 0.457 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose new approaches to exploit compression algorithms for clustering numerical data. Our first contribution is to design a measure that can score the quality of a given clustering result under the light of a fixed encoding scheme. We call this measure the Minimum Code Length (MCL). Our second contribution is to propose a general strategy to translate any encoding method into a cluster algorithm, which we call COOL (COding-Oriented cLustering). COOL has a low computational cost since it scales linearly with the data set size. The clustering results of COOL is also shown to minimize MCL. To illustrate further this approach, we consider the Gray Code as the encoding scheme to present GCOOL. G-COOL can find clusters of arbitrary shapes and remove noise. Moreover, it is robust to change in the input parameters; it requires only two lower bounds for the number of clusters and the size of each cluster, whereas most algorithms for finding arbitrarily shaped clusters work well only if all parameters are tuned appropriately. G-COOL is theoretically shown to achieve internal cohesion and external isolation and is experimentally shown to work well for both synthetic and real data sets.\nTitle:", "model_inf_time": 1.68}, {"id": "41219", "output": "A Game-Theoretic Approach for Dynamic Mode Switching in Intrusion Detection Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we address the problem of increasing the effectiveness of an intrusion detection system (IDS) for a cluster of nodes in ad hoc networks. To reduce the performance overhead of the IDS, a leader node is usually elected to handle the intrusion detection service on behalf of the whole cluster. However, most current solutions elect a leader randomly without considering the resource level of nodes. Such a solution will cause nodes with less remaining resources to die faster, reducing the overall lifetime of the cluster. It is also vulnerable to selfish nodes who do not provide services to others while at the same time benefiting from such services. Our experiments show that the presence of selfish nodes can significantly reduce the effectiveness of an IDS because less packets are inspected over time. To increase the effectiveness of an IDS in MANET, we propose a unified framework that is able to: (1) Balance the resource consumption among all the nodes and thus increase the overall lifetime of a cluster by electing truthfully and efficiently the most cost-efficient node known as leader-IDS. A mechanism is designed using Vickrey, Clarke, and Groves (VCG) to achieve the desired goal. (2) Catch and punish a misbehaving leader through checkers that monitor the behavior of the leader. A cooperative game-theoretic model is proposed to analyze the interaction among checkers to reduce the false-positive rate. A multi-stage catch mechanism is also introduced to reduce the performance overhead of checkers. (3) Maximize the probability of detection for an elected leader to effectively execute the detection service. This is achieved by formulating a zero-sum non-cooperative game between the leader and intruder. We solve the game by finding the Bayesian Nash Equilibrium where the leader's optimal detection strategy is determined. Finally, empirical results are provided to support our solutions.\nTitle:\nA game-theoretic intrusion detection model for mobile ad hoc networks\n\nAbstract:\nIn this paper, we consider the problem of reducing the number of false positives generated by cooperative Intrusion Detection Systems (IDSs) in Mobile Ad hoc Networks (MANETs). We define a flexible scheme using security classes, where an IDS is able to operate in different modes at each security class. This scheme helps in minimizing false alarms and informing the prevention system accurately about the severity of an intrusion. Shapley value is used to formally express the cooperation among all the nodes. To the best of our knowledge, there has not been any study for the case where the intrusions in MANETs are analyzed, in order to decrease false positives, using cooperative game theory. Our game theoretic model assists in analyzing the contribution of each mobile node on each security class in order to decrease false positives taking into consideration the reputation of nodes. Simulation results are given to validate the efficiency of our model in detecting intrusions and reducing false positives.\nTitle:\nA Cooperative Approach for Analyzing Intrusions in Mobile Ad hoc Networks\n\nAbstract:\nIn this paper, we consider the problem of increasing the effectiveness of an Intrusion Detection System (IDS) for a cluster of nodes in ad hoc networks. To solve such a prob- lem, a head cluster is elected by the nodes to handle the de- tection service. Current solution elects a leader randomly without considering the energy level of nodes. Such solu- tion is vulnerable to selfish nodes that do not provide IDS service to others while at the same time benefiting from oth- ers' services. From our experiments, selfish nodes reduce the effectiveness of an IDS since less packets are inspected over time. Here, we are modeling a distributed, truthful, and efficient mechanism for electing a leader IDS that han- dles the detection process in a cluster. Our solution is able to balance the energy among all the nodes and increase the overall lifetime of an IDS in a cluster. In our model, incen- tives are given in the form of reputation to encourage the nodes to cooperate in the leader election process. The rep- utation is used to track the cooperative behavior of nodes where miss-behaving nodes are punished by withholding the cluster's services. Reputations are calculated based on the truth-telling mechanism design known as Vickrey, Clarke, and Groves (VCG). Our analysis prove that truth-telling is the dominant strategy for all the nodes and therefore effi- ciency is guaranteed. Finally, simulation results show that our mechanism improves the performance of an IDS in an- alyzing packets and punishes misbehaving nodes.\nTitle:\nAn Efficient and Truthful Leader IDS Election Mechanism for MANET\n\nAbstract:\nIn this paper, we study using game theory the problem of detecting intrusions in wired infrastructure networks. Detection is accomplished by sampling a subset of the transmitted packets over selected network links or router interfaces. Given a total sampling budget, our framework aims at developing a network packet sampling strategy to effectively reduce the success chances of an intruder. We consider two different scenarios: (1) A well informed intruder divides his attack over multiple packets in order to increase his chances of successfully intruding a target domain. (2) Different cooperating intruders distribute the attack among themselves each send their attack fragments to the target node. Each of the packets containing a fragment of the attack is transmitted through a different path using multi-path routing, where each path is selected with a different probability. Knowing that, if these packets are independently analyzed then the intrusion will not be detected, i.e., a series of packets form an intrusion. To the best of our knowledge, there has not been any work done for the case where the attack is split over multiple packets or distributed over cooperative intruders using game theory. Non-cooperative game theory is used to formally express the problem, where the two players are: (1) the smart intruder or the cooperative intruders (depends on which scenario we are solving) and (2) the Intrusion Detection System (IDS). Our game theoretic framework will guide the intruder or the intruders to know their attack strategy and the IDS to have an optimal sampling strategy in order to detect the malicious packets.\nTitle:\nGame theoretic models for detecting network intrusions\n\nAbstract:\nIn this paper, we study the election of multiple leaders for intrusion detection in the presence of selfish nodes in mobile ad hoc networks (MANETs). To balance the resource consumption and prolong the lifetime of all nodes, each cluster should elect a node with the most remaining resources as its leader. However, without incentives for serving others, a node may behave selfishly by lying about its remaining resource and avoiding being elected. We present a solution based on mechanism design theory. More specifically, we design a scheme for electing cluster leaders that have the following two advantages: First, the collection of elected leaders is the optimal in the sense that the overall resource consumption will be balanced among all nodes in the network overtime. Second, the scheme provides the leaders with incentives in the form of reputation so that nodes are encouraged to honestly participate in the election process. The design of such incentives is based on the Vickrey, Clarke, and Groves (VCG) model by which truth-telling is the dominant strategy for each node. Simulation results show that our scheme can effectively prolong the overall lifetime of IDS in MANET and balance the resource consumptions among all the nodes.\nTitle:\nA Mechanism Design-Based Multi-Leader Election Scheme for Intrusion Detection in MANET\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.914, mean value for all the writers is 8.648, which makes it 0.227 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOne popular solution for reducing the resource consumption of Intrusion Detection System (IDS) in MANET is to elect a head-cluster (leader) to provide intrusion detection serviceto other nodes in the same cluster. However, such a moderate mode is only suitable when the probability of atack is low. Once the probability of attack is high, victim nodes should launch their own IDSs to detect and thwart intrusions. Such a robust mode is, however, costly with respect to energy and leads nodes to die faster. Clearly, to reduce the resource consumption of IDSs andyet keep its effectiveness, a critical issue is: When should we shift from moderate to robust mode? In this paper, we formalize this issue as a nonzero-sum noncooperative game theoretical model that takes into consideration the tradeoff between security and IDS resource consumption. The game solution will guide theleader-IDS to find the right moment for notifying the victimnode to launch its IDS once the security risk is high enough. To achieve this goal, the Bayesian game theory is used to analyze the interaction between the leader-IDS and intruder with incomplete information about the intruder. By solving such a game, we are able to find the threshold value for notifying the victim node to launch its IDS once the probability of attack exceeds that value. Simulation results show that our scheme can effectively reduce the IDS resource consumption without sacrificing security.\nTitle:", "model_inf_time": 1.95}, {"id": "41220", "output": "A Single Algorithm for Site Initialization, Recovery, and Backup in Distributed Databases", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nConcurrency control algorithms for database systems are usually regarded as methods for synchronizing Read and Write operations. Such methods are judged to be correct if they only produce serializable executions. However, Reads and Writes are sometimes inaccurate models of the operations executed by a database system. In such cases, serializability does not capture all aspects of concurrency control executions. To capture these aspects, we describe a proof schema for analyzing concurrency control correctness. We illustrate the proof schema by presenting two new concurrency algorithms for distributed database systems.\nTitle:\nAnalyzing Concurrency Control Algorithms When User and System Operations Differ\n\nAbstract:\nConcurrency control is the activity of synchronizing operations issued by concurrently executing programs on a shared database. The goal is to produce an execution that has the same effect as a serial (noninterleaved) one. In a multiversion database system, each write on a data item produces a new copy (or version) of that data item. This paper presents a theory for analyzing the correctness of concurrency control algorithms for multiversion database systems. We use the theory to analyze some new algorithms and some previously published ones.\nTitle:\nConcurrency control algorithms for multiversion database systems\n\nAbstract:\nConcurrency control is the activity of synchronizing operations issued by concurrently executing programs on a shared database. The goal is to produce an execution that has the same effect as a serial (noninterleaved) one. In a multiversion database system, each write on a data item produces a new copy (or version) of that data item. This paper presents a theory for analyzing the correctness of concurrency control algorithms for multiversion database systems. We use the theory to analyze some new algorithms and some previously published ones.\nTitle:\nMultiversion concurrency control\u2014theory and algorithms\n\nAbstract:\nA replicated database is a distributed database in which some data items are stored redundantly at multiple sites. The main goal is to improve system reliability. By storing critical data at multiple sites, the system can operate even though some sites have failed. However, few distributed database systems support replicated data, because it is difficult to manage as sites fail and recover. A replicated data algorithm has two parts. One is a discipline for reading and writing data item copies. The other is a concurrency control algorithm for synchronizing those operations. The read-write discipline ensures that if one transaction writes logical data item \u00d7, and another transaction reads or writes x, there is some physical manifestation of that logical conflict. The concurrency control algorithm synchronizes physical conflicts; it knows nothing about logical conflicts. In a correct replicated data algorithm, the physical manifestation of conflicts must be strong enough so that synchronizing physical conflicts is sufficient for correctness. This paper presents a theory for proving the correctness of algorithms that manage replicated data. The theory is an extension of serializability theory. We apply it to three replicated data algorithms: Gifford's \u201cquorum consensus\u201d algorithm, Eager and Sevcik's \u201cmissing writes\u201d algorithm, and Computer Corporation of America's \u201cavailable copies\u201d algorithm.\nTitle:\nThe failure and recovery problem for replicated databases\n\nAbstract:\nWe decompose the problem of concurrency control into the sub-problems of read-write and write-write synchronization. We present a series of timestamp- based algorithms (called synchronization techniques) that achieve read-write and/or write-write synchronization. And we show how to combine any read-write technique with any write-write technique to yield a complete concurrency control algorithm (called a method). Using this framework we describe 12 \"principal\" concurrency control methods in detail. Each principal method can be modified by refinements described in the paper, leading to more than 50 distinct concurrency control algorithms.\nTitle:\nTimestamp-based algorithms for concurrency control in distributed database systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.422, mean value for all the writers is 8.648, which makes it 1.046 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSite initialization is the problem of integrating a new site into a running distributed database system (DDBS). Site recovery is the problem of integrating an old site into a DDBS when the site recovers from failure. Site backup is the problem of creating a static backup copy of a database for archival or query purposes. We present an algorithm that solves the site initialization problem. By modifying the algorithm slightly, we get solutions to the other two problems as well. Our algorithm exploits the fact that a correct DDBS must run a serializable concurrency control algorithm. Our algorithm relies on the concurrency control algorithm to handle all intersite synchronization.\nTitle:", "model_inf_time": 1.46}, {"id": "41221", "output": "STEPS: A 3D Spatio-temporal Visualization Tool for Electric Power Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nRecent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed fo...\nTitle:\nGAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation.\n\nAbstract:\nWhile deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequ...\nTitle:\nActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models.\n\nAbstract:\nThe field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each tec...\nTitle:\nGLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration.\n\nAbstract:\nDeep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpre...\nTitle:\nVisual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers.\n\nAbstract:\nThe traditional data analysis tools support strong computational capabilities and numerous standard visualization techniques. However, they provide little visual interactions due to the fact that the tools maintain a wide applicability to diverse data domains, and thus any inherent meanings associated with the data domains are hardly allowed. To cover these limitations, we propose to augment Mat lab, one of the widely used data analysis tools and computational languages, by imposing the capabilities of handling semantic objects so that diverse essential interaction capabilities could be allowed such as brushing-and-linking, details-on-demand, and dynamic interactive updating on visualization. In our demonstration, we will show our audience how to import semantic data, how visual interactions are occurred, and how these functionalities are convenient using the movie similarity graph data set.\nTitle:\nInteractive Data Analysis Tool by Augmenting MATLAB with Semantic Objects\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.194, mean value for all the writers is 8.648, which makes it 0.466 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAs the bulk electric grid becomes more complex, power system operators and engineers have more information to process and interpret than ever before. The information overload they experience can be mitigated by effective visualizations that facilitate rapid and intuitive assessment of the system state. With the introduction of non-dispatchable renewable energy, flexible loads, and energy storage, the ability to temporally explore system states becomes critical. This paper introduces STEPS, a new 3D Spatio-temporal Electric Power Systems visualization tool suitable for steady-state operational applications.\nTitle:", "model_inf_time": 1.41}, {"id": "41222", "output": "C-DEM: Tri-Modal Querying and Navigation of Drosophila Embryo Images", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMotivation: Microarray profiling of mRNA abundance is often ill suited for temporal-spatial analysis of gene expressions in multicellular organisms such as Drosophila. Recent progress in image-based genome-scale profiling of whole-body mRNA patterns via in situ hybridization (ISH) calls for development of accurate and automatic image analysis systems to facilitate efficient mining of complex temporal-spatial mRNA patterns, which will be essential for functional genomics and network inference in higher organisms. Results: We present SPEX2, an automatic system for embryonic ISH image processing, which can extract, transform, compare, classify and cluster spatial gene expression patterns in Drosophila embryos. Our pipeline for gene expression pattern extraction outputs the precise spatial locations and strengths of the gene expression. We performed experiments on the largest publicly available collection of Drosophila ISH images, and show that our method achieves excellent performance in automatic image annotation, and also finds clusters that are significantly enriched, both for gene ontology functional annotations, and for annotation terms from a controlled vocabulary used by human curators to describe these images.\nTitle:\nSPEX2: automated concise extraction of spatial gene expression patterns from Fly embryo ISH images.\n\nAbstract:\nWe propose a method to handle approximate searching by image content in medical image databases. Image content is represented by attributed relational graphs holding features of objects and relationships between objects. The method relies on the assumption that a fixed number of \u201clabeled\u201d or \u201cexpected\u201d objects (e.g., \u201cheart\u201d, \u201clungs\u201d, etc.) are common in all images of a given application domain in addition to a variable number of \u201cunexpected\u201d or \u201cunlabeled\u201d objects (e.g., \u201ctumor\u201d, \u201chematoma\u201d, etc.). The method can answer queries by example, such as \u201cfind all X-rays that are similar to Smith's X-ray\u201d. The stored images are mapped to points in a multidimensional space and are indexed using state-of-the-art database methods (R-trees). The proposed method has several desirable properties: (a) Database search is approximate, so that all images up to a prespecified degree of similarity (tolerance) are retrieved. (b) It has no \u201cfalse dismissals\u201d (i.e., all images qualifying query selection criteria are retrieved). (c) It is much faster than sequential scanning for searching in the main memory and on the disk (i.e., by up to an order of magnitude), thus scaling-up well for large databases\nTitle:\nSimilarity searching in medical image databases\n\nAbstract:\nThis paper presents a max margin framework on image annotation and multimodal image retrieval as a structured prediction model. Following the max margin approach the image retrieval problem is formulated as a quadratic programming problem. By properly selecting joint feature representation between different modalities, our framework captures the dependency information between different modalities and avoids retraining the model from scratch when database undergoes dynamic updates. While this framework is a general approach which can be applied to multimodal information retrieval in any domains, we apply this approach to the Berkeley Drosophila embryo image database for the evaluation purpose. Experimental results show significant performance improvements over a state-of-the-art method.\nTitle:\nA Max Margin Framework On Image Annotation And Multimodal Image Retrieval\n\nAbstract:\nGiven a large collection of images, very few of which have labels, how can we guess the labels of the remaining majority, and how can we spot those images that need brand new labels, different from the existing ones? Current automatic labeling techniques usually scale super linearly with the data size, and/or they fail when only a tiny amount of labeled data is provided. In this paper, we propose QMAS (Querying, Mining And Summarization of Multi-modal Databases), a fast solution to the following problems: (i) low-labor labeling (L3) \u2013 given a collection of images, very few of which are labeled with keywords, find the most suitable labels for the remaining ones, and (ii) mining and attention routing \u2013 in the same setting, find clusters, the top-NO outlier images, and the top-NR representative images. We report experiments on real satellite images, two large sets (1.5GB and 2.25GB) of proprietary images and a smaller set (17MB) of public images. We show that QMAS scales linearly with the data size, being up to 40 times faster than top competitors (GCap), obtaining better or equal accuracy. In contrast to other methods, QMAS does low-labor labeling (L3), that is, it works even with tiny initial label sets. It also solves both presented problems and spots tiles that potentially require new labels.\nTitle:\nQMAS: Querying, Mining and Summarization of Multi-modal Databases\n\nAbstract:\nGiven a large collection of medical images of several conditions and treatments, how can we succinctly describe the characteristics of each setting? For example, given a large collection of retinal images from several different experimental conditions (normal, detached, reattached, etc.), how can data mining help biologists focus on important regions in the images or on the differences between different experimental conditions? If the images were text documents, we could find the main terms and concepts for each condition by existing IR methods (e.g., tf/idf and LSI). We propose something analogous, but for the much more challenging case of an image collection: We propose to automatically develop a visual vocabulary by breaking images into n \u8133 n tiles and deriving key tiles (\"ViVos\") for each image and condition. We experiment with numerous domain-independent ways of extracting features from tiles (color histograms, textures, etc.), and several ways of choosing characteristic tiles (PCA, ICA). We perform experiments on two disparate biomedical datasets. The quantitative measure of success is classification accuracy: Our \"ViVos\" achieve high classification accuracy (up to 83% for a nine-class problem on feline retinal images). More importantly, qualitatively, our \"ViVos\" do an excellent job as \"visual vocabulary terms\": they have biological meaning, as corroborated by domain experts; they help spot characteristic regions of images, exactly like text vocabulary terms do for documents; and they highlight the differences between pairs of images.\nTitle:\nViVo: Visual Vocabulary Construction for Mining Biomedical Images\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.159, mean value for all the writers is 8.648, which makes it 0.417 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe amount of biological data publicly available has experienced an exponential growth as the technology advances. Online databases are now playing an important role as information repositories as well as easily accessible platforms for researchers to communicate and contribute. Recent research projects in image bioinformatics produce a number of databases of images, which visualize the spatial expression pattern of a gene (eg. \"fj\"), and most of which also have one or several annotation keywords (eg., \"embryonic hindgut\"). C-DEM is an online system for Drosophila (= fruit-fly) Embryo images Mining. It supports queries from all three modalities to all three, namely, (a) genes, (b) images of gene expression, and (c) annotation keywords of the images. Thus, it can find images that are similar to a given image, and/or related to the desirable annotation keywords, and/or related to specific genes. Typical queries are what are most suitable keywords to assign to image insitu28465.jpg or find images that are related to gene \"fj\", and to the keyword \"embryonic hindgut\". C-DEM uses state-of-the-art feature extraction methods for images (wavelets and principal component analysis). It envisions the whole database as a tri-partite graph (one type for each modality), and it uses fast and flexible proximity measures, namely, random walk with restarts (RWR). In addition to flexible querying, C-DEM allows for navigation: the user can click on the results of an earlier query (image thumbnails and/or keywords and/or genes), and the system will report the most related images (and keywords, and genes). The demo is on a real Drosophila Embryo database, with 10,204 images, 2,969 distinct genes, and 113 annotation keywords. The query response time is below one second on a commodity desktop.\nTitle:", "model_inf_time": 1.97}, {"id": "41223", "output": "Automatically Detecting Semantic Bugs in Database-Centric Applications", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDatabase-centric applications (DCAs) are common in enterprise computing, and they use nontrivial databases. Testing of DCAs is increasingly outsourced to test centers in order to achieve lower cost and higher quality. When releasing proprietary DCAs, its databases should also be made available to test engineers, so that they can test using real data. Testing with real data is important, since fake data lacks many of the intricate semantic connections among the original data elements. However, different data privacy laws prevent organizations from sharing these data with test centers because databases contain sensitive information. Currently, testing is performed with fake data that often leads to worse code coverage and fewer uncovered bugs, thereby reducing the quality of DCAs and obliterating benefits of test outsourcing. We show that a popular data anonymization algorithm called k-anonymity seriously degrades test coverage of DCAs. We propose an approach that uses program analysis to guide selective application of k-anonymity. This approach helps protect sensitive data in databases while retaining testing efficacy. Our results show that for small values of k = 7, test coverage drops to less than 30% from the original coverage of more than 70%, thus making it difficult to achieve good quality when testing DCAs while applying data privacy.\nTitle:\nIs Data Privacy Always Good for Software Testing?\n\nAbstract:\nDatabase-centric applications (DCAs) are common in enterprise computing, and they use nontrivial databases. Testing of DCAs is increasingly outsourced to test centers in order to achieve lower cost and higher quality. When proprietary DCAs are released, their databases should also be made available to test engineers. However, different data privacy laws prevent organizations from sharing this data with test centers because databases contain sensitive information. Currently, testing is performed with anonymized data, which often leads to worse test coverage (such as code coverage) and fewer uncovered faults, thereby reducing the quality of DCAs and obliterating benefits of test outsourcing. To address this issue, we offer a novel approach that combines program analysis with a new data privacy framework that we design to address constraints of software testing. With our approach, organizations can balance the level of privacy with needs of testing. We have built a tool for our approach and applied it to nontrivial Java DCAs. Our results show that test coverage can be preserved at a higher level by anonymizing data based on their effect on corresponding DCAs.\nTitle:\nTesting software in age of data privacy: a balancing act\n\nAbstract:\nMany organizations deploy applications that use databases by sending Structured Query Language (SQL) statements to them and obtaining data that result from the execution of these statements. Since applications often share the same databases concurrently, database deadlocks routinely occur in these databases resulting in major performance degradation in these applications. Database engines do not prevent database deadlocks for the same reason that the schedulers of operating system kernels do not preempt processes in a way to avoid race conditions and deadlocks - it is not feasible to find an optimal context switching schedule quickly for multiple processes (and SQL statements), and the overhead of doing it is prohibitive. We created a novel approach that combines run-time monitoring, which automatically prevents database deadlocks, with static analysis, which detects hold-and-wait cycles that specify how resources (e.g., database tables) are held in contention during executions of SQL statements. We rigorously evaluated our approach. For a realistic case of over 1,200 SQL statements, our algorithm detects all hold-and-wait cycles in less than two seconds. We built a toolset and experimented with three applications. Our tool prevented all existing database deadlocks in these applications and increased their throughputs by up to three orders of magnitude.\nTitle:\nPreventing database deadlocks in applications\n\nAbstract:\nMany organizations deploy applications that use databases by sending Structured Query Language (SQL) statements to them and obtaining data that result from executions of these statements. Since applications often share the same databases concurrently, database deadlocks routinely occur in these databases. Testing applications to determine how they cause database deadlocks is important as part of ensuring correctness, reliability, and performance of these applications. Unfortunately, it is very difficult to reproduce database deadlocks, since it involves different factors such as the precise interleavings in executing SQL statements. We created a novel approach for Systematic TEsting in Presence of Database Deadlocks (STEPDAD) that enables testers to instantiate database deadlocks in applications with a high level of automation and frequency. We implemented STEPDAD and experimented with three applications. On average, STEPDAD detected a number of database deadlocks exceeding the deadlocks obtained with the baseline approach by more than an order of magnitude. In some cases, STEPDAD reproduced a database deadlock after running an application only twice, while no database deadlocks could be obtained after ten runs using the baseline approach.\nTitle:\nTesting Database-Centric Applications for Causes of Database Deadlocks\n\nAbstract:\nIn this demonstration, we will present a database deadlocks prevention system that visualizes our algorithm for detecting hold-and-wait cycles that specify how resources (e.g., database tables) are locked and waited on to be locked during executions of SQL statements and utilizes those cycles information to prevent database deadlocks automatically.\nTitle:\nREDACT: preventing database deadlocks from application-based transactions\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.224, mean value for all the writers is 8.648, which makes it 0.362 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDatabase-centric applications (DCAs) are widely used by many companies and organizations to perform various control and analytical tasks using large databases. Real-world databases are described by complex schemas that oftentimes contain hundreds of tables consisting of thousands of attributes. However, when software engineers develop DCAs, they may write code that can inadvertently violate the integrity of these databases. Alternatively, business analysts and database administrators can also make errors that lead to integrity violations (semantic bugs). To detect these violations, stakeholders must create assertions that check the validity of the data in the rows of the database tables. Unfortunately, creating assertions is a manual, laborious and error-prone task. Thus, a fundamental problem of testing DCAs is how to find such semantic bugs automatically. We propose a novel solution, namely DACITE, that enables stakeholders to automatically obtain constraints that semantically relate database attributes and code statements using a combination of static analysis of the source code and associative rule mining of the databases. We rely on SAT-solvers to validate if a solution to the combined constraints exists and issue warnings on possible semantic bugs to stakeholders. We evaluated our approach on eight open-source DCAs and our results suggest that semantic bugs can be found automatically with high precision. The results of the study with developers show that warnings produced by DACITE are useful and enable them to find semantic bugs faster.\nTitle:", "model_inf_time": 1.53}, {"id": "41224", "output": "Traceability of Emerging Forms of Software Engineering: TEFSE 2009", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe Sixth International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE 2011) will bring together researchers and practitioners to examine the challenges of recovering and maintaining traceability for the myriad forms of software engineering artifacts, ranging from user needs to models to source code. The objective of the 6th edition of TEFSE is to build on the work the traceability research community has completed in identifying the open traceability challenges. In particular, it is intended to be a working event focused on discussing the main problems related to software artifact traceability and propose possible solutions for such problems. Moreover, the workshop also aims at identifying key issues concerning the importance of maintaining the traceability information during software development, to further improve the cooperation between academia and industry and to facilitate technology transfer.\nTitle:\nSixth international workshop on traceability in emerging forms of software engineering: (TEFSE 2011)\n\nAbstract:\nThe challenges of implementing successful and cost-effective traceability have created a compelling research agenda that has addressed a broad range of traceability related issues, ranging from qualitative studies of traceability users in industry to very technical and quantitative studies. Unfortunately, advances are hampered by the significant time and effort needed to establish a traceability research environment and to perform comparative evaluations of new results against existing baselines. In this panel we discuss ongoing efforts by members of the Center of Excellence for Software Traceability (CoEST) to define the Grand Challenges of Traceability, develop benchmarks, and to construct TraceLab, an extensible and scalable visual environment for designing and executing a broad range of traceability experiments.\nTitle:\nGrand challenges, benchmarks, and TraceLab: developing infrastructure for the software traceability research community.\n\nAbstract:\nTraceability link recovery is an active research area in software engineering with a number of open research questions and challenges, due to the substantial costs and challenges associated with software maintenance. We propose Traceclipse, an Eclipse plug-in that integrates some similar characteristics of traceability link recovery techniques in one easy-to-use suite. The tool enables software developers to specify, view, and manipulate traceability links within Eclipse and it provides an API through which recovery techniques may be added, specified, and run within an integrated development environment. The paper also presents initial case studies aimed at evaluating the proposed plug-in.\nTitle:\nTraceclipse: an eclipse plug-in for traceability link recovery and management.\n\nAbstract:\nThis position paper discusses the situations when visualizing traceability links is opportune, as well as what information pertaining to these links should be visualized and how.It also presents a prototype tool, which is used to visualize traceability links to provide support for the user during recovery, maintenance, and browsing of such links.\nTitle:\nWhen and how to visualize traceability links?\n\nAbstract:\nExisting methods for recovering traceability links among software documentation artifacts analyze textual similarities among these artifacts. It may be the case, however, that related documentation elements share little terminology or phrasing. This paper presents a technique for indirectly recovering these traceability links in requirements documentation by combining textual with structural information as we conjecture that related requirements share related source code elements. A preliminary case study indicates that our combined approach improves the precision and recall of recovering relevant links among documents as compared to stand-alone methods based solely on analyzing textual similarities.\nTitle:\nCombining textual and structural analysis of software artifacts for traceability link recovery.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.691, mean value for all the writers is 8.648, which makes it 0.89 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTraceability of Emerging Forms of Software Engineering (TEFSE) 2009 will bring together researchers and practitioners to examine the challenges of recovering and maintaining traceability for the myriad forms of software engineering, from user needs to models to source code. In the 2007 instalment, TEFSE focused on the grand challenges of traceability. The 2009 instalment will focus on these and other emerging challenges in traceability.\nTitle:", "model_inf_time": 1.55}, {"id": "41225", "output": "Efficient Online Learning for Partial Monitoring with Prior Information", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe consider the optimal value of information problem, where the goal is to sequentially select a set of tests with a minimal cost, so that one can efficiently make the best decision based on the observed outcomes. Existing algorithms are either heuristics with no guarantees, or scale poorly (with exponential run time in terms of the number of available tests). Moreover, these methods assume a known distribution over the test outcomes, which is often not the case in practice.We propose a sampling-based online learning framework to address the above issues. First, assuming the distribution over hypotheses is known, we propose a dynamic hypothesis enumeration strategy, which allows efficient information gathering with strong theoretical guarantees. We show that with sufficient amount of samples, one can identify a near-optimal decision with high probability. Second, when the parameters of the hypotheses distribution are unknown, we propose an algorithm which learns the parameters progressively via posterior sampling in an online fashion. We further establish a rigorous bound on the expected regret. We demonstrate the effectiveness of our approach on a real-world interactive troubleshooting application, and show that one can efficiently make high-quality decisions with low cost.\nTitle:\nEfficient Online Learning For Optimizing Value Of Information: Theory And Application To Interactive Troubleshooting\n\nAbstract:\nIn the stochastic bandit problem, the goal is to maximize an unknown function via a sequence of noisy function evaluations. Typically, the observation noise is assumed to be independent of the evaluation and to satisfy a tail bound uniformly on the domain, which is a restrictive assumption for many applications. In this work, we consider the setting of heteroscedastic noise, where we explicitly allow the noise distribution to depend on the evaluation point. We show that this leads to new trade-offs for information and regret, which are not taken into account by existing approaches like upper confidence bound algorithms (UCB) or Thompson Sampling. To address these shortcomings, we introduce a frequentist regret framework, that is similar to the Bayesian analysis of Russo and Van Roy (2014), and we prove a new high-probability regret bound for general, possibly randomized policies, which depends on a quantity we refer to as regret-information ratio. From this bound, we define a frequentist version of Information Directed Sampling (IDS) to minimize a surrogate of the regret-information ratio over all possible action sampling distributions. In order to construct the surrogate function, we generalize known concentration inequalities for online least squares regression in separable Hilbert spaces to the case of heteroscedastic noise. This allows us to formulate several variants of IDS for linear and reproducing kernel Hilbert space response functions, yielding novel algorithms for Bayesian optimization. We also prove frequentist regret bounds, which in the homoscedastic case are comparable to known bounds for UCB, but can be much better when the noise is heteroscedastic. Empirically, we demonstrate in a linear setting, that some of our methods can outperform UCB and Thompson Sampling, even when the noise is homoscedastic.\nTitle:\nInformation Directed Sampling and Bandits with Heteroscedastic Noise.\n\nAbstract:\nHow should we gather information to make effective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hypotheses. Our goal is to drive uncertainty into a single decision region as quickly as possible. We identify necessary and sufficient conditions for correctly identifying a decision region that contains all hypotheses consistent with observations. We develop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove that is competitive with the intractable optimal policy. Our efficient implementation of the algorithm relies on computing subsets of the complete homogeneous symmetric polynomials. Finally, we demonstrate its effectiveness on two practical applications: approximate comparison-based learning and active localization using a robot manipulator.\nTitle:\nNear Optimal Bayesian Active Learning for Decision Making.\n\nAbstract:\nWe consider the problem of training generative models with a Generative Adversarial Network (GAN). Although GANs can accurately model complex distributions, they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem. In this paper, we view the problem of training GANs as finding a mixed strategy in a zero-sum game. Building on ideas from online learning we propose a novel training method named Chekhov GAN. On the theory side, we show that our method provably converges to an equilibrium for semi-shallow GAN architectures, i.e. architectures where the discriminator is a one-layer network and the generator is arbitrary. On the practical side, we develop an efficient heuristic guided by our theoretical results, which we apply to commonly used deep GAN architectures. On several real-world tasks our approach exhibits improved stability and performance compared to standard GAN training.\nTitle:\nAn Online Learning Approach to Generative Adversarial Networks.\n\nAbstract:\n  Which ads should we display in sponsored search in order to maximize our revenue? How should we dynamically rank information sources to maximize the value of the ranking? These applications exhibit strong diminishing returns: Redundancy decreases the marginal utility of each ad or information source. We show that these and other problems can be formalized as repeatedly selecting an assignment of items to positions to maximize a sequence of monotone submodular functions that arrive one by one. We present an efficient algorithm for this general problem and analyze it in the no-regret model. Our algorithm possesses strong theoretical guarantees, such as a performance ratio that converges to the optimal constant of 1 - 1/e. We empirically evaluate our algorithm on two real-world online optimization problems on the web: ad allocation with submodular utilities, and dynamically ranking blogs to detect information cascades. Finally, we present a second algorithm that handles the more general case in which the feasible sets are given by a matroid constraint, while still maintaining a 1 - 1/e asymptotic performance ratio. \nTitle:\nOnline Submodular Maximization under a Matroid Constraint with Application to Learning Assignments.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.246, mean value for all the writers is 8.648, which makes it 0.343 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nPartial monitoring is a general model for online learning with limited feedback: a learner chooses actions in a sequential manner while an opponent chooses outcomes. In every round, the learner suffers some loss and receives some feedback based on the action and the outcome. The goal of the learner is to minimize her cumulative loss. Applications range from dynamic pricing to label-efficient prediction to dueling bandits. In this paper, we assume that we are given some prior information about the distribution based on which the opponent generates the outcomes. We propose BPM, a family of new efficient algorithms whose core is to track the outcome distribution with an ellipsoid centered around the estimated distribution. We show that our algorithm provably enjoys near-optimal regret rate for locally observable partial-monitoring problems against stochastic opponents. As demonstrated with experiments on synthetic as well as real-world data, the algorithm outperforms previous approaches, even for very uninformed priors, with an order of magnitude smaller regret and lower running time.\nTitle:", "model_inf_time": 1.49}, {"id": "41226", "output": "Metro Maps for Navigating Information", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWhen information is abundant, it becomes increasingly difficult to fit nuggets of knowledge into a single coherent picture. Complex stories spaghetti into branches, side stories, and intertwining narratives. In order to explore these stories, one needs a map to navigate unfamiliar territory. We propose a methodology for creating structured summaries of information, which we call metro maps. Our proposed algorithm generates a concise structured set of documents maximizing coverage of salient pieces of information. Most importantly, metro maps explicitly show the relations among retrieved pieces in a way that captures story development. We first formalize characteristics of good maps and formulate their construction as an optimization problem. Then we provide efficient methods with theoretical guarantees for generating maps. Finally, we integrate user interaction into our framework, allowing users to alter the maps to better reflect their interests. Pilot user studies with a real-world dataset demonstrate that the method is able to produce maps which help users acquire knowledge efficiently.\nTitle:\nTrains of thought: generating information maps\n\nAbstract:\nAs the number of scientific publications soars, even the most enthusiastic reader can have trouble staying on top of the evolving literature. It is easy to focus on a narrow aspect of one's field and lose track of the big picture. Information overload is indeed a major challenge for scientists today, and is especially daunting for new investigators attempting to master a discipline and scientists who seek to cross disciplinary borders. In this paper, we propose metrics of influence, coverage and connectivity for scientific literature. We use these metrics to create structured summaries of information, which we call metro maps. Most importantly, metro maps explicitly show the relations between papers in a way which captures developments in the field. Pilot user studies demonstrate that our method helps researchers acquire new knowledge efficiently: map users achieved better precision and recall scores and found more seminal papers while performing fewer searches.\nTitle:\nMetro maps of science\n\nAbstract:\nThe process of extracting useful knowledge from large datasets has become one of the most pressing problems in today's society. The problem spans entire sectors, from scientists to intelligence analysts and web users, all of whom are constantly struggling to keep up with the larger and larger amounts of content published every day. With this much data, it is often easy to miss the big picture. In this paper, we investigate methods for automatically connecting the dots -- providing a structured, easy way to navigate within a new topic and discover hidden connections. We focus on the news domain: given two news articles, our system automatically finds a coherent chain linking them together. For example, it can recover the chain of events starting with the decline of home prices (January 2007), and ending with the ongoing health-care debate. We formalize the characteristics of a good chain and provide an efficient algorithm (with theoretical guarantees) to connect two fixed endpoints. We incorporate user feedback into our framework, allowing the stories to be refined and personalized. Finally, we evaluate our algorithm over real news data. Our user studies demonstrate the algorithm's effectiveness in helping users understanding the news.\nTitle:\nConnecting the dots between news articles\n\nAbstract:\nFinding information is becoming a major part of our daily life. Entire sectors, from Web users to scientists and intelligence analysts, are increasingly struggling to keep up with the larger and larger amounts of content published every day. With this much data, it is often easy to miss the big picture. In this article, we investigate methods for automatically connecting the dots---providing a structured, easy way to navigate within a new topic and discover hidden connections. We focus on the news domain: given two news articles, our system automatically finds a coherent chain linking them together. For example, it can recover the chain of events starting with the decline of home prices (January 2007), and ending with the health care debate (2009). We formalize the characteristics of a good chain and provide a fast search-driven algorithm to connect two fixed endpoints. We incorporate user feedback into our framework, allowing the stories to be refined and personalized. We also provide a method to handle partially-specified endpoints, for users who do not know both ends of a story. Finally, we evaluate our algorithm over real news data. Our user studies demonstrate that the objective we propose captures the users\u2019 intuitive notion of coherence, and that our algorithm effectively helps users understand the news.\nTitle:\nConnecting Two (or Less) Dots: Discovering Structure in News Articles\n\nAbstract:\nWe study the problem of learning personalized user models from rich user interactions. In particular, we focus on learning from clustering feedback (i.e., grouping recommended items into clusters), which enables users to express similarity or redundancy between different items. We propose and study a new machine learning problem for personalization, which we call collaborative clustering. Analogous to collaborative filtering, in collaborative clustering the goal is to leverage how existing users cluster or group items in order to predict similarity models for other users' clustering tasks. We propose a simple yet effective latent factor model to learn the variability of similarity functions across a user population. We empirically evaluate our approach using data collected from a clustering interface we developed for a goal-oriented data exploration (or sensemaking) task: asking users to explore and organize attractions in Paris. We evaluate using several realistic use cases, and show that our approach learns more effective user models than conventional clustering and metric learning approaches.\nTitle:\nPersonalized collaborative clustering\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.097, mean value for all the writers is 8.648, which makes it 1.323 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWhen information is abundant, it becomes increasingly difficult to fit nuggets of knowledge into a single coherent picture. Complex stories spaghetti into branches, side stories, and intertwining narratives. In order to explore these stories, one needs a map to navigate unfamiliar territory. We have developed a methodology for creating structured summaries of information, which we call metro maps. Our algorithm generates a concise structured set of documents which maxi- mizes coverage of salient pieces of information. Most importantly, metro maps explicitly show the relations among retrieved pieces in a way that captures the evolution of a story. We first for- malize characteristics of good maps and formulate their construction as an optimization problem. Then, we provide efficient methods with theoretical guarantees for generating maps. Finally, we integrate capabilities for supporting user interaction into the framework, allowing users to guide the formulation of the maps so as to better re ect their interests. Pilot user studies with a real- world dataset demonstrate that the method is able to produce maps which help users to acquire knowledge efficiently.\nTitle:", "model_inf_time": 1.24}, {"id": "41227", "output": "The LEAP Embedded Networked Sensor System: Adaptive Energy-Efficient Processing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA broad range of embedded networked sensor (ENS) systems for critical environmental monitoring applications now require complex, high peak power dissipating sensor devices, as well as on-demand high performance computing and high bandwidth communication. Embedded computing demands for these new platforms include support for computationally intensive image and signal processing as well as optimization and statistical computing. To meet these new requirements while maintaining critical support for low energy operation, a new multiprocessor node hardware and software architecture, Low Power Energy Aware Processing (LEAP), has been developed. The LEAP architecture integrates fine-grained energy dissipation monitoring and sophisticated power control scheduling for all subsystems including sensor subsystems. This paper also describes a new distributed node testbed demonstrating that by exploiting high high energy efficiency components and enabling proper on-demand scheduling, the LEAP architecture may meet both sensing performance and energy dissipation objectives for a broad class of applications.\nTitle:\nThe low power energy aware processing (LEAP)embedded networked sensor system\n\nAbstract:\nA broad range of embedded networked sensor (ENS) systems for critical environmental monitoring applications now require complex, high peak power dissipating sensor devices, as well as on-demand high performance computing and high bandwidth communication. Embedded computing demands for these new platforms include support for computationally intensive image and signal processing as well as optimization and statistical computing. To meet these new requirements while maintaining critical support for low energy operation, a new multiprocessor node hardware and software architecture, Low Power Energy Aware Processing (LEAP), has been developed. The LEAP architecture integrates fine-grained energy dissipation monitoring and sophisticated power control scheduling for all subsystems including sensor subsystems. The LEAP2 platform is a second generation LEAP system with even higher resolution energy monitoring as well as the unique ability to do per process and per application energy profiling via a dedicated high performance ASIC. Our demonstration will highlight this profiling capability through a custom monitoring application named etop.\nTitle:\netop: sensor network application energy profiling on the LEAP2 platform\n\nAbstract:\nThis paper describes a new embedded networked sensor platform architecture that combines hardware and software tools providing detailed, fine-grained real-time energy usage information. We introduce the LEAP2 platform, a qualitative step forward over the previously developed LEAP and other similar platforms. LEAP2 is based on a new low power ASIC system and generally applicable supporting architecture that provides unprecedented capabilities for directly observing energy usage of multiple subsystems in real-time. Real-time observation with microsecond-scale time resolution enables direct accounting of energy dissipation for each computing task as well as for each hardware subsystem. The new hardware architecture is exploited with our new software tools, etop and endoscope. A series of experimental investigations provide high-resolution power information in networking, storage, memory and processing for primary embedded networked sensing applications. Using results obtained in real-time we show that for a large class of wireless sensor network nodes, there exist several interdependencies in energy consumptionbetween different subsystems. Through the use of our measurement tools we demonstrate that by carefully selecting the system operating points, energy savings of over 60% can be achieved while retaining system performance.\nTitle:\nThe Energy Endoscope: Real-Time Detailed Energy Accounting for Wireless Sensor Nodes\n\nAbstract:\nDirect measurement of event and component resolved energy dissipation in computing systems is critical for energy optimization of computing and networking applications. Prior research focuses on development of energy consumption models and custom-built energy measurement systems, but suffers from critical drawbacks. This paper addresses these limitations and presents solutions using DEEP (Decision-support for Energy Efficient Processing). DEEP is a rapidly-deployed open-source energy measurement platform architecture. The platform provides an unprecedented ability to non-intrusively measure the energy consumption associated with execution of software application code. DEEP is implemented as both an online and offline version. Evaluation demonstrates processing and energy overheads less than 1% for offline and about 5% for the online implementation. The DEEP implementation investigates the impact of data compression on network data transport. An intelligent data compression and transport algorithm is developed using the decision-support capabilities of DEEP. The algorithm creates significant energy savings (38%) in network data transport using dynamic selection of compression schemes to adapt to varying system and wireless network conditions.\nTitle:\nEnergy efficient network data transport through adaptive compression using the DEEP platforms\n\nAbstract:\nDual-radio, dual-processor nodes are an emerging class of Wireless Sensor Network devices that provide both low- energy operation as well as substantially increased computational performance and communication bandwidth for applications. In such systems, the secondary radio and processor operates with sufficiently low power that it may remain always vigilant, while the the main processor and primary, high-bandwidth radio remain off until triggered by the application. By exploiting the high energy efficiency of the main processor and primary radio along with proper usage, net operating energy benefits are enabled for applications. The secondary radio provides a constantly available multi-hop network, while paths in the primary network exist only when required. This paper describes a topology control mechanism for establishing an end-to-end path in a network of dual-radio nodes using the secondary radios as a control channel to selectively wake up nodes along the required end-to-end path. Using numerical models as well as testbed experimentation, we show that our proposed mechanism provides significant energy savings of more than 60% compared to alternative approaches, and that it incurs only moderately greater application latency.\nTitle:\nEnd-to-End Routing for Dual-Radio Sensor Networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.207, mean value for all the writers is 8.648, which makes it 0.477 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA broad range of embedded networked sensing (ENS) applications have appeared for large-scale systems, introducing new requirements leading to new embedded architectures, associated algorithms, and supporting software systems. These new requirements include the need for diverse and complex sensor systems that present demands for energy and computational resources, as well as for broadband communication. To satisfy application demands while maintaining critical support for low-energy operation, a new multiprocessor node hardware and software architecture, Low Power Energy Aware Processing (LEAP), has been developed. In this article, we described the LEAP design approach, in which the system is able to adaptively select the most energy-efficient hardware components matching an application\u2019s needs. The LEAP platform supports highly dynamic requirements in sensing fidelity, computational load, storage media, and network bandwidth. It focuses on episodic operation of each component and considers the energy dissipation for each platform task by integrating fine-grained energy-dissipation monitoring and sophisticated power-control scheduling for all subsystems, including sensors. In addition to the LEAP platform\u2019s unique hardware capabilities, its software architecture has been designed to provide an easy way to use power management interface and a robust, fault-tolerant operating environment and to enable remote upgrade of all software components. LEAP platform capabilities are demonstrated by example implementations, such as a network protocol design and a light source event detection algorithm. Through the use of a distributed node testbed, we demonstrate that by exploiting high energy-efficiency components and enabling proper on-demand scheduling, the LEAP architecture may meet both sensing performance and energy dissipation objectives for a broad class of applications.\nTitle:", "model_inf_time": 1.73}, {"id": "41228", "output": "Conceptual Spaces for a More Cognitive Semantic Web", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCSML is a semantic markup language created for the publishing and sharing of conceptual spaces, which are geometric structures that represent semantics at the conceptual level. CSML can be used to describe semantics that are not captured well by the ontology languages commonly used in the Semantic Web. Measurement of the semantic similarity of concepts as well as the combination of concepts without shared properties are common human cognitive tasks. However, these operations present sources of difficulty for tools reliant upon set-theoretic and syllogistic reasoning on symbolic ontologies. In contrast, these operations can be modeled naturally using conceptual spaces. This paper describes the design decisions behind CSML, introduces the key component elements of a CSML document, and presents examples of its usage.\nTitle:\nConceptual Space Markup Language (CSML): Towards the Cognitive Semantic Web\n\nAbstract:\nThe modeling of concepts from a cognitive perspective is important for designing spatial information systems that interoperate with human users. Concept representations that are built using geometric and topological conceptual space structures are well suited for semantic similarity and concept combination operations. In addition, concepts that are more closely grounded in the physical world, such as many spatial concepts, have a natural fit with the geometric structure of conceptual spaces. Despite these apparent advantages, conceptual spaces are underutilized because existing formalizations of conceptual space theory have focused on individual aspects of the theory rather than the creation of a comprehensive algebra. In this paper we present a metric conceptual space algebra that is designed to facilitate the creation of conceptual space knowledge bases and inferencing systems. Conceptual regions are represented as convex polytopes and context is built in as a fundamental element. We demonstrate the applicability of the algebra to spatial information systems with a proof-of-concept application.\nTitle:\nA metric conceptual space algebra\n\nAbstract:\nMeasuring semantic similarity among concepts is the core method for assessing the degree of semantic interoperability within and between ontologies. In this paper, we propose to extend current semantic similarity measures by accounting for the spatial relations between different geospatial concepts. Such integration of spatial relations, in particular topologic and metric relations, leads to an enhanced accuracy of semantic similarity measurements. For the formal treatment of similarity the theory of conceptual vector spaces\u2014sets of quality dimensions with a geometric or topologic structure for one or more domains\u2014is utilized. These spaces allow for the measurement of semantic distances between concepts. A case study from the geospatial domain using Ordnance Survey's MasterMap is used to demonstrate the usefulness and plausibility of the approach.\nTitle:\nSpatial relations for semantic similarity measurement\n\nAbstract:\nDetermining the grade of semantic similarity between geospatial concepts is the basis for evaluating semantic interoperability of geographic information services and their users. Geometrical models, such as conceptual spaces, offer one way of representing geospatial concepts, which are modelled as n-dimensional regions. Previous approaches have suggested to measure semantic similarity between concepts based on their approximation by single points. This paper presents a way to measure semantic similarity between conceptual regions\u2014leading to more accurate results. In addition, it allows for asymmetries by measuring directed similarities. Examples from the geospatial domain illustrate the similarity measure and demonstrate its plausibility.\nTitle:\nMeasuring semantic similarity between geospatial conceptual regions\n\nAbstract:\nExploratory search, in which a user investigates complex concepts, is cumbersome with today's search engines. We present a new exploratory search approach that generates interactive visualizations of query concepts using thematic cartography (e.g. choropleth maps, heat maps). We show how the approach can be applied broadly across both geographic and non-geographic contexts through explicit spatialization, a novel method that leverages any figure or diagram -- from a periodic table, to a parliamentary seating chart, to a world map -- as a spatial search environment. We enable this capability by introducing explanatory semantic relatedness measures. These measures extend frequently-used semantic relatedness measures to not only estimate the degree of relatedness between two concepts, but also generate human-readable explanations for their estimates by mining Wikipedia's text, hyperlinks, and category structure. We implement our approach in a system called Atlasify, evaluate its key components, and present several use cases.\nTitle:\nExplanatory semantic relatedness and explicit spatialization for exploratory search\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.228, mean value for all the writers is 8.648, which makes it 0.495 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOne of the key deficiencies of the Semantic Web is its lack of cognitive plausibility. We argue that by accounting for people's reasoning mechanisms and cognitive representations, the usefulness of information coming from the Semantic Web will be enhanced. More specifically, the utilization and integration of conceptual spaces is proposed as a knowledge representation that affords two important human cognitive mechanisms, i.e., semantic similarity and concept combination. Formal conceptual space algebra serves as the basis for the Conceptual Space Markup Language (CSML), which facilitates the engineering of ontologies using a geometric framework. We demonstrate the usefulness of the approach through a concrete example and suggest directions for future work, especially the need for combining geometric representations and reasoning mechanisms with existing Semantic Web structures.\nTitle:", "model_inf_time": 1.27}, {"id": "41229", "output": "A Three-Tier Rule Discovery Framework for Fault Management", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAr the heart of the Internet revolution is global telecommunication systems. These systems, initially designed for voice traffic, provide the vast backbone bandwidth capabilities necessary for Internet traffic. They have built-in redundancy and complexity to ensure robustness and quality of service. To facilitate this, this requires complex fault identification and management systems. Fault identification and management is generally handled by reducing the amount of alarm events (symptoms) presented to the operating engineer through monitoring, filtering and masking. The ultimate goal is to determine and present the actual underlying fault. While en-route to automated fault identification it is useful to derive rules and techniques to attempt to present less symptoms with greater diagnostic assistance. With these objectives in mind computer-assisted human discovery and human-assisted computer discovery techniques are discussed.\nTitle:\nDiscovering Rules For Fault Management\n\nAbstract:\nTelecommunication systems are built with extensive redundancy and complexity to ensure robustness and quality of service. Such systems requires complex fault identification and management tools. Fault identification and management are generally handled by reducing the number of alarm events (symptoms) presented to the operating engineer through monitoring, filtering and masking. The goal is to determine and present the actual underlying fault. Fault management is a complex task, subject to uncertainty in the symptoms presented. In this paper two key fault management approaches are considered: (i) rule discovery to attempt to present fewer symptoms with greater diagnostic assistance for the more traditional rule based system approach and (ii) the induction of Bayesian Belief Networks (BBNs) for a complete 'intelligent' approach. The paper concludes that the research and development of the two target fault management systems can be complementary.\nTitle:\nFacing Fault Management as It Is, Aiming for What You Would Like It to Be\n\nAbstract:\nGlobal telecommunication systems are built with extensive redundancy and complex management systems to ensure robustness. Fault iden tification and management of this complexity is an open research issue with which data mining can greatly assist.This paper proposes a hybrid data mining architecture and a parallel genetic algorithm (PGA) applied to the mining of Bayesian Belief Networks (BBN) from Telecommunication Management Network (TMN) data.\nTitle:\nParallel Data Mining of Bayesian Networks from Telecommunications Network Data\n\nAbstract:\nAs the size and complexity of networks and communications continue to grow, there is a heightened need to develop new techniques capable of achieving a level of service with successful operations upon which users can place even more reliance. Key emerging strategies for meeting this demand is 'autonomic networks' and 'autonomic communications', concepts similar to autonomic computing while specific to the communications field. This paper considers the 'self-healing' aspect of autonomic networks, examining, in particular, techniques for event correlation to aid fault identification. A three-tier rule-discovery framework and associated support and analysis tools are described. These assist with the development, management and maintenance of correlation rules and beliefs.\nTitle:\nAutonomic networks: engineering the self-healing property\n\nAbstract:\nThis paper reports on the latest developments in a deployed Survivable Secure System in relation to enabling the longer term aim of moving from a reactive to a predictive system. At this stage it has been decided to redevelop the event/rule processor into a distributed system to facilitate the future requirements of a predictive system.\nTitle:\nTargeting Prediction: Engineering a Distributed Event Processor for an Autonomic Biometric System\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.842, mean value for all the writers is 8.648, which makes it 0.166 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nGlobal telecommunication systems have built-in redundancy to ensure robustness. Unfortunately, this means that each fault that occurs can trigger a cascade of alarm events as individual parts of the system discover and report failure. It is then difficult to locate the origin of the fault. One contribution to effective fault management is through practical intelligent assistance. The general goal is to find a rich set of strategies and rules to help locate faults from alarm event data. This paper presents a three-tier rule discovery framework that combines computer-assisted human discovery with human-assisted computer discovery techniques. The approach is illustrated with a small experimental case study performed on a test high-speed network at Nortel Networks.\nTitle:", "model_inf_time": 1.22}, {"id": "41230", "output": "Sparse Time-Domain Reconstruction of Incomplete Relative Transfer Functions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper addresses the estimation of Relative Transfer Function RTF between microphones from noisy recordings. We utilize an incomplete initial measurement of the RTF, which is known for only several frequency bins. The measurement is completed by finding its sparsest representation in the time domain. We propose to perform this reconstruction by solving a Second-Order Cone Program SOCP. Free parameters of this formulation represent distance of the completed RTF from the initial estimate. We select these parameters based on the theoretical performance of the initial estimate. In experiments with real-world data, this approach achieves a significant refinement of the RTF, especially in scenarios with low signal-to-noise ratios.\nTitle:\nImproving Relative Transfer Function Estimates Using Second-Order Cone Programming.\n\nAbstract:\nThis report proposes a novel variant of the generalized sidelobe canceler. It assumes that a set of prepared relative transfer functions (RTFs) is available for several potential positions of a target source within a confined area. The key problem here is to select the correct RTF at any time, even when the exact position of the target is unknown and interfering sources are present. We propose to select the RTF based on lp-norm, p \u2264 1, measured at the blocking matrix output in the frequency domain. Subsequent experiments show that this approach significantly outperforms previously proposed methods for selection when the target and interferer signals are speech signals.\nTitle:\nInformed generalized sidelobe canceler utilizing sparsity of speech signals\n\nAbstract:\nTime-domain algorithms for blind separation of audio sources can be classified as being based either on a partial or complete decomposition of an observation space. The decomposition, especially the complete one, is mostly done under a constraint to reduce the computational burden. However, this constraint potentially restricts the performance. The authors propose a novel time-domain algorithm that is based on a complete unconstrained decomposition of the observation space. The observation space may be defined in a general way, which allows application of long separating filters, although its dimension is low. The decomposition is done by an appropriate independent component analysis (ICA) algorithm giving independent components that are grouped into clusters corresponding to the original sources. Components of the clusters are combined by a reconstruction procedure after estimating microphone responses of the original sources. The authors demonstrate by experiments that the method works effectively with short data, compared to other methods.\nTitle:\nTime-Domain Blind Separation of Audio Sources on the Basis of a Complete ICA Decomposition of an Observation Space\n\nAbstract:\nMethods for Blind Source Separation (BSS) aim, at recovering signals from their mixture without prior knowledge about the signals and the mixing system. Among others, they provide tools for enhancing speech signals when they are disturbed by unknown noise or other interfering signals in the mixture. This paper considers a recent time-domain BSS method that is based on a complete decomposition of a signal sub-space into components that should be independent. The components are used to reconstruct images of original signals using an ad hoc weighting, which influences the final performance of the method markedly. We propose a novel weighting scheme that utilizes block-Toeplitz structure of signal matrices and relies thus on an established property. We provide experiments with blind speech separation and speech recognition that prove the better performance of the modified BSS method.\nTitle:\nBlind Speech Separation In Time-Domain Using Block-Toeplitz Structure Of Reconstructed Signal Matrices\n\nAbstract:\nEstimation of instantaneous frequency of a narrow-band noise corrupted signal from discrete time phase-only data is considered. By aid of Kalman filter theory, filter and smoothing estimates are derived and their equivalence to estimates obtained by the MFT algorithm is established, MFT being a practical algorithm for multiple frequency tracking and smoothing.\nTitle:\nEstimation and smoothing of instantaneous frequency of noisy narrow band signals\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.635, mean value for all the writers is 8.648, which makes it 0.842 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nRelative transfer functions (RTF) between microphones can often be estimated accurately but only for certain frequencies. For example, this happens in situations where the RTF is estimated from a noise-free signal of a target source whose spectrum does not span the whole frequency range. By combining a conventional RTF estimator and a selection of the active frequencies, an incomplete measurement of the RTF is obtained. We propose to retrieve the whole RTF estimate through finding the sparsest representation of the incomplete measurement in the discrete or continuous time-domain and compare both approaches. The RTF estimates are evaluated in terms of attenuation rate that measures the target signal cancellation at the output of a blocking matrix. It is shown by experiments that the reconstructed estimate can achieve significantly better attenuation than the initial (complete) estimate.\nTitle:", "model_inf_time": 1.26}, {"id": "41231", "output": "Reframing Feedforward: Bridging the Gulf of Execution in Interaction Design", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDesign meetings with multidisciplinary stakeholders are instrumental for design projects. However, design teams face the challenges of synthetizing large amounts of information, often in a limited time, and with minimal common ground. We investigate these challenges through in-the-wild observations of six design meetings in three different projects, with professional design teams that follow a user-centered design methodology. We found that all the observed design meetings had a similar structure consisting of particular phases, in which design activities were organized around artefacts. These artefacts were used as input to disseminate and gather feedback of previous design outcomes, or as output to collect and process a variety of perspectives. From these findings, we synthetize practical guidelines to optimize artefact-based interactions during design meetings.\nTitle:\nUntangling Design Meetings: Artefacts as Input and Output of Design Activities.\n\nAbstract:\nThis paper presents a modular runtime architecture supporting our model-based user interface design approach for designing context-aware, distributable user interfaces for ambient intelligent environments.\nTitle:\nA task-driven user interface architecture for ambient intelligent environments\n\nAbstract:\nThis paper describes a framework to enable implicit interaction between mobile users in order to establish and maintain social networks according to the preferences and needs of each individual A user model is proposed which can be constructed by the user and appended with information regarding the user's privacy preferences Design choices and tool support regarding the framework are discussed.\nTitle:\nExtending social networks with implicit human-human interaction\n\nAbstract:\nPervasive applications are typically realized through ad-hoc service and user interface compositions. While many tools focus on the development of pervasive services by masking the complex technical side of a pervasive computing environment, the deployment of an application as a whole -- i.e. a set of services and user interfaces -- is often forgotten. We present an alternative design strategy and tool for pervasive applications in which pervasiveness is not considered a handicap, but rather as a situation that draws extra attention to the deployment of applications. By crafting pervasive applications and their services as independent context consumers and producers, we illustrate how the behaviour of a pervasive application deployed using our approach can be observed while it executes.\nTitle:\nPerCraft: Towards Live Deployment of Pervasive Applications\n\nAbstract:\nMulti-touch large display interfaces are becoming increasingly popular in public spaces. These spaces impose specific requirements on the accessibility of the user interfaces: most users are not familiar with the interface and expectations with regard to user experience are very high. Multi-touch interaction beyond the traditional move-rotate-scale interactions is often unknown to the public and can become exceedingly complex. We introduce TouchGhosts: visual guides that are embedded in the multi-touch user interface and that demonstrate the available interactions to the user. TouchGhosts are activated while using an interface, providing guidance on the fly and within the context-of-use. Our approach allows to define reconfigurable strategies to decide how or when a TouchGhost should be activated and which particular visualization will be presented to the user.\nTitle:\nGhosts in the interface: Meta-user interface visualizations as guides for multi-touch interaction\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.339, mean value for all the writers is 8.648, which makes it 1.443 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nFeedback and affordances are two of the most well-known principles in interaction design. Unfortunately, the related and equally important notion of feedforward has not been given as much consideration. Nevertheless, feedforward is a powerful design principle for bridging Norman's Gulf of Execution. We reframe feedforward by disambiguating it from related design principles such as feedback and perceived affordances, and identify new classes of feedforward. In addition, we present a reference framework that provides a means for designers to explore and recognize different opportunities for feedforward.\nTitle:", "model_inf_time": 1.4}, {"id": "41232", "output": "Investigating the Impact of Collaboration and Cooperation on User Experience in Shared Virtual Environments", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAs collaborative virtual environments (CVEs) are becoming more popular for both entertainment and professional activities, it is important to know which factors influence the collaboration between participants. This paper investigates two aspects of interaction in the context of collaboration between two users. A puzzle solving task in a basic interactive virtual environment was used as a case study. The influence of voice communication was assessed in a user experiment. Furthermore, as different I/O devices can be used with this kind of applications, the fact whether both participants are using the same device or not, was also checked.From this study, we can conclude that the inclusion of voice communication is important when working on the same task in a CVE, since this allows the participants to explicitly divide the work. On the other hand, the usage of the same kind of device by both users does not significantly influence the collaboration. The availability of different devices shows not to be a problem for the acceptance of CVEs, and therefore it is not necessary to impose a certain device, which is obviously preferable, especially when dealing with home users.\nTitle:\nInvestigating The Influence Of Communication And Input Devices On Collaboration In Virtual Environments\n\nAbstract:\nThe collaborative nature of many modern multiplayer games raises a lot of questions in cooperative game design. We address one of them in this paper by analyzing cooperative game patterns in remote gameplay in order to define benefits and drawbacks for each one. With the help of a user experiment, we analyzed player experience in a set of existing cooperative patterns for games played remotely without communication. By comparing patterns, supporting closely- and loosely-coupled collaboration, we discovered that the first type provided a more enjoyable experience but introduced additional challenges in case of a lack of communication. By analyzing patterns for both closely- and loosely-coupled interaction, we determined the most beneficial pattern within each type. We concluded with the results of a pattern comparison in co-located and remote setups.\nTitle:\nThe influence of cooperative game design patterns for remote play on player experience\n\nAbstract:\nThis paper presents a study investigating cooperation between players in casual games. Although widely used in co-located or asynchronous settings, cooperative gameplay elements are not popular in networked synchronous casual games. In our study, we have analyzed different types of cooperation between players in casual games. Each of these is based on a certain cooperative game design pattern, and can be classified as either closely- or loosely-coupled. Six game patterns have been selected and an equal number of games developed, each targeting one pattern. By means of a user experiment we have investigated which cooperation types fit most of the criteria that define casual games. More specifically, we have focused on the applicability of close coupling between players. Based on the games used in the experiment, most patterns with closely-coupled interaction have shown an overall higher user evaluation than loosely-coupled, satisfying criteria of casual games. These results indicate that introducing close coupling in the casual games under consideration is a potential way to increase the player experience.\nTitle:\nThe effect of closely-coupled interaction on player experience in casual games\n\nAbstract:\nLike most applications deployed on the Internet, modern multiplayer games are subject to the impact of transmission delays and the variability thereof. These delays can be introduced either by the physical limitations of signal transmission speed or overload and queuing problems in intermediate nodes. The influence of this delay is far-reaching and impacts most interactive applications. More specifically, quantitative and qualitative studies have been conducted on competitive game genres, such as first person shooter and racing games. In contrast, this work investigates how network delay affects player experience in cooperative games, where players have to interact with shared objects and obstacles. In this game genre, one might expect an increased sensitivity to detrimental network factors due to the reliance on the (near-)perfect synchronization of actions between participants. In this paper, a series of consecutive user tests were carried out with one of the most recent games, Little Big Planet 2; which focuses primarily on the cooperative aspect. Analysis has shown that delays over 100 ms significantly decrease player performance and the way in which network quality is perceived. At the same time jitter negatively affects user performance, though players do not perceive this impairment as disturbing.\nTitle:\nInfluence of network delay and jitter on cooperation in multiplayer games\n\nAbstract:\nActivity-Based Computing (ABC) has been proposed as an organisational structure for local desktop management and knowledge work. Knowledge work, however, typically occurs in partially overlapping subgroups and involves the use of multiple devices. We introduce co-Activity Manager, an ABC approach that (i) supports activity sharing for multiple collaborative contexts, (ii) includes collaborative tools into the activity abstraction and (iii) supports multiple devices by seamlessly integrated cloud support for documents and activity storage. Our 14 day field deployment in a multidisciplinary software development team showed that activity sharing is used as a starting point for long-term collaboration while integrated communication tools and cloud support are used extensively during the collaborative activities. The study also showed that activities are used in different ways ranging from project descriptions to to-do lists, thereby confirming that a document-driven activity roaming model seems to be a good match for collaborative knowledge work\nTitle:\nCo-activity manager: integrating activity-based collaboration into the desktop interface\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.57, mean value for all the writers is 8.648, which makes it 1.64 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIncreased interest in shared virtual environments has resulted in a necessity to investigate different factors that influence user interaction, both with the environment and other users. Introducing different types of joint activities into computer games can support a high level of realism and user engagement. This paper investigates two types of shared work: collaboration where users individually perform actions in the environment towards a common goal, and cooperation where users simultaneously act on the same objects in a shared environment. A basic computer game which requires joint work between all players was developed and used as a case study. We analyzed user performance and enjoyment in order to see which type of shared work is more preferable. Furthermore, the influence of user expertise on their enjoyment was also checked. From this particular study, we conclude that participants prefer cooperation over collaboration, as it provides more active and realistic performance. User expertise does not significantly influence enjoyment in this type of shared work, indicating that inexperienced players can enjoy the game equally as the experienced.\nTitle:", "model_inf_time": 1.68}, {"id": "41233", "output": "Integrating Security Models in Model Driven Development", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nModel-driven approach has recently received much attention in developing secure software and systems. In addition, software developers have attempted to employ such an emerging approach in the early stage of software development life cycle. However, security concerns are rarely considered and practiced due to the lack of appropriate systematic mechanisms and tools. In this paper, we introduce a multilayered software development life cycle (SDLC), which is based on an assurance management framework (AMF), focusing on the development of authorization systems. AMF facilitates comprehensive realization of formal security model, security policy specification and verification, generation of security enforcement codes, and rigorous conformance testing. We also articulate our experience in analyzing role-based authorization requirements and realizing those requirements in constructing a role-based authorization system.\nTitle:\nConstructing Authorization Systems Using Assurance Management Framework\n\nAbstract:\nVerification and testing are the important step for software assurance. However, such crucial and yet challenging tasks have not been widely adopted in building access control systems. In this paper we propose a methodology to support automatic analysis and conformance testing for access control systems, integrating those features to Assurance Management Framework (AMF). Our methodology attempts to verify formal specifications of a role-based access control model and corresponding policies with selected security properties. Also, we systematically articulate testing cases from formal specifications and validate conformance to the system design and implementation using those cases. In addition, we demonstrate feasibility and effectiveness of our methodology using SAT and Alloy toolset.\nTitle:\nEnabling verification and conformance testing for access control model\n\nAbstract:\nEven though role-based access control (RBAC) can tremendously help us minimize the complexity in administering users, it is still needed to realize the notion of roles at the resource level. In this paper, we propose a practical cryptographic RBAC model, called role-key hierarchy model, to support various security features including signature and encryption based on role-key hierarchy. With the help of rich algebraic structure of elliptic curve, we introduce a role-based cryptosystem construction to verify the rationality and validity of our proposed model. Also, a proof-of-concept prototype implementation and performance evaluation are discussed to demonstrate the feasibility and efficiency of our mechanisms.\nTitle:\nCryptographic role-based security mechanisms based on role-key hierarchy\n\nAbstract:\nEmerging computing technologies such as web services, service-oriented architecture, and cloud computing has enabled us to perform business services more efficiently and effectively. However, we still suffer from unintended security leakages by unauthorized actions in business services while providing more convenient services to Internet users through such a cutting-edge technological growth. Furthermore, designing and managing web access control policies are often error-prone due to the lack of effective analysis mechanisms and tools. In this paper, we represent an innovative policy anomaly analysis approach for web access control policies, focusing on extensible access control markup language policy. We introduce a policy-based segmentation technique to accurately identify policy anomalies and derive effective anomaly resolutions, along with an intuitive visualization representation of analysis results. We also discuss a proof-of-concept implementation of our method called XAnalyzer and demonstrate how our approach can efficiently discover and resolve policy anomalies.\nTitle:\nDiscovery and Resolution of Anomalies in Web Access Control Policies\n\nAbstract:\nThe advent of emerging technologies such as Web services, service-oriented architecture, and cloud computing has enabled us to perform business services more efficiently and effectively. However, we still suffer from unintended security leakages by unauthorized actions in business services while providing more convenient services to Internet users through such a cutting-edge technological growth. Furthermore, designing and managing Web access control policies are often error-prone due to the lack of effective analysis mechanisms and tools. In this paper, we represent an innovative policy anomaly analysis approach for Web access control policies. We focus on XACML (eXtensible Access Control Markup Language) policy since XACML has become the de facto standard for specifying and enforcing access control policies for various Web-based applications and services. We introduce a policy-based segmentation technique to accurately identify policy anomalies and derive effective anomaly resolutions. We also discuss a proof-of-concept implementation of our method called XAnalyzer and demonstrate how efficiently our approach can discover and resolve policy anomalies.\nTitle:\nAnomaly discovery and resolution in web access control policies\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.515, mean value for all the writers is 8.648, which makes it 0.113 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThere still exists an open question on how formal models can be fully realized in the system development phase. The Model Driven Development (MDD) approach has been recently introduced to deal with such a critical issue for building high assurance software systems. There still exists an open question on how formal models can be fully realized in the system development phase. The Model Driven Development (MDD) approach has been recently introduced to deal with such a critical issue for building high assurance software systems. The MDD approach focuses on the transformation of high-level design models to system implementation modules. However, this emerging development approach lacks an adequate procedure to address security issues derived from formal security models. In this paper, we propose an empirical framework to integrate security model representation, security policy specification, and systematic validation of security model and policy, which would be eventually used for accommodating security concerns during the system development. We also describe how our framework can minimize the gap between security models and the development of secure systems. In addition, we overview a proof-of-concept prototype of our tool that facilitates existing software engineering mechanisms to achieve the above-mentioned features of our framework.\nTitle:", "model_inf_time": 1.23}, {"id": "41234", "output": "FlowGuard: Dynamic Firewall Policy Enforcement in OpenFlow Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nOpenFlow, as the prevailing technique for Software-Defined Networks (SDNs), introduces significant programmability, granularity, and flexibility for many network applications to effectively manage and process network flows. However, because OpenFlow attempts to keep the SDN data plane simple and efficient, it focuses solely on L2/L3 network transport and consequently lacks the fundamental ability of stateful forwarding for the data plane. Also, OpenFlow provides a very limited access to connection-level information in the SDN controller. In particular, for any network access management applications on SDNs that require comprehensive network state information, these inherent limitations of OpenFlow pose significant challenges in supporting network services. To address these challenges, we propose an innovative connection tracking framework called STATEMON that introduces a global state-awareness to provide better access control in SDNs. STATEMON is based on a lightweight extension of OpenFlow for programming the stateful SDN data plane, while keeping the underlying network devices as simple as possible. To demonstrate the practicality and feasibility of STATEMON, we implement and evaluate a stateful network firewall and port knocking applications for SDNs, using the APIs provided by STATEMON. Our evaluations show that STATEMON introduces minimal message exchanges for monitoring active connections in SDNs with manageable overhead (3.27% throughput degradation).\nTitle:\nState-aware Network Access Management for Software-Defined Networks.\n\nAbstract:\nWe propose and implement an innovative remote attestation framework called DR@FT for efficiently measuring a target system based on an information flow-based integrity model. With this model, the high integrity processes of a system are first measured and verified, and these processes are then protected from accesses initiated by low integrity processes. Toward dynamic systems with frequently changed system states, our framework verifies the latest state changes of a target system instead of considering the entire system information. Our attestation evaluation adopts a graph-based method to represent integrity violations, and the graph-based policy analysis is further augmented with a ranked violation graph to support high semantic reasoning of attestation results. As a result, DR@FT provides efficient and effective attestation of a system's integrity status, and offers intuitive reasoning of attestation results for security administrators. Our experimental results demonstrate the feasibility and practicality of DR@FT.\nTitle:\nRemote Attestation with Domain-Based Integrity Model and Policy Analysis\n\nAbstract:\nIdentifying and protecting the trusted computing base (TCB) of a system is an important task, which is typically performed by designing and enforcing a system security policy and verifying whether an existing policy satisfies security objectives. To efficiently support these, an intuitive and cognitive policy analysis mechanism is desired for policy designers or security administrators due to the high complexity of policy configurations in contemporary systems. In this paper, we present a graph-based policy analysis methodology to identify TCBs with the consideration of different system applications and services. Through identifying information flows violating the integrity protection of TCBs, we also propose resolving principles to using our developed graph-based policy analysis tool.\nTitle:\nTowards System Integrity Protection with Graph-Based Policy Analysis\n\nAbstract:\nTraditional network security technologies such as firewalls and intrusion detection systems usually work according to a static ruleset only. We believe that a better approach to network security can be achieved if we use quantified levels of risk as an input. In this paper, we describe a dynamic access control architecture which uses risk to determine whether to allow or deny access by a source connection into the network. A simulation of our architecture shows favorable and promising results.\nTitle:\nDynamic and risk-aware network access management\n\nAbstract:\nMobile Ad hoc Networks (MANET) have been highly vulnerable to attacks due to the dynamic nature of its network infrastructure. Among these attacks, routing attacks have received considerable attention since it could cause the most devastating damage to MANET. Even though there exist several intrusion response techniques to mitigate such critical attacks, existing solutions typically attempt to isolate malicious nodes based on binary or nai\u0308ve fuzzy response decisions. However, binary responses may result in the unexpected network partition, causing additional damages to the network infrastructure, and nai\u0308ve fuzzy responses could lead to uncertainty in countering routing attacks in MANET. In this paper, we propose a risk-aware response mechanism to systematically cope with the identified routing attacks. Our risk-aware approach is based on an extended Dempster-Shafer mathematical theory of evidence introducing a notion of importance factors. In addition, our experiments demonstrate the effectiveness of our approach with the consideration of several performance metrics.\nTitle:\nRisk-Aware Mitigation for MANET Routing Attacks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.313, mean value for all the writers is 8.648, which makes it 0.286 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSoftware-Defined Networking (SDN) introduces significant granularity, visibility and flexibility to networking, but at the same time brings forth new security challenges. One of the fundamental challenges is to build robust firewalls for protecting OpenFlow-based networks where network states and traffic are frequently changed. To address this challenge, we introduce FlowGuard, a comprehensive framework, to facilitate not only accurate detection but also effective resolution of firewall policy violations in dynamic OpenFlow-based networks. FlowGuard checks network flow path spaces to detect firewall policy violations when network states are updated. In addition, FlowGuard conducts automatic and real-time violation resolutions with the help of several innovative resolution strategies designed for diverse network update situations. We also implement our framework and demonstrate the efficacy and efficiency of the proposed detection and resolution approaches in FlowGuard through experiments with a real-world network topology.\nTitle:", "model_inf_time": 1.43}, {"id": "41235", "output": "Joint Charge and Thermal Management of Batteries in Hybrid Power Sources", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe goal of a dynamic power management policy is to reduce the power consumption of an electronic system by putting system components into different states, each representing a certain performance and power consumption level. The policy determines the type and timing of these transitions based on the system history, workload, and performance constraints. In this paper we propose a new abstract mod...\nTitle:\nStochastic modeling of a power-managed system-construction andoptimization\n\nAbstract:\nEnergy efficiency has always been an important design criterion for portable embedded systems. To compensate for the shortcomings of electrochemical batteries such as low power density, limited cycle life, and the rate capacity effect, supercapacitors have been employed as complementary power supplies for electrochemical batteries, i.e., hybrid power supplies comprised of batteries and supercapacitors have been proposed. In this work, we consider a portable embedded system with a hybrid power supply and executing periodic real-time tasks. We perform system power management from both the power supply side and the power consumption side to maximize the system service time. Specifically, we use feedback control for maintaining the supercapacitor energy at a certain level by regulating the discharging current of the battery, such that the supercapacitor has the capability to buffer the load current fluctuation. At the power consumption side, we perform task scheduling to assist supercapacitor energy maintenance. Experimental results demonstrate that the proposed joint optimization framework of task scheduling and power supply control successfully prolongs the total service time by up to 57%.\nTitle:\nPower supply and consumption co-optimization of portable embedded systems with hybrid power supply\n\nAbstract:\nCycle life of a battery largely varies according to the battery operating conditions, especially the battery temperature. In particular, batteries age much faster at high temperature. Extensive experiments have shown that the battery temperature varies dramatically during continuous charge or discharge process. This paper introduces a forced convection cooling technique for the batteries that power a portable system. Since the cooling fan is also powered by the same battery, it is critical to develop a highly effective, low power-consuming solution. In addition, there is a fundamental tradeoff between the service time of a battery equipped with fans and the cycle life of the same battery. In particular, as the fan speed is increased, the power dissipated by the fan goes up and hence the full charge capacity of the battery is lost at a faster rate, but at the same time, the battery temperature remains lower and hence the battery longevity increases. This is the first work that formulates the adaptive thermal management problem for batteries (ATMB) in portable systems and provides a systematic solution for it. A hierarchical algorithm combining reinforcement learning at the lower level and dynamic programming at the upper level is proposed to derive the ATMB policy.\nTitle:\nAdaptive thermal management for portable system batteries by forced convection cooling\n\nAbstract:\nThis paper is concerned with the problem of maximizing capacity utilization of the battery power source in a portable electronic system under latency and loss rate constraints. First, a detailed stochastic model of a power-managed, battery powered electronic system is presented. The model, which is based on the theories of continuous-time Markovian decision processes and stochastic networks, captures two important characteristics of today's rechargeable battery cells, i.e., the current rate-capacity characteristic and the relaxation-induced recovery. Next, the battery-aware dynamic power management problem is formulated as a policy optimization problem and solved exactly by using a linear programming approach. Experimental results show that the proposed method outperforms existing heuristic methods for battery management by as much as 17% in terms of the average energy delivered per unit weight of battery cells.\nTitle:\nBattery-aware power management based on Markovian decision processes\n\nAbstract:\nThe growing packing density and power consumption of very large scale integration (VLSI) circuits have made thermal effects one of the most important concerns of VLSI designers. The increasing variability of key process parameters in nanometer CMOS technologies has resulted in larger impact of the substrate and metal line temperatures on the reliability and performance of the devices and interconn...\nTitle:\nThermal Modeling, Analysis, and Management in VLSI Circuits: Principles and Methods\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.346, mean value for all the writers is 8.648, which makes it 1.449 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper introduces a joint charge and thermal management problem for batteries in a battery-supercapacitor hybrid power source of a portable system, which has been equipped with a forced convection cooling technique, such as a fan. A key consideration in such a system is that the battery aging depends strongly on the battery temperature, which is in turn a function of the workload running on th...\nTitle:", "model_inf_time": 1.31}, {"id": "41236", "output": "Phase-Aware Dictionary Refinement for NMF-Based Music Transcription", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nGiven a musical audio recording, the goal of music transcription is to determine a score-like representation of the piece underlying the recording. Most current transcription methods employ variants of non-negative matrix factorization (NMF), which often fails to robustly model instruments producing non-stationary sounds. Using entire time-frequency patterns to represent sounds, non-negative matrix deconvolution (NMD) can capture certain types of nonstationary behavior but is only applicable if all sounds have the same length. In this paper, we present a novel method that combines the non-stationarity modeling capabilities available with NMD with the variable note lengths possible with NMF. Identifying frames in NMD patterns with states in a dynamical system, our method iteratively generates sound-object candidates separately for each pitch, which are then combined in a global optimization. We demonstrate the transcription capabilities of our method using piano pieces assuming the availability of single note recordings as training data.\nTitle:\nA Dynamic Programming Variant Of Non-Negative Matrix Deconvolution For The Transcription Of Struck String Instruments\n\nAbstract:\nNon-negative blind signal decomposition methods are widely used for musical signal processing tasks, such as automatic transcription and source separation. A spectrogram can be decomposed into a dictionary of full spectrum basis atoms and their corresponding time activation vectors using methods such as Non-negative Matrix Factorisation (NMF) and Non-negative K-SVD (NN-K-SVD). These methods are constrained by their learning order and problems posed by overlapping sources in the time and frequency domains of the source spectrogram. We consider that it may be possible to improve on current results by providing prior knowledge on the number of sources in a given spectrogram and on the individual structure of the basis atoms, an approach we refer to as structure-aware dictionary learning. In this work we consider dictionary recoverability of harmonic atoms, as harmonicity is a common structure in music signals. We present results showing improvements in recoverability using structure-aware decomposition methods, based on NN-K-SVD and NMF. Finally we propose an alternative structure-aware dictionary learning algorithm incorporating the advantages of NMF and NN-K-SVD.\nTitle:\nStructure-aware dictionary learning with harmonic atoms\n\nAbstract:\nAutomatic music transcription (AMT) can be performed by deriving a pitch-time representation through decomposition of a spectrogram with a dictionary of pitch-labelled atoms. Typically, non-negative matrix factorisation (NMF) methods are used to decompose magnitude spectrograms. One atom is often used to represent each note. However, the spectrum of a note may change over time. Previous research considered this variability using different atoms to model specific parts of a note, or large dictionaries comprised of datapoints from the spectrograms of full notes. In this paper, the use of subspace modelling of note spectra is explored, with group sparsity employed as a means of coupling activations of related atoms into a pitched subspace. Stepwise and gradient-based methods for non-negative group sparse decompositions are proposed. Finally, a group sparse NMF approach is used to tune a generic harmonic subspace dictionary, leading to improved NMF-based AMT results.\nTitle:\nNon-Negative Group Sparsity with Subspace Note Modelling for Polyphonic Transcription.\n\nAbstract:\nNon-negative Matrix Factorisation (NMF) is a commonly used tool in many musical signal processing tasks, including Automatic Music Transcription (AMT). However unsupervised NMF is seen to be problematic in this context, and harmonically constrained variants of NMF have been proposed. While useful, the harmonic constraints may be constrictive in mixed signals. We have previously observed that recovery of overlapping signal elements using NMF is improved through introduction of a sparse coding step, and propose here the incorporation of a sparse coding step using the Hellinger distance into a NMF algorithm. Improved AMT results for unsupervised NMF are reported.\nTitle:\nNon-negative matrix factorisation incorporating greedy hellinger sparse coding applied to polyphonic music transcription\n\nAbstract:\nSeveral probabilistic models involving latent components have been proposed for modelling time-frequency (TF) representations of audio signals (such as spectrograms), notably in the nonnegative matrix factorization (NMF) literature. Among them, the recent high resolution NMF (HR-NMF) model is able to take both phases and local correlations in each frequency band into account, and its potential has been illustrated in applications such as source separation and audio inpainting. In this paper, HR-NMF is extended to multichannel signals and to convolutive mixtures. A fast variational expectation-maximization (EM) algorithm is proposed to estimate the enhanced model. This algorithm is applied to a stereophonic piano signal, and proves capable of accurately modelling reverberation and restoring missing observations.\nTitle:\nMultichannel HR-NMF for modelling convolutive mixtures of non-stationary signals in the time-frequency domain.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.669, mean value for all the writers is 8.648, which makes it 0.871 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTechniques based on non-negative matrix factorization (NMF) have been successfully used to decompose a spectrogram of a music recording into a dictionary of templates and activations. While advanced NMF variants often yield robust signal models, there are usually some inaccuracies in the factorization since the underlying methods are not prepared for phase cancellations that occur when sounds with similar frequency are mixed. In this paper, we present a novel method that takes phase cancellations into account to refine dictionaries learned by NMF-based methods. Our approach exploits the fact that advanced NMF methods are often robust enough to provide information about how sound sources interact in a spectrogram, where they overlap, and thus where phase cancellations could occur. Using this information, the distances used in NMF are weighted entry-wise to attenuate the influence of regions with phase cancellations. Experiments on full-length, polyphonic piano recordings indicate that our method can be successfully used to refine NMF-based dictionaries.\nTitle:", "model_inf_time": 1.53}, {"id": "41237", "output": "Efficient Urban Object Recognition from Mobile Imagery Using Informative SIFT Descriptors", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe propose reliable outdoor object detection on mobile phone imagery from off-the-shelf devices. With the goal to provide both robust object detection and reduction of computational complexity for situated interpretation of urban imagery, we propose to apply the 'Informative Descriptor Approach' on SIFT features (i-SIFT descriptors). We learn an attentive matching of i-SIFT keypoints, resulting in a significant improvement of state-of-the-art SIFT descriptor based keypoint matching. In the off-line learning stage, firstly, standard SIFT responses are evaluated using an information theoretic quality criterion with respect to object semantics, rejecting features with insufficient conditional entropy measure, producing both sparse and discriminative object representations. Secondly, we learn a decision tree from the training data set that maps SIFT descriptors to entropy values. The key advantages of informative SIFT (i-SIFT) to standard SIFT encoding are argued from observations on performance complexity, and demonstrated in a typical outdoor mobile vision experiment on the MPG-20 reference database.\nTitle:\nBuilding detection from mobile imagery using informative SIFT descriptors\n\nAbstract:\nThe presented work settles attention in the architecture of ambient intelligence, in particular, for the application of mobile vision tasks in multimodal interfaces. A major issue for the performance of these services is uncertainty in the visual information which roots in the requirement to index into a huge amount of reference images. We propose a system implementation --the Attentive Machine Interface (AMI) --that enables contextual processing of multi-sensor information in a probabilistic framework, for example to exploit contextual information from geo-services with the purpose to cut down the visual search space into a subset of relevant object hypotheses. We present a proof-of-concept with results from bottom-up information processing from experimental tracks and image capture in an urban scenario, extracting object hypotheses in the local context from both (i) mobile image based appearance and (ii) GPS based positioning, and verify performance in recognition accuracy ( 10%) using Bayesian decision fusion. Finally, we demonstrate that top-down information processing --geo-information priming the recognition method in feature space --can yield even better results ( 13%) and more economic computing, verifying the advantage of multi-sensor attentive processing in multimodal interfaces.\nTitle:\nAn Attentive Machine Interface Using Geo-Contextual Awareness for Mobile Vision Tasks\n\nAbstract:\nAutonomous mobile agents require object recognition for high level interpretation and localization in complex scenes. In urban environments, recognition of buildings might play a dominant role in robotic systems that need object based navigation, that take advantage of visual feedback and multimodal information for self-localization, or that enable association to related information from the identified semantics. We present a new method - the informative local features approach - based on an information theoretic saliency measure that is rapidly derived from a local Parzen window density estimation in feature subspace. From the learning of a decision tree based mapping to informative features, it enables attentive access to discriminative information and thereby significantly speeds up the recognition process. This approach is highly robust with respect to severe degrees of partial occlusion, noise, and tolerant to some changes in scale and illumination. We present performance evaluation on our publicly available reference object database (TSG-20) that demonstrates the efficiency of this approach, case wise even outperforming the SIFT feature approach [1]. Building recognition will be advantageous in various application domains, such as, mobile mapping, unmanned vehicle navigation, and systems for car driver assistance.\nTitle:\nUrban Object Recognition From Informative Local Features\n\nAbstract:\nThis paper presents semantic analysis in mobile eye tracking studies as a novel application for video analysis in usability research and marketing analyses. Image analysis in the context of human attention within natural environments is recognized as a specific challenge on object detection and localization. In a characteristic study setting of urban marketing analysis, we propose an innovative method to extract semantic information from public media screens mediating content to passengers in public transportation. In an extensive performance test within a unique 100 subject study on real world data the method performed very successful in robustness and accuracy with respect to detection and thus proved being a valuable tool for semi-automated annotation of mobile eye tracking studies, requiring manual interaction in only \u224814% of image data.\nTitle:\nSemantic analysis of human visual attention in mobile eye tracking applications\n\nAbstract:\nThis work is about a novel methodology for window detection in urban environments and its multiple use in vision system applications. The presented method for window detection includes appropriate early image processing, provides a multi-scale Haar wavelet representation for the determination of image tiles which is then fed into a cascaded classifier for the task of window detection. The classifier is learned from a Gentle Adaboost driven cascaded decision tree [1] on masked information from training imagery and is tested towards window based ground truth information which is - together with the original building image databases - publicly available [10, 11, 13]. The experimental results demonstrate that single window detection is to a sufficient degree successful, e.g., for the purpose of building recognition, and, furthermore, that the classifier is in general capable to provide a region of interest operator for the interpretation of urban environments. The extraction of this categorical information is beneficial to index into search spaces for urban object recognition as well as aiming towards providing a semantic focus for accurate post-processing in 3D information processing systems. Targeted applications are (i) mobile services on uncalibrated imagery, e.g. , for tourist guidance, (ii) sparse 3D city modeling, and (iii) deformation analysis from high resolution imagery.\nTitle:\nWindow Detection in Facades\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.54, mean value for all the writers is 8.648, which makes it 0.761 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe present a computer vision system for the detection and identification of urban objects from mobile phone imagery, e.g., for the application of tourist information services. Recognition is based on MAP decision making over weak object hypotheses from local descriptor responses in the mobile imagery. We present an improvement over the standard SIFT key detector [7] by selecting only informative (i-SIFT) keys for descriptor matching. Selection is applied first to reduce the complexity of the object model and second to accelerate detection by selective filtering. We present results on the MPG-20 mobile phone imagery with severe illumination, scale and viewpoint changes in the images, performing with \u2248 98% accuracy in identification, efficient (100%) background rejection, efficient (0%) false alarm rate, and reliable quality of service under extreme illumination conditions, significantly improving standard SIFT based recognition in every sense, providing - important for mobile vision - runtimes which are \u2248 8 (\u224824) times faster for the MPG-20 (ZuBuD) database.\nTitle:", "model_inf_time": 1.64}, {"id": "41238", "output": "Teaching Consistency Between Data, Functional, and Process Models", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nGeneral theories of software engineering must balance between providing full understanding of a single case and providing partial understanding of many cases. In this paper we argue that for theories to be useful in practice, they should give sufficient understanding of a sufficiently large class of cases, without having to be universal or complete. We provide six strategies for developing such theories of the middle range.\nTitle:\nSix strategies for generalizing software engineering theories.\n\nAbstract:\nTraditionally, software measurement literature considers the uncertainty of cost drivers in project estimation as a challenge and treats it as such. This paper develops the position that uncertainty can be seen as an asset. It draws on results of a case study in which we replicated an approach to balancing uncertainties of project context characteristics in requirements-based effort estimation for ERP implementations.\nTitle:\nUncertainty in ERP Effort Estimation: A Challenge or an Asset?\n\nAbstract:\nA key issue in Requirements Engineering (RE) for Enterprise Resource Planning (ERP) in a crossorganizational context is how to find a match between the ERP application modules and requirements for business coordination. This paper proposes a conceptual framework for analyzing coordination requirements in inter-organizational ERP projects from a coordination theory perspective. It considers the undocumented assumptions for coordination that may have significant implications for ERP adopting organizations. In addition, we build a library of existing coordination mechanisms supported by modern ERP systems, and use it to make a proposal for how to improve the match between ERP implementations and supported business coordination processes. We discuss the implications of our framework for practicing requirements engineers. Our framework and library are based on a literature survey and the experience with ERP implementation of one of us (Daneva). In future empirical research we will further validate and refine our framework.\nTitle:\nRequirements Engineering for Cross-organizational ERP Implementation: Undocumented Assumptions and Potential Mismatches\n\nAbstract:\nThis paper presents a two-site case study on requirements-based effort estimation practices in enterprise resource planning projects. Specifically, the case study investigated the question of how to handle qualitative data and highly volatile values of project context characteristics. We counterpart this challenge and expound upon the integration of portfolio management concepts and simulation concepts into a classic effort estimation model (COCOMO II).\nTitle:\nIntegrating Portfolio Management and Simulation Concepts in the ERP Project Estimation Practice\n\nAbstract:\nThis paper deals with the development and use of business cases in support of cross-organizational enterprise resource planning (ERP)-enabled e-business integration initiatives. In order to ensure that such a project starts successfully, we will focus on pre-implementation activities. We propose a set of business case guidelines that emphasize the importance of benefits management during ERP implementations.\nTitle:\nCross-organizational ERP management: how to create a successful business case?\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.182, mean value for all the writers is 8.648, which makes it 2.162 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nConsider the situation that you have a data model, a functional model and a process model of a system, perhaps made by different analysts at different times. Are these models consistent with each other? A relevant question in practice - and therefore we think it should also be addressed in our courses. However, UML modelling textbooks don't discuss it, so we developed our own teaching materials. In this position paper we explain why and how.\nTitle:", "model_inf_time": 1.11}, {"id": "41239", "output": "Methodological Guidelines for Design Science Research: Navigating Practical and Knowledge Problems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCurrent proposals for combining action research and design science start with a concrete problem in an organization, then apply an artifact to improve the problem, and finally reflect on lessons learned. The aim of these combinations is to reduce the tension between relevance and rigor. This paper proposes another way of using action research in design science, which starts with an artifact, and then tests it under conditions of practice by solving concrete problems with them. The aim of this way of using action research in design science is to bridge the gap between the idealizations made when designing the artifact and the concrete conditions of practice that occur in real-world problems. The paper analyzes the role of idealization in design science and compares it with the requirements of rigor and relevance. It then proposes a way of bridging the gap between idealization and practice by means of action research, called technical action research (TAR) in this paper. The core of TAR is that the researcher plays three roles, which must be kept logically separate, namely of artifact developer, artifact investigator, and client helper. Finally, TAR is compared to other approaches of using action research in design science, and with canonical action research.\nTitle:\nTechnical action research as a validation method in information systems design science\n\nAbstract:\nThis tutorial presents a sound methodology for technical action research, which consist of testing a new artifact by using it to solve a real problem. Such a test would be useless if we could not generalize from it, and the tutorial introduces architectural inference as a way of supporting generalizations by technical action research.\nTitle:\nDesigning technical action research and generalizing from real-world cases\n\nAbstract:\nFor several decades there has been a debate in the computing sciences about the relative roles of design and empirical research, and about the contribution of design and research methodology to the relevance of research results. In this minitutorial we review this debate and compare it with evidence about the relation between design and research in the history of science and technology. Our review shows that research and design are separate but concurrent activities, and that relevance of research results depends on problem setting rather than on rigorous methods. We argue that rigorous scientific methods separate design from research, and we give simple model for how to do this in a problem-driven way.\nTitle:\nDesign Science, Engineering Science and Requirements Engineering\n\nAbstract:\nThis paper was triggered by concerns about the methodological soundness of many RE papers. We present a conceptual framework that distinguishes design papers from research papers, and show that in this framework, what is called a research paper in RE is often a design paper. We then present and motivate two lists of evaluation criteria, one for research papers and one for design papers. We apply both of these lists to two samples drawn from the set of all submissions to the RE\u201903 conference. Analysis of these two samples shows that most submissions of the RE\u201903 conference are design papers, not research papers, and that most design papers present a solution to a problem but neither validate this solution nor investigate the problems that can be solved by this solution. We conclude with a discussion of the soundness of our results and of the possible impact on RE research and practice.\nTitle:\nThe methodological soundness of requirements engineering papers: a conceptual framework and two case studies\n\nAbstract:\nScientific evaluation papers investigate existing problem situations or validate proposed solutions with scientific means, such as by experiment or case study. There is a growing amount of literature about how to report about empirical research in software engineering, but there is still some confusion about the difference between a scientific evaluation paper and other kinds of research papers. This is related to lack of clarity about the relation between empirical research, engineering, and industrial practice. In this minitutorial we give a brief rundown on how to structure a scientific evaluation papers as a special kind of research paper, using experiment reports and case study reports as examples. We give checklists of items that a reader should be able to find in these papers, and sketch the dilemmas that writers and readers of these papers face when applying these checklists.\nTitle:\nHow to Write and Read a Scientific Evaluation Paper\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.882, mean value for all the writers is 8.648, which makes it 0.2 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDesign science emphasizes the connection between knowledge and practice by showing that we can produce scientific knowledge by designing useful things. However, without further guidelines, aspiring design science researchers tend to identify practical problems with knowledge questions, which may lead to methodologically unsound research designs. To solve a practical problem, the real world is changed to suit human purposes, but to solve a knowledge problem, we acquire knowledge about the world without necessarily changing it. In design science, these two kinds of problems are mutually nested, but this nesting should not blind us for the fact that their problem-solving and solution justification methods are different. This paper analyzes the mutual nesting of practical problems and knowledge problems, derives some methodological guidelines from this for design science researchers, and gives an example of a design science project following this problem nesting.\nTitle:", "model_inf_time": 1.56}, {"id": "41240", "output": "Identifying and Obfuscating Hardware Trojans in FPGA Cryptographic Implementations", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSoft microprocessors are vital components of many embedded FPGA systems. As the application domain for FPGAs expands, the security of the software used by soft processors increases in importance. Although software confidentiality approaches (e.g. encryption) are effective, code obfuscation is known to be an effective enhancement that further deters code understanding for attackers. The availability of specialization in FPGAs provides a unique opportunity for code obfuscation on a per-application basis with minimal hardware overhead. In this paper we describe a new technique to obfuscate soft microprocessor code which is located outside the FPGA chip in an unprotected area. Our approach provides customizable, data-dependent control flow modification to make it difficult for attackers to easily understand program behavior. The application of the approach to three benchmarks illustrates a control flow cyclomatic complexity increase of about 7X with a modest logic overhead for the soft processor.\nTitle:\nHardware-assisted code obfuscation for FPGA soft microprocessors\n\nAbstract:\nEmbedded memory blocks are important resources in contemporary FPGA devices. When targeting FPGAs, application designers often specify high-level memory functions which exhibit a range of sizes and control structures. These logical memories must be mapped to FPGA embedded memory resources such that physical design objectives are met. In this work a set of power-aware logical-to-physical RAM mapping algorithms are described which convert user-defined memory specifications to on-chip FPGA memory block resources. These algorithms minimize RAM dynamic power by evaluating a range of possible embedded memory block mappings and selecting the most power-efficient choice. Our automated approach has been integrated into a commercial FPGA compiler and tested with 40 large FPGA benchmarks. Through experimentation, we show that, on average, embedded memory dynamic power can be reduced by 21% and overall core dynamic power can be reduced by 7% with a minimal loss (1%) in design performance.\nTitle:\nPower-aware RAM mapping for FPGA embedded memory blocks\n\nAbstract:\nThis paper describes a system-level approach to improve the latency of FPGA designs by performing optimization of the design specification on a functional level prior to highlevel synthesis. The approach uses Taylor Expansion Diagrams (TEDs), a functional graph-based design representation, as a vehicle to optimize the dataflow graph (DFG) used as input to the subsequent synthesis. The optimization focuses on critical path compaction in the functional representation before translating it into a structural DFG representation. Our approach engages several passes of a traditional high-level synthesis (HLS) process in a simulated annealing-based loop to make efficient cost tradeoffs. The algorithm is time efficient and can be used for fast design space exploration. The results indicate a latency performance improvement of 22% on average versus HLS with the initial DFG for a series of designs mapped to Altera Stratix II devices.\nTitle:\nFPGA latency optimization using system-level transformations and DFG restructuring\n\nAbstract:\nThis paper describes a complete off-chip memory security solution for embedded systems. Our security core is based on a one-time pad (OTP) encryption circuit and a CRC-based integrity checking module. These modules safeguard external memory used by embedded processors against a series of well-known attacks, including replay attacks, spoofing attacks and relocation attacks. Our implementation limits on-chip memory space overhead to less than 33% versus memory used by a standard microprocessor and reduces memory latency achieved by previous approaches by at least half. The performance loss for software execution with our solution is only 10% compared with a non-protected implementation. An FPGA prototype of our security core has been completed to validate our findings.\nTitle:\nA security approach for off-chip memory in embedded microprocessor systems\n\nAbstract:\nIn this research a refined interface between high-level design languages and hardware verification platforms is developed. Our interface methodology is demonstrated through the integration of a communication system design, written in C and SystemC, with a multi-FPGA logic emulator from Ikos Systems. We show that as designs are refined from a high-level to a gate-level representation, our methodology improves verification performance while maintaining verification fidelity across a range of abstraction levels.\nTitle:\nThe Integration of SystemC and Hardware-Assisted Verification\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.893, mean value for all the writers is 8.648, which makes it 0.644 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn contrast to ASICs, hardware Trojans can potentially be injected into FPGA designs post-manufacturing by bit stream alteration. Hardware Trojans which target cryptographic primitives are particularly interesting for an adversary because a weakened primitive can lead to a complete loss of system security. One problem an attacker has to overcome is the identification of cryptographic primitives in a large bit stream with unknown semantics. As the first contribution, we demonstrate that AES can be algorithmically identified in a look-up table-level design for a variety of implementation styles. Our graph-based approach considers AES implementations which are created using several synthesis and technology mapping options. As the second contribution, we present and discuss the drawbacks of a dynamic obfuscation countermeasure which allows for the configuration of certain crucial parts of a cryptographic primitive after the algorithm has been loaded into the FPGA. As a result, reverse-engineering and modifying a primitive in the bit stream is more challenging.\nTitle:", "model_inf_time": 1.53}, {"id": "41241", "output": "A Time Wave Neural Network for Global Optimal Project Scheduling", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe paper considers the dynamic job shop scheduling problem (DJSSP) with job release dates which arises widely in practical production systems. The principle characteristic of DJSSP considered in the paper is that the jobs arrive continuously in time and the attributes of the jobs, such as the release dates, routings and processing times are not known in advance, whereas in the classical job shop scheduling problem (CJSSP), it is assumed that all jobs to be processed are available at the beginning of the scheduling process. Reactive scheduling approach is one of the effective approaches for DJSSP. In the paper, a heuristic is proposed to implement the reactive scheduling of the jobs in the dynamic production environment. The proposed heuristic decomposes the original scheduling problem into a number of sub problems. Each sub problem, in fact, is a dynamic single machine scheduling problem with job release dates. The scheduling technique applied in theproposed heuristic is priority scheduling, which determines the next state of the system based on priority values of certain system elements. The system elements are prioritized with the help of scheduling rules (SRs). An approach based on gene expression programming (GEP) is also proposed in the paper to construct efficient SRs for DJSSP. The rules constructed by GEP are evaluated in the comparison of the rules constructed by GP and several prominent human made rules selected from literatures on extensive problem sets with respect to various measures of performance.\nTitle:\nReactive scheduling in a job shop where jobs arrive over time\n\nAbstract:\nWelding is one of the most important technologies in manufacturing industries due to its extensive applications. Welding scheduling can affect the efficiency of the welding process greatly. Thus, welding scheduling problem is important in welding production. This paper studies a challenging problem of dynamic scheduling in a real-world welding industry. To satisfy needs of dynamic production, three types of dynamic events, namely, machine breakdown, job with poor quality and job release delay, are considered. Furthermore, controllable processing times (CPT), sequence-dependent setup times (SDST) and job-dependent transportation times (JDTT) are also considered. Firstly, we formulate a model for the multi-objective dynamic welding scheduling problem (MODWSP). The objectives are to minimize the makespan, machine load and instability simultaneously. Secondly, we develop a hybrid multi-objective grey wolf optimizer (HMOGWO) to solve this MODWSP. In the HMOGWO, a modified social hierarchy is designed to improve its exploitation and exploration abilities. To further enhance the exploration, genetic operator is embedded into the HMOGWO. Since one characteristic of this problem is that multiple machines can handle one operation at a time, the solution is encoded as a two-part representation including a permutation vector and a machine assignment matrix. To evaluate the effectiveness of the proposed HMOGWO, we compare it with other well-known multi-objective metaheuristics including NSGA-II, SPEA2, and multi-objective grey wolf optimizer. Experimental studies demonstrate that the proposed HMOGWO outperforms other algorithms in terms of convergence, spread and coverage. In addition, the case study shows that this method can solve the real-world welding scheduling problem well. A multi-objective model for a dynamic welding scheduling problem is constructed.A hybrid multi-objective discrete grey optimizer is proposed to solve this problem.A new encoding scheme is designed to accommodate the characteristic of this problem.The proposed method outperforms other methods on most instances.\nTitle:\nA hybrid multi-objective grey wolf optimizer for dynamic scheduling in a real-world welding industry.\n\nAbstract:\nThis paper proposes a total of nine algorithms to minimize the makespan for the hybrid flowshop scheduling problem with sequence-dependent setup times. The first six algorithms are trajectory-based metaheuristics, including three variants of iterated local search and three variants of iterated greedy. The remaining three algorithms are population-based metaheuristics, namely, the improved fruit fly optimization, the improved migrating birds optimization, and the discrete artificial bee colony optimization. We present some advanced and effective technologies, including three mixed neighborhood structures, an enhanced perturbation method, and an enhanced destruction and construction procedure for the trajectory-based metaheuristics. We propose a path-relinking-based cooperative search, a diversity control scheme, and a diversified initialization approach for the improved fruit fly optimization. We calibrate the parameters and operators for the proposed algorithms by means of a design of experiments approach. To evaluate the proposed algorithms, we present several adaptations of other recent well-known meta-heuristics for the problem and conduct a comprehensive set of computational and statistical experiments to demonstrate the effectiveness of the presented algorithms. Among them, the discrete artificial bee colony optimization is the best-performing algorithm and it is able to improve 126 out of the 240 best known solutions for the benchmarks in the literature.\nTitle:\nEffective metaheuristics for scheduling a hybrid flowshop with sequence-dependent setup times.\n\nAbstract:\nIn this paper, we proposed an effective genetic algorithm for solving the flexible job-shop scheduling problem (FJSP) to minimize makespan time. In the proposed algorithm, Global Selection (GS) and Local Selection (LS) are designed to generate high-quality initial population in the initialization stage. An improved chromosome representation is used to conveniently represent a solution of the FJSP, and different strategies for crossover and mutation operator are adopted. Various benchmark data taken from literature are tested. Computational results prove the proposed genetic algorithm effective and efficient for solving flexible job-shop scheduling problem.\nTitle:\nAn effective genetic algorithm for the flexible job-shop scheduling problem\n\nAbstract:\nThe printed circuit boards (PCBs) grouping problem (PGP) is an essential part of PCBs assembly that has attracted much attention in recent years. In this paper, we propose a mathematical model and an iterated greedy (IG) algorithm, called IGP, for solving the PGP with setup time criterion. Based on the problem characteristics, two speed-up theorems are proposed and applied in the IGP. In the IGP, a new solution representation consisting of PCBs assignment sequence and component assignment sequence is adapted, and a heuristic based on PCB pairs and an iterated scheme is presented to create an initial solution. Then a merge operator is introduced to further improve the initial solution. A local search method based on the shift and swap operators is applied to improve solutions from the construction phase. To ensure the diversity of solutions, an acceptance criterion with probability is presented. Additionally, a detailed design experiment is carried out to calibrate the parameters for the presented IGP algorithm. The IGP is assessed by comparing it with the state-of-the-art algorithms in the literature. The experimental results show that the proposed IGP achieves the best performance among the tested methods for PGP. (C) 2021 Elsevier B.V. All rights reserved.\nTitle:\nAn effective iterated greedy algorithm for PCBs grouping problem to minimize setup times\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.985, mean value for all the writers is 8.648, which makes it 2.847 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper considers the time-dependent project scheduling problem (TPSP). We propose a time wave neural network (TWNN) framework that is able to achieve the global optimal solution (viz., the optimal project schedule) of the TPSP, which is very difficult to obtain using conventional methods (e.g., Dijkstra\u2019s algorithm). The proposed TWNN is a time wave neuron-based neural network without a requirement for any training. In the design of a TWNN, the overall project network of the TPSP is viewed as a neural network, while each node is considered as a wave-based neuron. With this new perspective, the wave-based neuron is constructed based on seven parts: an input, a wave receiver, a neuron state, a time-window selector, a wave generator, a wave sender, and an output. The first three parts are used to receive the waves coming from the predecessor neurons, the fourth part is used to choose the optimal feasible time window, and the remaining three parts are utilized to generate waves for the successive neurons. The main idea of a TWNN is based on the following mechanism: a wave generated from a neuron (node) means that all previous arcs (subprojects) of this neuron have been completed. In particular, the global optimal project scheduling is obtained when a wave is generated by the final destination neuron. To evaluate the performance of a TWNN, the well-known project scheduling problem library data sets are modified and considered in a comparative analysis. Numerical examples are also utilized to demonstrate the robustness of the method.\nTitle:", "model_inf_time": 1.68}, {"id": "41242", "output": "Dynamic Big Data Searching for Efficient Exploration", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDue to the emerging Big Data paradigm, traditional data management techniques result inadequate in many real life scenarios. In particular, the availability of huge amounts of data pertaining to social interactions among users calls for advanced analysis strategies. Furthermore, heterogeneity and high speed of this data require suitable data storage and management tools to be designed from scratch. In this paper, we describe a framework tailored for analysing user searches when they are connected to a social network in order to quickly identify users able to spread their influence across the network. It is worth noting that, gathering information about user preferences is crucial in several scenarios like viral marketing, tourism promotion and food education.\nTitle:\nEffective Information Spreading in Social Networks\n\nAbstract:\nDue to the increasing availability of huge amounts of data, traditional data management techniques result inadequate in many real life scenarios. Furthermore, heterogeneity and high speed of this data require suitable data storage and management tools to be designed from scratch. In this paper, we describe a framework tailored for analyzing user interactions with intelligent systems while seeking for some domain specific information (e.g., choosing a good restaurant in a visited area). The framework enhances user quest for information by performing a data exchange activity (called data posting) which enriches the information sources with additional background information and knowledge derived from experiences and behavioral properties of domain experts and users.\nTitle:\nA Framework Enhancing The User Search Activity Through Data Posting\n\nAbstract:\nBig Data rise made traditional data management techniques inadequate in many real life scenarios. In particular, the availability of huge amounts of data pertaining to user suggestions and searches calls for advanced analysis strategies in order to profitably leverage these data. Furthermore, heterogeneity and high speed of this data require suitable data storage and management tools to be designed from scratch. In this paper, we describe our proposal for analysing the way user searches and suggestions influence their social environment in order to quickly identify users able to spread their influence across the network. It is worth noting that, gathering information about user preferences is crucial in several scenarios like viral marketing, tourism promotion and food education.\nTitle:\nEvaluating the Influence of User Searches on Neighbors\n\nAbstract:\nIn this paper, we address the problem of trajectory data streams warehousing and querying, that revealed really challenging as we deal with data (trajectories) for which the order of elements is relevant. We propose an end to end framework in order to make the querying step quite effective. We performed several tests on real world datasets that confirmed the efficiency and effectiveness of the proposed techniques.\nTitle:\nWarehousing and querying trajectory data streams with error estimation\n\nAbstract:\nInformation management in healthcare is nowadays experiencing a great revolution. After the impressive progress in digitizing medical data by private organizations, also the federal government and other public stakeholders have also started to make use of healthcare data for data analysis purposes in order to extract actionable knowledge. In this paper, we propose an architecture for supporting interoperability in healthcare systems by exploiting Big Data techniques. In particular, we describe a proposal based on big data techniques to implement a nationwide system able to improve EHR data access efficiency and reduce costs.\nTitle:\nEnhancing Ehr Systems Interoperability By Big Data Techniques\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.22, mean value for all the writers is 8.648, which makes it 0.365 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDue to the emerging Big Data applications traditional data management techniques result inadequate in many real life scenarios. In particular, OLAP techniques require substantial changes in order to offer useful analysis due to huge amount of data to be analyzed and their velocity and variety. In this paper, we describe an approach for dynamic Big Data searching that based on data collected by a suitable storage system, enrich data in order to guide users through data exploration in a efficient and effective way.\nTitle:", "model_inf_time": 1.06}, {"id": "41243", "output": "Implicit Detection of Broad Phonemic Class Boundaries in Continuous Speech", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this work, we present the performance evaluation of an implicit approach for the automatic segmentation of continuous speech signals into broad phonemic classes as encountered in Greek language Our framework was evaluated with clear speech and speech with white, pink, bubble, car and machine gun additive noise Our framework's results were very promising since an accuracy of 76.1% was achieved for the case of clear speech (for distances less than 25 msec to the actual segmentation point), without presenting over-segmentation on the speech signal An average reduction of 4% in the total accuracy of our segmentation framework was observed in the case of wideband distortion additive noise environment.\nTitle:\nDetecting broad phonemic class boundaries from greek speech in noise environments\n\nAbstract:\nThe automatic phonetic time-alignment of speech databases is essential for the development cycle of a Text-to-Speech (TTS) system. Furthermore, the quality of the synthesized speech signals is strongly related to the precision of the produced alignment. In the present work we study the performance of a new HMM-based speech segmentation method. The method is based on hybrid embedded and isolated-unit trained models, and has proved to improve the phonetic segmentation accuracy in the multiple speaker task. Here it is employed on the single speaker segmentation task, utilizing a Greek-speech database. The evaluation of the method showed significant improvement in terms of phonetic segmentation accuracy as well as in the perceptual quality of synthetic speech, when compared to the baseline system.\nTitle:\nUsing Hybrid HMM-Based Speech Segmentation to Improve Synthetic Speech Quality\n\nAbstract:\nIn the present work, we propose a hybrid architecture for automatic alignment of speech waveforms and their corresponding phone sequence. The proposed architecture does not exploit any phone boundary information. Our approach combines the efficiency of embedded training techniques and the high performance of isolated-unit training. Evaluating on the established for the task of phone segmentation TIMIT database, we achieved an accuracy of 83.56%, which corresponds to improving the baseline system's accuracy by 6.09 %.\nTitle:\nA hybrid architecture for automatic segmentation of speech waveforms\n\nAbstract:\nIn the present work we study the applicability of Support Vector Machines (SVMs) on the phoneme recognition task. Specifically, the Least Squares version of the algorithm (LS-SVM) is employed in recognition of the Greek phonemes in the framework of telephone-driven voice-enabled information service. The N-best candidate phonemes are identified and consequently feed to the speech and language recognition components. In a comparative evaluation of various classification methods, the SVM-based phoneme recognizer demonstrated a superior performance. Recognition rate of 74.2% was achieved from the N-best list, for N=5, prior to applying the language model.\nTitle:\nRecognition of greek phonemes using support vector machines\n\nAbstract:\nIn the present work we study the appropriateness of a number of linear and non-linear regression methods, employed on the task of speech segmentation, for combining multiple phonetic boundary predictions which are obtained through various segmentation engines. The proposed fusion schemes are independent of the implementation of the individual segmentation engines as well as from their number. In order to illustrate the practical significance of the proposed approach, we employ 112 speech segmentation engines based on hidden Markov models (HMMs), which differ in the setup of the HMMs and in the speech parameterization techniques they employ. Specifically we relied on sixteen different HMMs setups and on seven speech parameterization techniques, four of which are recent and their performance on the speech segmentation task have not been evaluated yet. In the evaluation experiments we contrast the performance of the proposed fusion schemes for phonetic boundary predictions against some recently reported methods. Throughout this comparison, on the established for the phonetic segmentation task TIMIT database, we demonstrate that the support vector regression scheme is capable of achieving more accurate predictions, when compared to other fusion schemes reported so far.\nTitle:\nSpeech segmentation using regression fusion of boundary predictions\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.138, mean value for all the writers is 8.648, which makes it 1.271 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we evaluate an implicit approach for the automatic detection of broad phonemic class boundaries of continuous speech signals. The reported method is consisted of the prior segmentation of speech signal into pitch-synchronous segments, using pitchmarks location, for the computation of adjacent broad phonemic class boundaries. The approach's validity was tested on a phonetically rich speech corpus of Greek language as well as on the DARPA-TIMIT American-English language corpus. Our framework's results were very promising since by this method we achieved 25 msec accuracy of 76% and 74,9% respectively, without presenting over-segmentation on the speech signal.\nTitle:", "model_inf_time": 1.32}, {"id": "41244", "output": "Phone Duration Modeling for Emotional Speech Synthesis:  Improving Accuracy via Feature Selection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAccurate modeling of prosody is prerequisite for the production of synthetic speech of high quality. Phone duration, as one\n of the key prosodic parameters, plays an important role for the generation of emotional synthetic speech with natural sounding.\n In the present work we offer an overview of various phone duration modeling techniques, and consequently evaluate ten models,\n based on decision trees, linear regression, lazy-learning algorithms and meta-learning algorithms, which over the past decades\n have been successfully used in various modeling tasks. Furthermore, we study the opportunity for performance optimization\n by applying two feature selection techniques, the RReliefF and the Correlation-based Feature Selection, on a large set of\n numerical and nominal linguistic features extracted from text, such as: phonetic, phonologic and morphosyntactic ones, which\n have been reported successful on the phone and syllable duration modeling task. We investigate the practical usefulness of\n these phone duration modeling techniques on a Modern Greek emotional speech database, which consists of five categories of\n emotional speech: anger, fear, joy, neutral, sadness. The experimental results demonstrated that feature selection significantly\n improves the accuracy of phone duration prediction regardless of the type of machine learning algorithm used for phone duration\n modeling. Specifically, in four out of the five categories of emotional speech, feature selection contributed to the improvement\n of the phone duration modeling, when compared to the case without feature selection. The M5p trees based phone duration model was observed to achieve the best phone duration prediction accuracy in terms of RMSE and\n MAE.\nTitle:\nPhone duration modeling: overview of techniques and performance optimization via feature selection in the context of emotional speech\n\nAbstract:\nThe present work studies the effect of emotional speech on a smart-home application. Specifically, we evaluate the recognition performance of the automatic speech recognition component of a smart-home dialogue system for various categories of emotional speech. The experimental results reveal that word recognition rate for emotional speech varies significantly across different emotion categories.\nTitle:\nThe Effect of Emotional Speech on a Smart-Home Application\n\nAbstract:\nIn the present work we address the problem of phonetic segmentation of emotional speech. Investigating various traditional and recent HMM-based methods for speech segmentation, which we elaborated for the specifics of emotional speech segmentation, we demonstrate that the HMM-based method with hybrid embedded-isolated training offers advantageous segmentation accuracy, when compared to other HMM-based models used so far. The increased precision of the segmentation is a consequence of the iterative training process employed in the hybrid-training method, which refines the model parameters and the estimated phonetic boundaries taking advantage of the estimations made at previous iterations. Furthermore, we demonstrate the benefits of using purposely-built models for each target category of emotional speech, when compared to the case of one common model built solely from neutral speech. This advantage, in terms of segmentation accuracy, justifies the effort for creating and employing the purposely-built segmentation models per emotion category, since it significantly improves the overall segmentation accuracy.\nTitle:\nPhonetic Segmentation Of Emotional Speech With Hmm-Based Methods\n\nAbstract:\nThe present paper deals with the design and the annotation of a Greek real-world emotional speech corpus. The speech data consist of recordings collected during the interaction of naive users with a smart-home dialogue system. Annotation of the speech data with respect to the uttered command and emotional state was performed. Initial experimentations towards recognizing negative emotional states were performed and the experimental results indicate the range of difficulties when dealing with real-world data.\nTitle:\nA Real-World Emotional Speech Corpus for Modern Greek\n\nAbstract:\nIn the present work we aim at performance optimization of a speaker-independent emotion recognition system through speech feature selection process. Specifically, relying on the speech feature set defined in the Interspeech 2009 Emotion Challenge, we studied the relative importance of the individual speech parameters, and based on their ranking, a subset of speech parameters that offered advantageous performance was selected. The affect-emotion recognizer utilized here relies on a GMM-UBM-based classifier. In all experiments, we followed the experimental setup defined by the Interspeech 2009 Emotion Challenge, utilizing the FAU Aibo Emotion Corpus of spontaneous, emotionally coloured speech. The experimental results indicate that the correct choice of the speech parameters can lead to better performance than the baseline one.\nTitle:\nEnhancing emotion recognition from speech through feature selection\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.583, mean value for all the writers is 8.648, which makes it 0.798 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the present work we address the problem of phone duration modeling for the needs of emotional speech synthesis Specifically, relying on ten well known machine learning techniques, we investigate the practical usefulness of two feature selection techniques, namely the Relief and the Correlation-based Feature Selection (CFS) algorithms, for improving the accuracy of phone duration modeling The feature selection is performed over a large set of phonetic, morphologic and syntactic features In the experiments, we employed phone duration models, based on decision trees, linear regression, lazy-learning algorithms and meta-learning algorithms, trained on a Modern Greek speech database of emotional speech, which consists of five categories of emotional speech: anger, fear, joy, neutral, sadness The experimental results demonstrated that feature selection significantly improves the accuracy of phone duration modeling regardless of the type of machine learning algorithm used for phone duration modeling.\nTitle:", "model_inf_time": 1.58}, {"id": "41245", "output": "Addressing Class Imbalance in Linguistic Applications with Bayesian Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAttempting to automatically learn to identify verb complements from natural language corpora without the help of sophisticated linguistic resources like grammars, parsers or treebanks leads to a significant amount of noise in the data. In machine learning terms, where learning from examples is performed using class-labelled feature-value vectors, noise leads to an imbalanced set of vectors: assuming that the class label takes two values (in this work complement/non-complement), one class (complements) is heavily underrepresented in the data in comparison to the other. To overcome the drop in accuracy when predicting instances of the rare class due to this disproportion, we balance the learning data by applying one-sided sampling to the training corpus and thus by reducing the number of non-complement instances. This approach has been used in the past in several domains (image processing, medicine, etc) but not in natural language processing. For identifying the examples that are safe to remove, we use the value difference metric, which proves to be more suitable for nominal attributes like the ones this work deals with, unlike the Euclidean distance, which has been used traditionally in one-sided sampling. We experiment with different learning algorithms which have been widely used and their performance is well known to the machine learning community: Bayesian learners, instance-based learners and decision trees. Additionally we present and test a variation of Bayesian belief networks, the COr-BBN (Class-oriented Bayesian belief network). The performance improves up to 22% after balancing the dataset, reaching 73.7% f-measure for the complement class, having made use only a phrase chunker and basic morphological information for preprocessing.\nTitle:\nLearning verb complements for modern greek: Balancing the noisy dataset\n\nAbstract:\nImbalanced training sets, where one class is heavily underrepresented compared to the others, have a bad effect on the classification of rare class instances. We apply One-sided Sampling for the first time to a lexical acquisition task (learning verb complements from Modern Greek corpora) to remove redundant and misleading training examples of verb non-dependents and thereby balance our training set. We experiment with well-known learning algorithms to classify new examples. Performance improves up to 22% in recall and 15% in precision after balancing the dataset.\nTitle:\nLearning Greek verb complements: addressing the class imbalance\n\nAbstract:\nThe present paper discusses the issue of enhancing classification performance by means other than improving the ability of certain Machine Learning algorithms to construct a precise classification model. On the contrary, we approach this significant problem from the scope of an extended coding of training data. More specifically, our method attempts to generate more features in order to reveal the hidden aspects of the domain, modeled by the available training examples. We propose a novel feature construction algorithm, based on the ability of Bayesian networks to represent the conditional independence assumptions of a set of features, thus projecting relational attributes which are not always obvious to a classifier when presented in their original format. The augmented set of features results in a significant increase in terms of classification performance, a fact that is depicted to a plethora of machine learning domains (i.e. data sets from the UCI ML repository and the Artificial Intelligence group) using a variety of classifiers, based on different theoretical backgrounds.\nTitle:\nBayesian feature construction\n\nAbstract:\nFor the present work, we attempt to study the issue of automatic acquisition of intonational phrase breaks. A mathematically well-formed framework is suggested, which is based on Bayesian theory. Based on two different assumptions regarding the conditional independence of the input attributes, we have come up with two Bayesian implementations, namely the Naive Bayes and the Bayesian Networks classifiers. As a performance benchmark, we evaluated the experimental result against CART, an acclaimed algorithm in the field of intonational phrase -break detection that has demonstrated stat-of-the-art figures. Our approach utilizes minimal morphological and syntactic resources in a finite length window, i.e. the POS label and the type of syntactic phrase boundary, a novel attribute that has not been applied to the specific task before. On a 5500 word training set, the Bayesian networks approach proved to be the most effective, depicting precision and recall figures in the range of 82% and 77% respectively.\nTitle:\nA Data-Driven Framework for Intonational Phrase Break Prediction\n\nAbstract:\nLearning Bayesian Belief Networks from corpora has been applied to the automatic acquisition of verb subcategorization frames for Modern Greek (MG). We are incorporating minimal linguistic resources, i.e. morphological tagging and phrase chunking, since a general-purpose syntactic parser for MG is currently unavailable. Comparative experimental results have been evaluated against Naive Bayes classification, which is based on the conditional independence assumption along with two widely used methods, Log-Likelihood (LLR) and Relative Frequencies Threshold (RFT). We have experimented with a balanced corpus in order to assure unbiased behavior of the training model. Results have depicted that obtaining the inferential dependencies of the training data could lead to a precision improvement of about 4% compared to that of Naive Bayes and 7% compared to LLR and RFT Moreover, we have been able to achieve a precision exceeding 87% on the identification of subcategorization frames which are not known beforehand, while limited training data are proved to endow with satisfactory results.\nTitle:\nInfluence of Conditional Independence Assumption on Verb Subcategorization Detection\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.518, mean value for all the writers is 8.648, which makes it 0.742 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nFor the present work, we deal with the significant problem of high imbalance in data in binary or multi-class classification problems. We study two different linguistic applications. The former determines whether a syntactic construction (environment) co-occurs with a verb in a natural text corpus consists a subcategorization frame of the verb or not. The latter is called Name Entity Recognition (NER) and it concerns determining whether a noun belongs to a specific Name Entity class. Regarding the subcategorization domain, each environment is encoded as a vector of heterogeneous attributes, where a very high imbalance between positive and negative examples is observed (an imbalance ratio of approximately 1:80). In the NER application, the imbalance between a name entity class and the negative class is even greater (1:120). In order to confront the plethora of negative instances, we suggest a search tactic during training phase that employs Tomek links for reducing unnecessary negative examples from the training set. Regarding the classification mechanism, we argue that Bayesian networks are well suited and we propose a novel network structure which efficiently handles heterogeneous attributes without discretization and is more classification-oriented. Comparing the experimental results with those of other known machine learning algorithms, our methodology performs significantly better in detecting examples of the rare class.\nTitle:", "model_inf_time": 1.54}, {"id": "41246", "output": "Ontology Maintenance in Federated Collaborative Product Development Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents an ontology maintenance approach aiming at dynamically maintaining the federation collaboration ontology in High Level Architecture HLA federation development and execution process, with the objective to provide guidelines on how to use, organize and manipulate federation collaboration ontology to meet various requirements. The proposed approach includes two algorithms: ontology maintenance + and ontology maintenance -, corresponding to federate joining and resigning. It adopts an axiom-based deduction strategy and takes heavy-weighted ontologies into consideration. It can find all the explicit and derived inter-ontology relations. A case study shows that the proposed approach has great potential to improve the efficiency of federation development and execution processes, reduce the workload for adaptive adjustment of existing platforms, and enhance the applicability and flexibility of HLA based collaborative product development systems.\nTitle:\nOntology maintenance in high level architecture federation development and execution process\n\nAbstract:\nThis paper presents a novel ontology fusion approach which aims to establish a mutual understanding among HLA (High Level Architecture) -based distributed heterogeneous collaborative product development systems. The approach includes three steps: ontology mapping, ontology alignment and ontology merging. It adopts an axiom-based deduction ontology fusion strategy, and takes heavy weighted ontologies into consideration. It can find all the explicit and derived inter-ontology relations, and furthermore it reaches the active upper bounds of implicit equivalent inter-ontology relations searching. The proposed approach has great potential to improve the efficiency of preparation for HLA-based collaborative product development, reduce the work load for adaptive adjustment of ever-existing platforms, and enhance the applicability and flexibility of collaborative development systems.\nTitle:\nOntology fusion in HLA-based collaborative product development\n\nAbstract:\nIn high-level-architecture (HLA)-based distributed heterogeneous collaborative engineering environments (CEEs), the construction of federation object model files is time consuming. This paper presents an ontology fusion approach aiming at establishing a common understanding in such collaborative environments. The proposed approach has three steps: ontology mapping, ontology alignment, and ontology merging. Ontology mapping employs a top-down approach to explore all bridge relations between two terms from different ontologies based on bridge axioms and deduction rules. Ontology alignment adopts a bottom-up approach to discover implicit bridge relations between two terms from different domain ontologies based on equivalent inference. Ontology merging generates a new collaboration ontology from discovered equivalent bridge relations. It adopts an axiom-based ontology fusion strategy and takes heavy-weighted ontologies into consideration. It can find all the explicit and derived interontology relations. In a typical CEE, the proposed approach has a great potential to improve the efficiency of preparation for HLA-based collaborative engineering processes, reduce the work load for adaptive adjustment of existing platforms, and enhance the reusability and flexibility of CEEs. A case study has been conducted to validate the feasibility of the proposed approach.\nTitle:\nOntology Fusion in High-Level-Architecture-Based Collaborative Engineering Environments\n\nAbstract:\nThe reuse of existing systems is an important objective of High Level Architecture (HLA) based collaborative product development systems. However, in order to reuse an existing system, its interoperation interface has to be modified so as to comply with the objective and interaction representations defined in a corresponding Federation Object Model (FOM). Such modifications imply added time and effort, which diminishes the efficiency of system reuse in collaborative product development. This paper presents a heavy-weighted ontology-based construction method for interoperation models to support the reuse of subsystems in various collaborative contexts. In this method ontologies are used to specify the semantics of object classes and interaction classes in subsystems in a formal and computer readable fashion. In doing so, a Formal Concept Analysis (FCA) like construction method is introduced to establish the original interoperation ontology from scratch. An automatic transforming method from Simulation Object Model (SOM) into interoperation ontology is also described to make existing HLA based systems easy to adopt this approach. Then a consistency verification method is introduced to guarantee the consistency of the interoperation ontologies. A case study is used to demonstrate the feasibility of the proposed method. As a human-friendly modeling method, compared with existing interoperation modeling methods the proposed method is more flexible, efficient and reliable.\nTitle:\nOntology-based interoperation model of collaborative product development\n\nAbstract:\nIn collaborative enterprise networks, semantic heterogeneity is an important factor that hinders collaboration of various information systems. Ontology-driven semantic integration is an important category of solutions for the semantic integration problem. However, in many domains, there are no explicit and formal ontologies available. This paper proposes to adopt ontological views to address such challenges. It investigates the theoretical foundation of ontologies and ontological views. It presents a framework as a solution, based on the theoretical foundation, including the architecture of a semantic integration enabled environment, the modeling and representation of ontological views, and the semantic equivalence relationship discovered from the ontological views.\nTitle:\nOntological View-Driven Semantic Integration in Collaborative Networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.104, mean value for all the writers is 8.648, which makes it 0.389 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents a novel approach aiming to dynamically maintain the collaboration ontology in the execution of an ontology-based federated collaborative product development system when a federate joins or has resigned from a given federation. The proposed approach includes two algorithms: ontology maintenance (+) and ontology maintenance (-), corresponding to joining and resigning situations. It adopts an axiom-based deduction ontology fusion strategy, and takes heavy-weighted ontologies into consideration. It can find all the explicit and derived inter-ontology relations, and furthermore it reaches the active upper bounds of implicit equivalent inter-ontology relations searching. This paper also discusses some implementation issues on the basis of TH_RTI, a RTI (Run Time Infrastructure) version developed by National CIMS ERC, Tsinghua University. The proposed approach has great potential to improve the efficiency of ontology-based federated collaboration executions, reduce the work load for adaptive adjustment of ever-existing platforms, and enhance the applicability and flexibility of collaborative product development systems.\nTitle:", "model_inf_time": 1.35}, {"id": "41247", "output": "Task Assignment Mechanisms in Free/Libre Open Source Software Development", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe apparent success of free/libre open source software (FLOSS) development projects such as Linux, Apache, and many others has raised the question, what lessons from FLOSS development can be transferred to main- stream software development? In this paper, we use coordination theory to analyze coordination mechanisms in FLOSS development and compare our analysis with existing literature on coordination in proprietary software development. We examined developer interaction data from three active and successful FLOSS projects and used content analysis to identify the coordination mechanisms used by the participants. We found that there were similarities between the FLOSS groups and the reported practices of the proprietary project in the coordination mechanisms used to manage task-task dependencies. However, we found clear differences in the coordination mechanisms used to manage task-actor dependencies. While published descriptions of proprietary software development involved an elaborate system to locate the developer who owned the relevant piece of code, we found that \"self-assignment\" was the most common mechanism across three FLOSS projects. This coordination mechanism is consistent with expectations for distributed and largely volunteer teams. We conclude by discussing whether these emergent practices can be usefully transferred to mainstream practice and indicating directions for future research.\nTitle:\nCoordination of Free/Libre Open Source Software Development\n\nAbstract:\nWe seek to identify work practices that make Free/Libre Open Source Software (FLOSS) development teams effective. Particularly important to team effectiveness is decision making, In this paper, we report on an inductive qualitative analysis of 160 decision episode, of six FLOSS development teams. Our analysis revealed diversity in decision-making practices that seem to be related to differences in overall team characteristics and effectiveness.\nTitle:\nEmergent Decision-Making Practices in Free/Libre Open Source Software (Floss) Development Teams\n\nAbstract:\nWe review the literature on Free/Libre Open Source Software (FLOSS) development and on software development, distributed work and teams more generally to develop a theoretical model to explain the performance of FLOSS teams. The proposed model is based on Hackman's model of effectiveness of work teams, with coordination theory and collectivemind to extend Hackman's model by elaborating team practices relevant to effectiveness in software development. We propose a set of propositions to guide further research.\nTitle:\nEffective Work Practices for FLOSS Development: A Model and Propositions\n\nAbstract:\nFree/Libre Open Source Software (FLOSS) is primarily developed by distributed teams. Developers contribute from around the world and coordinate their activity almost exclusively by means of email and bulletin boards. FLOSS development teams some how profit from the advantages and evade the challenges of distributed software development. Despite the relevance of the FLOSS both for research and practice, few studies have inves- tigated the work practices adopted by these development teams. In this paper we investi- gate the structure and the coordination practices adopted by development teams during the bug-fixing process, which is considered one of main areas of FLOSS project success. In particular, based on a codification of the messages recorded in the bug tracking system of four projects, we identify the accomplished tasks, the adopted coordination mechanisms, and the role undertaken by both the FLOSS development team and the FLOSS community. We conclude with suggestions for further research.\nTitle:\nCoordination practices within FLOSS development teams: The bug fixing process\n\nAbstract:\nFree/Libre open source software (FLOSS, e.g., Linux or Apache) is primarily developed by distributed teams. Developers contribute from around the world and coordinate their activity almost exclusively by means of email and bulletin boards, yet some how profit from the advantages and evade the challenges of distributed software development. In this article we investigate the structure and the coordination practices adopted by development teams during the bug-fixing process, which is considered one of main areas of FLOSS project success. In particular, based on a codification of the messages recorded in the bug tracking system of four projects, we identify the accomplished tasks, the adopted coordination mechanisms, and the role undertaken by both the FLOSS development team and the FLOSS community. We conclude with suggestions for further research.\nTitle:\nBug Fixing Practices within Free/Libre Open Source Software Development Teams\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.105, mean value for all the writers is 8.648, which makes it 1.243 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper provides empirical evidence about how free/libre open source software development teams self-organize their work, specifically, how tasks are assigned to project team members. Following a case study methodology, we examined developer interaction data from three active and successful FLOSS projects using qualitative research methods, specifically inductive content analysis, to identify the task-assignment mechanisms used by the participants. We found that 'self-assignment' was the most common mechanism across three FLOSS projects. This mechanism is consistent with expectations for distributed and largely volunteer teams. We conclude by discussing whether these emergent practices can be usefully transferred to mainstream practice and indicating directions for future research.\nTitle:", "model_inf_time": 1.31}, {"id": "41248", "output": "Z-Ring: Fast Prefix Routing for Peer-to-Peer Overlays", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we argue that enforcing routing consistency i n key- based routing (KBR) protocols can simplify P2P application design and make structured P2P overlays suitable for more applications. We define two levels of routing consistency semantics, namel y weakly consistent KBR and strongly consistent KBR. We focus on an algorithm that provides strong consistency based on group membership service and weakly consistent KBR. The algorithm provides a continuum of consistency levels for applications with a tunable parameter.\nTitle:\nEnforcing Routing Consistency in Structured Peer-to-Peer Overlays: Should We and Could We?\n\nAbstract:\nIn this paper we present a rigorous treatment to structured overlay maintenance in decentralized peer-to-peer (P2P) systems subject to various system and network failures. We present a precise specification that requires the overlay maintenance protocols to be decentralized, preserve overlay connectivity, always converge to the desired structure whenever possible, and only maintain a small local state independent of the size of the system. We then provide a complete protocol with proof showing that it satisfies the specification. The protocol solves a number of subtle issues caused by decentralization and concurrency in the system. Our specification and the protocol overcomes a number of limitations of existing overlay maintenance protocols, such as the reliance on a centralized and continuously available bootstrap system, the assumption of a known system stabilization time, and the need to maintain large local membership lists.\nTitle:\nDecentralized, connectivity-preserving, and cost-effective structured overlay maintenance\n\nAbstract:\nIn this paper, we study two tightly coupled topics in online social networks (OSN): relationship classification and information propagation. The links in a social network often reflect social relationships among users. In this work, we first investigate identifying the relationships among social network users based on certain social network property and limited pre-known information. Social networks have been widely used for online marketing. A critical step is the propagation maximization by choosing a small set of seeds for marketing. Based on the social relationships learned in the first step, we show how to exploit these relationships to maximize the marketing efficacy. We evaluate our approach on large scale real-world data from Renren network, showing that the performances of our relationship classification and propagation maximization algorithm are pretty good in practice.\nTitle:\nRelationship classification in large scale online social networks and its impact on information propagation\n\nAbstract:\nIn social networks, users and artifacts (documents, discussions or videos) can be modelled as directed bi-type heterogeneous networks. Most existing works for community detection is either with undirected links or in homogeneous networks. In this paper, we propose an efficient algorithm OcdRank (Overlapping Community Detection and Ranking), which combines overlapping community detection and community-member ranking together in directed heterogeneous social network. The algorithm has low time complexity and supports incremental update. Experiments show that our method can detect better community structures as compared to other existing community detection methods.\nTitle:\nOverlapping Community Detection In Directed Heterogeneous Social Network\n\nAbstract:\nCloseness centrality is an important concept in social network analysis. In a graph representing a social network, closeness centrality measures how close a vertex is to all other vertices in the graph. In this paper, we combine existing methods on calculating exact values and approximate values of closeness centrality and present new algorithms to rank the top-kvertices with the highest closeness centrality. We show that under certain conditions, our algorithm is more efficient than the algorithm that calculates the closeness-centralities of all vertices.\nTitle:\nRanking of Closeness Centrality for Large-Scale Social Networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.342, mean value for all the writers is 8.648, which makes it 0.592 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we introduce Z-Ring, a fast prefix routing protocol for peer-to-peer overlay networks. Z-Ring incorporates cost-efficient membership protocol to achieve fast routing with small maintenance cost. Z-Ring achieves routing in logGN steps, where N is the network size and G is the size of a group that can be maintained by a membership protocol with low cost. With G=4096, it translates to one-hop routing for intranet environments (N\nTitle:", "model_inf_time": 1.46}, {"id": "41249", "output": "Resilient Routing Reconfiguration for Congestion-Free Network Protection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn recent years, there has been an increasing interest in peer-to-peer (P2P) multimedia streaming. In this paper, we consider constructing a high-bandwidth overlay tree for streaming services. We observe that underlay information such as link connectivity and link bandwidth is important in tree construction, because two seemingly disjoint overlay paths may share common links on the underlay. We hence study how to construct a high-bandwidth overlay tree given the underlay topology. We formulate the problem as building a Maximum Bandwidth Multicast Tree (MBMT) or a Minimum Stress Multicast Tree (MSMT), depending on whether link bandwidth is available or not. We prove that both problems are NP-hard and are not ap-proximable within a factor of (2/3 + epsiv), for any epsiv > 0, unless P = NP. We then present approximation algorithms to address them and analyze the algorithm performance. Furthermore, we discuss some practical issues (e.g., group dynamics, resilience and scalability) in system implementation. We evaluate our algorithms on Internet-like topologies. The results show that our algorithms can achieve high tree bandwidth and low link stress with low penalty in end-to-end delay. Measurement study based on Plan-etLab further confirms this. Our study shows that the knowledge of underlay is important for constructing efficient overlay trees.\nTitle:\nOn Maximizing Tree Bandwidth for Topology-Aware Peer-to-Peer Streaming\n\nAbstract:\nConfigurations for today's IP networks are becoming increasingly complex. As a result, configuration management is becoming a major cost factor for network providers and configuration errors are becoming a major cause of network disruptions. In this paper, we present and evaluate the novel idea of shadow configurations. Shadow configurations allow configuration evaluation before deployment and thus can reduce potential network disruptions. We demonstrate using real implementation that shadow configurations can be implemented with low overhead.\nTitle:\nShadow configuration as a network management primitive\n\nAbstract:\nWe present a novel algorithm that combines a recurrent neural network (RNN) and two swarm intelligence (SI) methods to infer a gene regulatory network (GRN) from time course gene expression data. The algorithm uses ant colony optimization (ACO) to identify the optimal architecture of an RNN, while the weights of the RNN are optimized using particle swarm optimization (PSO). Our goal is to construct an RNN whose response mimics gene expression data generated by time course DNA microarray experiments. We observed promising results in applying the proposed hybrid SI-RNN algorithm to infer networks of interaction from simulated and real-world gene expression data\nTitle:\nInference of Gene Regulatory Networks from Time Course Gene Expression Data Using Neural Networks and Swarm Intelligence\n\nAbstract:\nIn this work, we consider the sensor localization problem from a novel perspective by treating it as a functional dual of target tracking. In traditional tracking problems, static location-aware sensors track and predict the position/speed of a moving target. As a dual, we utilize a moving location-assistant (LA) (with global positioning system (GPS) or pre-defined moving path) to help location-unaware sensors to accurately discover their positions. We call our proposed system Landscape. In Landscape, an LA (an aircraft, for example) periodically broadcasts its current location (we call it beacon) while it moves around or through a sensor field. Each sensor collects the location beacons, measures the distance between itself and the LA based on received signal strength (RSS), and individually calculates their locations via an Unscented Kalman Filter (UKF) based algorithm. Our contributions are at least twofold. (1) Landscape is a distributed scheme, it does not rely on measured distances among neighbors (as used by most current proposals), which makes it robust to topology and density; Landscape involves zero sensor-to-sensor communication overhead, and is highly scalable to network size. (2) By introducing UKF in sensor localization problem, we reap multiple benefits: our UKF-based algorithm nicely exploits the constraints increasingly added by the beacons; it elegantly solves the nonlinear problem with low computation cost and complexity; and most importantly, it efficiently reduces the effects of measurement errors, making Landscape robust to ranging errors. Extensive simulations and evaluations with the state-of-the-art systems show that Landscape is a high-performance sensor positioning scheme for outdoor sensor networks.\nTitle:\nA Novel Distributed Sensor Positioning System Using the Dual of Target Tracking\n\nAbstract:\nMany large content publishers use multiple content distribution networks to deliver their content, and many commercial systems have become available to help a broader set of content publishers to benefit from using multiple distribution networks, which we refer to as content multihoming. In this paper, we conduct the first systematic study on optimizing content multihoming, by introducing novel algorithms to optimize both performance and cost for content multihoming. In particular, we design a novel, efficient algorithm to compute assignments of content objects to content distribution networks for content publishers, considering both cost and performance. We also design a novel, lightweight client adaptation algorithm executing at individual content viewers to achieve scalable, fine-grained, fast online adaptation to optimize the quality of experience (QoE) for individual viewers. We prove the optimality of our optimization algorithms and conduct systematic, extensive evaluations, using real charging data, content viewer demands, and performance data, to demonstrate the effectiveness of our algorithms. We show that our content multihoming algorithms reduce publishing cost by up to 40%. Our client algorithm executing in browsers reduces viewer QoE degradation by 51%.\nTitle:\nOptimizing cost and performance for content multihoming\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.019, mean value for all the writers is 8.648, which makes it 0.317 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nNetwork resiliency is crucial to IP network operations. Existing techniques to recover from one or a series of failures do not offer performance predictability and may cause serious congestion. In this paper, we propose Resilient Routing Reconfiguration (R3), a novel routing protection scheme that is (i) provably congestion-free under a large number of failure scenarios; (ii) efficient by having low router processing overhead and memory requirements; (iii) flexible in accommodating different performance requirements (e.g., handling realistic failure scenarios, prioritized traffic, and the trade-off between performance and resilience); and (iv) robust to both topology failures and traffic variations. We implement R3 on Linux using a simple extension of MPLS, called MPLS-ff. We then conduct extensive Emulab experiments and simulations using realistic network topologies and traffic demands. Our results show that R3 achieves near-optimal performance and is at least 50% better than the existing schemes under a wide range of failure scenarios.\nTitle:", "model_inf_time": 1.59}, {"id": "41250", "output": "A large-scale ad hoc network testbed at Niigata University", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe have built a wireless mesh network testbed, which is tolerant of disaster and aim to resolve a digital divide issue in rural mountain areas. This testbed is located in two areas and has total 22 nodes which are installed on the power poles in these areas. Two areas are connected to a control server at Niigata university with broadband wireless access systems using 5 GHz band via a gate way node at the local-government office, which is located at the center of two areas. Each node has two wireless LAN interfaces with IEEE 802.11b/g and a wireless LAN access point. We measure the throughput and the packet transmission successful ratio of links in our testbed. Moreover, we investigate the technical issues to manage and operate the mesh network in the rural mountain areas.\nTitle:\nA wireless mesh network testbed in rural mountain areas\n\nAbstract:\nWireless Mesh Networks (WMN) is an essential technology based on mobile ad hoc networks (MANETs). A three sector antenna system applied to WMN was proposed in early 2010, and system performance was analyzed. In this paper, we introduce a real-world three sector antenna system set up in the current WMN test bed in Niigata University, which employs IEEE 802.11 wireless LAN and supports multi-interface and multi-channel. Channel assignment is performed to the system, and two network patterns are acquired. A series of real-world single-flow and multi-flow throughput measuring experiments are performed to the two network patterns using three sector antenna system. The performance and characteristics of the system are discussed. The test bed node has the exchange function of three sector antenna and omni directional antenna. We discuss a preliminary comparison between the two antenna system. And the three sector antenna system achieves much better throughput performance than the omni directional antenna system on the basis of the one-hop throughput measuring experiment.\nTitle:\nPerformance of Wireless Mesh Networks with Three Sector Antenna\n\nAbstract:\nAn emergency node-combined wireless network that uses balloons, electric vehicles, and unmanned aerial vehicles (UAVs) was proposed. We discuss the construction and specific issues of this network. The communication area is evaluated by changing the node elevation. In this network, the elevations of balloon and UAV nodes vary, and the communication area changes with elevation. In addition, the network may be temporarily disconnected by changes of the communication area. To solve this problem, we employ a delay-tolerant network technique. We evaluate the number of nodes and node density according to the desired application and regional characteristics.\nTitle:\nConstruction of a node-combined wireless network for large-scale disasters\n\nAbstract:\nAs emergency communication systems for the large scale disaster, a wireless network using balloons, SKYMESH, was proposed so far. In this paper, a network construction method for SKYMESH is proposed. At first, network construction requirements for SKYMESH are clarified. Based on these requirements, we discuss a node placement method and a network construction method. The proposed network construction method is implemented, and its performance is evaluated by experiments. As a result, the network can be constructed by the proposed method within allowable elapsed time. In addition, interference between links can be suppressed, and high throughput can be achieved in the constructed network.\nTitle:\nNetwork construction management for emergency communication system SKYMESH in large scale disaster\n\nAbstract:\nIEEE 802.11 wireless interface supports multiple transmission rates. It is an important problem how to select the transmission rate so that the throughput is maximized. In a wireless mesh network (WMN), nodes are stationary placed in general. The characteristic of each link will not change significantly over relatively long time. It is effective to assign fixed transmission rate which achieves the best transmission performance at each link while link quality does not change significantly. There are two of for fixed transmission rate assignment schemes. One type assigns a transmission rate at each link (link-fixed), and the other assigns it at each node (node-fixed). In this paper, we propose a novel node-fixed transmission rate assignment scheme. Our proposed scheme tries to increase throughput on the condition that the network connectivity of all nodes in the WMN is kept. We evaluate the proposed scheme by experimental measurement using our testbed. Compared with the scheme that the same transmission rate is assigned to all nodes, our proposed scheme can improve the minimum throughput although the average throughput of both schemes is almost the same.\nTitle:\nExperimental evaluation of a novel transmission rate assignment scheme in wireless mesh networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 12.298, mean value for all the writers is 8.648, which makes it 3.114 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA large-scale ad hoc network testbed was built at Niigata university in November 2004. This testbed has 50 nodes on the building and along streets. Each node is composed of an SH3 processor with embedded Linux and an IEEE 802.11b wireless LAN device. All nodes are connected to the control PC via the control network, which consists of a wired LAN and one-hop wireless links. Several tools to support automatic operations of experiments are provided in the testbed.\nTitle:", "model_inf_time": 1.52}, {"id": "41251", "output": "Connecting Specialized User Interfaces for a Personal Navigation Service", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we describe a public navigation system which uses adaptive displays as directional signs. The displays are mounted to walls where they provide passersbys with directional information. Each sign is an autonomous, wirelessly networked digital displays connected to a central server. The signs are position-aware and able to adapt their display content in accordance with their current position. Advantages of such a navigation system include improved flexibility, dynamic adaptation and ease of setup and maintenance.\nTitle:\nAdaptive navigation support with public displays\n\nAbstract:\nWe are exploring a class of decision-theoretic handheld systems that give a user personalized advice about how to explore an indoor area in search of products or information. An initial user test in a simple mockup of a shopping mall showed that even novice PDA users accepted the system immediately and were able to achieve their shopping goals faster than when using a paper map of the mall. A key issue is the extent to which spontaneous user behavior can be accommodated within this framework.\nTitle:\nUser acceptance of a decision-theoretic location-aware shopping guide\n\nAbstract:\nIn this paper we regard the navigation aid provided by mobile navigation systems in a real environment and the effects of these mobile assistants to the development of spatial knowledge. Therefore, we report on a user study concerning the acquisition of spatial knowledge. This study sets up on a former study described by Kr\u00fcger and colleagues and sheds light on problems concerning the acquisition of survey knowledge while being navigated by a mobile handheld PC.\nTitle:\nAcquisition of spatial knowledge in location aware mobile pedestrian navigation systems\n\nAbstract:\nThe design of personal mobile navigation devices needs to take into account the context of use, including different types of input in order to react to changing cognitive as well as to haptic constraints. In this work we propose a laboratory evaluation framework for pedestrian navigation devices that aims to maximize the significance of results obtained in a virtual environment for later field usage. In order to increase the degree of physical motion of test users, we have designed a Virtual Environment (VE) with a treadmill-based walking interface. In order to validate our approach we present preliminary results from a study comparing over-ground walking with treadmill walking, which shows the applicability of the treadmill VE. With this work we would like to combine methodologies coming from cognitive psychology with field-study methods often used in user-interface design.\nTitle:\nA laboratory evaluation framework for pedestrian navigation devices\n\nAbstract:\nCommon Geographic Information Systems (GIS) require a high degree of expertise from its users, making them difficult to be operated by laymen. This paper describes novel approaches to easily perform typical basic spatial tasks within a GIS: e.g. pan-, zoom- and selection-operations by using multi-touch gestures in combination with foot gestures. We are interested in understanding how non-expert users interact with such multi-touch surfaces. We provide a categorization and a framework of multi-touch hand gestures for interacting with a GIS. This framework is based on an initial evaluation. We present results of a more detailed in situ-study mainly focusing on multi-user multi-touch interaction with geospatial data. Furthermore we extend our framework using a combination of multi-touch gestures with a small set of foot gestures to solve geospatial tasks.\nTitle:\nWhole Body Interaction with Geospatial Data\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.577, mean value for all the writers is 8.648, which makes it 0.793 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nNavigation services can be found in different situations and contexts: while connected to the web through a desktop PC, in cars, and more recently on PDAs while on foot. These services are usually well designed for their specific purpose, but fail to work in other situations. In this paper we present an approach that connects a variety of specialized user interfaces to achieve a personal navigation service spanning different situations. We describe the concepts behind the \\bf BPN (BMW Personal Navigator), an entirely implemented system that combines a desktop event and route planner, a car navigation system, and a multi-modal, in- and outdoor pedestrian navigation system for a PDA. Rather than designing for one unified UI, we focus on connecting specialized UIs for desktop, in-car and on-foot use.\nTitle:", "model_inf_time": 1.17}, {"id": "41252", "output": "Automatic Classification of Reading Disorders in Children Using Speech Analysis", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn clinical practice, reading disorders are still evaluated perceptually. In order to alleviate this problem, we propose to use automatic speech processing techniques to classify reading disorders. Therefore, we recorded 38 children who were suspected to have a reading disorder. The recordings were performed using a German standard test for reading disorders. Each child was perceptually assessed and the number of reading errors per child was recorded. Furthermore, the reading duration was stored for each child. If either of both values exceeded an age-dependent limit, the child was diagnosed having a reading disorder. In 30 of the 38 children the reading disorder was confirmed. In this paper, we present the results on the automatic evaluation concerning a single word reading test. We achieve up to 78.9% recognition rate in the detection of the exceedance of the reading error limit and 97.4% recognition rate in the classification for reading disorder.\nTitle:\nAutomatic classification of reading disorders in a single word reading test\n\nAbstract:\nWe present a novel system to automatically diagnose reading disorders. The system is based on a speech recognition engine with a module for prosodic analysis. The reading disorder test is based on eight different subtests. In each of the subtests, the system achieves a recognition accuracy of at least 95&percnt;. As in the perceptual version of the test, the results of the subtests are then joined into a final test result to determine whether the child has a reading disorder. In the final classification stage, the system identifies 98.3&percnt; of the 120 children correctly. In the future, our system will facilitate the clinical evaluation of reading disorders.\nTitle:\nAn automatic version of a reading disorder test\n\nAbstract:\nThis paper discusses the automatic evaluation of speech of children with cleft lip and palate (CLP). CLP speech shows special characteristics such as hypernasality, backing, and weakening of plosives. In total five criteria were subjectively assessed by an experienced speech expert on the phone level. This subjective evaluation was used as a gold standard to train a classification system. The automatic system achieves recognition results on frame, phone, and word level of up to 75.8 % CL. On speaker level significant and high correlations between the subjective evaluation and the automatic system of up to 0.89 are obtained.\nTitle:\nAutomatic Evaluation Of Characteristic Speech Disorders In Children With Cleft Lip And Palate\n\nAbstract:\nWe present a novel system for the automatic evaluation of speech and voice disorders. The system can be accessed via the internet platform-independently. The patient reads a text or names pictures. His or her speech is then analyzed by automatic speech recognition and prosodic analysis. For patients who had their larynx removed due to cancer and for children with cleft lip and palate we show that we can achieve significant correlations between the automatic analysis and the judgment of human experts in a leave-one-out experiment (p\nTitle:\nPEAKS - A system for the automatic evaluation of voice and speech disorders\n\nAbstract:\nIn speech therapy and rehabilitation, a patient's voice has to be evaluated by the therapist. Established methods for objective, automatic evaluation analyze only recordings of sustained vowels. However, an isolated vowel does not reflect a real communication situation. In this paper, a speech recognition system and a prosody module are used to analyze a text that was read out by the patients. The correlation between the perceptive evaluation of speech intelligibility by five medical experts and measures like word accuracy (WA), word recognition rate (WR), and prosodic features was examined. The focus was on the influence of reading errors on this correlation.The test speakers were 85 persons suffering from cancer in the larynx. 65 of them had undergone partial laryngectomy, i.e. partial removal of the larynx. The correlation between the human intelligibility ratings on a five-point scale and the machine was r= ---0.61 for WA, r\u2248 0.55 for WR, and r\u2248 0.60 for prosodic features based on word duration and energy. The reading errors did not have a significant influence on the results. Hence, no special preprocessing of the audio files is necessary.\nTitle:\nInfluence of Reading Errors on the Text-Based Automatic Evaluation of Pathologic Voices\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.548, mean value for all the writers is 8.648, which makes it 1.621 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we present an automatic classification approach to identify reading disorders in children. This identification is based on a standardized test. In the original setup the test is performed by a human supervisor who measures the reading duration and notes down all reading errors of the child at the same time. In this manner we recorded tests of 38 children who were suspected to have reading disorders. The data was confronted to an automatic system which employs speech recognition and prosodic analysis to identify the reading errors. In a subsequent classification experiment -- based on the speech recognizer's output, the duration of the test, and prosodic features -- 94.7 % of the children could be classified correctly.\nTitle:", "model_inf_time": 1.37}, {"id": "41253", "output": "Hardware Implementations of Atomic Primitives", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nShared memory is an appealing abstraction for parallel programming. It must be implemented with caches in order to perform well, however and caches require a coherence mechanism to ensure that processors reference current data. Hardware coherence mechanisms for large-scale machines are complex and costly, but existing software mechanisms for message-passing machines have not provided a performance-competitive solution. We claim that an intermediate hardware option-memory-mapped network interfaces that support a global physical address space-can provide most of the performance benefits of hardware cache coherence. We present a software coherence protocol that runs on this class of machines and greatly narrows the performance gap between hardware and software coherence. We compare the performance of the protocol to that of existing software and hardware alternatives and evaluate the tradeoffs among various cache-write policies. We also observe that simple program changes can greatly improve performance. For the programs in our test suite and with the changes in place, software coherence is often faster and never more than 55% slower than hardware coherence.\nTitle:\nSoftware cache coherence for large scale multiprocessors\n\nAbstract:\nTraditional supercomputers use a flat multi-bank SRAM memory organization to supply high bandwidth at low latency. Most other computers use a hierarchical organization with a small SRAM cache and slower, cheaper DRAM for main memory. Such systems rely heavily on data locality for achieving optimum performance. This paper evaluates cache-based memory systems for vector supercomputers. We develop a simulation model for a cache-based version of the Cray Research C90 and use the NAS parallel benchmarks to provide a large scale workload. We show that while caches reduce memory traffic and improve the performance of plain DRAM memory, they still lag behind cacheless SRAM. We identify the performance bottle-necks in DRAM-based memory systems and quantify their contribution to program performance degradation. We find the data fetch strategy to be a significant parameter affecting performance, evaluate the performance of several fetch policies, and show that small fetch sizes improve performance by maximizing the use of available memory bandwidth.\nTitle:\nCache performance in vector supercomputers\n\nAbstract:\nEfficient synchronization is important for achieving good performance in parallel programs, especially on large-scale multiprocessors. Most synchronization algorithms have been designed to run on a dedicated machine, with one application process per processor, and can suffer serious performance degradation in the presence of multiprogramming. Problems arise when running processes block or, worse, busy-wait for action on the part of a process that the scheduler has chosen not to run. We show that these problems are particularly severe for scalable synchronization algorithms based on distributed data structures. We then describe and evaluate a set of algorithms that perform well in the presence of multiprogramming while maintaining good performance on dedicated machines. We consider both large and small machines, with a particular focus on scalability, and examine mutual-exclusion locks, reader-writer locks, and barriers. Our algorithms vary in the degree of support required from the kernel scheduler. We find that while it is possible to avoid pathological performance problems using previously proposed kernel mechanisms, a modest additional widening of the kernel/user interface can make scheduler-conscious synchronization algorithms significantly simpler and faster, with performance on dedicated machines comparable to that of scheduler-oblivious algorithms.\nTitle:\nScheduler-conscious synchronization\n\nAbstract:\nScalable busy-wait synchronization algorithms are essential for achieving good parallel program performance on large scale multiprocessors. Such algorithms include mutual exclusion locks, reader-writer locks, and barrier synchronization. Unfortunately, scalable synchronization algorithms are particularly sensitive to the effects of multiprogramming: their performance degrades sharply when processors are shared among different applications, or even among processes of the same application. In this paper we describe the design and evaluation of scalable scheduler-conscious mutual exclusion locks, reader-writer locks, and barriers, and show that by sharing information across the kernel/application interface we can improve the performance of scheduler-oblivious implementations by more than an order of magnitude.\nTitle:\nHigh performance synchronization algorithms for multiprogrammed multiprocessors\n\nAbstract:\nShared memory is widely regarded as a more intuitive model than message passing for the development of parallel programs. A shared memory model can be provided by hardware, software, or some combination of both. One of the most important problems to be solved in shared memory environments is that of cache coherence. Experience indicates, unsurprisingly, that hardware-coherent multiprocessors greatly outperform distributed shared-memory (DSM) emulations on message-passing hardware. Intermediate options, however, have received considerably less attention. We argue in this position paper that one such option---a multiprocessor or network that provides a global physical address space in which processors can make non-coherent accesses to remote memory without trapping into the kernel or interrupting remote processors---can provide most of the performance of hardware cache coherence at little more monetary or design cost than traditional DSM systems. To support this claim we have developed the Cashmere family of software coherence protocols for NCC-NUMA (Non-Cache-Coherent, Non-Uniform-Memory Access) systems, and have used execution-driven simulation to compare the performance of these protocols to that of full hardware coherence and distributed shared memory emulation. We have found that for a large class of applications the performance of NCC-NUMA multiprocessors rivals that of fully hardware-coherent designs, and significantly surpasses the performance realized on more traditional DSM systems.\nTitle:\nEfficient shared memory with minimal hardware support\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.677, mean value for all the writers is 8.648, which makes it 0.828 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper we consider several hardware implementations of the general-purpose atomic primitives fetch and \u03a6, compare and swap, load linked, and store conditional on large-scale shared-memory multiprocessors. These primitives have proven popular on small-scale bets-based machines, but have yet to become widely available on large-scale, distributed shared memory machines. We propose several alternative hardware implementations of these primitives, and then analyze the performance of these implementations for various data sharing patterns. Our results indicate that good overall performance can be obtained by implementing compare and swap in the cache controllers, and by providing an additional instruction to load an exclusive copy of a cache line\nTitle:", "model_inf_time": 1.24}, {"id": "41254", "output": "Complexity of Branch-and-Cut with Lifted-Cover Inequalities for 0-1 Integer Programming", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper addresses the problem of finding cutting planes for multistage stochastic integer programs. We give a general method for generating cutting planes for multistage stochastic integer programs based on combining inequalities that are valid for the individual scenarios. We apply the method to generate cuts for a stochastic version of a dynamic knapsack problem and for stochastic lot-sizing problems. We give computational results, which show that these new inequalities are very effective in a branch-and-cut algorithm.\nTitle:\nCutting Planes for Multistage Stochastic Integer Programs\n\nAbstract:\nFacet-defining inequalities lifted from minimal covers are used as strong cutting planes in algorithms for solving 0-1 integer programming problems. In this paper we extend the result of Balas and Zemel by giving a set of inequalities that determines the lifting coefficients of facet-defining inequalities of the 0-1 knapsack polytope for any ordering of the variables to be lifted. We further generalize the result to obtain facet-defining inequalities for the 0-1 knapsack problem with generalized upper bound constraints.\nTitle:\nLifted cover facets of the 0-1 knapsack polytope with GUB constraints\n\nAbstract:\nIn linear programming based branch-and-bound algorithms, many heuristics have been developed to improve primal solutions, but on the dual side we rely solely on cutting planes to improve dual bounds. We design a dual heuristic which incorporates relaxation algorithms within a branch-and-bound framework to improve dual bounds. We study the effect of solving various relaxations with dual heuristics by conducting a series of computational tests on the multi-dimensional knapsack problem.\nTitle:\nA dual heuristic for mixed integer programming\n\nAbstract:\nWe study the mixed 0-1 knapsack polytope, which is defined by a single knapsack constraint that contains 0-1 and bounded continuous variables. We develop a lifting theory for the continuous variables. In particular, we present a pseudo-polynomial algorithm for the sequential lifting of the continuous variables and we discuss its practical use.\nTitle:\nLifted Inequalities for 0-1 Mixed Integer Programming: Basic Theory and Algorithms\n\nAbstract:\nWe present the implementation of a branch-and-cut algorithm for bound constrained nonconvex quadratic programs. We use a class of inequalities developed in [12] as cutting planes. We present various branching strategies and compare the algorithm to several other methods to demonstrate its effectiveness.\nTitle:\nA branch-and-cut algorithm for nonconvex quadratic programs with box constraints\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.066, mean value for all the writers is 8.648, which makes it 0.357 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\n<P>We investigate several complexity issues related to branch-and-cut algorithms for 0-1 integer programming based on lifted-cover inequalities (LCIs). We show that given a fractional point, determining a violated LCI over all minimal covers is NP-hard. The main result is that there exists a class of 0-1 knapsack instances for which any branch-and-cut algorithm based on LCIs has to evaluate an exponential number of nodes to prove optimality.</P>\nTitle:", "model_inf_time": 1.6}, {"id": "41255", "output": "Filtering-Based Recursive Generalized Least Squares Algorithm for Multivariate Pseudo-Linear Autoregressive Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper focuses on the parameter estimation problem of multivariate output-error autoregressive systems. Based on the data filtering technique and the auxiliary model identification idea, we derive a filtering-based auxiliary model recursive generalized least squares algorithm. The key is to filter the input\u2013output data and to derive two identification models, one of which includes the system parameters and the other contains the noise parameters. Compared with the auxiliary model-based recursive generalized least squares algorithm, the proposed algorithm requires less computational burden and can generate more accurate parameter estimates. Finally, an illustrative example is provided to verify the effectiveness of the proposed algorithm.\nTitle:\nAuxiliary Model-Based Recursive Generalized Least Squares Algorithm for Multivariate Output-Error Autoregressive Systems Using the Data Filtering\n\nAbstract:\nThis paper studies the parameter estimation algorithms of multivariate pseudo-linear autoregressive systems. A decomposition-based recursive generalised least squares algorithm is deduced for estimating the system parameters by decomposing the multivariate pseudo-linear autoregressive system into two subsystems. In order to further improve the parameter accuracy, a decomposition based multi-innovation recursive generalised least squares algorithm is developed by means of the multi-innovation theory. The simulation results confirm that these two algorithms are effective.\nTitle:\nDecomposition-Based Recursive Least Squares Identification Methods For Multivariate Pseudo-Linear Systems Using The Multi-Innovation\n\nAbstract:\nParameter estimation has wide applications in one-dimensional and multidimensional signal processing and filtering. This paper focuses on the parameter estimation problem of multivariate output-error autoregressive systems. Based on the data filtering technique and the auxiliary model identification idea, we derive a filtering based auxiliary model generalized stochastic gradient algorithm. The key is to choose an appropriate filter to filter the input-output data and to study a novel method to get the system model parameters and noise model parameters respectively. By employing the multi-innovation identification theory, a filtering based auxiliary model multi-innovation generalized stochastic gradient algorithm is proposed. Compared with the auxiliary model generalized stochastic gradient algorithm, the proposed algorithms can generate more accurate parameter estimates. Finally, an illustrative example is provided to verify the effectiveness of the proposed algorithms.\nTitle:\nThe data filtering based generalized stochastic gradient parameter estimation algorithms for multivariate output-error autoregressive systems using the auxiliary model.\n\nAbstract:\nThis paper derives a data filtering-based two-stage stochastic gradient algorithm and a data filtering-based multistage recursive least-squares algorithm for input nonlinear output-error autoregressive systems (i.e., Hammerstein systems). The output of the system is expressed as a linear combination of all system parameters based on the key term separation technique. The basic idea of the proposed algorithm is to filter the input---output data and to separate the parameter vector into several vectors and to interactively identify each parameter vector. The data filtering-based two-stage stochastic gradient algorithm has higher convergence rate than the stochastic gradient algorithm. Compared with the recursive generalized least-squares algorithm, the dimensions of the involved covariance matrices in the data filtering-based multistage recursive least-squares algorithm become small, and thus the data filtering-based multistage recursive least-squares algorithm has a higher computational efficiency. The numerical simulation results indicate that the proposed algorithms are effective.\nTitle:\nFiltering-Based Multistage Recursive Identification Algorithm for an Input Nonlinear Output-Error Autoregressive System by Using the Key Term Separation Technique.\n\nAbstract:\nThis paper studies the parameter estimation algorithms of multivariate equation-error autoregressive systems. By using the decomposition technique, the multivariate equation-error autoregressive system is decomposed into two subsystems, and a decomposition-based generalized stochastic gradient algorithm is deduced for estimating the parameters of these two subsystems. In order to further improve the parameter accuracy, a decomposition-based multi-innovation generalized stochastic gradient algorithm is developed by means of the multi-innovation theory. The simulation results confirm that these two algorithms are effective.\nTitle:\nDecomposition-Based Gradient Estimation Algorithms for Multivariate Equation-Error Autoregressive Systems Using the Multi-innovation Theory.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.578, mean value for all the writers is 8.648, which makes it 2.5 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper concerns the parameter identification methods of multivariate pseudo-linear autoregressive systems. A multivariate recursive generalized least squares algorithm is presented as a comparison. By using the data filtering technique, a multivariate pseudo-linear autoregressive system is transformed into a filtered system model and a filtered noise model, and a filtering based multivariate recursive generalized least squares algorithm is developed for estimating the parameters of these two models. The proposed algorithm achieves a higher computational efficiency than the multivariate recursive generalized least squares algorithm, and the simulation results prove that the proposed method is effective.\nTitle:", "model_inf_time": 1.61}, {"id": "41256", "output": "Link Layer Reliability Mechanisms and Their Impact on TCP Performance in 4G Satellite Links", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this article, we evaluate the impact of link layer retransmissions on the performance of TCP in the context of aeronautical communications. We present the architecture of aeronautical networks, which is manly driven by an important channel access delay, and the various retransmission strategies that can be implemented at both link and transport layers. We consider a worst case scenario to illustrate the benefits provided by the ARQ scheme at the link layer in terms of transmission delay. We evaluate the trade-off between allowing a fast data transmission and a low usage of satellite capacity by adjusting link layer parameters.\nTitle:\nOn the Impact of Link Layer Retransmissions on TCP for Aeronautical Communications.\n\nAbstract:\n  Considering real physical (MAC/PHY) traces inside network simulations is a complex task that might lead to complex yet approximated models. However, realistic cross-layer analysis with the upper layer and in particular the transport layer cannot be driven without considering the MAC/PHY level. In this paper, we propose to cope with this problem by introducing a software that translates real physical events from a given trace in order to be used inside a network simulator such as $ns$-2. The main objective is to accurately perform analysis of the impact of link layer reliability schemes (obtained by the use of real physical traces) on transport layer performance. We detail the internal mechanisms and the benefits of this software with a focus on 4G satellite communications scenarios and present the resulting metrics provided by CLIFT to perform consistent cross-layer analysis. \nTitle:\nCLIFT: a Cross-Layer InFormation Tool to perform cross-layer analysis based on real physical traces\n\nAbstract:\nWe propose a novel DTN routing algorithm, called DQN, specifically designed for quasi-deterministic networks with an application to satellite constellations. We demonstrate that our proposal efficiently forwards the information over a satellite network derived from the Orbcomm topology while keeping a low replication overhead. We compare our algorithm against other well-known DTN routing schemes and show that we obtain the lowest replication ratio with a delivery ratio of the same order of magnitude than a reference theoretical optimal routing. We also analyze the impact of terrestrial gateways density and analyze DQN performances in heterogeneous cases. Copyright (C) 2015 John Wiley & Sons, Ltd.\nTitle:\nDtn Routing For Quasi-Deterministic Networks With Application To Leo Constellations\n\nAbstract:\n  We present an NS-2 module, Physical Channel Access (PCA), to simulate different access methods on a link shared with Multi-Frequency Time Division Multiple Access (MF-TDMA). This tech- nique is widely used in various network technologies, such as satellite communication. In this context, different access methods at the gateway induce different queuing delays and available capacities, which strongly impact transport layer performance. Depending on QoS requirements, design of new congestion and flow control mechanisms and/or access methods requires evaluation through simulations.   PCA module emulates the delays that packets will experience using the shared link, based on descriptive parameters of lower layers characteris- tics. Though PCA has been developed with DVB-RCS2 considerations in mind (for which we present a use case), other MF-TDMA-based appli- cations can easily be simulated by adapting input parameters. Moreover, the presented implementation details highlight the main methods that might need modifications to implement more specific functionality or emulate other similar access methods (e.g., OFDMA). \nTitle:\nPhysical Channel Access (PCA): Time and Frequency Access Methods Emulation in NS-2\n\nAbstract:\nTCP(Transmission Control Protocol) uses a loss-based algorithm to estimate whether the network is congested or not. The main difficulty for this algorithm is to distinguish spurious from real network congestion events. Other research studies have proposed to enhance the reliability of this congestion estimation by modifying the internal TCP algorithm. In this paper, we propose an original congestion event algorithm implemented independently of the TCP source code. Basically, we propose a modular architecture to implement a congestion event detection algorithm to cope with the increasing complexity of the TCP code and we use it to understand why some spurious congestion events might not be detected in some complex cases. We show that our proposal is able to increase the reliability of TCP NewReno congestion detection algorithm that might help to the design of detection criterion independent of the TCP code. We find out that solutions based only on RTT (Round-Trip Time) estimation are not accurate enough to cover all existing cases. Furthermore, we evaluate our algorithm with and without network reordering where other inaccuracies, not previously identified, occur.\nTitle:\nTransport congestion events detection (TCED): towards decorrelating congestion detection from TCP\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.446, mean value for all the writers is 8.648, which makes it 0.681 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe study the impact of reliability mechanisms introduced at the link layer on the performance of transport protocols in the context of 4G satellite links. Specifically, we design a software module that performs realistic analysis of the network performance, by utilizing real physical layer traces of a 4G satellite service. Based on these traces, our software module produces equivalent link layer traces, as a function of the chosen link layer reliability mechanism. We further utilize the link layer traces within the ns-2 network simulator to evaluate the impact of link layer schemes on the performance of selected Transmission Control Protocol (TCP) variants. We consider erasure coding, selective-repeat automatic request (ARQ) and hybrid-ARQ link layer mechanisms, and TCP Cubic, Compound, Hybla, New Reno and Westwood. We show that, for all target TCP variants, when the throughput of the transport protocol is close to the channel capacity, using the ARQ mechanism is most beneficial for TCP performance improvement. In conditions where the physical channel error rate is high, hybrid-ARQ results in the best performance for all TCP variants considered, with up to 22% improvements compared to other schemes. Copyright (c) 2014 John Wiley & Sons, Ltd.\nTitle:", "model_inf_time": 1.67}, {"id": "41257", "output": "Online and Offline Defragmentation of Reconfigurable Hardware", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nRecent generations of FPGAs allow run-time partial reconfiguration. To increase the efficacy of reconfigurable computing, multitasking on FPGAs is proposed. One of the challenging problems in multitasking systems is online template placement. In this paper, we describe how existing algorithms work, and propose a new free space manager which is one main part of the placement algorithm. The decision where to place a new module depends on its finishing time mobility. Therefore the proposed algorithm is a combination of scheduling and placement. The simulation results show a better performance against existing methods.\nTitle:\nA Dynamic Scheduling and Placement Algorithm for Reconfigurable Hardware\n\nAbstract:\nFPGAs(Field Programmable Gate Arrays) are often used as reconfigurable device. Because the functions to be implemented in FPGAs are often too big to fit in one device, they are divided into several partitions or configurations which can fit in the device. According to dependencies given in the function a Schedule is calculated. The partitions are successively downloaded in the device in accordance with the schedule until the complete function is computed. Often the time needed for reconfiguration is too high compared to the computation time [1, 11]. This paper presents a novel method for the reduction of the total reconfiguration time of a function by the generation of a minimal number of configurations. We present the framework that we developed for the fast and easy generation of configurations from a function modeled as DFG (dataflow graph).\nTitle:\nTemporal Partitioning and Sequencing of Dataflow Graphs on Reconfigurable Systems\n\nAbstract:\nReconfigurable hardware components such as FPGAs are used more and more in embedded systems, since such components offer a sufficient capacity for a complete SoC(System on a Chip) or even NoC(Network on a Chip). In order to use efficiently the dynamic and partial reconfiguration possibility on such components, one needs a support in the form of operating systems to manage both software and reconfigurable hardware. Online placement is one of these management issues that is investigated in this paper. Here we present a new approach for online placement of modules on reconfigurable devices. Also an optimisation of communication between running modules themselves and outside of the chip is proposed. The experimental results show a considerable decrease in communication and routing costs. Also task scheduling between hardware and software is investigated and a communication infrastructure for online placement has been proposed and implemented.\nTitle:\nOnline Placement For Dynamically Reconfigurable Devices\n\nAbstract:\nWe present a new concept as well as the implementation of an FPGA-based reconfigurable platform, the Erlangen Slot Machine (ESM). The main advantages of this platform are: first, the possibility for each module to access its peripheries independent from its location through a programmable crossbar, and distributed SRAMs among slices. This allows an unrestricted relocation of modules on the device. Second, the intermodule structure allows an unlimited communication among running modules.\nTitle:\nThe Erlangen Slot Machine: A Highly Flexible FPGA-Based Reconfigurable Platform\n\nAbstract:\nThe focus in this paper is put onto extensions to typical FPGA hardware architectures in order to support some operating system functions. In particular we examine the problem of preemptive multitasking and hardware defragmentation on a reconfigurable system based on FPGAs with a dynamically changing set of hardware tasks that can be replaced considering internal states.\nTitle:\nFpga Architecture Extensions For Preemptive Multitasking And Hardware Defragmentation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.686, mean value for all the writers is 8.648, which makes it 0.886 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nModern generations of field-programmable gate arrays (FPGAs) allow for partial reconfiguration. In an online context, where the sequence of modules to be loaded on the FPGA is unknown beforehand, repeated insertion and deletion of modules leads to progressive fragmentation of the available space, making defragmentation an important issue. We address this problem by propose an online and an offline component for the defragmentation of the available space. We consider defragmenting the module layout on a reconfigurable device. This corresponds to solving a two-dimensional strip packing problem. Problems of this type are NP-hard in the strong sense, and previous algorithmic results are rather limited. Based on a graph-theoretic characterization of feasible packings, we develop a method that can solve two-dimensional defragmentation instances of practical size to optimality. Our approach is validated for a set of benchmark instances.\nTitle:", "model_inf_time": 1.3}, {"id": "41258", "output": "Robust Authentication Watermarking for 3D Models Against Compression", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe article addresses the problem of copyright protection for 3D motion-captured data by designing a robust blind watermarking mechanism. The mechanism segments motion capture data and identifies clusters of 3D points per segment. A watermark can be embedded and extracted within these clusters by using a proposed extension of 3D quantization index modulation. The watermarking scheme is blind in nature and the encoded watermarks are shown to be imperceptible, and secure. The resulting hiding capacity has bounds based on cluster size. The watermarks are shown to be robust against attacks such as uniform affine transformations (scaling, rotation, and translation), cropping, reordering, and noise addition. The time complexity for watermark embedding and extraction is estimated as O(n log n) and O(n2 log n), respectively.\nTitle:\nBlind robust watermarking of 3d motion data\n\nAbstract:\nThis paper presents a fragile watermarking technique to tamper proof (Mocap) motion capture data. The technique visualizes 3D Mocap data as a series of clusters of points. Watermarks are embedded using clusters of points, where a bit is encoded in each cluster. The four point encoding mechanism uses a combination of one point encoding and three point encoding schemes. Using these schemes it is possible to distinguish between affine transformations, noise addition and reverse ordering attacks. The bits are encoded and decoded in this scheme using an extension of quantization index modulation. It has been shown that distortions are reduced to achieve imperceptibility of the watermark. The bit encoding schemes give the flexibility to achieve better accuracy in tamper detection. In addition, the paper suggests a probabilistic model, which is a function of the watermark size. Using this model, it has been proved that larger watermark sizes achieve higher accuracy in tamper detection.\nTitle:\nTamper proofing 3d motion data streams\n\nAbstract:\nDigital watermarking for copyright protection of 3-D meshes cannot be directly applied to point clouds, since we need to derive consistent connectivity information, which might change due to attacks, such as noise addition and cropping. Schemes for point clouds operate only on the geometric data and, hence, are generic and applicable to mesh-based representations of 3-D models. For building generic copyright schemes for 3-D models, this paper presents a robust blind watermarking mechanism for 3-D point-sampled geometry. The basic idea is to find a cluster tree from clusters of 3-D points. Using the cluster tree, watermarks can be embedded and extracted by deriving an order among points at global (intracluster) and local levels (intercluster). The multiple bit watermarks are encoded/decoded inside each cluster based on an extension of the cluster structure-based 3-D quantization index modulation. The encoding mechanism makes the technique robust against uniform affine transformations (rotation, scaling, and transformation), reordering, cropping, simplification, and noise addition attacks. The technique when applied to 3-D meshes also achieves robustness against retriangulation and progressive compression techniques. Customization of the bit-encoding scheme achieves high hiding capacity with embedding rates that are equal to 4 b/point, while maintaining the imperceptibility of the watermark with low distortions. The estimated time complexity is O(n log n), where n is the number of 3-D points.\nTitle:\nRobust blind watermarking of point-sampled geometry\n\nAbstract:\nAbstract In this paper, scene-based fingerprinting method for traitor tracing is proposed which is computationally less complex and handles large user group, say 1011 users while requiring few frames to embed the watermark. The proposed method uses QR code as a watermark due to its three main features: (1) inherent templates, (2) noise resiliency, and (3) compact size. The proposed method creates the QR code watermark on-the-fly which is then segmented and embedded parallely inside the scenes of video using the watermarking key. The features of QR code, segmentation, and watermarking key not only help the proposed method in supporting a large user group but also make it computationally fast. Further, synchronization issues may arise due to addition and deletion of scenes. To avoid such scenarios, the proposed method matches the inherent templates present in QR code with the templates present in the segments of the extracted watermark. Experimental results show that the proposed method is computationally efficient and is robust against attacks such as collusion, scene dropping, scene addition, and other common signal processing attacks.\nTitle:\nScene-based fingerprinting method for traitor tracing\n\nAbstract:\nNowadays, the Internet provides a convenient medium for sharing complex 3D models online. However, transmitting 3D progressive meshes over networks may encounter the problem of packets loss that can lead to connectivity inconsistency and distortion of the reconstructed meshes. In this paper, we combine reliable and unreliable channels to reduce both time delay and mesh distortion, and we propose an error-concealment scheme for tolerating packet loss when the meshes are transmitted over unreliable network channels. When the loss of connectivity data occurs, the decoder can predict the geometry data and mesh connectivity information, and construct an approximation of the original mesh. Therefore, the proposed error-concealment scheme can significantly reduce the data size required to be transmitted over reliable channels. The results show that both the computational cost of our error-concealment scheme and the distortion introduced by our scheme are small.\nTitle:\nLoss tolerance scheme for 3D progressive meshes streaming over networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.213, mean value for all the writers is 8.648, which makes it 0.482 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we propose a scheme to enhance the robustness of authentication based watermarking algorithms for 3D models (3D meshes) against compression and decompression. We target different compression techniques such as Compressed Progressive Meshes (CPM) [9], Progressive Forest Split (PFS) compression [11], and Progressive Mesh (PM) [7]. The idea behind the algorithm is to compute those set of vertices that are never used for compression. We have three sets of such vertices that are never used for compression by these techniques. The first set of vertices includes the boundary vertices that are critical for deciding the shape of the 3D model. The second set includes the neighboring vertices to the split vertex. The third set of vertices comprise of edges that do not form simple triangle. After applying the compression algorithms for 3D models, we apply the watermarking algorithm that uses geometric information only. We then watermark the vertices that are obtained from the above three sets. These watermarks are shown to be preserved even after repeated decompression and compression of 3D models. Due to the generic nature of the algorithm, we can apply the above idea to any watermarking algorithm used for other applications such as copyright protection as well.\nTitle:", "model_inf_time": 1.54}, {"id": "41259", "output": "Scaling Effects and Correction of Leaf Area Index Retrieved from Remote Sensing Data", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n\u2022The suitability of the PROSAIL model for LAI estimation is evaluated.\u2022The PROSAIL model is suitable for LAI estimation of maize, potato, and sunflower.\u2022The accuracy of LAI estimation from dual-angle data is higher than that from single-angle data.\u2022Different LUT sizes have negligible influences on the accuracy of LAI estimation.\u2022Different cost functions have significant effects on the accuracy of LAI estimation.\nTitle:\nInversion of the PROSAIL model to estimate leaf area index of maize, potato, and sunflower fields from unmanned aerial vehicle hyperspectral data.\n\nAbstract:\nThis paper makes an attempt to establish a generalized neural network for simultaneously retrieving atmospheric profiles and surface temperature from hyperspectral thermal infrared data. To generate the simulated data covering the whole actual situations, the distributions of surface material, temperature and atmospheric profiles are elaborated carefully. The simulated at-sensor radiances are divided into two sub-ranges, one in atmospheric window and another in water absorption band. The simulated data are transformed in the eigen-domain in both sub-ranges and used as the network inputs. The atmospheric profiles, surface temperature and emissivity are used as the outputs after the eigen-domain transformation. The validation of the trained network indicates that a RMSE of surface temperature around 1.6K, a RMSE of temperature profiles around 2K in troposphere and a RMSE of total water content around 0.3g/cm2 can be obtained. The results from the net can be used as initial guess of the physical retrieval model.\nTitle:\nA generalized neural network for simultaneous retrieval of atmospheric profiles and surface temperature from hyperspectral thermal infrared data\n\nAbstract:\nSoil moisture (SM) is a major concern in the earth science. In previous studies, in the process of retrieval of SM from remote sensing data, surface roughness effects on the retrieval of SM is obvious and a priori knowledge of surface roughness is necessary. In this paper, a simple method to retrieve SM from passive microwave data is proposed. Using the proposed method, surface roughness effects on soil moisture retrieval can be reduced. Result of sensitivity analysis shows it can be a promising method to retrieve SM. Both simulated data and actual data have been used to retrieve SM with the proposed method in this work. The result shows that the SM can be obtained with a RMSE of 1.14% from the simulated data and 1.7% from actual data.\nTitle:\nReduction of surface roughness effects on the soil moisture retrieval from AMSR-E data\n\nAbstract:\nWith the development of quantitative remote sensing, scale issues have attracted more and more the attention of scientists. Research is now suffering from a severe scale discrepancy between data sources and the models used. Consequently, both data interpretation and model application become difficult due to these scale issues. Therefore, effectively scaling remotely sensed information at different scales has already become one of the most important research focuses of remote sensing. The aim of this paper is to demonstrate scale issues from the points of view of analysis, processing and modeling and to provide technical assistance when facing scale issues in remote sensing. The definition of scale and relevant terminologies are given in the first part of this paper. Then, the main causes of scale effects and the scaling effects on measurements, retrieval models and products are reviewed and discussed. Ways to describe the scale threshold and scale domain are briefly discussed. Finally, the general scaling methods, in particular up-scaling methods, are compared and summarized in detail.\nTitle:\nScale issues in remote sensing: a review on analysis, processing and modeling.\n\nAbstract:\nLand surface temperature (LST) is a critical parameter for numerical weather forecasting, drought monitoring, water resources management and global climate change studies. Because of the supercooled temperature, the cirrus cloud can significantly reduce the LST retrieved from thermal infrared data. This paper focused on analyzing and reducing the influence of thin cirrus cloud on the accuracy of LST retrieved using the generalized split-window (GSW) algorithm. A correction method was proposed with the LST retrieval error expressed as linear functions of cirrus optical depth (COD). The slopes of the linear functions were further written as the combination of the difference and mean of two used channels emissivities and cirrus cloud top height (CTH). The results showed that the LST retrieval accuracy could be significantly improved with root mean square error (RMSE) of LST changing from 14.4 K before LST error correction to 1.8 K after LST error correction for COD equivalent to 0.3.\nTitle:\nInfluence of thin cirrus clouds on land surface temperture retrieval using the generalized split-window algorithm from thermal infrared data\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 13.806, mean value for all the writers is 8.648, which makes it 4.401 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper makes an attempt to address the scaling problem of Leaf Area Index (LAI) and to analyze the propagation of scaling effect of LAI. On the basis of the Taylor series expansion and following the general scaling procedure, it is demonstrated that the magnitude of the scaling effect is the product of the degree of the non-linearity of the retrieval model and the spatial heterogeneity of input variables involved in this model. Finally, a scaling correction model is proposed to correct for the scaling effect of LAI. The validation using the simulated data indicates that the proposed scaling correction model of LAI gives promising accuracy if the spatial heterogeneity is well characterized by its wavelet variance. The RMSE and relative error of retrieved LAI induced by the scale effect can be greatly reduced after scaling correction. The scaling propagation analysis of LAI reveals that the scaling effects caused by several non-linear components may compensate for each other, which would enhance our confidence in using LAI product over heterogeneity areas.\nTitle:", "model_inf_time": 1.53}, {"id": "41260", "output": "OID Join Algorithm: Utilizing Path Indexes for Efficient Object-Oriented Query Processing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n We propose query optimization techniquesthat fully utilize the advantages of path indexesin object-oriented database systems. Althoughpath indexes provide an efficient accessto complex objects, little research has beendone on query optimization that fully utilizepath indexes. We first devise a generalizedindex intersection technique, adapted to thestructure of the path index extended fromconventional indexes, for utilizing multiple(path) indexes to access each class in a query.We... \nTitle:\nQuery Optimization Techniques Utilizing Path Indexes in Object-Oriented Database Systems\n\nAbstract:\nThe intermediate result cardinality \u2014 the number of objects satisfying a condition given in a query \u2014 is an important factor for estimating the cost of the query in query optimization. In this paper we show that an object-oriented query often involves partial participation of classes in a relationship. We then present a new technique for estimating the intermediate result cardinality in such a query. Partial participation has not been considered seriously in existing techniques. Since the proposed technique uses detailed statistics to accommodate partial participation, it estimates the intermediate result cardinality more accurately than existing ones. We also show that these statistics are easily obtained by using inherent properties of object-oriented databases.\nTitle:\nA new method for estimating the number of objects satisfying an object-oriented query involving partial participation of classes\n\nAbstract:\nWe propose XIR, a novel method for processing partial match queries on heterogeneous XML documents using information retrieval (IR) techniques. A partial match query is defined as the one having the descendent-or-self axis \u201c//\u201d in its path expression. In its general form, a partial match query has branch predicates forming branching paths. The objective of XIR is to efficiently support this type of queries for large-scale documents of heterogeneous schemas. XIR has its basis on the conventional schema-level methods using relational tables and significantly improves their efficiency using two techniques: an inverted index technique and a novel prefix match join. The former indexes the labels in label paths as keywords in texts, and allows for finding the label paths matching the queries more efficiently than string match used in the conventional methods. The latter supports branching path expressions, and allows for finding the result nodes more efficiently than containment joins used in the conventional methods. We compare the efficiency of XIR with those of XRel and XParent using XML documents crawled from the Internet. The results show that XIR is more efficient than both XRel and XParent by several orders of magnitude for linear path expressions, and by several factors for branching path expressions.\nTitle:\nEfficient evaluation of partial match queries for XML documents using information retrieval techniques\n\nAbstract:\nThis paper presents a tunable two-dimensional class hierarchy indexing technique (2D-CHI) for object-oriented databases. We use a two-dimensional file organization as the index structure. 2D-CHI deals with the problem of clustering objects in a two-dimensional domain space consisting of the key attribute domain and the class identifier domain. In conventional class indexing techniques using one-dimensional index structures such as the B+-tree, the clustering property is exclusively owned by one attribute. These indexing techniques do not efficiently handle the queries that address both the attribute keys and the class identifiers. 2D-CHI enhances query performance by adjusting the degree of clustering between the key value domain and the class identifier domain based on the pre-collected usage pattern. For performance evaluation, we first compare 2D-CHI with the conventional class indexing techniques using an analytic cost model based on the assumption of uniform object distribution, and then, verify the cost model through experiments using the multilevel grid file as the two-dimensional index. We further perform experiments with non-uniform object distributions. Our experiments show that our proposed method does indeed build optimal class index structures regardless of query types and object distributions. We strongly believe that our paper significantly contributes to building a self-tunable database system by supporting automatically tunable index structure.\nTitle:\n2D-CHI: A Tunable Two-Dimensional Class Hierarchy Index for Object-Oriented Databases\n\nAbstract:\nThe visual object query language (VOQL) recently proposed for object databases has been successful in visualizing path expressions and set-related conditions, and providing formal semantics. However, VOQL has several problems. Due to unrealistic assumptions, only set-related conditions can be represented in VOQL. Due to lack of the explicit language construct for the notion of variables, queries are often awkward and less intuitive.\nTitle:\nVOQL*: A Visual Object Query Language With Inductively Defined Formal Semantics\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.192, mean value for all the writers is 8.648, which makes it 1.317 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAlthough various types of path indexes-indexes on path expressions-have been proposed for efficient processing of object-oriented queries, conventional join algorithms do not effectively utilize them. We propose a new join algorithm called OID join algorithm that effectively utilizes (multiple) path indexes in object-oriented databases. When (multiple) path indexes are available for a query, the OID join algorithm may reduce the query evaluation cost significantly by taking full advantage of the path indexes. We present a cost analysis for the OID join algorithm and compare it with those of conventional ones\nTitle:", "model_inf_time": 1.57}, {"id": "41261", "output": "Temporal Rule Induction for Clinical Outcomes Analysis", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present the complete classification of recursive relationships and the criteria that contribute to the structural validity of modeling recursive relationships within the entity-relationship (ER) diagram. Unlike typical other analyses that use only maximum cardinality constraints, we have used both maximum and minimum cardinality constraints in defining the properties and their structural validity criteria. We used the notions of role uniqueness, path connectivity, and cardinality constraints to derive a complete and comprehensive set of decision rules. Five rules and three corollaries were established to determine structural validity of recursive relationships. The contribution of this paper is to present a complete taxonomy of recursive relationships with their properties as well as the decision rules for their structural validity. These decision rules can be readily applied to real world data models regardless of their complexity. The rules can easily be incorporated into the database modeling and designing process, or extended into case tool implementations.\nTitle:\nA Taxonomy of Recursive Relationships and Their Structural Validity in ER Modeling\n\nAbstract:\nWe explore the criteria that contribute to the structural validity of modeling structures within the entity-relationship (ER) diagram. Our approach examines cardinality constraints in conjunction with the degree of the relationship to address constraint consistency, state compliance, and role uniqueness issues to derive a complete and comprehensive set of decision rules. Unlike typical other analyses that use only maximum cardinality constraints, we have used both maximum and minimum cardinality constraints in defining the properties and their structural validity criteria yielding a complete analysis of the structural validity of recursive, binary, and ternary relationship types. Our study evaluates these relationships as part of the overall diagram and our rules address these relationships as they coexist in a path structure within the model. The contribution of this paper is to provide a comprehensive set of decision rules to determine the structural validity of any ERD containing recursive, binary, and ternary relationships. These decision rules can be readily applied to real world data models regardless of their complexity. The rules can easily be incorporated into the database modeling and designing process, or extended into case tool implementations.\nTitle:\nAn analysis of structural validity in entity-relationship modeling\n\nAbstract:\nWe review a set of rules identifying which combinations of ternary and binary relationships can be combined simultaneously in semantically related situations. We investigate the effect of these rules on decomposing ternary relationships to simpler, multiple binary relationships. We also discuss the relevance of these decomposition strategies to ER modeling. We show that if at least one 1:1 or 1:M binary constraint can be identified within the construct of the ternary itself, then any ternary relationship can be decomposed to a binary format. From this methodology we construct a heuristic-the Constrained Ternary Decomposition (CTD) rule\nTitle:\nTernary relationship decomposition strategies based on binary imposition rules\n\nAbstract:\nOur paper seeks to provide an analysis of ternary relationship logic with an objective of identifying whether they can be decomposed given only the constructs and constraints provided during conceptual entity relationship (ER) modeling.\nTitle:\nBinary Representations of Ternary Relationships in ER Conceptual Modelling\n\nAbstract:\nThis paper describes our ongoing research on data exploration and knowledge discovery in a patient wellness tracking (PWT) information system developed for a nurse-managed community health center. The center employs an innovative and transdisciplinary care model that fully integrates behavioral and various wellness services into primary care to form a team approach. We have developed the PWT system that integrates clinical data collected in an electronic medical record (EMR) system with the data generated by a spectrum of healthy living programs and wellness services. While data is being collected rapidly in large volumes, it is imperative to develop effective tools in helping clinicians explore data and discover knowledge. In this paper, we present (1) an exploratory data browser based on information content in information theory for searching granularity patient data, and (2) a knowledge discovery component based on probabilistic graphical models for diagnosis, prognosis, and revealing clinical cause-effect interactions.\nTitle:\nData exploration and knowledge discovery in a patient wellness tracking (PWT) system at a nurse-managed health services center\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.541, mean value for all the writers is 8.648, which makes it 1.615 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nClinical outcomes analysis normally covers a particular time period. The sample under study is constantly changing as patients are censored, leave the study or die. In this paper, we present a novel data mining approach to mine temporal rules that reflect characteristics of outcomes analysis. We apply our temporal rule induction algorithm to a set of cancer patients, clinical records that were prospectively collected for 20 years. We analyse clinical data not only based on the static event, such as local recurrence for survival analysis, but also based on the temporal event with censored data for each time unit. The rules extracted from our temporal rule induction algorithm are compared to results from statistical analysis. The importance of this paper is that this novel temporal rule induction algorithm provides valuable insights for clinical data assessment and complements traditional statistical analysis.\nTitle:", "model_inf_time": 1.12}, {"id": "41262", "output": "Voice User Identification for Personalized Car Applications", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we present a real automatic meteorological information system that, not only provides friendly voice access to real-time data coming from automatic sensors, but also establishes an automatic warning service on the weather. It aims to extend the availability, personalization and friendliness of the meteorological information by means of a reusable easy-to-use friendly oral natural language interface. This interface takes advantage of the improvements in speech processing, dialogue handling and the great growth of mobile telephony. After the description of the functionalities of the system and its architecture, we present in detail the features of the dialogue manager. The main goals we have considered are: to provide the right information, to design a friendly interface, and to help the user never getting lost during the dialogue.\nTitle:\nDialogue management in an automatic meteorological information system\n\nAbstract:\nIn this paper we present a real automatic meteorological information system that, not only provides friendly voice access to real-time data coming from automatic sensors, but also establishes an automatic warning service on the weather. It aims to extend the availability, personalization and friendliness of the meteorological information by means of a reusable easy-to-use friendly oral natural language interface. This interface takes advantage of the improvements in speech processing, dialogue handling and the great growth of mobile telephony. After the description of the functionalities of the system and its architecture, we present in detail the features of the dialogue manager. The main goals we have considered are: to provide the right information and to design a friendly interface.\nTitle:\nArchitecture and dialogue design for a voice operated information system\n\nAbstract:\nIn this article, we present the evaluation results for the task of speaker diarization of broadcast news, which was part of the Albayzin 2010 evaluation campaign of language and speech technologies. The evaluation data consists of a subset of the Catalan broadcast news database recorded from the 3/24 TV channel. The description of five submitted systems from five different research labs is given, marking the common as well as the distinctive system features. The diarization performance is analyzed in the context of the diarization error rate, the number of detected speakers and also the acoustic background conditions. An effort is also made to put the achieved results in relation to the particular system design features.\nTitle:\nSpeaker Diarization of Broadcast News in Albayzin 2010 Evaluation Campaign.\n\nAbstract:\nIn this paper, the authors describe the UPC speaker identification system submitted to the CLEAR'07 (Classification of Events, Activities and Relationships) evaluation. Firstly, the UPC single distant microphone identification system is described. Then the use of combined microphone inputs in two different approaches is also considered. The first approach combines signals from several microphones to obtain a single enhanced signal by means a delay and sum algorithm. The second one fuses the decision of several single distant microphone systems. In our experiments, the latter approach has provided the best results for this task.\nTitle:\nRobust Speaker Identification for Meetings: UPC CLEAR'07 Meeting Room Evaluation System\n\nAbstract:\nDetecting the location and identity of users is a first step in creating context-aware applications for technologically-endowed\n environments. We propose a system that makes use of motion detection, person tracking, face identification, feature-based\n identification, audio-based localization, and audio-based identification modules, fusing information with particle filters\n to obtain robust localization and identification. The data streams are processed with the help of the generic client-server\n middleware SmartFlow, resulting in a flexible architecture that runs across different platforms.\nTitle:\nMultimodal identification and localization of users in a smart environment\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.204, mean value for all the writers is 8.648, which makes it 1.328 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCar applications demand more and more the use of speech technologies. Drivers must concentrate on controlling the car and the non-use of hands makes the voice a valuable tool. Here we analyze the possibility of identifying the user of a car through her/his voice in order to develop some useful applications, and establish preferences, some of them related to music. The identification will be done in parallel to speech commands which will be given to devices in the car in the future. Once the user is identified, the system loads a personal profile. It includes music preferences which can be downloaded from the Internet databases using e.g. MPEG-7 .\nTitle:", "model_inf_time": 1.11}, {"id": "41263", "output": "A Joint System for 2D-Face and 3D-Head Tracking in Smart Rooms", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe paper introduces a novel detection and tracking system that pro- vides both frame-view and world-coordinate human location infor- mation, based on video from multiple synchronized and calibrated cameras with overlapping fields of view. The system is developed and evaluated for the specific scenario of a seminar lecturer present- ing in front of an audience inside a \"smart room\", its aim being to track the lecturer's head centroid in the three-dimensional (3D) space and also yield two-dimensional (2D) face information in the avail- able camera views. The proposed approach is primarily based on a statistical appearance model of human faces by means of well- known AdaBoost-like face detectors, extended to address the head pose variation observed in the smart room scenario of interest. The appearance module is complemented by two novel components and assisted by a simple tracking drift detection mechanism. The first component of interest is the initialization module, which employs a spatio-temporal dynamic programming approach with appropriate penalty functions to obtain optimal 3D location hypotheses. The sec- ond is an adaptive subspace learning based 2D tracking scheme with a novel forgetting mechanism, introduced as a means to reduce track- ing drift and increase robustness to illumination and head pose vari- ation. System performance is benchmarked on an extensive database of realistic human interaction in the lecture smart room scenario, col- lected as part of the European integrated project \"CHIL\". The system consistently achieves excellent tracking precision, with a 3D mean tracking error of less than 16 cm, and is demonstrated to outperform four alternative tracking schemes. Furthermore, the proposed system performs relatively well in detecting frontal and near-frontal faces in the available frame views.\nTitle:\nJoint face and head tracking inside multi-camera smart rooms\n\nAbstract:\nWe present a robust vision system for single person tracking inside a smart room using multiple synchronized, calibrated, stationary cameras. The system consists of two main components, namely initialization and tracking, assisted by an additional component that detects tracking drift. The main novelty lies in the adaptive tracking mechanism that is based on subspace learning of the tracked person appearance in selected two-dimensional camera views. The sub-space is learned on the fly, during tracking, but in contrast to the traditional literature approach, an additional \"forgetting\" mechanism is introduced, as a means to reduce drifting. The proposed algorithm replaces mean-shift tracking, previously employed in our work. By combining the proposed technique with a robust initialization component that is based on face detection and spatio-temporal dynamic programming, the resulting vision system significantly outperforms previously reported systems for the task of tracking the seminar presenter in data collected as part of the CHIL project\nTitle:\nPerson Tracking in Smart Rooms using Dynamic Programming and Adaptive Subspace Learning\n\nAbstract:\nVisual detection and tracking of humans in complex scenes is a challenging problem with a wide range of applications, for example surveillance and human-computer interaction. In many such applications, time-synchronous views from multiple calibrated cameras are available, and both frame-view and space-level human location information is desired. In such scenarios, efficiently combining the strengths of face detection and person tracking is a viable approach that can provide both levels of information required and improve robustness. In this paper, we propose a novel vision system that detects and tracks human faces automatically, using input from multiple calibrated cameras. The method uses an Adaboost algorithm variant combined with mean shift tracking applied on single camera views for face detection and tracking, and fuses the results on multiple camera views to check for consistency and obtain the three-dimensional head estimate. We apply the proposed system to a lecture scenario in a smart room, on a corpus collected as part of the CHIL European Union integrated project. We report results on both frame-level face detection and three-dimensional head tracking. For the latter, the proposed algorithm achieves similar results with the IBM \u201cPeopleVision\u201d system.\nTitle:\nA joint system for person tracking and face detection\n\nAbstract:\nRobust face detection presents a difficult problem in real interaction scenarios, that, in order to achieve, most often requires employing additional sources of information. In this paper, we consider two such sources: temporal information, available in the form of video sequences, and spatial information, available from multiple calibrated cameras with synchronous, overlapping fields of view of the 3D scene of interest. These two sources are exploited jointly, using a novel dynamic programming approach, for a lecture scenario inside appropriately equipped smart rooms, aiming at robust face detection of the lecturer within the available 2D camera views. Experimental results, reported on the CHIL project database, demonstrate that the proposed approach outperforms purely frame-based face detection\nTitle:\nRobust Multi-View Multi-Camera Face Detection inside Smart Rooms Using Spatio-Temporal Dynamic Programming\n\nAbstract:\nIn this paper, we describe the IBM system submitted to the NIST Rich Transcription Spring 2006 (RT06s) evaluation campaign for automatic speech activity detection (SAD). This SAD system has been developed and evaluated on CHIL lecture meeting data using far-field microphone sensors, namely a single distant microphone (SDM) configuration and a multiple distant microphone (MDM) condition. The IBM SAD system employs a three-class statistical classifier, trained on features that augment traditional signal energy ones with features that are based on acoustic phonetic likelihoods. The latter are obtained using a large speaker-independent acoustic model trained on meeting data. In the detection stage, after feature extraction and classification, the resulting sequence of classified states is further collapsed into segments belonging to only two classes, speech or silence, following two levels of smoothing. In the MDM condition, the process is repeated for every available microphone channel, and the outputs are combined based on a simple majority voting rule, biased towards speech. The system performed well at the RT06s evaluation campaign, resulting to 8.62% and 5.01% \u201cspeaker diarization error\u201d in the SDM and MDM conditions respectively.\nTitle:\nThe IBM RT06s evaluation system for speech activity detection in CHIL seminars\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.821, mean value for all the writers is 8.648, which makes it 1.001 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe present the IBM systems submitted and evaluated within the CLEAR'06 evaluation campaign for the tasks of single person visual 3D tracking (localization) and 2D face tracking on CHIL seminar data. The two systems are significantly inter-connected to justify their presentation within a single paper as a joint vision system for single person 2D-face and 3D-head tracking, suitable for smart room environments with multiple synchronized, calibrated, stationary cameras. Indeed, in the developed system, face detection plays a pivotal role in 3D person tracking, being employed both in system initialization as well as in detecting possible tracking drift. Similarly, 3D person tracking determines the 2D frame regions where a face detector is subsequently applied. The joint system consists of a number of components that employ detection and tracking algorithms, some of which operate on input from all four corner cameras of the CHIL smart rooms, while others select and utilize two out of the four available cameras. Main system highlights constitute the use of AdaBoost-like multi-pose face detectors, a spatio-temporal dynamic programming algorithm to initialize 3D location hypotheses, and an adaptive subspace learning based tracking scheme with a forgetting mechanism as a means to reduce tracking drift. The system is benchmarked on the CLEAR'06 CHIL seminar database, consisting of 26 lecture segments recorded inside the smart rooms of the UKA and ITC CHIL partners. Its resulting 3D single-person tracking performance is 86% accuracy with a precision of 88 mm, whereas the achieved face tracking score is 54% correct with 37% wrong detections and 19% misses. In terms of speed, an inefficient system implementation runs at about 2 fps on a P4 2.8 GHz desktop.\nTitle:", "model_inf_time": 2.01}, {"id": "41264", "output": "iCO(2): A Game for Illuminating Eco-Driving Behaviors", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAgriculture, while of uttermost importance for society, may also have a strong negative impact on the environment. Hence we propose a game that offers players the opportunity to experience the effects of different styles of agriculture on the environment. The game was built with the purpose of promoting the awareness of agriculture issues, such as, (1) the impact of fertilizers in sources of fresh water, (2) the problems related to deforestation and impact on the weather, and (3) the importance of balancing environmental and economic perspectives in order to produce food of good quality with low impact on the environment\u2014and at the same time keep the activity sustainable. To make players care about these issues, we added a direct impact of the players' actions on a population of non-player characters, the villagers, whose simple autonomous behaviors resemble that of living entities. The game was implemented in the multi-user online three-dimensional (3-D) virtual world platform Open-Simulator, which supports an immersive user experience and high accessibility. An experiment was performed and showed that the game improved players' knowledge about agriculture and their awareness of the environmental impact of agriculture.\nTitle:\nAgriVillage: A Game to Foster Awareness of the Environmental Impact of Agriculture\n\nAbstract:\nAgriculture has a strong impact in the environment; it has played key role in the development of food security and food safety. In this paper, we are proposing a 3D multilingual Internet game, developed by using the on-line virtual world platform, that lets the player experience the potential effects of agriculture in the environment. The main idea is to foster awareness of agriculture's environmental issues, by not only making the player learn the impact of fertilizers and deforestation that affect the sources of water and weather, respectively, but also enhance the importance of food quality. To make players care for these issues, the game creates a direct impact by using the happiness of people that is represent by the villagers. To reduce the language barrier when sharing knowledge across the countries is needed, the game also supports multi-language to make it more understandable to the player. This paper describes details of the game design, the system architecture and the experiment. The experiment conducted with the game showed promising results.\nTitle:\nAgriVillage: 3D multi-language internet game for fostering agriculture environmental awareness\n\nAbstract:\nAgents cannot be decoupled from their environment. An agent perceives and acts in a world and the model of the world influences how the agent makes decisions. Most systems with virtual embodied agents simulate the environment within a specific realization engine such as the graphics engine. As a consequence, these agents are bound to a particular kind of environment which compromises their reusability across different applications. We propose the ION Framework, a framework for simulating virtual environments which separates the simulation environment from the realization engine. In doing so, it facilitates the integration and reuse of the several components of the system. The ION Framework was used to create several 3D virtual worlds populated with autonomous embodied agents that were tested with hundreds of users.\nTitle:\nION Framework --- A Simulation Environment for Worlds with Virtual Agents\n\nAbstract:\nIn this paper we describe a method to create strategically and visually rich game map environments for turn-based strategy multiplayer browser games. Our method creates dynamic maps which expand according to the players' subscriptions pattern and adapt to the players' choices. This method mitigates current level designer limitations and contributes to the solution of balancing problems in turn-based massively multiplayer browser games.\nTitle:\nSelf-adapting dynamically generated maps for turn-based strategic multiplayer browser games\n\nAbstract:\nThe idea of robotic companions capable of establishing meaningful relationships with humans remains far from being accomplished. To achieve this, robots must interact with people in natural ways, employing social mechanisms that people use while interacting with each other. One such mechanism is empathy, often seen as the basis of social cooperation and prosocial behaviour. We argue that artificial companions capable of behaving in an empathic manner, which involves the capacity to recognise another's affect and respond appropriately, are more successful at establishing and maintaining a positive relationship with users. This paper presents a study where an autonomous robot with empathic capabilities acts as a social companion to two players in a chess game. The robot reacts to the moves played on the chessboard by displaying several facial expressions and verbal utterances, showing empathic behaviours towards one player and behaving neutrally towards the other. Quantitative and qualitative results of 31 participants indicate that users towards whom the robot behaved empathically perceived the robot as friendlier, which supports our hypothesis that empathy plays a key role in human-robot interaction.\nTitle:\nThe influence of empathy in human-robot relations\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.702, mean value for all the writers is 8.648, which makes it 0.899 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe iCO(2) game collects human driving behavior data at scale and better illuminates user behaviors, including which eco-driving practices drivers apply. The game uses the Distributed Virtual Environments (DiVE) networking framework for multiuser 3D virtual world applications. Tests of DiVE's \"area of interest\" method showed promising results on two important metrics: bandwidth and frames per second. A small marketing campaign also collected data from users interacting with the iPad version of iCO(2) in the wild.\nTitle:", "model_inf_time": 1.58}, {"id": "41265", "output": "Axiomatic Analysis of Task-Oriented Negotiation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper introduces an axiomatic model for bargaining analysis. We describe a bargaining situation in propositional logic and represent bargainers' preferences in total pre-orders. Based on the concept of minimal simultaneous concessions, we propose a solution to n-person bargaining problems and prove that the solution is uniquely characterized by five logical axioms: Consistency, Comprehensiveness, Collective rationality, Disagreement, and Contraction independence. This framework provides a naive solution to multi-person, multi-issue bargaining problems in discrete domains. Although the solution is purely qualitative, it can also be applied to continuous bargaining problems through a procedure of discretization, in which case the solution coincides with the Kalai-Smorodinsky solution.\nTitle:\nA logic-based axiomatic model of bargaining\n\nAbstract:\nThis paper presents a logical extension of Nash's Cooperative Bargaining Theory. We introduce a concept of entrenchment measurement, which maps propositions to real numbers, as a vehicle to represent agent's belief states and attitudes towards bargaining situations. We show that Nash's bargaining solution can be restated in terms of bargainers belief states. Negotiable items, bargaining outcomes and conflicting arguments can then be explicitly expressed in propositional logic meanwhile Nash's numerical solution to bargaining problem is still applicable.\nTitle:\nA logical model of Nash bargaining solution\n\nAbstract:\nThis paper presents a logical axiomatization of bargaining so- lutions. A bargaining situation is described in propositional logic and the bargainers' preferences are quantified in terms of the logical structure of the bargaining situation. A solution to the n-person bargaining problems is proposed based on the maxmin rule over the degrees of bargainers' satisfaction. We show that the solution is uniquely characterized by four nat- ural and intuitive axioms as well as three other fundamental assumptions. All the axioms and assumptions are represented in logical statements and most of them have a game-theoretic counterpart. The framework would help us to identify the log- ical and numerical reasoning behind bargaining processes. This paper aims to develop a logical theory of bargain- ing in cooperative model. We shall represent bargainers' negotiation items in propositional logic and quantify the bargainers' preferences over their negotiation items in ac- cordance with the logical structure of the items. We pro- pose a solution to the n-person bargaining problems based\nTitle:\nReasoning about Bargaining Situations\n\nAbstract:\nThis paper proposes a logical framework for bargaining with integrity constraints (IC) in multi-agent and multi-issue bargaining environments. We construct a simultaneous concession solution for bargaining games under IC, and show that the solution is uniquely characterised by a set of logical properties. In addition, we prove that the solution also satisfies the most fundamental game theoretic properties such as symmetry and Pareto optimality. Finally, we discuss the relationship between merging operators and bargaining solutions under integrity constraints.\nTitle:\nA Logical Framework of Bargaining with Integrity Constraints\n\nAbstract:\nThis paper presents an axiomatic approach to negotiation protocol analysis. We consider a negotiation procedure as multiple stages of mutual belief revision. A set of postulates in AGM-style of belief revision axe proposed to specify rational behavior of negotiation. An explicit construction of negotiation function is given in which negotiation process is viewed as the interaction of two iterated revision operations. As a result the proposed axiomatic system is proved to be consistent. Finally, we examine our approach with an instantiation of Rosenschein and Zlotkin's Monotonic Concession Protocol of Negotiation.\nTitle:\nAxiomatic Analysis of Negotiation Protocols\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 6.867, mean value for all the writers is 8.648, which makes it 1.52 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents an axiomatic analysis of negotiation problems within task-oriented domains (TOD). We start by applying three classical bargaining solutions of Nash, Kalai-Smorodinsky and Egalitarian to the domains of problems with a preprocess of randomization on possible agreements. We find out that these three solutions coincide within any TOD and can be characterized by the same set of axioms, which specify a solution of task oriented negotiation as an outcome of dual-process of maximizing cost reduction and minimizing workload imbalance. This axiomatic characterization is then used to produce an approximate solution to the domain of problems without randomization on possible agreements.\nTitle:", "model_inf_time": 1.17}, {"id": "41266", "output": "Power-Efficient Bluetooth Receiver Based on Noncoherent Sequence Detection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nTwo new Bluetooth receiver designs, the so-called modified limiter-discriminator detector with integrate and dump filtering (LDI) and noncoherent sequence detection (NSD), have recently been proposed in the literature. These receivers have been shown to improve the Bluetooth system performance in terms of physical-layer measures like bit-error rate (BER) and packet-error rate (PER) compared to the conventional LDI receiver. In this paper, we present a more comprehensive performance comparison for these receivers, which includes consideration of the spatial distribution of Bluetooth devices, channel propagation, data traffic, scheduling, automatic repeat request (ARQ), and baseband packet selection. The performance is measured in terms of practically relevant metrics such as end-to-end delay and throughput. Our numerical and simulation results verify that incorporating the new receivers into Bluetooth devices can considerably improve data-rate and quality-of-service performance compared to employing state-of-the-art LDI receivers.\nTitle:\nPerformance Comparison of Bluetooth LDI, Modified LDI, and NSD Receivers.\n\nAbstract:\nRecently, a new noncoherent sequence detection receiver for Bluetooth systems has been developed. The receiver is based on Rimoldi's decomposition of the Bluetooth transmit signal and its power efficiency was shown by software simulations. In this paper, a hardware implementation of the decoder for this receiver is presented. In particular, we describe a low-complexity Viterbi decoder that performs noncoherent sequence detection in a two-state trellis using per-survivor processing. The prototype decoder is implemented in a field programmable gate array (FPGA). The bit-error-rate performance of the implemented decoder is compared with that of the reference software simulations\nTitle:\nPer-survivor processing Viterbi decoder for Bluetooth applications\n\nAbstract:\nIn this paper, we present a receiver design for Bluetooth transmission based on Laurent&#39;s decomposition of the Bluetooth transmit signal. The main features of this receiver are 1) its low complexity compared to alternative solutions; 2) its excellent performance close to the theoretical limit; and 3) its high robustness against frequency offsets, phase noise, and modulation index variations, which...\nTitle:\nBluetooth Receiver Design Based on Laurent's Decomposition.\n\nAbstract:\nBluetooth is a popular short-range low-power radio standard for wireless personal area networks. Bluetooth transmitters employ Gaussian frequency shift keying (GFSK) and simple block codes for error correction. Recently, two new receiver designs for Bluetooth devices, which are the so-called modified limiter-discriminator detector with integrate-and-dump filtering (LDI) and noncoherent sequence de...\nTitle:\nPerformance Evaluation of Bluetooth Systems With LDI, Modified LDI, and NSD Receivers\n\nAbstract:\nCoded continuous phase modulation based on a feedback-free modulator with noncoherent detection is discussed. Low-complexity receiver processing is achieved by using only two or three linear filters for demodulation and applying noncoherent sequence estimation with reduced-state Viterbi decoding and simple branch metric calculation. Overall, the proposed noncoherent receiver provides significant a...\nTitle:\nCoded continuous phase modulation with low-complexity noncoherent reception\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.911, mean value for all the writers is 8.648, which makes it 0.224 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe design of power efficient receivers for Bluetooth systems is a challenging task due to stringent complexity constraints. In this paper, we tackle this problem and present a receiver design consisting of a single filter and a subsequent noncoherent sequence detector. This receiver outperforms the conventional discriminator detector by more than 4 dB for typical Bluetooth channels. Thereby, the proposed noncoherent sequence detection (NSD) algorithm is both favorably low complex as it operates on a two-state trellis and highly robust against channel phase variations caused by low-cost local oscillators. The particular filter design accomplishes effective out-of-band interference suppression. Different from previous work on sequence detector receivers published in the literature, we take possible variations of the Bluetooth modulation parameters into account, and we also devise efficient methods for combined NSD and forward error correction decoding. Hence, the presented receiver design is an attractive solution for practical Bluetooth devices.\nTitle:", "model_inf_time": 1.27}, {"id": "41267", "output": "Product-Form Analysis of Batch Queues with Special Arrivals and Partial Batch Discarding", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn queueing networks with blocking, stations wishing to transmit customers to a full queue are blocked and need to take alternative action on completing a service. In general, product-forms, i.e. separable solutions for such a network's equilibrium state probabilities, do not exist but some product-forms have been obtained over the years in special cases, using a variety of techniques. We show that the Reversed Compound Agent Theorem (RCAT) can obtain these diverse results in a uniform way by its direct application, so unifying product-forms in networks with and without blocking. New product-forms are also constructed for a type of blocking we call `skipping', where a blocked station sends its output-customers to the queue after the one causing the blocking in that customer's path. Finally, we investigate a novel congestion management scheme for networks of finite-capacity queues in which a station with a full queue transmits signals that delete customers from upstream queues in order to reduce incoming traffic.\nTitle:\nA unifying approach to product-forms in networks with finite capacity constraints\n\nAbstract:\nTwo new methodological results are obtained: first, a way to perturb a network into one with a product-form solution for its equilibrium state probabilities, and secondly, a new compositional approach to deriving corresponding response time distributions. The Reversed Compound Agent Theorem (RCAT) is used to construct suitable perturbations in near-product-form networks that render them separable by satisfying the conditions of this theorem. Response time calculations in stochastic networks are usually developed in terms of sample path analyses beginning in an equilibrium state. We consider the joint probability distribution of the sojourn times of a tagged task at each node of a path in a network and observe that this is the same in both the forward and reversed processes. Therefore if the reversed process is known, each node-sojourn time can be taken from either process. In particular, the reversed process can be used for the first node in a path and the forward process for the other nodes in a recursive analysis. This approach derives, quickly and systematically, existing results for response time probability densities in tandem, open and closed tree-like, and overtake-free Markovian networks of queues. An example shows that the technique is far more widely applicable, constructing a perturbed network with product-form, a new result in its own right, and then finding a very simple expression for its response time probability density function.\nTitle:\nResponse time distributions and network perturbation into product-form\n\nAbstract:\nThe reversed compound agent theorem (RCAT) is a compositional result that uses Markovian process algebra (MPA) to derive the reversed process of certain interactions between two continuous time Markov chains at equilibrium. From this reversed process, together with the given, forward process, the joint state probabilities can be expressed as a product-form, although no general algorithm has previously been given. This paper first generalises RCAT to multiple (more than two) cooperating agents, which removes the need for multiple applications and inductive proofs in cooperations of an arbitrary number of processes. A new result shows a simple stochastic equivalence between cooperating, synchronised processes and corresponding parallel, asynchronous processes. This greatly simplifies the proof of the new, multi-agent theorem, which includes a statement of the desired product-form solution itself as a product of given state probabilities in the parallel components. The reversed process and product-form thus derived rely on a solution to certain rate equations and it is shown, for the first time, that a unique solution exists under mild conditions--certainly for queueing networks and G-networks.\nTitle:\nSeparable equilibrium state probabilities via time reversal in Markovian process algebra\n\nAbstract:\nWe obtain the queue length probability distribution at equilibrium for a multi-server, single queue with generalised exponential (GE) service time distribution and a Markov modulated compound Poisson arrival process (MMCPP) \u2013 i.e., a Poisson point process with bulk arrivals having geometrically distributed batch size whose parameters are modulated by a Markovian arrival phase process. This arrival process has been considered appropriate in ATM networks and the GE service times provide greater flexibility than the more conventionally assumed exponential distribution. The result is exact and is derived, for both infinite and finite capacity queues, using the method of spectral expansion applied to the two dimensional (queue length by phase of the arrival process) Markov process that describes the dynamics of the system. The Laplace transform of the interdeparture time probability density function is then obtained. The analysis therefore could provide the basis of a building block for modelling networks of switching nodes in terms of their internal arrival processes, which may be both correlated and bursty.\nTitle:\nThe MMCPP/GE/c Queue\n\nAbstract:\nProduct-form models facilitate the efficient analysis of large stochastic models and have been sought after for some three decades. Apart from the dominating work on queueing networks, some product-forms were found for stochastic Petri nets (SPNs) that allow fork-join constructs and for queueing networks extended to include special customers called signals, viz. G-networks. We appeal to the Reversed Compound Agent Theorem (RCAT) to prove new product-form solutions for SPNs in which there are special transitions, the firings of which act in a similar way to signals in G-networks, but which may be generated by synchronised firings (or service completions) and may affect several places simultaneously. We show that SPNs with signals are strict generalisations of G-networks with negative customers, triggers and catastrophes, and illustrate with copious examples.\nTitle:\nAnalysis of stochastic Petri nets with signals\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.4, mean value for all the writers is 8.648, which makes it 0.212 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIt is shown that a Markovian queue, with bulk arrivals and departures having any probability mass functions for their batch sizes, has geometrically distributed queue length at equilibrium (when this exists) provided there is an additional special bulk arrival stream, with particular rate and batch size distribution, when the server is idle. It is shown that the time-averaged input rate of the special arrivals tends to zero as the queue becomes saturated, and a heavy-traffic limit for the queue without special arrivals is derived by martingale methods. This is shown to give the same asymptotic queue length probabilities as the geometric model. The product form is then extended to tandem networks of batch queues using the reversed compound agent theorem (RCAT). In order to obtain the product form in this case, it is required that, in addition to special arrival streams, so-called 'partial batches' are discarded immediately from the network when there are not enough customers in the queue to fill an entire departing batch. Somewhat surprisingly it turns out that, in heavy traffic, the product-form network does not always agree with the regulated Brownian motion (RBM) diffusion limit for the standard network without special arrivals and where partial batches are not discarded, but forwarded to the next node. Indeed, we show that the two models agree in heavy traffic if and only if the skew-symmetry condition for the RBM to have a product form is satisfied. When the condition does hold, our theoretical and numerical results thus validate the use of the product-form batch networks as moderate-traffic approximations to the analogous standard queueing network model without special arrivals and where partial batches may be forwarded to the next node instead of being lost. In the case that the condition does not hold, we obtain a new product-form stationary distribution for the associated non-RBM diffusion limit.\nTitle:", "model_inf_time": 1.86}, {"id": "41268", "output": "Semantic Web Services for Intelligent Multimedia Adaptation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAbstract Intelligent, server-side adaptation of multimedia resources is becoming increasingly important and challenging for two reasons. First, the market continuously brings up new mobile end-user devices to which the content has to be adapted as these devices support different display formats and operate on various types of networks. On the other hand, with the help of metadata annotations which are now available in the MPEG-7 and MPEG-21 standard, advanced forms of resource adaptations on the content level become possible. As none of the existing multimedia transformation tools and libraries can support all these different forms of basic and advanced adaptation operations, an intelligent multimedia adaptation server has to integrate such external tools and algorithms and perform an adequate sequence of adaptation operations on the original resource before sending it to the client. In this paper we present the results of the ISO/IEC MPEG Core Experiment on using Semantic Web Services technology as a tool for declaratively describing the semantics of adaptation services and constructing multi-step adaptation sequences in an open and extensible multimedia adaptation framework. We show how the semantics of adaptation operations can be captured in the form of input, output, precondition, and effects, how the problem of finding adequate adaptation sequences can be viewed as an Artificial Intelligence planning problem, and finally, how the existing MPEG standards are technically integrated into the service descriptions and how they serve as the shared ontology of the domain. Our approach both introduces declarative, knowledge-based technology into the involved multimedia communities and on the other hand broadens the application scope of Semantic Web Service technology in the area of general semantic service descriptions and automated program construction.\nTitle:\nKnowledge-based multimedia adaptation for ubiquitous multimedia consumption\n\nAbstract:\nPersonalized delivery of multimedia content over the Internet opens new business perspectives for future multimedia applications and thus plays an important role in the ongoing MPEG-7 and MPEG-21 multimedia standardization efforts. Based on these standards, next-generation multimedia services will be able to automatically prepare the digital content before delivery according to the client's device capabilities, the network conditions, or even the user's content preferences. However, these services will have to deal with a variety of different end user devices, media formats, as well as with additional metadata when adapting the original media resources. In parallel, an increasing number of commercial or open-source media transformation tools will be available, capable of exploiting such descriptive metadata or dealing with new media formats; thus it is not realistic that a single tool will support all possible transformations.In this paper, we present a novel, fully knowledge-based approach for building such multimedia adaptation services, addressing the above mentioned issues of openness, extensibility, and concordance with existing and upcoming standards. In our approach, the original media is transformed in multiple adaptation steps performed by an extensible set of external tools, where the construction of adequate adaptation sequences is solved in an Artificial Intelligence planning process. The interoperability issue is addressed by exploiting standardizedSemantic Web Services technology. This technology allows us to express tool capabilities and execution semantics in a declarative and well-defined form. In this context, existing multimedia standards serve as a shared domain ontology.The presented approach was implemented and successfully evaluated in an official ISO/IEC MPEG (Moving Picture Experts Group) Core Experiment and is currently under further evaluation by the standardization body.\nTitle:\nA knowledge-based framework for multimedia adaptation\n\nAbstract:\nMultimedia content is becoming increasingly important in many areas not only for pure entertainment but also for commercial or educational purposes like, e.g., distance learning or online training. In parallel, the rapid evolution in the hardware sector brought up various new (mobile) end user devices like pocket PCs or mobile phones that are capable of displaying such content. Due to the different capabilities and usage environments of these devices, the basic multimedia content has to be adapted in order to fit the specific devices' capabilities and requirements, whereby such transformations typically include changes in the display size or quality adaptation. Based on the capabilities of the target device that can be expressed using recent multimedia standards like MPEG-21, these adaptation steps are typically carried out by the video server or a proxy node before the data is transferred to the client. In this paper, we present a software framework and implementation of such a multimedia server add-on that advances state-of-the-art technology in two ways. First, the framework supports the integration of various (already existing) multimedia transformation tools based on declarative interface and semantic capability descriptions in a way comparable to Semantic Web Services approaches. Second, by using the components' capability descriptions and the usage environment of the end user device, we employ a knowledge-based planning approach for dynamically constructing and executing the needed transformation program for a specific multi-media content request.\nTitle:\nAn extensible framework for knowledge-based multimedia adaptation\n\nAbstract:\nThe rapid evolution in the hardware sector brought up various (mobile) end user devices like PDAs or cell phones on which online multimedia content can be consumed. Due to different capabilities of these devices as well as individual user preferences, the original multimedia resources have to be adapted in order to fit the specific devices\u00fd constraints and to meet the users\u00fd requirements. Given the high variety of possible adaptation operations both on the format as well as the content level, an intelligent multimedia server must be able to integrate multiple existing and specialized adaptation tools. In this paper, we demonstrate how the usage of modular software components and declarative descriptions of component behavior enables us to develop extensible multimedia adaptation systems. The precise semantics of the utilized functionality description mechanism as well as the defined vocabulary from existing and emerging multimedia standards also allows us to automatically assemble adaptation chains that are executed on a given resource involving multiple, externally provided software components.\nTitle:\nA Knowledge and Component Based Multimedia Adaptation Framework\n\nAbstract:\nThis paper describes how a multimedia adaptation framework can automatically decide the sequence of operations to be executed in order to adapt an MPEG-21 Digital Item to the MPEG-21 description of the usage environment in which it will be consumed. The main innovation of this work with respect to previous multimedia adaptation decision models is that in the proposed approach decisions can be made without knowing the exact behaviour of the operations that are going to be executed.\nTitle:\nMultimedia Adaptation Decisions Modelled as Non-deterministic Operations\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.509, mean value for all the writers is 8.648, which makes it 0.119 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nUniversal Multimedia Access (UMA), where users can consume any multimedia resource anywhere at any time.. is the driving vision of ongoing ISO/IEC Moving Picture Experts Group (MPEG) standardization efforts. In that context, intelligent adaptation means that before resources are sent over the network, they are prepared according to the client's device capabilities, the network conditions, or even the user's content preferences. In this paper, we argue that Semantic Web Services can serve as a key enabling technology to achieve the goals of UMA. As the standards evolve. more and more specialized software tools will be available that provide specific functionalitiess for adapting the media in different dimensions. When the functionality of such tools is described declaratively with the means of Semantic Web Services technology, intelligent adaptation network nodes can be developed, capable of automatically composing multi-step adaptation sequences and dynamically integrating such services available on the Web. This paper describes the architecture and a prototypical implementation of an intelligent adaptation node that supports automatic, knowledge-based service composition which is made possible by the shared domain ontology defined in MPEG metadata standards.\nTitle:", "model_inf_time": 1.39}, {"id": "41269", "output": "Virtual Sales Assistants: Beyond Search and Query Forms", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe metaphor of a virtual sales assistant symbolizes an interactive Web application that serves online shoppers just as their human counterparts in brick & mortar businesses would do. They help to create an enjoyable shopping experience for users and are intended to bring a social and emotional perspective to e-commerce environments. Although, researchers have investigated dozens of interesting pilot applications, few have actually been commercialized. There is a big gap between the technological state of the art and what pays off for e-commerce sites from an economical point of view. Based on our industrial experience we discuss success factors for the development of cost-efficient and persuasive virtual sales assistants and contribute a framework for conversational sales recommender systems that includes a design environment for the efficient creation of virtual characters. Furthermore, this paper reports on experiences gathered from deployed applications.\nTitle:\nCost-Efficient Development of Virtual Sales Assistants\n\nAbstract:\nRecommender Systems (RS) suggest useful and interesting items to users in order to increase user satisfaction and online conversion rates. They typically exploit explicit or implicit user feedback such as ratings, buying records or clickstream data and apply statistical methods to derive recommendations. This paper focuses on explicitly formulated customer requirements as the sole type of user feedback. Its contribution lies in comparing different techniques such as knowledge- and utility-based methods, collaborative filtering, association rule mining as well as hybrid variants when user models consist solely of explicit customer requirements. We examine how this type of user feedback can be exploited for personalization in e-commerce scenarios. Furthermore, examples of actual online shops are developed where such contextual user information is available, demonstrating how more efficient RS configurations can be implemented. Results indicate that, especially for new users, explicit customer requirements are a useful source of feedback for personalization and hybrid configurations of collaborative and knowledge-based techniques achieve best results.\nTitle:\nCase-studies on exploiting explicit customer requirements in recommender systems\n\nAbstract:\nThe sales of customisable products and services over the internet is a challenging task within the area of electronic commerce. In this chapter we will present a case study which shows how the offering and selling of complex products and services from the telecommunication industry is supported within a generic framework for customer-adaptive distributed online configuration. Following the paradigm of mass customisation, products and services are nowadays sold to customers in many variants according to specific customer requirements. In a Web-based environment special emphasis must be given to the customer interaction with the sales system. Therefore we sketch how a personalised Web-interaction may imitate a good salesperson that adapts his expert advice according to the customers interests and skills. The digital economy of the 21st century will be based on flexibly integrated webs of highly specialised solution providers. Regarding configuration technology itself, the joint configuration of organisationa lly and geographically distributed products and services must be supported. This requires the extension of current configuration technology to include distributed knowledge bases and co- operative problem solving behaviour. The developed framework is designed generic enough to be also applicable to other industries with similar requirements for electronic commerce systems such as the areas of facility management equipment or building and construction industry.\nTitle:\nWeb-based Commerce of Complex Products and Services with Multiple Suppliers\n\nAbstract:\nThis paper summarizes our experiences gained from several industrial advisory applications that were developed with the knowledge-based ADVISOR SUITE framework over the last years and gives an outlook on future extensions of the presented system. In the 'experiences' section of the paper, we first address aspects related to the development of such applications, such as knowledge engineering, software maintenance, or testing. In addition, we describe the main requirements for such an advisory application to be perceived as an intelligent, value-adding service by the end users and finally summarize the results of an industrial study on how advisory applications are able to influence the buying behavior of online shoppers. The second part of the paper discusses current and future extensions of our system. The main lines of research addressed in this section are 'Extended debugging support', 'Automated extraction of product data from web sources', 'Log mining and advanced data analysis', and 'Community-adapted advisory systems'.\nTitle:\nKnowledge-Based Sales Advisory: Experiences and Future Directions\n\nAbstract:\nProduct configuration is a key technology in today's highly specialized economy. Within the scope of state-of-the-art B2B frameworks and eProcurement solutions, various initiatives take into account the provision of configuration services. However, they all are based on the idea of defining quasi-standards for many-to-many relationships between customers and vendors. When moving towards networked markets, where suppliers dynamically form supply-side consortia, more flexible approaches to B2B integration become necessary. The emerging paradigm of Web services has therefore a huge potential in business application integration. This paper presents an application scenario for configurationWeb services, that is currently under development in the research project CAWICOMS. An ontology-based approach allows the advertisement of services and a configuration specific protocol defines the operational processes. However, the lack of standards for the semantic annotation of Web services is still a major shortcoming of current Web technology.\nTitle:\nSemantic Configuration Web Services in the CAWICOMS Project\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.795, mean value for all the writers is 8.648, which makes it 0.125 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe metaphor of a virtual sales assistant has become commonplace in many online shops over the past decade. It raises expectations that Web applications serve online customers in the same way as sales persons of brick & mortar businesses do. However, in reality this promise is rarely kept. In many online shops the term is misused and denotes simple search tools or static query forms that do not provide personalized interactive behavior - let alone a social or emotional perspective. The purpose of this talk is to give an overview on current state-of-the-art techniques for Web personalisation and user interaction with e-commerce platforms and present selected examples. Furthermore, current research results are given and challenges for future work discussed.\nTitle:", "model_inf_time": 1.39}, {"id": "41270", "output": "Quotes: An AI-Powered E-Sourcing Application", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nStructured 3D Virtual Environments are 3D virtual spaces where some users' interactions are regulated by a subjacent Organisation Centered Multi Agent System (OCMAS) ---an Electronic Institution (EI). They are task-oriented hybrid systems, where staff (organisational) software agents support --based on activities' specification and current EI state--human users in their task achievement. The contribution of this paper is a conversational task-oriented structured 3D environment, where users interact with staff bots (i.e. the embodiment of staff agents) using natural language and, as a result, the communication is improved. With this aim, we extend the Artificial Intelligence Mark-up Language (AIML) with special tags to enable the complex flow of task-oriented conversations. They are characterised by different conversation' states --such as asked, responded, failed or confirmed--and by data types from the EI ontology. We evaluate the usability of our conversational proposal and compare it to a previous command-based interaction system. As expected, data analysis on users' skills --command-based novice vs expert profile-- suggests the co-existence both conversational and command-based user-agent interaction styles. But overall, results show a higher users' satisfaction with the conversational approach which, in average, also performs better in terms of efficiency, effectiveness and errors.\nTitle:\nConversational Structured Hybrid 3D Virtual Environments\n\nAbstract:\nThe automated synthesis of norms for coordination of multi-agent systems remains an open and complex problem. In this paper we present the Intelligent Robust On-line Norm Synthesis Machine (IRON), a system whose goal is the automated synthesis of norms. IRON is capable of synthesising norms that are at the same time effective (to ensure coordination) and necessary (to avoid over-regulation). IRON has been tested on a simulated traffic scenario to successfully synthesise norms that help cars avoid collisions. IRON is equipped with visualization features that provide support for an intuitive and informed monitoring of the synthesis process.\nTitle:\nIRON: a machine for the automated synthesis of normative systems\n\nAbstract:\nCoordination infrastructures play a central role in the engineering of multiagent systems. Since the advent of agent technology, research on coordination infrastructures has produced a significant number of infrastructures with varying features. In this paper, we review the the state-of-the-art coordination infrastructures with the purpose of identifying open research challenges that next generation coordination infrastructures should address. Our analysis concludes that next generation coordination infrastructures must address a number of challenges: (i) to become socially aware, by facilitating human interaction within a MAS; (ii) to assist agents in their decision making by providing decision support that helps them reduce the scope of reasoning and facilitates the achievement of their goals; and (iii) to increase openness to support on-line, fully decentralised design and execution. Furthermore, we identify some promising approaches in the literature, together with the research issues worth investigating, to cope with such challenges.\nTitle:\nTowards next generation coordination infrastructures.\n\nAbstract:\nWithin the area of multi-agent systems, normative systems are a widely used framework for the coordination of interdependent activities. A crucial problem associated with normative systems is that of synthesising norms that will effectively accomplish a coordination task and that the agents will comply with. Many works in the literature focus on the on-line synthesis of a single,  (convention) whose compliance forms a rational choice for the agents and that effectively coordinates them in  particular coordination situation that needs to be identified and modelled as a game in advance. In this work, we introduce a framework for the automatic off-line synthesis of  that coordinate the agents in multiple  that cannot be easily identified in advance nor resolved separately. Our framework roots in evolutionary game theory. It considers multi-agent systems in which the potential conflict situations can be automatically enumerated by employing MAS simulations along with basic domain information. Our framework simulates an evolutionary process whereby successful norms prosper and spread within the agent population, while unsuccessful norms are discarded. The outputs of such a natural selection process are sets of  that, together, effectively coordinate the agents in multiple interdependent situations and are evolutionarily stable. We empirically show the effectiveness of our approach through empirical evaluation in a simulated traffic domain.\nTitle:\nOff-line synthesis of evolutionarily stable normative systems.\n\nAbstract:\nThis paper investigates feature selection based on rough sets for dimensionality reduction in Case-Based Reasoning classifiers. In order to be useful, Case-Based Reasoning systems should be able to manage imprecise, uncertain and redundant data to retrieve the most relevant information in a potentially overwhelming quantity of data. Rough Set Theory has been shown to be an effective tool for data mining and for uncertainty management. This paper has two central contributions: (1) it develops three strategies for feature selection, and (2) it proposes several measures for estimating attribute relevance based on Rough Set Theory. Although we concentrate on Case-Based Reasoning classifiers, the proposals are general enough to be applicable to a wide range of learning algorithms. We applied these proposals on twenty data sets from the UCI repository and examined the impact of feature selection over classification performance. Our evaluation shows that all three proposals benefit the basic Case-Based Reasoning system. They also present robustness in comparison to well-known feature selection strategies.\nTitle:\nRough set based approaches to feature selection for Case-Based Reasoning classifiers\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.057, mean value for all the writers is 8.648, which makes it 0.504 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the everyday business world, the sourcing process of multiple goods and services usually involves complex negotiations (via telephone, fax, etc.) that include discussion of product and service features. Nowadays, this is a high-cost process due to the scarce use of tools that streamline this negotiation process and assist purchasing managers' decision-making. With the advent of Internet-based technologies, it has become feasible the idea of tools enabling low-cost, assisted, fluid, on-line dialogs between buyer enterprises and their providers wherever they are located. Consequently, several commercial systems to support on-line negotiations (e-sourcing tools) have been released. It is our view that there is still a need for these systems to incorporate effective decision support. This article presents the foundations of Quotes, a commercial sourcing application developed by iSOCO that, in addition to cover the whole sequence of sourcing tasks, incorporates decision support facilities based on Artificial Intelligence (AI) techniques that successfully address previous limitations within a single and coherent framework. The paper focuses on the computational realization of sourcing tasks along with the decision support facilities they require. While supported negotiation events are Request for Quotations/Proposals (RFQs/RFPs) and reverse auctions, decision support facilities include offer generation, offer comparison, and optimal bid set computation (winner determination) in combinatorial negotiations. Additionally, the paper presents a compound of experiences and lessons learned when using Quotes for real sourcing processes.\nTitle:", "model_inf_time": 1.57}, {"id": "41271", "output": "Autonomous Mobile Sink Routing for Energy-Balanced Data Gathering", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDeploying multiple mobile sinks is an attractive approach to enhance the performance of wireless sensor networks. In this paper, we address the scenario of two mobile sinks. The two sinks can travel in the same region or in the divided regions separately. The problems are formulated by a lattice-based network model. We deduce the network lifetime and delay of data delivery in different mobility patterns. The relation between network performance and number of mobile sinks is also discussed. Simulation results are provided to validate our theoretic analysis followed by performance comparison of different mobility patterns.\nTitle:\nDeploying Multiple Mobile Sinks in Event-Driven WSNs\n\nAbstract:\nIntroducing heterogeneous mobile devices, such as mobile phones into the large scale sparse wireless sensor networks is a promising research direction. These devices acting as mobile sinks offer many benefits to the network. For instance, they help to improve scalability, maintain load balance, conserve energy, prolong network lifetime and implement commercially. The paper investigates the impacts of different features and behavior of mobile sinks on the hybrid wireless sensor networks. Analysis and simulation results show that, instead of deploying mobile sinks as much as possible, choosing appropriate number, transmission range, velocity and gathering mode of the sink nodes can significantly decrease the average end-to-end data delivery delay and improve the energy conservation. The comparisons of performance metrics between fixed sinks and mobile sinks are also made in sparse networks along with the results that mobile sinks can bring higher data success rate and energy balance.\nTitle:\nThe hybrid mobile wireless sensor networks for data gathering\n\nAbstract:\nThis paper reviews the state-of-the-art features introduced by sink mobility into wireless sensor networks (WSN), and introduces the architecture of mobile enabled Wireless Sensor Network (mWSN) to realize large-scale information gathering via networked wireless sensors and mobile sinks. After introducing the mobile sensing scenarios, some fundamental design parameters in mWSN have been investigated, such as cluster size, sink velocity, transmission range, and packet length. Our contributions include: (1) A cluster formation method has been proposed via multihop forwarding to form a cluster around the expected position of a mobile sink, in order to guarantee packet delay and minimize energy consumption. (2) Analysis of the performance influence by sink mobility leads to the conclusion that the optimal sink velocity must make a compromise between sink-sensor meeting delay and message delivery delay. (3) Finding that large transmission range and short packet length are both of benefit to lower the outage probability of packet transmission. Extensive simulations have been designed to evaluate the performance of mWSN in terms of packet delay, energy consumption and outage probability of packet transmission.\nTitle:\nmWSN for Large Scale Mobile Sensing\n\nAbstract:\nConsidering sensor nodes deployed densely and uniformly a mobile sink moving through the sensing field queries a specific area of interest for monitoring information. The Query packet, injected by the mobile sink, is routed to the specific area and the corresponding Response packet is expected to return via multi-hop communication. In this paper, we analyze such a network model to address the problem of efficient data collection for mobile wireless monitoring applications. We first propose a meeting position-aware routing (MPAR) protocol for routing the Response packet efficiently and then propose an efficient query-based data collection scheme (QBDCS) for mobile wireless monitoring applications based on the MPAR. In order to minimize the energy consumption and packet delivery latency, the QBDCS chooses the optimal query time of injecting the Query packet and tailors the routing mechanism for sensor nodes forwarding packets. Simulation study has verified the analysis and demonstrated that the QBDCS can significantly reduce the energy consumption and end to end delivery latency.\nTitle:\nEfficient Query-Based Data Collection for Mobile Wireless Monitoring Applications\n\nAbstract:\nMobility has been incorporated into wireless sensor networks in recent years in order to improve energy efficiency and network coverage. In particular, at the cost of increased message delivery delay, the energy conservation can be achieved by limiting the hop count between the communicating sensor-sink pair. In this paper we allow multihop routing between one sensor and mobile sinks, and consider the delay*energy metric, which is a compound metric to evaluate the performance of diverse data collection schemes. Through extensive simulation experiments of AODV performance over IEEE 802.15.4 MAC with clustered wireless sensor network setting, we investigate the fundamental problems of AODV without information of 802.15.4 MAC, and analyze the influence of incorporating multiple sinks.\nTitle:\nSimulation Study of AODV Performance over IEEE 802.15.4 MAC in WSN with Mobile Sinks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.571, mean value for all the writers is 8.648, which makes it 0.787 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSink mobility has attracted much research interest in recent years because it can improve network performance such as energy efficiency and throughput. An energy-unconscious moving strategy is potentially harmful to the balance of the energy consumption among sensor nodes so as to aggravate the hotspot problem of sensor networks. In this paper, we propose an autonomous moving strategy for the mobile sinks in data-gathering applications. In our solution, a mobile sink approaches the nodes with high residual energy to force them to forward data for other nodes and tries to avoid passing by the nodes with low energy. We performed simulation experiments to compare our solution with other three data-gathering schemes. The simulation results show that our strategy cannot only extend network lifetime notably but also provides scalability and topology adaptability.\nTitle:", "model_inf_time": 1.38}, {"id": "41272", "output": "Complete axiomatization of a relative modal logic with composition and intersection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n This paper is devoted to the completeness issue of RMLCI ---the relative modal logic with composition and intersection --- a restriction ofthe propositional dynamic logic with intersection. The trouble with RMLCIis that the operation of intersection is not modally definable. Using thenotion of mosaics, we give a new proof of a theorem considered in a previouspaper &quot;Complete axiomatization of a relative modal logic with compositionand intersection&quot;. The theorem asserts that the proof... \nTitle:\nA new proof of completeness for a relative modal logic with composition and intersection\n\nAbstract:\nWe devote this article to the axiomatization/completeness of PRSPDL0-a variant of iteration-free PDL with parallel composition. Our results are based on the following: although the program operation of parallel composition is not modally definable in the ordinary language of PDL, it becomes definable in a modal language strengthened by the introduction of propositional quantifiers. Instead of using axioms to define the program operation of parallel composition in the language of PDL enlarged with propositional quantifiers, we add an unorthodox rule of proof that makes the canonical model standard for the program operation of parallel composition and we use large programs for the proof of the Truth Lemma.\nTitle:\nIteration-free PDL with storing, recovering and parallel composition: a complete axiomatization.\n\nAbstract:\n This paper presents the axiomatization --- without the rule of irreflexivity--- of the modal logic of inequality as well as a method for provingits completeness. This method uses the technics of the frame of subordination.1 IntroductionIntroduced by von Wright [14] and furthered by Segerberg [12], the modallogic with inequality [11] [13] --- also called logic of elsewhere [1] [3] [12]--- is superior in expressive power to the modal logic without inequality[2] [5] [9]. Its linguistic... \nTitle:\nInequality without irreflexivity\n\nAbstract:\nWe devote this paper to the completeness of an axiom system for PDL^{\u2229}_{0} \u2014 a variant of PDL which includes the program operations of composition and intersection. Most of the difficulty in the proof of the completeness theorem for PDL^{\u2229}_{0} lies in the fact that intersection of accessibility relations is not modally definable. We overcome this difficulty by considering the concepts of theory and large program. Theories are sets of formulas that contain PDL^{\u2229}_{0} and are closed under the inference rule of modus ponens. Large programs are built up from program variables and theories by means of the operations of composition and intersection, just as programs are built up from program variables and tests. Adapting these concepts to the subordination method, we can prove the completeness of our deductive system for PDL^{\u2229}_{0}.\nTitle:\nEliminating Unorthodox Derivation Rules in an Axiom System for Iteration-free PDL with Intersection\n\nAbstract:\nWe study a fragment of propositional modal logics using the universal modality given by a restriction on the modal depth of\n modal formulas.\n \nTitle:\nBoolean logics with relations\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.167, mean value for all the writers is 8.648, which makes it 0.41 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe consider the question of the complete axiomatization of a relativemodal logic with composition and intersection.1 IntroductionAs an abstraction of a system introduced by Pratt, propositional dynamiclogic --- PDL --- is a powerful instrument with which it ispossible to reason about programs [9, 6, 7, 11, 8]. The linguistic basisof PDL is the propositional calculus enlarged with a family of modaloperators of the form [ff] where ff is a program in \\Pi c composed inductivelyvia regular...\nTitle:", "model_inf_time": 1.36}, {"id": "41273", "output": "Dynamic RSU Re-routing for Reducing Multi-hop Wireless Delay in Vehicular Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nVehicular wireless networks (VWNs) offer wireless multi-hop communications between vehicles and roadside units (RSUs). A multi-hop wireless network with carry-and-forward routing may suffer from longer delay in packet delivery. However, the applications may demand stringent delay requirement in multi-hop VWNs. Therefore, this paper proposes an adaptive RSU re-routing strategy to alter the current associated RSU to the best RSU under delay or throughput constrains for applications in multi-hop VWNs. Performance results show that the proposed strategy is able to adaptively select a suitable packet size for the trade-off between delay and throughput to satisfy the diverse requirements of applications in multi-hop VWNs.\nTitle:\nAdaptive Rsu Re-Routing Under Delay Constraint In Multi-Hop Vehicular Networks\n\nAbstract:\nVehicular wireless networks (VWNs) offer both high-priority safety and low-priority non-safety applications. In VWNs, the high-priority applications are urgent. Transmitting urgent information demands low delay and high bandwidth. When the traffic is heavy, how to reduce the multi-hop wireless delay will be critical for urgent information. In this paper, we propose a blocking strategy to preempt the transmission of low-priority vehicles and to decrease the number of contending vehicles. Thus, the multi-hop wireless delay of high-priority vehicles will be greatly reduced even if the traffic is heavy. Moreover, we also propose a mathematical model to evaluate the performance of the proposed strategy and compare the existing strategy. Performance results show that the proposed strategy can reduce the multi-hop wireless delay significantly.\nTitle:\nReducing Multi-hop Wireless Delay for High-Priority Applications in Vehicular Wireless Networks\n\nAbstract:\nIn recent years, the IEEE 802.11p/1609 wireless access in vehicular environments standards adopt the dedicated short-range communications multi-channel architecture for vehicular wireless networks. To utilize the multi-channel architecture, each vehicle equipped with two sets of transceivers can operate concurrently on three different channels. For example, in cluster-based multi-channel schemes, a cluster head vehicle coordinates and assigns an appropriate channel to its cluster members. However, these schemes are unsuitable for a single channel device performing on only one RF channel at a time which would waste channel resource and increase time to allocate a channel. Another approach, called LEACH-based scheme, selects channels randomly and ensures that each channel is selected once within a round in each vehicle. However, this leads to a situation that different vehicles might select the same channel in short-term duration. In this paper, we propose a multi-channel selection scheme, called minimum duration counter (MDC) scheme, which could apply to a single channel device, while utilizing the multi-channel architecture of an 802.11p/1609 network. In addition, we compare the MDC scheme with pure random (PR) and LEACH-based schemes in terms of fairness index (FI) and utilization to emphasize the fairness and to balance the traffic of multi-channel usage. Furthermore, we analyze the counter overflow probability distribution and propose solutions to the MDC scheme. Numerical results show that our scheme outperforms the PR and LEACH-based schemes in terms of multi-channel usage, traffic balancing, and fairness.\nTitle:\nA Fair Scheme for Multi-channel Selection in Vehicular Wireless Networks\n\nAbstract:\nWe propose a pointer forwarding scheme for the mobile resource reservation protocol (MRSVP) to reduce the resource reservation cost on the wireless Internet. We show that the pointer forwarding scheme could significantly degrade the reservation cost when a mobile host performs locality movement.\nTitle:\nResource reservation with pointer forwarding schemes for the mobile RSVP\n\nAbstract:\nInternet protocol multicasting is a mechanism widely used in the Internet to disseminate information. Multicast delivery paths may need to be frequently restructured along with host migrations, at the expense of nontrivial overhead or multicast latency. This paper presents a scheme with short multicast latency, while retaining acceptable overhead in adapting the delivery paths to mobile host locations. We exploit the locality in host movements and keep active the networks that mobile hosts have most recently visited. As long as hosts migrate within these active networks, multicast delivery paths can remain unchanged upon host movements. Performance results show that, compared with the best known proposal, our scheme is promising.\nTitle:\nA new locality-based IP multicasting scheme for mobile hosts\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.672, mean value for all the writers is 8.648, which makes it 0.874 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAbstractVehicular wireless networks offer wireless multi-hop communications between vehicles and roadside units RSUs. To reduce deployment cost, the distance between two RSUs could be long; that is,the communications between an RSU and a vehicle may be carried out through multi-hops among intermediate vehicles. When a vehicle is driven from one RSU to another, the wireless multi-hop delay becomes more serious as the number of multi-hop relays increases. The wireless multi-hop delay is critical for some emergency service. For instance, in a traffic accident, when a patient was sent to the hospital by ambulance, the life information of the patient must be transmitted to the hospital on time through the multi-hop wireless network. If the ambulance is moved from one RSU to another, the wireless multi-hop delay becomes more and more serious as the ambulance is closing to another RSU. In this paper, we propose an RSU re-routing strategy that dynamically alters multi-hop communications until the best RSU with the shortest path using location information is found. Moreover, we compare the proposed strategy with the existing strategy in terms of broadcasting costs, re-routing delay, and wireless multi-hop delay of data transmission. Performance results show that the proposed strategy can reduce the wireless multi-hop delay significantly. Copyright \u00a9 2014 John Wiley & Sons, Ltd.\nTitle:", "model_inf_time": 1.78}, {"id": "41274", "output": "On the E-measure and Subdomain Testing Effectiveness", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe investigate the efficacy of subdomain testing and random testing using the expected number of failures detected (the E-measure) as a measure of effectiveness. Simple as it is, the E-measure does provide a great deal of useful information about the fault detecting capability of testing strategies. With the E-measure, we obtain new characterizations of subdomain testing, including several new conditions that determine whether subdomain testing is more or less effective than random testing. Previously, the efficacy of subdomain testing strategies has been analyzed using the probability of detecting at least one failure (the P-measure) for the special case of disjoint subdomains only. On the contrary, our analysis makes use of the E-measure and considers also the general case in which subdomains may or may not overlap. Furthermore, we discover important relations between the two different measures. From these relations, we also derive corresponding characterizations of subdomain testing in terms of the P-measure\nTitle:\nOn the expected number of failures detected by subdomain testing and random testing\n\nAbstract:\nSubdomain testing is a very general approach to the selection of test cases. It captures the characteristics of testing strategies that require the test suite to cover some predefined testing requirements. This paper attempts to characterise precisely the failure distributions for the best and worst case of any given subdomain testing strategy. Our analysis has revealed some crucial factors and principles that affect the effectiveness of subdomain testing strategies.\nTitle:\nOn Some Characterisation Problems of Subdomain Testing\n\nAbstract:\nTo compare the performance of different testing strategies, P-measure and E-measure are two effectiveness measures used in previous analytical studies. P-measure, which is defined as the probability of detecting at least one failure, is a measure of how likely it is that failure-causing inputs are selected at least once as test cases. E-measure, which is defined as the expected number of failures ...\nTitle:\nA New Perspective of the Proportional Sampling Strategy\n\nAbstract:\nWeyuker and Jeng have investigated the conditions that affect the performance of partition testing and have compared analytically the fault-detecting ability of partition testing and random testing. Chen and Yu have generalized some of Weyuker and Jeng's results. In this paper, we extend the analysis to subdomain testing in which subdomains may overlap. We derive several results for a special case and demonstrate a technique to extend some of our results to more general cases. We believe that this technique should be very useful in further investigating the behavior of subdomain testing.\nTitle:\nOn the Analysis of Subdomain Testing Strategies\n\nAbstract:\nFailure patterns describe typical ways in which inputs revealing program failure are distributed across the input domain\u2014in many cases, clustered together in contiguous regions. Based on these observations several debug testing methods have been developed. We examine the upper bound of debug testing effectiveness improvements possible through making assumptions about the shape, size and orientation of failure patterns. We consider the bounds for testing strategies with respect to minimizing the F-measure, maximizing the P-measure, and maximizing the E-measure. Surprisingly, we find that the empirically measured effectiveness of some existing methods that are not based on these assumptions is close to the theoretical upper bound of these strategies. The assumptions made to obtain the upper bound, and its further implications, are also examined.\nTitle:\nAn upper bound on software testing effectiveness\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.983, mean value for all the writers is 8.648, which makes it 0.286 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe expected number of failures detected (the E-measure) has been found to be a useful measure of the effectiveness of testing strategies. This paper takes a fresh perspective on the formulation of the E-measure. From this, we deduce new sufficient conditions for subdomain testing to be better than random testing. These conditions are simpler and more easily applicable than many of those previously found. Moreover, we obtain new characterisations of subdomain testing strategies in terms of the E-measure.\nTitle:", "model_inf_time": 1.22}, {"id": "41275", "output": "Ordering Guidelines for Secondary Task Image Attributes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe found that established display design guidelines for focal images cannot be extended to images displayed as a secondary task in a dual-task situation. This paper describes an experiment that determines a new ordering guideline for secondary task image attributes according to human cognitive ability to extract information. The imperative for alternate guidelines is based on the difference in an image's ability to convey meaning, which decreases when moved from a focal to a secondary task situation. Secondary task attribute ordering varies with the level of degradation in the primary task.\nTitle:\nAn ordering of secondary task display attributes\n\nAbstract:\nPeople often need to quickly access or maintain awareness of secondary information while busy with other primary tasks. Information visualizations provide rapid, effective access to information, but are generally designed to be examined by users as the primary focus of their attention. The goal of this research is to discover how to design information visualizations intended for the periphery and to understand how quickly and effectively people can interpret information visualizations while they are busily performing other tasks. We evaluated how several factors of a visualization (visual density, presence time, and secondary task type) impact people's abilities to continue with a primary task and to complete secondary tasks related to the visualization. Our results suggest that, with relaxed time pressure, reduced visual information density and a single well-defined secondary task, people can effectively interpret visualizations with minimal distraction to their primary task.\nTitle:\nAn evaluation of information visualization in attention-limited environments\n\nAbstract:\nLarge high-resolution displays have been shown to improve user performance over standard displays on many large-scale visualization tasks. But what is the reason for the improvement? The two most cited reasons for the advantage are (1) the wider field of view that exploits peripheral vision to provide context, and (2) the opportunity for physical navigation (e.g. head turning, walking, etc.) to visually access information. Which of these two factors is the key to advantage? Or, do they both work together to produce a combined advantage? This paper reports on an experiment that separates peripheral vision and physical navigation as independent variables. Results indicate that, for most of the tasks tested, increased physical navigation opportunity is more critical to improving performance than increased field of view. Some evidence indicates a valuable combined role.\nTitle:\nThe effects of peripheral vision and physical navigation on large scale visualization\n\nAbstract:\nFrom an exploratory user study using a fictional textual intelligence analysis task on a large, high-resolution vertical display, we investigated how pairs of users interact with the display to construct spatial schemas and externalize information, as well as how they establish shared and private territories. We investigated users' space management strategies depending on the design philosophy of the user interface (visualization- or document-centric). We classified the types of territorial behavior exhibited in terms of how the users interacted with information on the display (integrated or independent workspaces). Next, we examined how territorial behavior impacted the common ground between the pairs of users. Finally, we offer design suggestions for building future co-located collaborative visual analytics tools for use on large, high-resolution vertical displays.\nTitle:\nLarge high resolution displays for co-located collaborative sensemaking: Display usage and territoriality\n\nAbstract:\nThe main challenge associated with visual analysis using multiple displays is tied to the fact that a user must maintain awareness of and synthesize scattered information across separate displays\u2014some of which may be out of the user\u2019s immediate field of vision. To address this need, we present Spatially Aware Visual Links (SAViL), a cross-display visual link technique capable of (1) guiding the user\u2019s attention to relevant information and (2) visually connecting related information across displays. In essence, SAViL visually represents the direct connections among different types of visual objects on separate displays to help users create semantic layers of documents spread over different displays. To test the efficacy of this system, we evaluated the impact of visual linking on the sensemaking process for text data utilizing multiple heterogeneous displays. The results of our evaluation indicate that cross-display links enable users to effectively forage for, organize, and synthesize relevant information scattered across multiple displays, integrating the different displays into a single cohesive visual workspace to support their sensemaking tasks.\nTitle:\nSAViL: cross-display visual links for sensemaking in display ecologies.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.753, mean value for all the writers is 8.648, which makes it 0.09 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe found that established display design guidelines for focal images cannot be extended to images displayed as a secondary task in a dual-task situation. This paper describes an experiment that determines a new ordering guideline for secondary task image attributes according to human cognitive ability to extract information. The imperative for alternate guidelines is based on the difference in an image's ability to convey meaning, which decreases when moved from a focal to a secondary task situation. Secondary task attribute ordering varies with the level of degradation in the primary task. Furthermore, attribute effectiveness may be particular to types of visual operations relating to cognitive tasks.\nTitle:", "model_inf_time": 1.22}, {"id": "41276", "output": "A Framework for Spatiotemporal Query Processing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMost researches in spatiotemporal database focus on data model and query processing, and few are towards the implementation of spatiotemporal database systems. In this paper, we present OSTM, a spatiotemporal database module which is extended on Oracle9i. As an Object-Relational DBMS, Oracle provides extensibility of data types and functions for complex applications. Based on our previous spatiotemporal data model, we develop spatiotemporal data types and functions for Oracle9i. After a brief introduction to the spatiotemporal data model, the implemental details of OSTM are discussed. OSTM supports traditional SQL language and thus provides a practical way for spatiotemporal data management.\nTitle:\nOSTM: A Spatiotemporal Extension to Oracle\n\nAbstract:\nPrevious research on spatiotemporal database were mainly focused on modeling, indexing, and query processing, and little work has be done in the implementation of spatiotemporal database management systems. In this paper, we present an extension of Oracle, named STOC (Spatio-Temporal Object Cartridge), to support spatiotemporal data management in a practical way. The extension is developed as a PL/SQL package and can be integrated into Oracle to offer spatiotemporal data types as well as spatiotemporal operations for various applications. Users are allowed to use standard SQL to access spatiotemporal data and functions. After an overview of the general features of STOC, we discuss the architecture and implementation of STOC. And finally, a case study of STOC's demonstration is presented.\nTitle:\nSTOC: extending oracle to support spatiotemporal data management\n\nAbstract:\nTypical spatiotemporal information systems, such as traffic management and land management, need to trace the spatiotemporal changes of objects. Different applications have different requirements on describing spatiotemporal changes. It is necessary to make a systematic research on spatiotemporal changes in order to design a general spatiotemporal data model for different spatiotemporal applications. Based on the semantics of objects and spatiotemporal changes in the real world, this paper proposes a systematic classification on spatiotemporal changes of objects and a new approach to describe spatiotemporal changes, which is based on object identity and descriptor. The new approach uses object-level spatiotemporal changes and attribute-level spatiotemporal changes to describe the evolving history of objects. It overcomes the shortcomings of previous approaches, which are lacking in completeness and systematization on representing spatiotemporal changes, and can describe spatiotemporal changes completely.\nTitle:\nSemantics and Modeling of Spatiotemporal Changes\n\nAbstract:\nSpatiotemporal databases have received much attention since more and more applications, such as environment management and land management, have shown urgent requirements on the management of spatiotemporal information. But different applications have different requirements on describing spatiotemporal objects and spatiotemporal changes, and there is no systematic foundation for the modeling of spatiotemporal data. In this paper, we first study the characteristics of spatiotemporal changes. And then we build a semantic framework for the representation of spatiotemporal data. Spatiotemporal changes are classified into six types. We use three elements to represent these spatiotemporal changes, which are called lifecycle, descriptor and transformation. Through the three elements, a fundamental framework to model spatiotemporal data is achieved.\nTitle:\nA Semantic Framework for Spatiotemporal Data Representation\n\nAbstract:\nIn this paper, we present a new conceptual modeling tool, called STXER (SpatioTemporal eXtended Entity-Relationship), to support spatiotemporal database applications. STXER provides most functions for database conceptual modeling, such as graphical user interface, SQL script generation, XML schema definition, and commercial-DBMS-oriented database creation. Compared with previous tools, STXER is ER-compatible and supports richer spatiotemporal semantics. Thus it can support both non-spatiotemporal and spatiotemporal applications. After an overview of the general features of STXER, we discuss the architecture of STXER. And finally, a case study of STXER's demonstration is presented.\nTitle:\nAugmenting traditional ER method to support spatiotemporal database applications\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.797, mean value for all the writers is 8.648, which makes it 0.726 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSpatiotemporal data, or time-varying spatial data, occurs pervasively in many database applications. Such applications may benefit substantially from a DBMS with built-in spatiotemporal support. Current object-relational database products, such as Informix, Oracle and DB2, provide extensibility facilities that enable users to define spatiotemporal data types and operations.. According to this framework, spatiotemporal query processing is treated as relational query processing, and no additional query optimization is performed. Virtually, spatiotemporal support is needed to integrate spatiotemporal operations into the query optimization and processing framework. This paper presents a framework for spatiotemporal query processing, which is based a bottom object-relational DBMS and a top middleware layer. An object-relational DBMS is used to build add-in spatiotemporal support, which results in an extended spatiotemporal package. A spatiotemporal middleware layer on top of object-relational DBMS accepts SQL-represented spatiotemporal queries and makes query optimization. Two crucial issues to realize such a spatiotemporal query processing system are discussed, which are the spatiotemporal extension to object-relational DBMS and the middleware layer to process spatiotemporal queries.\nTitle:", "model_inf_time": 1.28}, {"id": "41277", "output": "IndoorDB: An Extension of Oracle for Indoor Moving Objects Management", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents a novel method to generate semantic-based trajectories for indoor moving objects. Indoor moving objects management has been a research focus in recent years. In order to get the trajectory data of indoor moving objects, we have to deply numerous positioning equipments, such as RFID readers and tags. In addition, it is a very complex and costly process to construct different environment settings for various indoor applications. To solve those problems, we propose to use virtual positioning equipments, e.g. RFID readers and tags, to simulate indoor environment. Furthermore, we present a semantic-based approach to generating trajectories for indoor moving objects, which takes into account the type of moving objects, the relationship between moving objects and locations, and the distribution of the trajectories. Compared with previous approaches, our method is more realistic for the simulation of indoor scenarios, and can provide useful trajectory data for further indoor data management analysis. Finally, we design and implement a tool for the generation of semantic-based trajectories for indoor moving objects, and conduct a case study to demonstrate its effectiveness. The results show that it can generate semantic-based trajectories for indoor moving objects according to different parameters and semantic settings.\nTitle:\nGenerating semantic-based trajectories for indoor moving objects\n\nAbstract:\nThe increasing deployment of indoor positioning technologies like RFID, Wi-fi, and Bluetooth offers the possibility to obtain users' trajectories in indoor spaces. In this paper, based on indoor moving-object trajectories, we aim to detect hotspots from indoor trajectory data. Such information is helpful for users to understand the surrounding locations as well as to enable indoor trajectory mining and location recommendation. We first define a new kind of query called indoor hotspot query. Then, we introduce a pre-processing step to remove meaningless locations and obtain indoor stay trajectories. Further, we propose a new approach to answering indoor hotspot queries w.r.t. two factors: (1) users' interests in indoor locations, and (2) the mutual reinforcement relationship between users and indoor locations. Particularly, we construct a user-location matrix and use an iteration-based technique to compute the hotness of indoor locations. We evaluate our proposal on 223,564 indoor tracking records simulating 100 users' movements over a period of one month in a six-floor building. The results in terms of MAP, P@n, and nDCG show that our proposal outperforms baseline methods like rank-by-visit, rank-by-density, and rank-by-duration.\nTitle:\nDetecting Hotspots From Trajectory Data In Indoor Spaces\n\nAbstract:\nIn this paper, we present a CASE tool supporting conceptual modeling for moving objects database applications, which is called STXER (spatio-temporal extended entity-relational model). The main purpose of STXER is to support the database design for moving objects applications on typical object-relational DBMS. The STXER tool enhances the traditional ER model with moving characteristics. It can express rich spatiotemporal semantics for moving objects applications. After an overview of the general features of STXER, we discuss the architecture of STXER. And finally, a case study of STXER's demonstration is presented.\nTitle:\nConceptual Modeling for Moving Objects Database Applications\n\nAbstract:\nIn this paper, we present a flexible simulation environment for the performance evaluation of flash-aware algorithms, which is called Flash-DBSim. The main purpose of Flash-DBSim is to provide a configurable virtual flash disk for upper systems, such as file system and DBMS, so that the algorithms in those systems can be easily evaluated on different types of flash disks. Moreover, it also offers a prototyping environment for those algorithms inside flash disk, e.g. the algorithms for garbage collection or wear-leveling. After an overview of the general features of Flash-DBSim, we discuss the architecture of Flash-DBSim. And finally, a case study of Flash-DBSim's demonstration is presented.\nTitle:\nA flexible simulation environment for flash-aware algorithms\n\nAbstract:\nSpatiotemporal data, or time-varying spatial data, occurs pervasively in many database applications. Such applications may benefit substantially from a DBMS with built-in spatiotemporal support. Current object-relational database products, such as Informix, Oracle and DB2, provide extensibility facilities that enable users to define spatiotemporal data types and operations.. According to this framework, spatiotemporal query processing is treated as relational query processing, and no additional query optimization is performed. Virtually, spatiotemporal support is needed to integrate spatiotemporal operations into the query optimization and processing framework. This paper presents a framework for spatiotemporal query processing, which is based a bottom object-relational DBMS and a top middleware layer. An object-relational DBMS is used to build add-in spatiotemporal support, which results in an extended spatiotemporal package. A spatiotemporal middleware layer on top of object-relational DBMS accepts SQL-represented spatiotemporal queries and makes query optimization. Two crucial issues to realize such a spatiotemporal query processing system are discussed, which are the spatiotemporal extension to object-relational DBMS and the middleware layer to process spatiotemporal queries.\nTitle:\nA Framework For Spatiotemporal Query Processing Based On Object-Relational Dbms And Middleware\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.3, mean value for all the writers is 8.648, which makes it 0.297 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nManaging moving objects in indoor space has been a research focus in recent years, as most people live and work in indoor space, e.g. working in office, living in apartment, etc. In this paper, we present an extension of Oracle named IndoorDB to support indoor moving objects management in a practical way. The extension is developed as a PL/SQL package and can be integrated into Oracle to offer new data types and operations for indoor location-based queries such as indoor navigation, hot spots detection, KNN, range queries, and so on. After an overview of the general features of IndoorDB, we discuss the architecture and implementation of IndoorDB. And finally, a case study of IndoorDB's demonstration is presented. \u00a9 Springer-Verlag 2013.\nTitle:", "model_inf_time": 1.53}, {"id": "41278", "output": "Efficient Sampling for Third-Order Tensors", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n  We propose a new method for robust PCA -- the task of recovering a low-rank matrix from sparse corruptions that are of unknown value and support. Our method involves alternating between projecting appropriate residuals onto the set of low-rank matrices, and the set of sparse matrices; each projection is {\\em non-convex} but easy to compute. In spite of this non-convexity, we establish exact recovery of the low-rank matrix, under the same conditions that are required by existing methods (which are based on convex optimization). For an $m \\times n$ input matrix ($m \\leq n)$, our method has a running time of $O(r^2mn)$ per iteration, and needs $O(\\log(1/\\epsilon))$ iterations to reach an accuracy of $\\epsilon$. This is close to the running time of simple PCA via the power method, which requires $O(rmn)$ per iteration, and $O(\\log(1/\\epsilon))$ iterations. In contrast, existing methods for robust PCA, which are based on convex optimization, have $O(m^2n)$ complexity per iteration, and take $O(1/\\epsilon)$ iterations, i.e., exponentially more iterations for the same accuracy.   Experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations. \nTitle:\nNon-convex Robust PCA.\n\nAbstract:\nWe consider the problem of solving mixed random linear equations with $k$ components. This is the noiseless setting of mixed linear regression. The goal is to estimate multiple linear models from mixed samples in the case where the labels (which sample corresponds to which model) are not observed. We give a tractable algorithm for the mixed linear equation problem, and show that under some technical conditions, our algorithm is guaranteed to solve the problem exactly with sample complexity linear in the dimension, and polynomial in $k$, the number of components. Previous approaches have required either exponential dependence on $k$, or super-linear dependence on the dimension. The proposed algorithm is a combination of tensor decomposition and alternating minimization. Our analysis involves proving that the initialization provided by the tensor method allows alternating minimization, which is equivalent to EM in our setting, to converge to the global optimum at a linear rate.\nTitle:\nSolving a Mixture of Many Random Linear Equations by Tensor Decomposition and Alternating Minimization.\n\nAbstract:\nWe develop a new algorithm to cluster sparse unweighted graphs -- i.e. partition the nodes into disjoint clusters so that there is higher density within clusters, and low across clusters. By sparsity we mean the setting where both the in-cluster and across cluster edge densities are very small, possibly vanishing in the size of the graph. Sparsity makes the problem noisier, and hence more difficult to solve. Any clustering involves a tradeoff between minimizing two kinds of errors: missing edges within clusters and present edges across clusters. Our insight is that in the sparse case, these must be {\\em penalized differently}. We analyze our algorithm's performance on the natural, classical and widely studied ``planted partition'' model (also called the stochastic block model); we show that our algorithm can cluster sparser graphs, and with smaller clusters, than all previous methods. This is seen empirically as well.\nTitle:\nClustering Sparse Graphs.\n\nAbstract:\nThe task of sparse linear regression consists of finding an unknown sparse vector from linear measurements. Solving this task even under \u201chigh-dimensional\u201d settings, where the number of samples is fewer than the number of variables, is now known to be possible via methods such as the LASSO. We consider the multiple sparse linear regression problem, where the task consists of recovering several related sparse vectors at once. A simple approach to this task would involve solving independent sparse linear regression problems, but a natural question is whether one can reduce the overall number of samples required by leveraging partial sharing of the support sets, or nonzero patterns, of the signal vectors. A line of recent research has studied the use of $\\\\ell _{1}/\\\\ell _{q}$ norm block-regularizations with $q 1$ for such problems. However, depending on the level of sharing, these could actually perform worse in sample complexity when compared to solving each problem independently. We present a new \u201cadaptive\u201d method for multiple sparse linear regression that can leverage support and parameter overlap when it exists, but not pay a penalty when it does not. We show how to achieve this using a very simple idea: decompose the parameters into two components and regularize these differently. We show, theoretically and empirically, that our method strictly and noticeably outperforms both $\\\\ell _{1}$ or $\\\\ell _{1}/\\\\ell _{q}$ methods, over the entire range of possible overlaps (except at boundary cases, where we match the best method), even under high-dimensional scaling.\nTitle:\nA Dirty Model for Multiple Sparse Regression\n\nAbstract:\nLinear encoding of sparse vectors is widely popular, but is commonly data-independent -- missing any possible extra (but a-priori unknown) structure beyond sparsity. In this paper we present a new method to learn linear encoders that adapt to data, while still performing well with the widely used $ell_1$ decoder. The convex $ell_1$ decoder prevents gradient propagation as needed in standard gradient-based training. Our method is based on the insight that unrolling the convex decoder into $T$ projected subgradient steps can address this issue. Our method can be seen as a data-driven way to learn a compressed sensing measurement matrix. We compare the empirical performance of 10 algorithms over 6 sparse datasets (3 synthetic and 3 real). Our experiments show that there is indeed additional structure beyond sparsity in the real datasets. Our method is able to discover it and exploit it to create excellent reconstructions with fewer measurements (by a factor of 1.1-3x) compared to the previous state-of-the-art methods. We illustrate an application of our method in learning label embeddings for extreme multi-label classification. Our experiments show that our method is able to match or outperform the precision scores of SLEEC, which is one of the state-of-the-art embedding-based approaches for extreme multi-label learning.\nTitle:\nLearning a Compressed Sensing Measurement Matrix via Gradient Unrolling.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 6.927, mean value for all the writers is 8.648, which makes it 1.468 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper we propose new techniques to sample arbitrary third-order tensors, with an objective of speeding up tensor algorithms that have recently gained popularity in machine learning. Our main contribution is a new way to select, in a biased random way, only $O(n^{1.5}/\\epsilon^2)$ of the possible $n^3$ elements while still achieving each of the three goals: \\\\ {\\em (a) tensor sparsification}: for a tensor that has to be formed from arbitrary samples, compute very few elements to get a good spectral approximation, and for arbitrary orthogonal tensors {\\em (b) tensor completion:} recover an exactly low-rank tensor from a small number of samples via alternating least squares, or {\\em (c) tensor factorization:} approximating factors of a low-rank tensor corrupted by noise. \\\\ Our sampling can be used along with existing tensor-based algorithms to speed them up, removing the computational bottleneck in these methods.\nTitle:", "model_inf_time": 1.47}, {"id": "41279", "output": "Semidefinite Programming Relaxations for Semialgebraic Problems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe compare algorithms for global optimization of polynomial functions in many\nvariables. It is demonstrated that existing algebraic methods (Gr\\\"obner bases,\nresultants, homotopy methods) are dramatically outperformed by a relaxation\ntechnique, due to N.Z. Shor and the first author, which involves sums of\nsquares and semidefinite programming. This opens up the possibility of using\nsemidefinite programming relaxations arising from the Positivstellensatz for a\nwide range of computational problems in real algebraic geometry.\n  This paper was presented at the Workshop on Algorithmic and Quantitative\nAspects of Real Algebraic Geometry in Mathematics and Computer Science, held at\nDIMACS, Rutgers University, March 12-16, 2001.\nTitle:\nMinimizing Polynomial Functions\n\nAbstract:\nIn the past decade there has been a surge of interest in algebraic approaches to optimization problems defined by multivariate polynomials. Fundamental mathematical challenges that arise in this area include understanding the structure of nonnegative polynomials, the interplay between efficiency and complexity of different representations of algebraic sets, and the development of effective algorithms. Remarkably, and perhaps unexpectedly, convexity provides a new viewpoint and a powerful framework for addressing these questions. This naturally brings us to the intersection of algebraic geometry, optimization, and convex geometry, with an emphasis on algorithms and computation. This emerging area has become known as convex algebraic geometry. This tutorial will focus on basic and recent developments in convex algebraic geometry, and the associated computational methods based on semidefinite programming for optimization problems involving polynomial equations and inequalities. There has been much recent progress, by combining theoretical results in real algebraic geometry with semidefinite programming to develop effective computational approaches to these problems. We will make particular emphasis on sum of squares decompositions, general duality properties, infeasibility certificates, approximation/inapproximability results, as well as survey the many exciting developments that have taken place in the last few years.\nTitle:\nConvex algebraic geometry and semidefinite optimization\n\nAbstract:\nSemialgebraic computations, i.e., the manipulation of sets and logical conditions defined by polynomial inequalities in real variables, are an essential \u201dprimitive\u201d in the analysis and design of hybrid dynamical systems. Fundamental tasks such as reachability analysis, abstraction verification, and the computation of stability and performance certificates, all use these operations extensively, and can quickly become the computational bottleneck in the design process. Although there is a well-developed body of both basic theory and algorithms for these tasks, the practical performance of most available methods is still far from being satisfactory on real-world problems. While there are several possible causes for this (besides the NP-hardness of the task), a sensible explanation lies in the purely algebraic nature of the usual methods, as well as the insistence on exact (as opposed to approximate or \u201drelaxed\u201d) solutions. For these reasons, there is a strong interest in the development of efficient techniques for (perhaps restricted) classes of semialgebraic problems. In this talk we review the basic elements and present several new results on the SOS approach to semialgebraic computations, that combines symbolic and numerical techniques from real algebra and convex optimization. Its main defining feature is the use and computation of sum of squares (SOS) decompositions for multivariate polynomials via semidefinite programming. These are extended, using the Positivstellensatz, to structured infeasibility certificates for polynomial equations and inequalities. The developed techniques unify and generalize many well-known existing methods. In particular, we will discuss semialgebraic problems with at most two quantifier alternations (i.e., classical polynomial optimization problems as well as the related games and minimax problems). As an example, we will solve in detail a class of zero-sum two-person games with an infinite number of pure strategies, where the payoff function is a polynomial expression of the actions of the players. Although particular emphasis will be given to the hybrid systems viewpoint, the basic ideas and algorithms, as well as these recent extensions, will be illustrated with examples drawn from a broad range of related domains, including dynamical systems and geometric theorem proving.\nTitle:\nSOS methods for semi-algebraic games and optimization\n\nAbstract:\nInspired by a question of Lov\u00e1sz, we introduce a hierarchy of nested semidefinite relaxations of the convex hull of real solutions to an arbitrary polynomial ideal called theta bodies of the ideal. These relaxations generalize Lov\u00e1sz's construction of the theta body of a graph. We establish a relationship between theta bodies and Lasserre's relaxations for real varieties which allows, in many cases, for theta bodies to be expressed as feasible regions of semidefinite programs. Examples from combinatorial optimization are given. Lov\u00e1sz asked to characterize ideals for which the first theta body equals the closure of the convex hull of its real variety. We answer this question for vanishing ideals of finite point sets via several equivalent characterizations. We also give a geometric description of the first theta body for all ideals.\nTitle:\nTheta Bodies for Polynomial Ideals\n\nAbstract:\nWe introduce the framework of path-complete graph Lyapunov functions for approximation of the joint spectral radius. The approach is based on the analysis of the underlying switched system via inequalities imposed among multiple Lyapunov functions associated to a labeled directed graph. Inspired by concepts in automata theory and symbolic dynamics, we define a class of graphs called path-complete graphs, and show that any such graph gives rise to a method for proving stability of the switched system. This enables us to derive several asymptotically tight hierarchies of semidefinite programming relaxations that unify and generalize many existing techniques such as common quadratic, common sum of squares, path-dependent quadratic, and maximum/minimum-of-quadratics Lyapunov functions. We compare the quality of approximation obtained by certain classes of path-complete graphs including a family of dual graphs and all path-complete graphs with two nodes on an alphabet of two matrices. We derive approximation guarantees for several families of path-complete graphs, such as the De Bruijn graphs. This provides worst-case performance bounds for path-dependent quadratic Lyapunov functions and a constructive converse Lyapunov theorem for maximum/minimum-of-quadratics Lyapunov functions.\nTitle:\nJoint Spectral Radius and Path-Complete Graph Lyapunov Functions.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.575, mean value for all the writers is 8.648, which makes it 0.062 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\n.\u00a0\u00a0\u2002A hierarchy of convex relaxations for semialgebraic problems is introduced. For questions reducible to a finite number of\n polynomial equalities and inequalities, it is shown how to construct a complete family of polynomially sized semidefinite\n programming conditions that prove infeasibility. The main tools employed are a semidefinite programming formulation of the\n sum of squares decomposition for multivariate polynomials, and some results from real algebraic geometry. The techniques provide\n a constructive approach for finding bounded degree solutions to the Positivstellensatz, and are illustrated with examples\n from diverse application fields.\nTitle:", "model_inf_time": 1.63}, {"id": "41280", "output": "Variational Tomographic Reconstruction via Moment Estimation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn many applications of tomography, the fundamental quantities of interest in an image are geometric ones. In these instances, pixel-based signal processing and reconstruction is at best inefficient, and, at worst, nonrobust in its use of the available tomographic data. Classical reconstruction techniques such as filtered back-projection tend to produce spurious features when data is sparse and noisy; these \"ghosts\" further complicate the process of extracting what is often a limited number of rather simple geometric features. In this paper, we present a framework that, in its most general form, is a statistically optimal technique for the extraction of specific geometric features of objects directly from the noisy projection data. We focus on the tomographic reconstruction of binary polygonal objects from sparse and noisy data. In our setting, the tomographic reconstruction problem is essentially formulated as a (finite-dimensional) parameter estimation problem. In particular, the vertices of binary polygons are used as their defining parameters. Under the assumption that the projection data are corrupted by Gaussian white noise, we use the maximum likelihood (ML) criterion, when the number of parameters is assumed known, and the minimum description length (MDL) criterion for reconstruction when the number of parameters is not known. The resulting optimization problems are nonlinear and thus are plagued by numerous extraneous local extrema, making their solution far from trivial. In particular, proper initialization of any iterative technique is essential for good performance. To this end, we provide a novel method to construct a reliable yet simple initial guess for the solution. This procedure is based on the estimated moments of the object, which may be conveniently obtained directly from the noisy projection data.\nTitle:\nReconstructing binary polygonal objects from projections: a statistical view\n\nAbstract:\nIn this work, we develop statistically based algorithms to reconstruct binary polygonal objects from sparse and noisy tomographic-based observation data. Traditional approaches to the reconstruction of geometric objects from projection data often lead to highly nonlinear estimation problems. To avoid the difficulties associated with such nonlinear problems, we first examine the problem of reconstruction of an object based on knot location measurements, i.e., measurements of the locations of abrupt change in the projections. The ties between this problem and that of multitarget radar tracking enable us to develop a sequential hypothesis-testing algorithm requiring only the solution of a series of linear estimation problems. In particular, data association hypotheses are generated, under each of which the inversion is linear. The complexity of the association possibilities are kept in check through the use of constraints on the reconstruction imposed by the tomography problem. The solution of this first problem is then used as an initialization to a more complete reconstruction which, while utilizing all the projection data, is nonlinear. We demonstrate that the estimates provided by the first, efficient algorithm are of good quality on their own, and, when combined with a fully nonlinear inversion, produce excellent object estimates.\nTitle:\nTomographic reconstruction of polygons from knot location and chord length measurements\n\nAbstract:\nWe use a natural pixel-type representation of an object, originally developed for incomplete data tomography problems, to construct nearly orthonormal multiscale basis functions. The nearly orthonormal behavior of the multiscale basis functions results in a system matrix, relating the input (the object coefficients) and the output (the projection data), which is extremely sparse. In addition, the coarsest scale elements of this matrix capture any ill conditioning in the system matrix arising from the geometry of the imaging system. We exploit this feature to partition the system matrix by scales and obtain a reconstruction procedure that requires inversion of only a well-conditioned and sparse matrix. This enables us to formulate a tomographic reconstruction technique from incomplete data wherein the object is reconstructed at multiple scales or resolutions. In case of noisy projection data we extend our multiscale reconstruction technique to explicitly account for noise by calculating maximum a posteriori probability (MAP) multiscale reconstruction estimates based on a certain self-similar prior on the multiscale object coefficients. The framework for multiscale reconstruction presented can find application in regularization of imaging problems where the projection data are incomplete, irregular, and noisy, and in object feature recognition directly from projection data.\nTitle:\nTomographic reconstruction and estimation based on multiscale natural-pixel bases.\n\nAbstract:\nThe authors represent the standard ramp filter operator of the filtered-back-projection (FBP) reconstruction in different bases composed of Haar and Daubechies compactly supported wavelets. The resulting multiscale representation of the ramp-filter matrix operator is approximately diagonal. The accuracy of this diagonal approximation becomes better as wavelets with larger numbers of vanishing moments are used. This wavelet-based representation enables the authors to formulate a multiscale tomographic reconstruction technique in which the object is reconstructed at multiple scales or resolutions. A complete reconstruction is obtained by combining the reconstructions at different scales. The authors' multiscale reconstruction technique has the same computational complexity as the FBP reconstruction method. It differs from other multiscale reconstruction techniques in that (1) the object is defined through a one-dimensional multiscale transformation of the projection domain, and (2) the authors explicitly account for noise in the projection data by calculating maximum a posteriori probability (MAP) multiscale reconstruction estimates based on a chosen fractal prior on the multiscale object coefficients. The computational complexity of this maximum a posteriori probability (MAP) solution is also the same as that of the FBP reconstruction. This result is in contrast to commonly used methods of statistical regularization, which result in computationally intensive optimization algorithms.\nTitle:\nA wavelet-based method for multiscale tomographic reconstruction\n\nAbstract:\nWhen segmenting images of low quality or with missing data, statistical prior information about the shapes of the objects to be segmented can significantly aid the segmentation process. However, defining probability densities in the space of shapes is an open and challenging problem. In this paper, we propose a nonparametric shape prior model for image segmentation problems. In particular, given example training shapes, we estimate the underlying shape distribution by extending a Parzen density estimator to the space of shapes. Such density estimates are expressed in terms of distances between shapes, and we consider the L2 distance between signed distance functions for shape density estimation, in addition to a distance measure based on the template metric. In particular, we consider the case in which the space of shapes is interpreted as a manifold embedded in a Hilbert space. We then incorporate the learned shape prior distribution into a maximum a posteriori (MAP) estimation framework for segmentation. This results in an optimization problem, which we solve using active contours. We demonstrate the effectiveness of the resulting algorithm in segmenting images that involve low-quality data and occlusions. The proposed framework is especially powerful in handling ''multimodal'' shape densities.\nTitle:\nNonparametric shape priors for active contour-based image segmentation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.87, mean value for all the writers is 8.648, which makes it 0.189 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe describe a variational framework for the tomographic reconstruction of an image from the maximum likelihood (ML) estimates of its orthogonal moments. We show how these estimated moments and their (correlated) error statistics can be computed directly, and in a linear fashion from given noisy and possibly sparse projection data. Moreover, thanks to the consistency properties of the Radon transform, this two-step approach (moment estimation followed by image reconstruction) can be viewed as a statistically optimal procedure. Furthermore, by focusing on the important role played by the moments of projection data, we immediately see the close connection between tomographic reconstruction of nonnegative valued images and the problem of nonparametric estimation of probability densities given estimates of their moments. Taking advantage of this connection, our proposed variational algorithm is based on the minimization of a cost functional composed of a term measuring the divergence between a given prior estimate of the image and the current estimate of the image and a second quadratic term based on the error incurred in the estimation of the moments of the underlying image from the noisy projection data. We show that an iterative refinement of this algorithm leads to a practical algorithm for the solution of the highly complex equality constrained divergence minimization problem. We show that this iterative refinement results in superior reconstructions of images from very noisy data as compared with the classical filtered back-projection (FBP) algorithm.\nTitle:", "model_inf_time": 1.57}, {"id": "41281", "output": "Equivalence Relations Among Graph Testing, DNA Complex Screening, Superimposed Codes, and Secure Key Distribution", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we give an overview of Combinatorial Group Testing algo- rithms which are applicable to DNA Library Screening. Our survey focuses on several classes of constructions not discussed in previous surveys, provides a general view on pooling design constructions and poses several open questions arising from this view.\nTitle:\nA Survey on Combinatorial Group Testing Algorithms with Applications to DNA Library Screening\n\nAbstract:\nMotivated by applications in social networks, a new type of dominating set has been studied in the literature. In this paper, we present results regarding the complexity and approximation in general graphs.\nTitle:\nNew dominating sets in social networks\n\nAbstract:\nThe study of gene functions requires the development of a DNA library of high quality through much of testing and screening. Pooling design is a mathematical tool to reduce the number of tests for DNA library screening. The transversal design is a special type of pooling design, which is good in implementation. In this paper, we present a new construction for transversal designs. We will also extend our construction to the error-tolerant case.\nTitle:\nNew construction for transversal design.\n\nAbstract:\nIn this paper, we consider the maximum and minimum versions of degree-concentrated fault-tolerant spanning subgraph problem which has many applications in network communications. We prove that both this two problems are NP-hard. For the maximum version, we use DC programming relaxation to propose a heuristic algorithm. Numerical tests indicate that the proposed algorithm is efficient and effective. For the minimum version, we also formulate it as a DC program, and show that the DC algorithm does not perform well for this problem.\nTitle:\nSolving the degree-concentrated fault-tolerant spanning subgraph problem by DC programming.\n\nAbstract:\nThe study of gene functions requires a DNA library of high quality, such a library is obtained from a large mount of testing and screening. Pooling design is a very helpful tool for reducing the number of tests for DNA library screening. In this paper, we present new one- and two-stage pooling designs, together with new probabilistic pooling designs. The approach in this paper works for both error-free and error-tolerance scenarios.\nTitle:\nNew constructions of one- and two-stage pooling designs.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.993, mean value for all the writers is 8.648, which makes it 0.294 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper discusses the relation among four problems: graph testing, DNA complex screening, superimposed codes and secure\n key distribution. We prove a surprising equivalence relation among these four problems, and use this equivalence to improve\n current results on graph testing. In the rest of this paper, we give a lower bound for the minimum number of tests on DNA\n complex screening model.\nTitle:", "model_inf_time": 1.51}, {"id": "41282", "output": "Constant-Gap Performance of Decode-and-Forward and Compress-and-Forward in Interference Relay Channels", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe capacity of a class of Interference Relay Channels (IRC) is investigated. We derive a novel outer and three inner bounds which are based on a careful use of all existing cooperative strategies together with the adequate interference decoding technique. Our main result is the necessity of three different cooperative strategies to achieve a constant gap to the capacity for each SNR regime of the Gaussian IRC. Surprisingly enough, this outcome appears to be in contrast with that of the standard Gaussian RC (Relay Channel) where several cooperative strategies yield constant-gap results in all regimes.\nTitle:\nConstant-gap results and cooperative strategies for a class of Interference Relay Channels\n\nAbstract:\nThe capacity of a class of Interference Relay Channels (IRC) \u2013the Injective Semideterministic IRC where the relay can only observe one of the sources\u2013 is investigated. We first derive a novel outer bound and two inner bounds which are based on a careful use of each of the available cooperative strategies together with the adequate interference decoding technique. The outer bound extends Telatar and Tse\u2019s work whereas the inner bounds contain several known results in the literature. Our main result is the characterization of the capacity region of the Gaussian class of IRCs studied within a fixed number of bits per dimension, constant gap. The proof relies on the use of the different cooperative strategies in specific SNR regimes due to their complexity. As a matter of fact, this issue reveals the complex nature of the Gaussian IRC where the combination of a single coding scheme for the Gaussian relay and interference channel may not lead to a good coding scheme for this problem, even when the focus is only on capacity within a constant gap over all possible fading statistics.\nTitle:\nCapacity Bounds for a Class of Interference Relay Channels\n\nAbstract:\nThis paper investigates the problem of information transmission over the\nsimultaneous relay channel with two users (or two possible channel outcomes)\nwhere for one of them the more suitable strategy is Decode-and-Forward (DF)\nwhile for the other one is Compress-and-Forward (CF). In this setting, it is\nassumed that the source wishes to send common and private informations to each\nof the users (or channel outcomes). This problem is relevant to: (i) the\ntransmission of information over the broadcast relay channel (BRC) with\ndifferent relaying strategies and (ii) the transmission of information over the\nconventional relay channel where the source is oblivious to the coding strategy\nof relay. A novel coding that integrates simultaneously DF and CF schemes is\nproposed and an inner bound on the capacity region is derived for the case of\ngeneral memoryless BRCs. As special case, the Gaussian BRC is studied where it\nis shown that by means of the suggested broadcast coding the common rate can be\nimproved compared to existing strategies. Applications of these results arise\nin broadcast scenarios with relays or in wireless scenarios where the source\ndoes not know whether the relay is collocated with the source or with the\ndestination.\nTitle:\nBroadcasting over the Relay Channel with Oblivious Cooperative Strategy\n\nAbstract:\nConsider the composite relay channel consisting of a set of relay channels associated to a probability measure. The current channel is a draw from its probability and in some cases arbitrary small error probability cannot be guaranteed for all channels in the set. In this paper, instead of finding the maximum achievable rate subject to a small error probability (EP) for all the channels in the set, we look at the asymptotic behavior of EP for a given rate. The notion of achievable EP is introduced as a novel performance measure for wireless relay channels. We can intuitively define it as the smallest EP that can be asymptotically achieved for a given rate. The behavior of EP is directly related to the epsilon-capacity of each channel in the set. It is shown that the behavior of EP is upper and lower bounded by the outage probability of a region which is referred to as the full error region. Then every code with a rate belonging to this region yields EP equal to one. Finally, new coding for oblivious cooperation simultaneously exploiting both Decode-and-Forward (DF) and Compress-and-Forward (CF) strategies is investigated. The Gaussian relay channel with slow fading is also discussed.\nTitle:\nOn the asymptotic error probability of composite relay channels.\n\nAbstract:\nThis paper investigates the potential gain of cooperation in large wireless networks with multiple sources and relays, where the nodes form an homogeneous Poisson point process. The source nodes may choose their nearest neighbor from the set of inactive nodes as their relay. Although cooperation can potentially lead to significant improvements on the asymptotic error probability of a communication pair, relaying causes additional interference in the network, increasing the average noise. We address the basic question: how should source nodes optimally balance cooperation vs. interference to guarantee reliability in all communication pairs. Based on the decode-and-forward (DF) scheme at the relays, we derive closed-form approximations to the upper bounds on the error probability, averaging over all node positions. Surprisingly, in the small outage probability regime, there is an almost binary behavior that dictates -depending on network parameters- the activation or not of all relay nodes.\nTitle:\nCooperation Versus Interference In Large Wireless Relay Networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.919, mean value for all the writers is 8.648, which makes it 1.084 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we derive a new outer and two inner bounds for a class of Interference Relay Channels (IRCs) which includes the Gaussian IRC as a special case. The inner bounds are based on the idea of rate-splitting at the sources with either decode-and-forward (DF) or compress-and-forward (CF) at the relay. Our main contribution lies in showing for a wide range of regimes of the Gaussian IRC that either the DF or the CF inner bound achieves the upper bound to within a constant gap regardless of the channel parameters.\nTitle:", "model_inf_time": 1.98}, {"id": "41283", "output": "View-Conditioned Causality for Explaining Unexpected Query Results", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nOutsourcing data streams and desired computations to a third party such as the cloud is a desirable option to many companies. However, data outsourcing and remote computations intrinsically raise issues of trust, making it crucial to verify results returned by third parties. In this context, we propose a novel solution to verify outsourced grouped aggregation queries (e.g., histogram or SQL Group-by queries) that are common in many business applications. We consider a setting where a data owner employs an untrusted remote server to run continuous grouped aggregation queries on a data stream it forwards to the server. Untrusted clients then query the server for results and efficiently verify correctness of the results by using a small and easy-to-compute signature provided by the data owner. Our work complements previous works on authenticating remote computation of selection and aggregation queries. The most important aspect of our solution is that it is publicly verifiable \u2014 unlike most prior works, we support untrusted clients (who can collude with other clients or with the server). Experimental results on real and synthetic data show that our solution is practical and efficient.\nTitle:\nPublicly verifiable grouped aggregation queries on outsourced data streams\n\nAbstract:\nIn a modern web application, a single high-level action like a mouse click triggers a flurry of asynchronous events on the client browser and remote web servers. We introduce Domino, a new tool which automatically captures and analyzes end-to-end, asynchronous causal relationship of events that span clients and servers. Using Domino, we found uncharacteristically long event chains in Bing Maps, discovered data races in the WinJS implementation of promises, and developed a new server-side scheduling algorithm for reducing the tail latency of server responses.\nTitle:\nDomino: understanding wide-area, asynchronous event causality in web applications\n\nAbstract:\nAd networks for mobile apps require inspection of the visual layout of their ads to detect certain types of placement frauds. Doing this manually is error prone, and does not scale to the sizes of today's app stores. In this paper, we design a system called DECAF to automatically discover various placement frauds scalably and effectively. DECAF uses automated app navigation, together with optimizations to scan through a large number of visual elements within a limited time. It also includes a framework for efficiently detecting whether ads within an app violate an extensible set of rules that govern ad placement and display. We have implemented DECAF for Windows-based mobile platforms, and applied it to 1,150 tablet apps and 50,000 phone apps in order to characterize the prevalence of ad frauds. DECAF has been used by the ad fraud team in Microsoft and has helped find many instances of ad frauds.\nTitle:\nDECAF: detecting and characterizing ad fraud in mobile apps\n\nAbstract:\nWebcams, microphones, pressure gauges and other sensors provide exciting new opportunities for querying and monitoring the physical world. In this paper we focus on querying wide area sensor databases, containing (XML) data derived from sensors spread over tens to thousands of miles. We present the first scalable system for executing XPATH queries on such databases. The system maintains the logical view of the data as a single XML document, while physically the data is fragmented across any number of host nodes. For scalability, sensor data is stored close to the sensors, but can be cached elsewhere as dictated by the queries. Our design enables self starting distributed queries that jump directly to the lowest common ancestor of the query result, dramatically reducing query response times. We present a novel query-evaluate gather technique (using XSLT) for detecting (1) which data in a local database fragment is part of the query result, and (2) how to gather the missing parts. We define partitioning and cache invariants that ensure that even partial matches on cached data are exploited and that correct answers are returned, despite our dynamic query-driven caching. Experimental results demonstrate that our techniques dramatically increase query throughputs and decrease query response times in wide area sensor databases.\nTitle:\nCache-and-query for wide area sensor databases\n\nAbstract:\nThe rise of smartphones equipped with various sensors has enabled personalization of various applications based on user contexts extracted from sensor readings. At the same time it has raised serious concerns about the privacy of user contexts. In this paper, we present MASKIT, a technique to filter a user context stream that provably preserves privacy. The filtered context stream can be released to applications or be used to answer their queries. Privacy is defined with respect to a set of sensitive contexts specified by the user. MASKIT limits what adversaries can learn from the filtered stream about the user being in a sensitive context - even if the adversaries are powerful and have knowledge about the filtering system and temporal correlations in the context stream. At the heart of MASKIT is a privacy check deciding whether to release or suppress the current user context. We present two novel privacy checks and explain how to choose the one with the higher utility for a user. Our experiments on real smartphone context traces of 91 users demonstrate the high utility of MASKIT.\nTitle:\nMaskIt: privately releasing user context streams for personalized mobile applications\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.222, mean value for all the writers is 8.648, which makes it 0.363 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA surprising query result is often an indication of errors in the query or the underlying data. Recent work suggests using causal reasoning to find explanations for the surprising result. In practice, however, one often has multiple queries and/or multiple answers, some of which may be considered correct and others unexpected. In this paper, we focus on determining the causes of a set of unexpected results, possibly conditioned on some prior knowledge of the correctness of another set of results. We call this problem View-Conditioned Causality. We adapt the definitions of causality and responsibility for the case of multiple answers/views and provide a non-trivial algorithm that reduces the problem of finding causes and their responsibility to a satisfiability problem that can be solved with existing tools. We evaluate both the accuracy and effectiveness of our approach on a real dataset of user-generated mobile device tracking data, and demonstrate that it can identify causes of error more effectively than static Boolean influence and alternative notions of causality.\nTitle:", "model_inf_time": 1.48}, {"id": "41284", "output": "A Control Plane for High-Quality, Scalable, and Responsive Live Video Delivery", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nLive video delivery is expected to reach a peak of 50 Tbps this year. This surging popularity is fundamentally changing the Internet video delivery landscape. CDNs must meet users' demands for fast join times, high bitrates, and low buffering ratios, while minimizing their own cost of delivery and responding to issues in real-time. Wide-area latency, loss, and failures, as well as varied workloads (\"mega-events\" to long-tail), make meeting these demands challenging.\n\nAn analysis of video sessions concluded that a centralized controller could improve user experience, but CDN systems have shied away from such designs due to the difficulty of quickly handling failures, a requirement of both operators and users. We introduce VDN, a practical approach to a video delivery network that uses a centralized algorithm for live video optimization. VDN provides CDN operators with real-time, fine-grained control. It does this in spite of challenges resulting from the wide-area (e.g., state inconsistency, partitions, failures) by using a hybrid centralized+distributed control plane, increasing average bitrate by 1.7x and decreasing cost by 2x in different scenarios.\n\n\nTitle:\nPractical, Real-time Centralized Control for CDN-based Live Video Delivery\n\nAbstract:\nVideo viewership over the Internet is rising rapidly, and market predictions suggest that video will comprise over 90\\% of Internet traffic in the next few years. At the same time, there have been signs that the Content Delivery Network (CDN) infrastructure is being stressed by ever-increasing amounts of video traffic. To meet these growing demands, the CDN infrastructure must be designed, provisioned and managed appropriately. Federated telco-CDNs and hybrid P2P-CDNs are two content delivery infrastructure designs that have gained significant industry attention recently. We observed several user access patterns that have important implications to these two designs in our unique dataset consisting of 30 million video sessions spanning around two months of video viewership from two large Internet video providers. These include partial interest in content, regional interests, temporal shift in peak load and patterns in evolution of interest. We analyze the impact of our findings on these two designs by performing a large scale measurement study. Surprisingly, we find significant amount of synchronous viewing behavior for Video On Demand (VOD) content, which makes hybrid P2P-CDN approach feasible for VOD and suggest new strategies for CDNs to reduce their infrastructure costs. We also find that federation can significantly reduce telco-CDN provisioning costs by as much as 95%.\nTitle:\nAnalyzing the potential benefits of CDN augmentation strategies for internet video workloads\n\nAbstract:\nOver the past few years video viewership over the Internet has risen dramatically and market predictions suggest that video will account for more than 50% of the traffic over the Internet in the next few years. Unfortunately, there has been signs that the Content Delivery Network (CDN) infrastructure is being stressed with the increasing video viewership load. Our goal in this paper is to provide a first step towards a principled understanding of how the content delivery infrastructure must be designed and provisioned to handle the increasing workload by analyzing video viewing behaviors and patterns in the wild. We analyze various viewing behaviors using a dataset consisting of over 30 million video sessions spanning two months of viewership from two large Internet video providers. In these preliminary results, we observe viewing patterns that have significant impact on the design of the video delivery infrastructure.\nTitle:\nUnderstanding internet video viewing behavior in the wild\n\nAbstract:\nImproving users' quality of experience (QoE) is crucial for sustaining the advertisement and subscription based revenue models that enable the growth of Internet video. Despite the rich literature on video and QoE measurement, our understanding of Internet video QoE is limited because of the shift from traditional methods of measuring video quality (e.g., Peak Signal-to-Noise Ratio) and user experience (e.g., opinion scores). These have been replaced by new quality metrics (e.g., rate of buffering, bitrate) and new engagement centric measures of user experience (e.g., viewing time and number of visits). The goal of this paper is to develop a predictive model of Internet video QoE. To this end, we identify two key requirements for the QoE model: (1) it has to be tied in to observable user engagement and (2) it should be actionable to guide practical system design decisions. Achieving this goal is challenging because the quality metrics are interdependent, they have complex and counter-intuitive relationships to engagement measures, and there are many external factors that confound the relationship between quality and engagement (e.g., type of video, user connectivity). To address these challenges, we present a data-driven approach to model the metric interdependencies and their complex relationships to engagement, and propose a systematic framework to identify and account for the confounding factors. We show that a delivery infrastructure that uses our proposed model to choose CDN and bitrates can achieve more than 20\\\\% improvement in overall user engagement compared to strawman approaches.\nTitle:\nDeveloping a predictive model of quality of experience for internet video\n\nAbstract:\nAn imminent challenge that content providers, CDNs, third-party analytics and optimization services, and video player designers in the Internet video ecosystem face is the lack of a single \"gold standard\" to evaluate different competing solutions. Existing techniques that describe the quality of the encoded signal or controlled studies to measure opinion scores do not translate directly into user experience at scale. Recent work shows that measurable performance metrics such as buffering, startup time, bitrate, and number of bitrate switches impact user experience. However, converting these observations into a quantitative quality-of-experience metric turns out to be challenging since these metrics are interrelated in complex and sometimes counter-intuitive ways, and their relationship to user experience can be unpredictable. To further complicate things, many confounding factors are introduced by the nature of the content itself (e.g., user interest, genre). We believe that the issue of interdependency can be addressed by casting this as a machine learning problem to build a suitable predictive model from empirical observations. We also show that setting up the problem based on domain-specific and measurement-driven insights can minimize the impact of the various confounding factors to improve the prediction performance.\nTitle:\nA quest for an Internet video quality-of-experience metric\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.381, mean value for all the writers is 8.648, which makes it 0.228 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nUser-created live video streaming is marking a fundamental shift in the workload of live video delivery. However, live-video-specific challenges and the viral nature of user-created content makes it difficult for current CDNs to deliver 1) high-quality, 2) highly-scalable, and 3) highly-responsive service. We present the design and implementation of VDN, a new control plane for CDNs designed to optimize the delivery of live streams within the CDN. VDN satisfies these requirements by using two approaches: 1) optimizing directly for video quality (not just throughput) and 2) combining centralized control with local control, allowing VDN to adapt to traffic dynamics and network failures at fine timescales.\nTitle:", "model_inf_time": 1.84}, {"id": "41285", "output": "Two-layered metadata service model and replica management in grid environment", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMetadata management is one of the key techniques in data grid. It is required to achieve two goals: high efficiency and availability. This paper presents a Replication Based Metadata Management System (RBMMS) as metadata server implemented in Global Distributed Storage System (GDSS). To address the above two goals RBMMS maintains a spares strongly connected graph to describe replica structure and relations among the replicas. The graph is used to propagate updating information and replica discovery in the process of replica addition and removal. Cache module is also implemented in RBMMS to further improve the performance of metadata access. The evaluation demonstrates that RBMMS attains high availability and efficiency of metadata management system.\nTitle:\nReplica based distributed metadata management in grid environment\n\nAbstract:\nData intensive task is becoming one of the most important applications in grid environment. The scale of data sets has been hundreds of terabytes and soon will be petabytes. The primary problem we face is how to organize the geographical distributed storage devices to support the collaborative operations on data in those resources. On the one hand, performance is critical to such application, but on the other hand the diverse network conditions prevent users from getting the same service quality. This paper focuses on how to resolve the above problem, and presents a 2-layered metadata service model in grid environment which utilizes the special locality of users\u2019 distribution and provides a platform for grid data management. We have implemented that 2-layered metadata service model in ChinaGrid Supporting Platform (CGSP) \u2013 the grid middleware for ChinaGrid project.\nTitle:\n2-layered metadata service model in grid environment\n\nAbstract:\nGrid information service influences quality of service for grid platforms directly. In this paper, based on network latency, network coordinate is introduced into grid information service to locate each grid node. By network coordinates, network latency between user and resource providers can be forecasted, and the result can be submitted to grid information service, which offers a list of resource providers with increasing of network latency. The proposal is suitable for time-sensitive applications and can enhance the ability of grid resource sharing and collaboration.\nTitle:\nGrid information service based on network latency\n\nAbstract:\nData grid is developed to facilitate sharing data and resources located in different parts of the world The major barrier to support fast data access in a data grid is the high latency of wide area networks and the Internet Data replication is adopted to improve data access performance When different sites hold replicas, there are significant benefits while selecting the best replica In this paper, we propose a new replica selection strategy based on classification techniques In this strategy the replica selection problem is regarded as a classification problem The data transfer history is utilized to help predicting the best site holding the replica The adoption of the switch mechanism of replica selection model avoids a waste of time for inaccurate classification results In this paper, we study and simulate KNN and SVM methods for different file access patterns and compare results with the traditional replica catalog model The results show that our replica selection model outperforms the traditional one for certain file access requests.\nTitle:\nUsing classification techniques to improve replica selection in data grid\n\nAbstract:\nChinaGrid is a nationwide grid project supported by the Chinese government that aims at integrating massive heterogeneous resources distributed on the China Education and Research Network (CERNET). The ChinaGrid Supporting Platform (CGSP) is a grid middleware developed for ChinaGrid. It provides a series of system-level services for ChinaGrid, helps to build application portals and integrate grid resources, and supports the secondary development of grid services. Data Management Services (DMS) is a group of system-level services in CGSP for managing storage and data resources, supporting transparent data access, and guaranteeing high-performance data transfer on the grid. It consists of metadata management service, storage resource management service, replication management service, storage agent and transfer client. It offers the fundamental support for data access on ChinaGrid. In this paper, we introduce the design principle and implementation of DMS and its transfer scheme.\nTitle:\nData management services and transfer scheme in ChinaGrid\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.16, mean value for all the writers is 8.648, which makes it 0.437 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe primary problem of data management in grid environment is how to effectively organise the geographical distributed storage devices to support the collaborative data operations. This paper focuses on how to resolve the above problem and presents a two-layered metadata service model in a grid environment which uses the locality of users' distribution and provides a platform for grid data management. It also presents a metadata replica model, which uses a sparse strongly connected graph to connect replica metadata. In this way it can obtain high performance and availability of metadata services in a grid environment.\nTitle:", "model_inf_time": 1.36}, {"id": "41286", "output": "EAPAC: An Application Placement Framework for Cloud Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nTraditional computing resource allocations in virtualization environment devote to provide fairness of resource distribution when the overall workload of host is heavy. That makes those allocations lack of efficiency under light workloads. To target this, we design and implement a lightweight resource allocation system, virtual resource allocation system (VRAS). Considering the fact that workloads can be balanced by migrating virtual machines to other hosts, we propose a request driven mechanism to focus on resource allocation under light workloads. We also present some allocation strategies used in VRAS to explain how it works on processor and memory resources. Our experiment results demonstrate that VRAS can result in throughput improvements of 28 % for RUBiS application, and the network overhead reduction of 81 %, comparing with the traditional allocation methods.\nTitle:\nVRAS: A Lightweight Local Resource Allocation System for Virtual Machine Monitor\n\nAbstract:\nVirtualization is a popular technology. Services and applications running on each virtual machine have to compete with each other for limited physical computer or network resources. Each virtual machine has different I/O requirement and special priority. Without proper scheduling resource management, a load surge in a virtual machine may inevitably degrade other's performance. In addition, each virtual machine may run different kinds of application, which have different disk bandwidth demands and service priorities. When assigning I/O resources, we should deal with each case on demand. In this paper, we propose a dynamic virtual machine disk bandwidth control mechanism in virtualization environment. A Disk Credit Algorithm is introduced to support a fine-gained disk bandwidth allocation mechanism among virtual machines. We can assign disk bandwidth according to each virtual machine's service priority/weight and its requirement. Related experiments show that the mechanism can improve the VMs' isolation and guarantee the performance of the specific virtual machine well.\nTitle:\nA disk bandwidth allocation mechanism with priority\n\nAbstract:\nThis paper presents a new resource allocation framework based on SLA (Service Level Agreements) classes for cloud computing environments. Our framework is proposed in the context of containers with two qualitative and two quantitative SLAs classes to meet the needs of users. The two qualitative classes represent the satisfaction time criterion, and the reputation criterion. Moreover, the two quantitative classes represent the criterion over the number of resources that must be allocated to execute a container and the redundancy (number of replicas) criterion. The novelty of our work is based on the possibility to adapt, dynamically, the scheduling and the resources allocation of containers according to the different qualitative and quantitative SLA classes and the activities peaks of the nodes in the cloud. This dynamic adaptation allows our framework a flexibility for efficient global scheduling of all submitted containers and for efficient management, on the fly, of the resources allocation. The key idea is to make the specification on resources demand less rigid and to ask the system to decide on the precise number of resources to allocate to a container. Our framework is implemented in C++ and it is evaluated using Docker containers inside the Grid'5000 testbed. Experimental results show that our framework gives expected results for our scenario and provides with good performance regarding the balance between objectives.\nTitle:\nA Resource Allocation Framework with Qualitative and Quantitative SLA Classes.\n\nAbstract:\nDue to its potential, the use of virtual machines in grid computing is attracting increasing attention. Most of the researches focus on how to create or destroy a virtual execution environments for different kinds of applications, the policy of managing the virtual environments isn't widely discussed. This paper proposes an adaptive and dependable virtual execution environment for grid computing called ADVE, which focuses on the policy of managing dependable virtual execution environments, such as when to create and destroy a new virtual execution execution environment, when to migrate applications from one virtual workspace to a new virtual workspace. We conduct experiments over a cluster to evaluate the performance of ADVE, and the experimental results show that ADVE can improve the throughput and the reliability of Grid resources with the adaptive management of virtual machines.\nTitle:\nADVE: adaptive and dependable virtual environments for grid computing\n\nAbstract:\nDue to its potential, using virtual machines in grid computing is attracting increasing attention. Most of the researches focus on how to create or destroy a virtual execution environments for different kinds of applications, while the policy of managing the virtual environments is not widely discussed. This paper proposes the design, implementation, and evaluation of an adaptive and dependable virtual execution environment for grid computing, ADVE, which focuses on the policy of managing virtual machines in grid environments. To build a dependable virtual execution environments for grid applications, ADVE provides an set of adaptive policies managing virtual machine, such as when to create and destroy a new virtual execution environment, when to migrate applications from one virtual execution environment to a new virtual execution environment. We conduct experiments over a cluster to evaluate the performance of ADVE, and the experimental results show that ADVE can improve the throughput and the reliability of grid resources with the adaptive management of virtual machines.\nTitle:\nAdapting grid computing environments dependable with virtual machines: design, implementation, and evaluations\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.551, mean value for all the writers is 8.648, which makes it 0.083 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA Cloud system, which is also often called a data center, may host a large number of application servers (termed as applications for short in this paper) that consume various types of resource, such as CPU, memory, and I/O. In such a scenario, the application placement strategy, which specifies how to host applications across the physical servers in the Cloud, is critical to achieve good system- and/or client-oriented performance. The existing application placement algorithms in literature assume that the consumption of system resources is proportional to the level of workloads submitted to the system. In this paper, we reveal that it may not be the case in some circumstances. Based on this observation, we design and implement an application placement framework, called EAPAC, for Clouds. The developed framework is able to achieve the optimized performance by judiciously allocating to the applications a proper mixture of different types of application request as well as an appropriate number of requests in each type. Further, we investigate the issue of resource conflicts among different applications when there exist concurrent requests in the system. We have conducted extensive experiments to evaluate the performance of the developed framework. The experiment results show that compared with the existing method in literature, EAPAC can improve the performance by 35% in terms of the reply rate.\nTitle:", "model_inf_time": 1.43}, {"id": "41287", "output": "CRP: A Heterogeneity Aware Clustering Protocol for Enhanced Network Stability in Sensor Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe lifetime of sensor network depends on the efficient utilization of resource-constrained sensor nodes. Several MAC protocols like DMAC and its variants have been proposed to save critical sensor resources through sleep-wakeup scheduling over data gathering tree. For applications where data aggregation is not possible, the sleep duration decreases gradually from the leaves to the root of the data gathering tree. This results early failure of sensor nodes near the sink, and affects network connectivity and coverage. Deploying redundant sensors can solve this problem where a faulty node is replaced by a redundant node to maintain network connectivity and coverage. However, the amount of redundancy depends on the node failure pattern, and thus more number of redundant nodes required to be deployed near the sink. This paper proposes a gradient based sensor deployment scheme for energy-efficient data gathering exploring the trade-off among connectivity, coverage, fault-tolerance and redundancy. The density of deployment is estimated based on the distance of a node from the sink while dealing with connectivity, coverage and fault-tolerance. The effectiveness of the proposed scheme has been analyzed both theoretically and with the help of simulation.\nTitle:\nExploring Gradient In Sensor Deployment Pattern For Data Gathering With Sleep Based Energy Saving\n\nAbstract:\nWireless sensor networks aim at a special kind of ad hoc networks, exposing an energy-constrained distributed computing environment. Proposed protocols have tried to incorporate power management schemas of the likes of reduced duty cycles and active synchronization. However this has led to a significant loss in the latency of data delivery. In this work we have introduced low latency medium access control (LLM), as a mechanism to deliver data with low latency without compromising the energy efficiency of the network. To this end we exploit the data aggregating properties of a sensor network and introduce a pre-notification packet to keep potential forwarding nodes aware of a forthcoming data packet. Our simulations show that LLM does perform as per expectations\nTitle:\nLLM:Low Latency MAC Protocol for Wireless Sensor Networks\n\nAbstract:\nSensor network deployed for the critical infrastructure monitoring requires high degree of reliability in sensory data gathering, in spite of arbitrary node or sink failures. This paper proposes a robust data gathering scheme specially designed to provide guaranteed delivery of the sensory data for applications on the critical infrastructure monitoring. Redundancy in a sensor network, in terms of both the number of deployed sensors and the amount of duplicate data delivery, is explored to design an effective protocol that ensures the reliable data delivery while assuring the timeliness, connectivity and the sensing coverage. A set of active sensors is selected from all the sensors deployed, based on the network connectivity and the sensing coverage criteria that participates in the data forwarding process. Rest of the sensors go to the sleep state, and act as a replacement on the failure of an active sensor. The proposed protocol aims to find out multiple node-disjoint paths to multiple sinks, so that the loss of connectivity in one path due to node failure does not disrupt application services. The effectiveness of the proposed scheme has been analyzed using simulation results, and compared with other protocols proposed in the literature for reliable data delivery.\nTitle:\nFault resilience in sensor networks: Distributed node-disjoint multi-path multi-sink forwarding\n\nAbstract:\nKey Predistribution Schemes (KPS) are efficient key management solutions that are well suited to establish lightweight symmetric keys even in resource starved environments, like low cost Internet of Things (IoT). This paper uses Chinese Remainder Theorem (CRT) to propose an energy efficient and deterministic KPS for distributed ad hoc networks, that we name as CRT-KPS. We theoretically establish the effectiveness of CRT-KPS in term of crucial metrics. Comparative study establishes that our proposals have better balance in overall performance as compared to state-of-the-art schemes and should find wide applications in IoT systems (specially for resource starved end devices).\nTitle:\nCrt-Kps: A Key Predistribution Schemes Using Crt\n\nAbstract:\nThe lifetime of a sensor network is influenced by the efficient utilization of the resource constrained sensor nodes. The tree-based data gathering offers good quality of service (QoS) for the running applications. However, data gathering at the sink reduces the network lifetime due to a fast failure of highly loaded nodes. Loss of connectivity and sensing coverage affect the performance of the applications that demand critical QoS. In this paper, a data gathering tree management scheme has been proposed to deal with arbitrary node failures in delay-sensitive sensor networks. A load-balanced distributed BFS tree construction procedure has been introduced for an efficient data gathering. Based on the initial tree construction, a tree maintenance scheme and an application message handler have been designed to ensure the reliable delivery of the application messages. The correctness of the proposed scheme has been verified both theoretically and with the help of simulation. The proposed scheme offers low overhead, enhanced network lifetime and good QoS in terms of delay and reliability of the application messages.\nTitle:\nTopology Management Ensuring Reliability in Delay Sensitive Sensor Networks with Arbitrary Node Failures.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.893, mean value for all the writers is 8.648, which makes it 1.915 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSensor nodes in a network consume energy in a non-uniform manner. Designing clustering protocols that are heterogeneity aware is still an open issue in sensor networks. This work focuses on hierarchically clustered heterogeneous sensor networks. Sensor nodes organize themselves in self organized groups called clusters. Each cluster consists of a cluster head and some member nodes. The sensor network consists of nodes of two different energy levels. We show that current heterogeneity aware protocols (such as SEP [1], DEEC [2] etc.) are unable to distribute the usage of energy amongst the sensor nodes uniformly. We propose CRP, Cluster Re-election Protocol, that is heterogeneity aware and enhances the network stability period (period before which the first node dies) over current protocols. We show, by simulation, that CRP improves network stability by reducing the variation in the residual energy of sensor nodes. Therefore. CRP is able to exploit the heterogeneity present in a sensor network in an efficient manner.\nTitle:", "model_inf_time": 1.63}, {"id": "41288", "output": "The \u03b1-graph as a boundary definition for sparse point sets", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe study the shape of a finite point set in Ill 2, where the points are not bound to a regular grid like Z 2. The shape of a connected point set in IR 2 is captured by its boundary. For a finite point set the boundary is a directed graph that connects points identified as boundary points. We argue that to serve as a proper boundary definition the directed graph should regulate scale, be minimal, have an increasing interior and be consistent with the boundary definition of connected objects. We propose to use the directed variant of the a-shape as defined by Edelsbrunner et al (1983), which we call the a-graph. The a-graph is based on a generalization of the convex hull. The computational aspects of the a-graph have been extensively studied, but little attention has been paid to the potential use of the a-graph as a shape descriptor or a boundary definition. In this paper we prove that the oz-graph satisfies the aforementioned criteria. We also prove a relation between the a-graph and the opening scale space from mathematical morphology. In fact, the a-hull provides a generalization of this scale space.\nTitle:\nShape of an arbitrary finite point set in IR2\n\nAbstract:\nIn many applications experts need to make decisions based on the analysis of multi-dimensional data. Various classification models can support the decision making process. To obtain an intuitive understanding of the classification model, interactive visualizations are essential. We argue that this is best done by a series of interactive 2D scatterplots. In this paper, we define a set of characteristics of the multi-dimensional classification model that have to be visually represented in those scatterplots. Our proposed method presents those characteristics in a uniform manner for both linear and non-linear classification methods. We combine a visualization of a Voronoi based representation of multi-dimensional decision boundaries with visualization of the distances of the data elements to these boundaries. To allow the developer of the model to refine the threshold of the classification model and instantly observe the results, we use interactive decision point selection on a performance curve. Finally, we show how the combination of those techniques allows exploration of multi-dimensional decision boundaries in 2D.\nTitle:\nVisualizing multi-dimensional decision boundaries in 2D\n\nAbstract:\nWe present a software tool for the analysis of 3D-line shaped objects in a 3D grey valued image. We discuss the extraction of the object from the image, the measurement of its shape features and its display.\nTitle:\nMeasurement of 3D-line shaped objects\n\nAbstract:\nSegmentation of (noisy) images containing a complex ensemble of objects is difficult to achieve on the basis of local image information only. It is advantageous to attack the problem of object boundary extraction by a model-based segmentation procedure. Segmentation is achieved by tuning the parameters of the geometrical model in such a way that the boundary template locates and describes the object in the image in an optimal way. The optimality of the solution is based on an objective function taking into account image information as well as the shape of the template. Objective functions in literature are mainly based on the gradient magnitude and a measure describing the smoothness of the template. In this contribution, we propose a new image objective function based on directional gradient information derived from Gaussian smoothed derivatives of the image data. The proposed method is designed to accurately locate an object boundary even in the case of a conflicting object positioned close to the object of interest. We further introduce a new smoothness objective to ensure the physical feasibility of the contour. The method is evaluated on artificial data. Results on real medical images show that the method is very effective in accurately locating object boundaries in very complex images.\nTitle:\nParameterized feasible boundaries in gradient vector fields\n\nAbstract:\nIn this contribution we introduce a new model-free method for object tracking. The tracking is posed as a seg- mentation problem which we solve using the watershed al- gorithm. A framework is defined to compute the required to- pographic surface from distances to the predicted contour, intensity edges and motion edges. This multifeature tracking approach yields accurate re- sults in the presence of object corners, image clutter, and camera motion. Results on real sequences confirm the stability and ro- bustness of the method. Objects are tracked over long se- quences and in the presence of fast object motion.\nTitle:\nMultifeature object tracking using a model-free approach\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.898, mean value for all the writers is 8.648, which makes it 0.64 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nPresents the shape of a sparse point set S in R2 . A crucial step in finding the shape of a sparse point set is the definition of its boundary. This boundary is a graph indicating a relation among the elements of S. No well defined definition of such a boundary is found in literature. For continuous point sets this problem does not exist as the boundary has a unique definition. The authors pose general criteria a boundary definition should satisfy and show that the \u03b1-graph satisfies those criteria. The boundary is a function of the scale parameter \u03b1. The authors further show that the \u03b1-graph has a strong relation with mathematical morphology. As an application the use of the \u03b1-graph in the multi-scale recognition of industrial objects is shown\nTitle:", "model_inf_time": 1.48}, {"id": "41289", "output": "A Multi-Agent Based Healthcare Support System in Ubiquitous Computing Environment", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe propose a multi-agent-based healthcare support system in ubiquitous computing environment. By utilizing knowledge about healthcare and various information including vital sign, physical location, and video data of a user under observation from real space, the system provides useful information regarding health condition effectively and in user-oriented manner. This paper describes a user-oriented healthcare support system based on concept of symbiotic computing, focusing on design and initial prototype implementation of the system.\nTitle:\nUser-oriented healthcare support system based on symbiotic computing\n\nAbstract:\nWe have been investigating a symbiotic healthcare support system in ubiquitous computing environment. By using knowledge about healthcare and various kinds of information including vital sign, location information, and multimedia data of multiple object persons under observation of real world, the system provides useful information regarding health condition effectively and in user-oriented manner. This paper focuses on effective and real-time service provisioning using the information. The data and information including vital sign, location information, environmental information, multimedia data, specialized knowledge, etc. contain significant diverse aspects in both quantitative and qualitative. By using existing inference mechanisms, we cannot cope with these kinds of information and knowledge in real-time. In this work, we present an effective inference mechanism combining various kinds of sensor data and huge amount of knowledge on healthcare for providing healthcare information and services in real time.\nTitle:\nAn effective inference method using sensor data for symbiotic healthcare support system\n\nAbstract:\nThere is a steady increase of number of people who are suffering from lifestyle-related diseases. Although much work has been done on healthcare support system, these existing systems are limited in ability of healthcare support service. This paper proposes an agent-based framework for advanced healthcare support system. In order to provide useful information for healthcare of an object person, not only to him/herself but also to the related people of that person, the system needs to acquire variety of information, knowledge, data, etc. and store/manage them in a systematic manner. This paper mainly focuses on the concept and design of the system, and also we describe the implementation details of several functions for the healthcare.\nTitle:\nAn Agent-Based Framework for Healthcare Support System\n\nAbstract:\nThe agent-based systems have been designed and developed using the latest agent technologies. However, the design and the debugging of these systems contain some difficult problems due to the situational and nondeterministic nature of the agents, and the effective design-supporting technologies have not been proposed so far. In order to make efficient design process of agent system, we propose an interactive development method of agent system based on the agent-repository-based multiagent framework which focuses on an essential feature of agent system design, i.e., the reuse of existing agents stored in the agent repository. In this paper, we propose an Interactive Design Environment of Agent system called IDEA and demonstrate the effectiveness of the proposed method.\nTitle:\nDesign and implementation of interactive design environment of agent system\n\nAbstract:\nIn this paper, we propose an agent-based mobile Ad-hoc service construction system named AMACS, which is an agent based middleware and improves usability of user communication on Ad-hoc networks. AMACS exchanges user information among computers and makes users possible to use various communication services by only selecting target user\u00fds name and service name. In this paper, we describe agent-oriented service composition method of AMACS, and show detailed design and implementation. Finally, through experiments using prototype system, we show effectiveness of AMACS.\nTitle:\nAn Agent-Based Middleware for Communication Service on Ad Hoc Network\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.58, mean value for all the writers is 8.648, which makes it 0.795 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents an advanced healthcare support system in ubiquitous computing environment. By utilizing knowledge about healthcare and various information including vital sign, physical location, and video data of a user under observation from real space, the system provides useful information regarding health condition effectively and in user-oriented manner. In this paper, we describe a user-oriented healthcare support system focusing on design and implementation of the system with multi-agent technology.\nTitle:", "model_inf_time": 1.36}, {"id": "41290", "output": "Regular Languages and Morphisms", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe consider a set of natural operations on languages, and prove that the orbit of any language L under the monoid generated by this set is finite and bounded, independently of L. This generalizes previous results about complement, Kleene closure, and positive closure.\nTitle:\nFinite orbits of language operations\n\nAbstract:\nAnswering two problems formulated by Marcus and Paun, we prove that it is decidable whether or not a context-free language can be written as the set of all finite prefixes of an infinite word and that it is decidable whether or not a regular language can be written as the set of all finite subwords of an infinite word.\nTitle:\nLanguages obtained from infinite words\n\nAbstract:\nRegular languages have many different characterizations in terms of automata, congruences, semigroups etc. We have a look at some more recent results, obtained mostly during the last two decades, namely characterizations using morphic compositions, equality sets and well orderings.\nTitle:\nCharacterizations of Regularity\n\nAbstract:\nLet M be a class of automata (in a precise sense to be defined) and Mc the class obtained by augmenting each automaton in M with finitely many reversal-bounded counters. We show that if the languages defined by M are effectively semilinear, then so are the languages defined by Mc, and, hence, their emptiness problem is decidable. We give examples of how this result can be used to show the decidability of certain problems concerning the equivalence of morphisms on languages. We also prove a surprising undecidability result for commutation of languages: given a fixed two-element code K, it is undecidable whether a given context-free language L commutes with K, i.e., LK=KL.\nTitle:\nSome Decision Problems Concerning Semilinearity and Commutation\n\nAbstract:\nWe consider a set of eight natural operations on formal languages Kleene closure, positive closure, complement, prefix, suffix, factor, subword, and reversal, and compositions of them. If x and y are compositions, we say x is equivalent to y if they have the same effect on all languages L. We prove that the number of equivalence classes of these eight operations is finite. This implies that the orbit of any language L under the elements of the monoid is finite and bounded, independent of L. This generalizes previous results about complement, Kleene closure, and positive closure. We also estimate the number of distinct languages generated by various subsets of these operations.\nTitle:\nComposition and orbits of language operations: finiteness and upper bounds\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 6.981, mean value for all the writers is 8.648, which makes it 1.422 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe prove that for any regular language L 0 , Reg \u2260 H \u22121 H (L 0 ) , where Reg denotes the family of all regular languages and H (resp. H \u22121 ) denotes the family of all morphisms (resp. inverse morphisms).\nTitle:", "model_inf_time": 0.84}, {"id": "41291", "output": "Finitary Two-Way Finite-State Transducers and MSO Definability", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDeterministic two-way finite state transductions are exactly the mso definable string transductions. Nondeterministic mso definable string transductions equal compositions of nondeterministic two-way finite state transductions that have the finite visit property. Both families of mso definable string transductions are characterized in terms of Hennie machines.\nTitle:\nTwo-Way Finite State Transducers and Monadic Second-Order Logic\n\nAbstract:\nWe extend a classic result of B\u00fcchi, Elgot, and Trakhtenbrot: MSO definable string transductions i.e., string-to-string functions that are definable by an interpretation using monadic second-order (MSO) logic, are exactly those realized by deterministic two-way finite-state transducers, i.e., finite-state automata with a two-way input tape and a one-way output tape. Consequently, the equivalence of two mso definable string transductions is decidable. In the nondeterministic case however, MSO definable string tranductions, i.e., binary relations on strings that are mso definable by an interpretation with parameters, are incomparable to those realized by nondeterministic two-way finite-state transducers. This is a motivation to look for another machine model, and we show that both classes of MSO definable string transductions are characterized in terms of Hennie machines, i.e., two-way finite-state transducers that are allowed to rewrite their input tape, but may visit each position of their input only a bounded number of times.\nTitle:\nMSO definable string transductions and two-way finite-state transducers\n\nAbstract:\nLet  be the identity relation on the monoid A\n*. It is shown that the problem  is undecidable for rational transductions . On the other hand, the problems  and = are easily seen to be decidable.\nTitle:\nIdentities and Transductions\n\nAbstract:\nThe forbidding and enforcing paradigm was introduced by Ehrenfeucht and Rozenberg as a way to define families of languages based on two sets of boundary conditions. Later, a variant of this paradigm was considered where an fe-system defines a single language. We investigate this variant further by studying fe-systems in which both the forbidding and enforcing sets are finite and show that they define regular languages. We prove that the class of languages defined by finite fe-systems is strictly between the strictly locally testable languages and the class of locally testable languages.\nTitle:\nFinite Language Forbidding-Enforcing Systems\n\nAbstract:\nThe pebble tree automaton and the pebble tree transducer are enhanced by additionally allowing an unbounded number of \"invisible\" pebbles (as opposed to the usual (\"visible\" ones). The resulting pebble tree automata recognize the regular tree languages (i.e., can validate all generalized DTD's) and hence can find all matches of MSO definable n-ary patterns. Moreover, when viewed as a navigational device, they lead to an XPath-like formalism that has a path expression for every MSO definable binary pattern. The resulting pebbletree transducers can apply arbitrary MSO definable tests to (the observable part of) their configurations, they (still) have a decidable typechecking problem, and they can model the recursion mechanism of XSLT. The time complexity ofthe typechecking problem for conjunctive queries that use MSO definable binary patterns can often be reduced through the use of invisible pebbles.\nTitle:\nXML transformation by tree-walking transducers with invisible pebbles\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 6.903, mean value for all the writers is 8.648, which makes it 1.489 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe hierarchy of arbitrary compositions of two-way nondeterministic finite-state transductions collapses when restricted to finitary transductions, i.e., transductions that produce a finite set of outputs for each input. The hierarchy collapses to the class of nondeterministic MSO definable transductions, which is inside the second level of that hierarchy. It is decidable whether a composition of two-way nondeterministic finite-state transducers realizes a finitary transduction (i.e., is MSO definable).\nTitle:", "model_inf_time": 1.51}, {"id": "41292", "output": "Crossroads: Time-Sensitive Programming for Efficient Autonomous Intersection Management", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nUtilizing intelligent transportation infrastructures can significantly improve the throughput of intersections of Connected Autonomous Vehicles (CAV), where an Intersection Manager (IM) assigns a target velocity to incoming CAVs in order to achieve a high throughput. Since the IM calculates the assigned velocity for a CAV based on the model of the CAV, it's vulnerable to model mismatches and possible external disturbances. As a result, IM must consider a large safety buffer around all CAVs to ensure a safe scheduling, which greatly degrades the throughput. In addition, IM has to assign a relatively lower speed to CAVs that intend to make a turn at the intersection to avoid rollover. This issue reduces the throughput of the intersection even more. In this paper, we propose a space and time-aware technique to manage intersections of CAVs that is robust against external disturbances and model mismatches. In our method, RIM, IM is responsible for assigning a safe Time of Arrival (TOA) and Velocity of Arrival (VOA) to an approaching CAV such that trajectories of CAVs before and inside the intersection does not conflict. Accordingly, CAVs are responsible for determining and tracking an optimal trajectory to reach the intersection at the assigned TOA while driving at VOA. Since CAVs track a position trajectory, the effect of bounded model mismatch and external disturbances can be compensated. In addition, CAVs that intend to make a turn at the intersection do not need to drive at a slow velocity before entering the intersection. Results from conducting experiments on a 1/10 scale intersection of CAVs show that RIM can reduce the position error at the expected TOA by 18X on average in presence of up to 10% model mismatch and an external disturbance with an amplitude of 5% of max range. In total, our technique can achieve 2.7X better throughput on average compared to velocity assignment techniques.\nTitle:\nRIM: Robust Intersection Management for Connected Autonomous Vehicles\n\nAbstract:\nReducing device dimensions, increasing transistor densities, and smaller timing windows, expose the vulnerability of processors to soft errors induced by charge carrying particles. Since these factors are only consequences of the inevitable advancement in processor technology, the industry has been forced to improve reliability on general purpose chip multiprocessors (CMPs). With the availability of increased hardware resources, redundancy-based techniques are the most promising methods to eradicate soft-error failures in CMP systems. In this work, we propose a novel customizable and redundant CMP architecture (UnSync) that utilizes hardware-based detection mechanisms (most of which are readily available in the processor), to reduce overheads during error-free executions. In the presence of errors (which are infrequent), the always forward execution enabled recovery mechanism provides for resilience in the system. The inherent nature of our architecture framework supports customization of the redundancy, and thereby provides means to achieve possible performance-reliability tradeoffs in many-core systems. We provide a redundancy-based soft-error resilient CMP architecture for both write-through and write-back cache configurations. We design a detailed RTL model of our UnSync architecture and perform hardware synthesis to compare the hardware (power/area) overheads incurred. We compare the same with those of the Reunion technique, a state-of-the-art redundant multicore architecture. We also perform cycle-accurate simulations over a wide range of SPEC2000, and MiBench benchmarks to evaluate the performance efficiency achieved over that of the Reunion architecture. Experimental results show that, our UnSync architecture reduces power consumption by 34.5 percent and improves performance by up to 20 percent with 13.3 percent less area overhead, when compared to the Reunion architecture for the same level of reliability achieved.\nTitle:\nUnSync-CMP: Multicore CMP Architecture for Energy-Efficient Soft-Error Reliability\n\nAbstract:\nSoftware-managed architectures, which use scratch-pad memories (SPMs), are a promising alternative to cached-based architectures for multicores. SPMs provide scalability but require explicit management. For example, to use an instruction SPM, explicit management code needs to be inserted around every call site to load functions to the SPM. such management code would check the state of the SPM and perform loading operations if necessary, which can cause considerable overhead at runtime. In this paper, we propose a compiler-based approach to reduce this overhead by identifying management code that can be removed or simplified. Our experiments with various benchmarks show that our approach reduces the execution time by 14% on average. In addition, compared to hardware caching, using our approach on an SPM-based architecture can reduce the execution times of the benchmarks by up to 15%.\nTitle:\nReducing code management overhead in software-managed multicores.\n\nAbstract:\nPublic transport in suburban cities (covers 80% of the urban landscape) of developing regions suffer from the lack of information in Google Transit, unpredictable travel times, chaotic schedules, absence of information board inside the vehicle. Consequently, passengers suffer from lack of information about the exact location where the bus is at present as well as the estimated time to be taken to reach the desired destination. We find that off-the-shelf deployment of existing (non-GPS) localization schemes exhibit high error due to sparsity of stable and structured outdoor landmarks (anchor points). Through rigorous experiments conducted over a month however, we realize that there are a certain class of volatile landmarks which may be useful in developing efficient localization scheme. Consequently, in this paper, we design a novel generalized energy-efficient outdoor localization scheme - UrbanEye, which efficiently combines the volatile and non-volatile landmarks using a specialized data structure, the probabilistic timed automata. UrbanEye uses speed-breakers, turns and stops as landmarks, estimates the travel time with a mean accuracy of \u00b12.5 mins and produces a mean localization accuracy of 50 m. Results from several runs taken in two cities, Durgapur and Kharagpur, reveal that UrbanEye provides more than 50% better localization accuracy compared to the existing system Dejavu [1], and consumes significantly less energy.\nTitle:\nUrbanEye: An outdoor localization system for public transport\n\nAbstract:\nScratchpad-memory (SPM) based memory hierarchy is a promising alternative to cache-based memory hierarchies, due to the difficulty in scaling caches to processors with high core count. However, explicit data management in software is required on SPM-based memory hierarchies. This paper focuses on optimizing the stack data management on SPM-based multicore processors, as memory accesses to call stack present in most applications. While previous works have developed techniques to enable correct stack pointer management, they have not optimized it. As a result, existing techniques still incur high overhead. This paper proposes an automated compiler-based scheme for efficient pointer management. Our experiments on MiBench benchmarks demonstrate that our scheme almost completely eliminates pointer management overhead. As a result, as compared to the state-of-the-art approach, our approach reduces the average execution time of the benchmarks by 52%. Furthermore, with our approach, the performance of stack management on SPM becomes better than hardware caches on average even with conservative estimates.\nTitle:\nEfficient pointer management of stack data for software managed multicores\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.613, mean value for all the writers is 8.648, which makes it 0.823 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nFor autonomous vehicles, intelligent autonomous intersection management will be required for safe and efficient operation. In order to achieve safe operation despite uncertainties in vehicle trajectory, intersection management techniques must consider a safety buffer around the vehicles. For truly safe operation, an extra buffer space should be added to account for the network and computational delay caused by communication with the Intersection Manager (IM). However, modeling the worst-case computation and network delay as additional buffer around the vehicle degrades the throughput of the intersection. To avoid this problem, AIM[1], a popular state-of-the-art IM, adopts a query-based approach in which the vehicle requests to enter at a certain arrival time dictated by its current velocity and distance to the intersection, and the IM replies yes/no. Although this solution does not degrade the position uncertainty, it ultimately results in poor intersection throughput. We present Crossroads, a time-sensitive programming method to program the interface of a vehicle and the IM. Without requiring additional buffer to account for the effect of network and computational delay, Crossroads enables efficient intersection management. Test results on a 1/10 scale model of intersection using TRAXXAS RC cars demonstrates that our Crossroads approach obviates the need for large buffers to accommodate for the network and computation delay, and can reduce the average wait time for the vehicles at a single-lane intersection by 24%. To compare Crossroads with previous approaches, we perform extensive Matlab simulations, and find that Crossroads achieves on average 1.62X higher throughput than a simple VT-IM with extra safety buffer, and 1.36X better than AIM.\nTitle:", "model_inf_time": 1.8}, {"id": "41293", "output": "Profiling-Based Scratch Pad Memory Optimization for Irregular Access Patterns in Embedded Applications", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nExploiting runtime memory access traces can be a complementary approach to compiler optimizations for the energy reduction in memory hierarchy. This is particularly important for emerging multimedia applications since they usually have input-sensitive runtime behavior which results in dynamic and/or irregular memory access patterns. These types of applications are normally hard to optimize by static compiler optimizations. The reason is that their behavior stays unknown until runtime and may even change during computation. To tackle this problem, we propose an integrated approach of software [compiler and operating system (OS)] and hardware (data access record table) techniques to exploit data reusability of multimedia applications in Multiprocessor Systems on Chip. Guided by compiler analysis for generating scratch pad data layouts and hardware components for tracking dynamic memory accesses, the scratch pad data layout adapts to an input data pattern with the help of a runtime scratch pad memory manager incorporated in the OS. The runtime data placement strategy presented in this paper provides efficient scratch pad utilization for the dynamic applications. The goal is to minimize the amount of accesses to the main memory over the entire runtime of the system, which leads to a reduction in the energy consumption of the system. Our experimental results show that our approach is able to significantly improve the energy consumption of multimedia applications with dynamic memory access behavior over an existing compiler technique and an alternative hardware technique.\nTitle:\nAdaptive Scratch Pad Memory Management for Dynamic Behavior of Multimedia Applications\n\nAbstract:\nCoarse-grained reconfigurable architecture aims to achieve both performance and flexibility. However, power consumption is no less important for the reconfigurable architecture to be used as a competitive processing core in embedded systems. In this paper, we show how power is consumed in a typical coarse-grained reconfigurable architecture. Based on the power breakdown data, we suggest a power-conscious configuration cache structure and code mapping technique, which reduce power consumption without performance degradation. Experimental results show that the proposed approach saves much power even with reduced configuration cache size\nTitle:\nPower-Conscious Configuration Cache Structure and Code Mapping for Coarse-Grained Reconfigurable Architecture\n\nAbstract:\nMany embedded processors have complex, irregular architectures resulting from the customization for the maximum performance and energy efficiency of target applications. One such example is the heterogeneous register architecture, which has fast, small-sized register files, for their specific uses, distributed over the data paths between different functional units. Although this architectural design may be good at achieving the H/W design goal of high speed, small area and low power, it requires highly expensive algorithms for optimal code generation. This is primarily because multiple registers contained in each file come with many different constraints subject to their design purposes, and often their names are aliased with each other; thus the final code quality is very sensitive to how properly such aliased, heterogeneous registers are utilized in every instruction. In this work, we propose a code generation approach to attack this complex problem. The experiments reveal that our approach is fast, practically running in polynomial time. In comparison with the related work, it achieves approximately 13% of code size reduction and 16% of speed increase.\nTitle:\nFast Code Generation for Embedded Processors with Aliased Heterogeneous Registers\n\nAbstract:\nRecently coarse-grained reconfigurable architectures (CGRAs) have drawn increasing attention due to their efficiency and flexibility. While many CGRAs have demonstrated impressive performance improvements, the effectiveness of CGRA platforms ultimately hinges on the compiler. Existing CGRA compilers do not model the details of the CGRA architecture, due to which they are, i) unable to map applications, even though a mapping exists, and ii) use too many PEs to map an application. In this paper, we model several CGRA details in our compiler and develop a graph mapping based approach (SPKM) for mapping applications onto CGRAs. On randomly generated graphs our technique can map on average 4.5X more applications than the previous approaches, while using fewer CGRA rows 62% times, without any penalty in mapping time. We observe similar results on a suite of benchmarks collected from Livermore Loops, Multimedia and DSPStone benchmarks.\n\n\nTitle:\nSPKM: a novel graph drawing based algorithm for application mapping onto coarse-grained reconfigurable architectures\n\nAbstract:\nCoarse-grained reconfigurable architectures (CGRAs) promise high performance at high power efficiency. They fulfil this promise by keeping the hardware extremely simple, and moving the complexity to application mapping. One major challenge comes in the form of data mapping. For reasons of power-efficiency and complexity, CGRAs use multibank local memory, and a row of PEs share memory access. In order for each row of the PEs to access any memory bank, there is a hardware arbiter between the memory requests generated by the PEs and the banks of the local memory. However, a fundamental restriction remains in that a bank cannot be accessed by two different PEs at the same time. We propose to meet this challenge by mapping application operations onto PEs and data into memory banks in a way that avoids such conflicts. To further improve performance on multibank memories, we propose a compiler optimization for CGRA mapping to reduce the number of memory operations by exploiting data reuse. Our experimental results on kernels from multimedia benchmarks demonstrate that our local memory-aware compilation approach can generate mappings that are up to 53&percnt; better in performance (26&percnt; on average) compared to a memory-unaware scheduler.\nTitle:\nMemory access optimization in compilation for coarse-grained reconfigurable architectures\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.042, mean value for all the writers is 8.648, which makes it 0.336 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMany embedded array-intensive applications have irregular access patterns that are not amenable to static analysis for extraction of access patterns, and thus prevent efficient use of a Scratch Pad Memory (SPM) hierarchy for performance and power improvement. We present a profiling based strategy that generates a memory access trace which can be used to identify data elements with fine granularity that can profitably be placed in the SPMs to maximize performance and energy gains. We developed an entire toolchain that allows incorporation of the code required to profitably move data to SPMs; visualization of the extracted access pattern after profiling; and evaluation/exploration of the generated application code to steer mapping of data to the SPM to yield performance and energy benefits.We present a heuristic approach that efficiently exploits the SPM using the profiler-driven access pattern behaviors. Experimental results on EEMBC and other industrial codes obtained with our framework show that we are able to achieve 36% energy reduction and reduce execution time by up to 22% compared to a cache based system.\nTitle:", "model_inf_time": 1.73}, {"id": "41294", "output": "Towards a Standardized Description and Evaluation of HCI Patterns for MBUID", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDespite intense research activities in the last years, HCI patterns still lack in a standardized description and organization. This makes it difficult for the developers to identify the relevant patterns for solving a problem as well as to apply them accordingly to the problem context. To fully benefit from HCI patterns within the engineering of interactive computer systems they have to be prepared for integration into a model-based user interface development process. This workshop on Pattern-driven Engineering of Interactive Computer Systems (PEICS) focuses on bringing together various research approaches in order to be conjointly conductive to the state of the art. We present contributions according to semantics, formalization, languages, support, research directions as well as tools.\nTitle:\nPEICS: towards HCI patterns into engineering of interactive systems\n\nAbstract:\nThis paper depicts potentialities of formal HCI pattern specifications with regard to facilitate the semi-automated generation of user interfaces for interactive applications. In a first step existing proven and well accepted techniques in the field of model-based user interface development are highlighted and briefly reviewed. Subsequently it is discussed how we combine model-based and pattern-oriented methods within our user interface modeling and development framework in order to partly enable automated user interface generation. In this context a concrete pattern definition approach is introduced and illustrated with tangible examples from the domain of interactive knowledge sharing applications.\nTitle:\nFormal pattern specifications to facilitate semi-automated user interface generation\n\nAbstract:\nIn this paper we demonstrate how patterns can act as a driving force for the development of interactive applications. As knowledge re-use is becoming more and more crucial, patterns can be an effective tool to represent knowledge of the HCI domain. Using a model-based development methodology, it is shown how patterns can act as building blocks for the establishment of these models. Starting from outlining the general process of pattern application, we discuss how and which patterns are suitable for several models. In particular we discuss the application and use of patterns for the task, dialog and presentation models. Furthermore, we suggest an interface for patterns using \"generic classes\" and give concrete examples to corroborate our approach. This allows for modular patterns reuse and plausible parameter exchange with the underlying system. Tool support is based on XML-representations of patterns using a template engine.\nTitle:\nPatterns In Model-Based Engineering\n\nAbstract:\nOne of the main indicators concerning the usability of an application is the corresponding level of accessibility provided by this application. Although a lot of work has been done in the software engineering domain, the accessibility problem has not been enough tackled in the HCI area. In this paper we present an idea to resolve the user-related accessibility problems since the modeling stage of smart environment applications while being assisted by patterns. The proposed idea is to provide two generic patterns used for any accessibility modeling problem, and additionally the creation of two pattern libraries presenting concrete solutions for the most common user-based accessibility problems.\nTitle:\nUser-oriented accessibility patterns for smart environments\n\nAbstract:\nThe development of interactive software typically requires the combined skills of software developers, HCI, platform and marketing specialists in order to create applications with good software quality, usability, and user experience. The combination of model-based user interface development practices with pattern-based approaches that specify HCI- and software-patterns in a formalized way and respect emerging standards, has the potential to facilitate and automate the software development process, reduce development costs, and provide solutions that can easily be adapted to varying contexts of use. To satisfy these goals we have developed a framework for Pattern-based Modeling and Generation of Interactive Systems PaMGIS. The paper at hand describes the topical version of the PaMGIS Pattern Specification Language PPSL which is designed to accomplish both, capturing all information required to support model-based user interface generation and offering the highest level of compatibility to existing pattern description languages.\nTitle:\nA Unified Pattern Specification Formalism to Support User Interface Generation.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.709, mean value for all the writers is 8.648, which makes it 0.052 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSince almost one decade HCI pattern languages are one popular form of design knowledge representations which can be used to facilitate the exchange of best practices, knowledge and design experience between the interdisciplinary team members and allow the formalization of different user interface aspects. Since patterns usually describe the rational in which context they should be applied (when), why a certain pattern should be used in a specific use context (why) and how to implement the solution part (how) they are suitable to describe different user interface aspects in a constructive way. But despite intense research activities in the last years, HCI pattern languages still lack in a lingua franca, a common language for the standardized description and organization of the pattern. This makes it difficult to design suitable tools that support the developers in applying HCI patterns in model-based user interface development (MBUID) processes. To enable the constructive use of HCI patterns in the model-based development process the informal textual, or graphical notation of HCI patterns has to be overcome. Besides that, evaluating the effectiveness of a pattern, i.e. when is a pattern a 'good' pattern is an important issue that has to be tackled to fully benefit from HCI patterns and to improve their applicability in future design processes.\nTitle:", "model_inf_time": 1.52}, {"id": "41295", "output": "Generalized Interfaces in JavaGI", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe abilities to extend a software module and to integrate a software module into an existing software system without changing existing source code are fundamental challenges in software engineering and programming-language design. We reconsider these challenges at the level of language expressiveness, by using the language concept of type classes, as it is available in the functional programming language Haskell. A detailed comparison with related work shows that type classes provide a powerful framework in which solutions to known software extension and integration problems can be provided. We also pinpoint several limitations of type classes in this context.\nTitle:\nSoftware extension and integration with type classes\n\nAbstract:\nThe 'Scrap your boilerplate' approach to generic programming allows the programmer to write generic functions that can traverse arbitrary data structures, and yet have type-specific cases. However, the original approach required all the type-specific cases to be supplied at once, when the recursive knot of generic function definition is tied. Hence, generic functions were closed. In contrast, Haskell's type classes support open, or extensible, functions that can be extended with new type-specific cases as new data types are defined. In this paper, we extend the 'Scrap your boilerplate' approach to support this open style. On the way, we demonstrate the desirability of abstraction over type classes, and the usefulness of recursive dictionarie.\nTitle:\nScrap your boilerplate with class: extensible generic functions\n\nAbstract:\nA heterogeneous collection is a datatype that is capable of storing data of different types, while providing operations for look-up, update, iteration, and others. There are various kinds of heterogeneous collections, differing in representation, invariants, and access operations. We describe HLIST - a Haskell library for strongly typed heterogeneous collections including extensible records. We illustrate HLIST's benefits in the context of type-safe database access in Haskell. The HLIST library relies on common extensions of Haskell 98. Our exploration raises interesting issues regarding Haskell's type system, in particular, avoidance of overlapping instances, and reification of type equality and type unification.\nTitle:\nStrongly typed heterogeneous collections\n\nAbstract:\n . Design patterns are widely used in object-oriented design.The application of design patterns in programming, however, is usuallybased on manual implementation in an ordinary object-oriented programminglanguage resulting in problems like the lack of encapsulation,traceability and reusability of the patterns. We present a design patternoriented programming model as an extension of the object orientedprogramming model. The model copes with the specification and applicationof... \nTitle:\nA Programming Language for Design Patterns\n\nAbstract:\nProgrammable rewriting strategies provide a valuable tool for implementing traversal functionality in grammar-driven (or schema-driven) tools. The working Haskell programmer has access to programmable rewriting strategies via two similar options: (i) the Strafunski bundle for generic functional programming and language processing, and (ii) the \u201cScrap Your Boilerplate\u201d approach to generic functional programming. Basic rewrite steps are encoded as monomorphic functions on datatypes. Rewriting strategies are polymorphic functions composed from appropriate basic strategy combinators.\nTitle:\nProgrammable Rewriting Strategies in Haskell: -- White Paper \n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 6.231, mean value for all the writers is 8.648, which makes it 2.062 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nJavaGI is an experimental language that extends Java 1.5 by generalizing the interface concept to incorporate the essential features of Haskell's type classes. In particular, generalized interfaces cater for retroactive and constrained interface implementations, binary methods, static methods in interfaces, default implementations for interface methods, interfaces over families of types, and existential quantification for interface-bounded types. As a result, many anticipatory uses of design patterns such as Adapter, Factory, and Visitor become obsolete; several extension and integration problems can be solved more easily. JavaGI's interface capabilities interact with subtyping (and subclassing) in interesting ways that go beyond type classes. JavaGI can be translated to Java 1.5. Its formal type system is derived from Featherweight GJ.\nTitle:", "model_inf_time": 0.98}, {"id": "41296", "output": "Adaptive Market Mechanisms for Discussion Forums", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDiscussion forums are one of the simplest, while successful, Internet-based systems to promote the spread of knowledge. Unfortunately, they sometimes lack appropriate incentives to encourage users participation. In this paper we discuss a simple market-oriented mechanism to promote the knowledge exchange activity in discussion forums. The proposed solution departs from other market-oriented approaches since the scarce resource we deal with, the effort to create information, has important peculiarities which must be taken into account. In addition to the basic framework, we have also developed a method to dynamically improve the performance of the system. This method uses statistics about users behavior in order to minimize the reponse time in which questions are satisfactorily answered.\nTitle:\nEncouraging knowledge exchange in discussion forums by market-oriented mechanisms\n\nAbstract:\nIn this paper we present a method that, given a communicating system, allows us to reduce on average the number of messages sent. The basic idea is to shortcut the most frequent dialogs appearing in the communications. By doing so, some large dialogs can be reduced to only two messages. In addition, not only the number of messages, but also the size of them, can be minimized, on average, by considering the probabilities of sending each message in each state.\nTitle:\nA formal framework to reduce communications in communication systems\n\nAbstract:\nIn this paper, we present a methodology to estimate the impact of modifying a given software system design. In addition, we will be able to evaluate its reusability as well as the coupling of its components. In order to do that, the designer defines the system in terms of its components, their dependencies, the properties they fulfill, and the properties each component requires to other components. Besides, some auxiliary functions are used to define the relations between properties and the cost associated with their modification. Putting together all this information the different ways to perform a modification can be systematically generated and studied. We have applied our methodology to a medium-size system. Specifically, we dealt with an on-line intelligent tutoring system allowing users to learn the programming language Haskell.\nTitle:\nA formal framework for analyzing reusability complexity in component-based systems\n\nAbstract:\nIn this paper we formally present a methodology that, by using genetics, allows us to adapt the optimal intelligence of the elements conforming a living system. We call these environments meta-adaptable systems. We propose systems in which the capability of the agents to adapt themself to the environment (via intelligence) is itself adapted according to the changes of their own environment. We show a simple but illustrative example of such a meta-adaptable system, which can be easily extended to deal with more real situations as the construction of software systems based on intelligent mobile agents. This system has been fully implemented, and some interesting conclusions have been extracted from it.\nTitle:\nDefining Meta-Adaptable Living Agents\n\nAbstract:\nFinding the optimal solution to NP-hard problems requires at least exponential time. Thus, heuristic methods are usually applied to obtain acceptable solutions to this kind of problems. In this paper we propose a new type of heuristic algorithms to solve this kind of complex problems. Our algorithm is based on river formation dynamics and provides some advantages over other heuristic methods, like ant colony optimization methods. We present our basic scheme and we illustrate its usefulness applying it to a concrete example: The Traveling Salesman Problem.\nTitle:\nUsing river formation dynamics to design heuristic algorithms\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.176, mean value for all the writers is 8.648, which makes it 0.403 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOne of the most successful and simplest Internet-based systems to promote the spread of knowledge are discussion forums. Unfortunately, not much research has been done to improve them by introducing adaptive capabilities. In this paper we discuss a market-oriented mechanism to promote the knowledge exchange activity in discussion forums. Our system provides a strategy to dynamically adapt the structure of the forums by using statistics about users behavior. By doing so, we minimize the response time in which questions are satisfactorily answered. In addition to that, the effort needed to generate useful information is also reduced.\nTitle:", "model_inf_time": 1.02}, {"id": "41297", "output": "Intrusion Detection for Emergency Call Services in VoIP Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nNowadays, voice over IP (VoIP) telephony networks are connected to classic public switched telephony networks (PSTNs). Emergency calls from VoIP peers to PSTN public service answering points (PSAPs) are possible. Through the connection of IP networks and PSTNs the PSAP may be a victim of new, more powerful denial of service (DoS) attacks. This paper describes the present and future architecture of a PSAP. Based on measurements at a PSAP the challenge of attack detection at the PSAP is revealed. Furthermore, first solutions are pointed out and evaluated.\nTitle:\nPresent and Future Challenges Concerning DoS-attacks against PSAPs in VoIP Networks\n\nAbstract:\nThis paper deals with the characteristics of multicast push to talk voice traffic sources for disaster area scenarios. The goal is to design models that can be used to assign voice traffic sources to nodes. The modelling is based on an analysis of real-life measurements during a catastrophe maneuver The analysis shows that about half of all calls originate from the communication head of a talk group. Based on this characteristic, different models are considered. Synthetic distributions of traffic sources for the different models are generated and evaluated by statistical analysis. Finally, the impact of the different models is evaluated in an exemplary simulative network performance analysis.\nTitle:\nHow To Assign Traffic Sources To Nodes In Disaster Area Scenarios\n\nAbstract:\nDisaster areas have been figured out as a typical usage scenario for mobile wireless ad-hoc networks (MANETs). In contrast to this, there are no specific mobility or traffic models for MANETs. In this paper we present a realistic approach to realize the mobility in disaster areas based on tactical issues of civil protection. The new model is analyzed and compared to Gauss-Markov and Random Waypoint mobility models. Furthermore, we present first simulation results. The mobility model analysis as well as the simulation are based on two real disasters that occurred in Germany in 1999 and 2001. We show that disaster area scenarios have specific characteristics. Thus, they should be considered in MANET performance evaluation.\nTitle:\nHuman Mobility in MANET Disaster Area Simulation - A Realistic Approach\n\nAbstract:\nThis paper deals with voice communication models for disaster area scenarios. The goal is to design models that can be used to generate realistic push to talk traffic for single talk groups. The modelling is based on an analysis of empirical measurements during a catastrophe maneuver. The analysis shows that the time series comprise heavy load periods and significant correlations. Based on these characteristics, different Markov and semi-Markov models are considered. Synthetic traffic streams for the different models are generated and evaluated by visual and statistical analysis. Finally, a case study outlines the impact of the different traffic models in network performance simulation\nTitle:\nModelling Voice Communication in Disaster Area Scenarios\n\nAbstract:\nAnalysing and modelling traffic is one important step in the performance evaluation of communication systems. In this paper we focus on first responder (FR) networks. The goal is to figure out models that can be used to generate realistic synthetic push to talk voice traffic for single talk groups to be used in network simulation. Our work is based on an empirical long-time measurement of one FR channel. The analysis of the trace shows significant short- and long-range correlations as well as variations of load over time. As the characteristics analysed are similar to ones observed in disaster area (DA) traces, we consider n-state and 3-state (Semi)-Markov models. After fitting the parameters of these models to the traces, synthetic traffic streams for the different models are generated and finally evaluated by both visual and statistical analysis.\nTitle:\nCharacterisation and Modelling of Voice Traffic in First Responder Networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.925, mean value for all the writers is 8.648, which makes it 0.236 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the recent years Voice over IP (VoIP) telephony started to migrate from research to the market. In the future, All-IP networks will substitute the classical Public Switched Telephone Networks (PSTNs). Nowadays, there is no All-IP network yet, but many VoIP-providers already enable calls from VoIP to a PSTN and vice versa. Thus, critical infrastructures within the PSTN like the emergency call service, are accessible from the VoIP network (e.g. the Internet) and get exposed to new security threats. In particular, there is the risk of Denial of Service (DoS) attacks originating from the VoIP network. An attacker could jam the emergency call service by generating a massive load of faked emergency calls, which could lead to the loss of lives in the worst case. For us, this was the motivation to analyse the applicability of the concept of Intrusion Detection (ID) in the emergency call context and develop an adapted ID-architecture including its implementation. In an evaluation of the ID-architecture, using real emergency call traces from the fire department of Cologne, we show that the developed concept can reliably detect emerging DoS attacks from VoIP networks up to a certain VoIP diffusion rate.\nTitle:", "model_inf_time": 1.42}, {"id": "41298", "output": "Pseudo-Chain Completeness of Pseudo-Linear Triangle Logic", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we introduce triangle algebras: a variety of residuated lattices equipped with approximation operators, and with a third angular point u, different from 0 and 1. We show that these algebras serve as an equational representation of interval-valued residuated lattices (IVRLs). Furthermore, we present triangle logic (TL), a system of many-valued logic capturing the tautologies of IVRLs. Triangle algebras are used to cast the essence of using closed intervals of L as truth values into a set of appropriate logical axioms. Our results constitute a crucial first step towards solving an important research challenge: the axiomatic formalization of residuated t-norm based logics on L^I, the lattice of closed intervals of [0,1], in a similar way as was done for formal fuzzy logics on the unit interval.\nTitle:\nTriangle algebras: A formal logic approach to interval-valued residuated lattices\n\nAbstract:\nFuzzy formal logics were introduced in order to handle graded truth values instead of only 'true' and 'false'. A wide range. of such logics were introduced successfully, like Monoidal T-norm based Logic, Basic Logic. Godel Logic, Lukasiewicz Logic etc. However, in general, fuzzy set theory is riot only concerned with vagueness, bill also with uncertainty. A possible solution is to rise intervals instead of real numbers as membership values. In this paper, we present air approach with triangle algebras, which tire algebraic characterizations of interval-valued residuated lattices. The variety of these structures corresponds in a sound and complete way to a logic that we introduce. called Triangle Logic (in the same. way as, e.g., BL-algebras and Basic Logic). We will show that this truth-functional approach, along with the residuation principle, has some consequence that seem to obstruct an easy and proper interpretation for the semantics of Triangle Logic.\nTitle:\nA Fuzzy Formal Logic for Interval-valued Residuated Lattices\n\nAbstract:\nIn this paper, we present triangle algebras: residuated lattices equipped with two modal, or approximation, operators and with a third angular point u, different from 0 (false) and 1 (true), intuitively denoting ignorance about a formula\u2019s truth value. We prove that these constructs, which bear a close relationship to several other algebraic structures including rough approximation spaces, provide an equational representation of interval-valued residuated lattices; as an important case in point, we consider $\\mathcal{L}^I$, the lattice of closed intervals of [0,1]. As we will argue, the representation by triangle algebras serves as a crucial stepping stone to the construction of formal interval-valued fuzzy logics, and in particular to the axiomatic formalization of residuated t-norm based logics on $\\mathcal{L}^I$, in a similar way as was done for formal fuzzy logics on the unit interval.\nTitle:\nTriangle algebras: towards an axiomatization of interval-valued residuated lattices\n\nAbstract:\nAs is well-known, residuated lattices (RLs) on the unit interval correspond to left-continuous t-norms. Thus far, a similar characterization has not been found for RLs on the set of intervals of [0,1], or more generally, of a bounded lattice L. In this paper, we show that the open problem can be solved if it is restricted, making only a few simple and intuitive assumptions, to the class of interval-valued residuated lattices (IVRLs). More specifically, we derive a full characterization of product and implication in IVRLs in terms of their counterparts on the base RL. To this aim, we use triangle algebras, a recently introduced variety of RLs that serves as an equational representation of IVRLs.\nTitle:\nA characterization of interval-valued residuated lattices\n\nAbstract:\nAn important concept in the theory of residuated lattices and other algebraic structures used for formal fuzzy logic, is that of a filter. Filters can be used, amongst others, to define congruence relations. Specific kinds of filters include Boolean filters and prime filters. In this paper, we define several different filters of residuated lattices and triangle algebras and examine their mutual dependencies and connections. Triangle algebras characterize interval-valued residuated lattices.\nTitle:\nFilters of residuated lattices and triangle algebras\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.914, mean value for all the writers is 8.648, which makes it 0.227 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTriangle algebras are equationally defined structures that are equivalent with certain residuated lattices on a set of intervals, which are called interval-valued residuated lattices (IVRLs). Triangle algebras have been used to construct triangle logic (TL), a formal fuzzy logic that is sound and complete w.r.t. the class of IVRLs. In this paper, we prove that the so-called pseudo-prelinear triangle algebras are subdirect products of pseudo-linear triangle algebras. This can be compared with MTL-algebras (prelinear residuated lattices) being subdirect products of linear residuated lattices. As a consequence, we are able to prove the pseudo-chain completeness of pseudo-linear triangle logic (PTL), an axiomatic extension of TL introduced in this paper. This kind of completeness is the analogue of the chain completeness of monoidal T-norm based logic (MTL). This result also provides a better insight in the structure of triangle algebras; it enables us, amongst others, to prove properties of pseudo-prelinear triangle algebras more easily. It is known that there is a one-to-one correspondence between triangle algebras and couples (L,@a), in which L is a residuated lattice and @a an element in that residuated lattice. We give a schematic overview of some properties of pseudo-prelinear triangle algebras (and a number of others that can be imposed on a triangle algebra), and the according necessary and sufficient conditions on L and @a.\nTitle:", "model_inf_time": 1.43}, {"id": "41299", "output": "Cuckoo-Inspired Algorithms: A Survey of Applications", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\ne-Remanufacturing has nowadays become a superior option for product recovery management system. So far, many different approaches have been followed in order to increase the efficiency of remanufacturing process. Swarm intelligence (SI), a relatively new bio-inspired family of methods, seeks inspiration in the behavior of swarms of insects or other animals. After applied in other fields with success, SI started to gather the interest of researchers working in the field of remanufacturing. In this paper we provide a survey of SI methods that have been used in e-remanufacturing.\nTitle:\nSwarm intelligence supported e-remanufacturing\n\nAbstract:\nIn manufacturing environment, an automated guided vehicle (AGV) system is composed of a set of driver-less vehicles that transport goods and materials between distinct workstations and storage locations of shops. In soft computing area, ant algorithms are a series of population-based approaches inspired by various behaviors of real ant colonies. During the last two decades, ant algorithms have achieved a great success in solving many combinatorial optimization problems. In this article we make an attempt to study the feasibility of applying ant algorithms to different problems encountered in AGV system design and control. By making use of ant algorithms' strengths, we hope to provide the readers with alternative options for solving conventional AGV system design and control problems, as well as to point out some new directions for AGV system research.\nTitle:\nCan ant algorithms make automated guided vehicle system more intelligent?\n\nAbstract:\nNew bio-technologies are being developed that allow high-throughput imaging of gene expressions, where each image captures the spatial gene expression pattern of a single gene in the tissue of interest. This paper addresses the problem of automatically inferring a gene interaction network from such images. We propose a novel kernel-based graphical model learning algorithm, that is both convex and consistent. The algorithm uses multi-instance kernels to compute similarity between the expression patterns of different genes, and then minimizes the L1 regularized Bregman divergence to estimate a sparse gene interaction network. We apply our algorithm on a large, publicly available data set of gene expression images of Drosophila embryos, where our algorithm makes novel and interesting predictions.\nTitle:\nInferring gene interaction networks from ISH images via kernelized graphical models\n\nAbstract:\nRemanufacturing has become a superior option for product recovery management system. It mainly consists of three stages: retrieval, reproduction, and redistribution. So far, many different approaches have been followed in order to improve the efficiency of a remanufacturing process. However, as the complexity increases, the use of computational intelligence (CI) in those problems is becoming a unique tool of imperative value. In this paper, different CI methods, such as artificial neural network (ANN), ant colony optimization (ACO), biogeography-based optimization (BBO), cuckoo search (CS) and fuzzy logic (FL), are utilized to solve the problems involved in retrieval and reproduction stages for remanufacturing. The key issues in implementing the proposed approaches are discussed, and finally the applicability of the proposed methods are illustrated through different examples.\nTitle:\nComputational Intelligence In Used Products Retrieval And Reproduction\n\nAbstract:\nRadio frequency identification (RFID) research based upon computational intelligence (CI) is currently attracting a lot effort from the research community. Characteristics of CI, such as adaptation, fault tolerance, high computational speed and error resilience, fit the requirements of RFID research. In this article we provide an overview of the research progress in applying CI methods to various problems within RFID research area. The findings of this review should be a good source for anyone who is interested in the application of CI approaches to RFID and its corresponding fields.\nTitle:\nThe applications of computational intelligence in radio frequency identification research\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.527, mean value for all the writers is 8.648, which makes it 0.103 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA number of cuckoo-inspired algorithms have been introduced recently in the literature. These procedures are based on the parasitic behavior observed in some cuckoo species, in combination with the Levy flight behavior discovered in some birds and fruit flies. Thus far, many different applications have been reported. In this paper we provide a quick overview of the current stage of cuckoo-inspired algorithms and their various applications.\nTitle:", "model_inf_time": 1.22}, {"id": "41300", "output": "Ensemble Classifiers and Neural Networks for Caller Interaction Classification in Interactive Voice Response Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA classification system that accurately categorizes caller behavior within Interactive Voice Response systems would assist in developing good automated self service applications. This paper details the implementation of such a classification system for a pay beneficiary application. Adaptive Neuro-Fuzzy Inference System (ANFIS), Feed forward Artificial Neural Network (ANN) and Support Vector Machine (SVM) classifiers were created. Exceptional results were achieved. The ANN classifiers are the preferred models. ANN classifiers achieved 100% classification on 'Say account', 'Say amount' and 'Select beneficiary' unseen data. The ANN classifier yielded 95.42% accuracy on 'Say confirmation' unseen data.\nTitle:\nAdaptive Neuro Fuzzy Inference System, Neural Network and Support Vector Machine for Caller Behavior Classification\n\nAbstract:\nAccurate classification of caller interactions within Interactive Voice Response systems would assist corporations to determine caller behavior within these telephony applications. This paper details the development of such a classification system for a pay beneficiary application. Fuzzy Inference Systems, Multi-Layer Perceptron, Support Vector Machine and ensemble of classifiers were developed. Accuracy, sensitivity and specificity performance metrics were computed as well as compared for these classification solutions. Ideally, a classifier should have high sensitivity and high specificity. Exceptional results were achieved. The ensemble of classifiers is the preferred solution, yielding an accuracy of 99.17%.\nTitle:\nGenetic Algorithms, Neural Networks, Fuzzy Inference System, Support Vector Machines for Call Performance Classification\n\nAbstract:\nA classification system that would aid businesses in selecting calls for analysis would improve the call recording selection process. This would assist in developing good automated self service applications. This paper details such a classification system for a pay beneficiary application. Fuzzy Inference System (FIS) classifiers were created. These classifiers were optimized using Genetic Algorithm (GA) and Simulated Annealing (SA). GA and SA performance in FIS classifier optimization were compared. Good results were achieved. In regards to computational efficiency, SA outperformed GA. When optimizing the FIS 'Say account' and 'Say confirmation' classifiers, GA is the preferred technique. Similarly. SA is the preferred method in FIS 'Say amount' and 'Select beneficiary classifier optimization. GA and SA optimized FIS field classifier outperformed previously developed FIS field classifiers.\nTitle:\nOptimization of Fuzzy Inference System Field Classifiers Using Genetic Algorithms and Simulated Annealing.\n\nAbstract:\nThis paper employs pattern classification methods for assisting contact centers in determining caller interaction at a 'Say account' field within an Interactive Voice Response application. Binary and real coded genetic algorithms (GAs) that employed normalized geometric ranking as well as tournament selection functions were utilized to optimize the Multi-Layer Perceptron neural network architecture. The binary coded genetic algorithm (GA) that used tournament selection function yielded the most optimal solution. However, this algorithm was not the most computationally efficient. This algorithm demonstrated acceptable repeatability abilities. The binary coded GA that used normalized geometric selection function yielded poor repeatability capabilities. GAs that employed normalized geometric ranking selection function were computationally efficient, but yielded solutions that were approximately equal. The real coded tournament selection function GA produced classifiers that were approximately 3% less accurate than the binary coded tournament selection function GA.\nTitle:\nCaller Interaction Classification: A Comparison of Real and Binary Coded GA-MLP Techniques\n\nAbstract:\n  This paper compares various optimization methods for fuzzy inference system optimization. The optimization methods compared are genetic algorithm, particle swarm optimization and simulated annealing. When these techniques were implemented it was observed that the performance of each technique within the fuzzy inference system classification was context dependent. \nTitle:\nFuzzy Inference Systems Optimization\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.789, mean value for all the writers is 8.648, which makes it 0.973 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA classification system that accurately categorizes caller interaction within Interactive Voice Response systems is essential in determining caller behaviour. Field and call performance classifier for pay beneficiary application are developed. Genetic Algorithms, Multi-Layer Perceptron neural network, Radial Basis Function neural network, Fuzzy Inference Systems and Support Vector Machine computational intelligent techniques were considered in this research. Exceptional results were achieved. Classifiers with accuracy values greater than 90% were developed. The preferred models for field 'Say amount', 'Say confirmation' and call performance classification are the ensemble of classifiers. However, the Multi-Layer Perceptron classifiers performed the best in field 'Say account' and 'Select beneficiary' classification.\nTitle:", "model_inf_time": 1.5}, {"id": "41301", "output": "Outlier Detection in Sparse Data with Factorization Machines", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA network with $n$ nodes contains O(n2) possible links. Even for networks of modest size, it is often difficult to evaluate all pairwise possibilities for links in a meaningful way. Furthermore, even though link prediction is closely related to missing value estimation problems, such as collaborative filtering, it is often difficult to use sophisticated models such as latent factor methods because of their computational complexity over very large networks. Due to this computational complexity, most known link prediction methods are designed for evaluating the link propensity over a specified subset of links, rather than for performing a global search over the entire networks. In practice, however, it is essential to perform an exhaustive search over the entire networks. In this paper, we propose an ensemble enabled approach to scaling up link prediction, which is able to decompose traditional link prediction problems into subproblems of smaller size. These subproblems are each solved with the use of latent factor models, which can be effectively implemented over networks of modest size. Furthermore, the ensemble enabled approach has several advantages in terms of performance. We show the advantage of using ensemble-based latent factor models with experiments on very large networks. Experimental results demonstrate the effectiveness and scalability of our approach.\n\n\nTitle:\nScaling up Link Prediction with Ensembles.\n\nAbstract:\nA network with n nodes contains O(n2) possible links. Even for networks of modest size, it is often difficult to evaluate all pairwise possibilities for links in a meaningful way. Further, even though link prediction is closely related to missing value estimation problems, it is often difficult to use sophisticated models such as latent factor methods because of their computational complexity on l...\nTitle:\nAn Ensemble Approach to Link Prediction.\n\nAbstract:\nThis paper studies the quality of web service prediction problem. We formalize the QoS prediction problem by incorporating multiple contextual characteristics via collective matrix factorization that simultaneously factor the user-service quality matrix and contextual information matrices. Using the service category and location context, we develop three context-aware QoS prediction models and algorithms to demonstrate the advantages of this modeling technique. The advantages of our proposed models are demonstrated via experiments on real-life data sets.\nTitle:\nQuality of Web Service Prediction by Collective Matrix Factorization\n\nAbstract:\nNetwork anomaly detection has become very popular in recent years because of the importance of discovering key regions of structural inconsistency in the network. In addition to application-specific information carried by anomalies, the presence of such structural inconsistency is often an impediment to the effective application of data mining algorithms such as community detection and classification. In this paper, we study the problem of detecting structurally inconsistent nodes that connect to a number of diverse influential communities in large social networks. We show that the use of a network embedding approach, together with a novel dimension reduction technique, is an effective tool to discover such structural inconsistencies. We also experimentally show that the detection of such anomalous nodes has significant applications: one is the specific use of detected anomalies, and the other is the improvement of the effectiveness of community detection.\nTitle:\nAn embedding approach to anomaly detection\n\nAbstract:\n  Computing shortest distances is one of the fundamental problems on graphs, and remains a {\\em challenging} task today. {\\em Distance} {\\em landmarks} have been recently studied for shortest distance queries with an auxiliary data structure, referred to as {\\em landmark} {\\em covers}. This paper studies how to apply distance landmarks for fast {\\em exact} shortest distance query answering on large road graphs. However, the {\\em direct} application of distance landmarks is {\\em impractical} due to the high space and time cost. To rectify this problem, we investigate novel techniques that can be seamlessly combined with distance landmarks. We first propose a notion of {\\em hybrid landmark covers}, a revision of landmark covers. Second, we propose a notion of {\\em agents}, each of which represents a small subgraph and holds good properties for fast distance query answering. We also show that agents can be computed in {\\em linear time}. Third, we introduce graph partitions to deal with the remaining subgraph that cannot be captured by agents. Fourth, we develop a unified framework that seamlessly integrates our proposed techniques and existing optimization techniques, for fast shortest distance query answering. Finally, we experimentally verify that our techniques significantly improve the efficiency of shortest distance queries, using real-life road graphs. \nTitle:\nDistance Landmarks Revisited for Road Graphs.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.927, mean value for all the writers is 8.648, which makes it 0.238 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn sparse data, a large fraction of the entries take on zero values. Some examples of sparse data include short text snippets (such as tweets in Twitter) or some feature representations of categorical data sets with a large number of values, in which traditional methods for outlier detection typically fail because of the difficulty of computing distances. To address this, it is important to use the latent relations between such values. Factorization machines represent a natural methodology for this, and are naturally designed for the massive-domain setting because of their emphasis on sparse data sets. In this study, we propose an outlier detection approach for sparse data with factorization machines. Factorization machines are also efficient due to their linear complexity in the number of non-zero values. In fact, because of their efficiency, they can even be extended to traditional settings for numerical data by an appropriate feature engineering effort. We show that our approach is both effective and efficient for sparse categorical, short text and numerical data by an extensive experimental study.\nTitle:", "model_inf_time": 1.38}, {"id": "41302", "output": "Spray Routing for Intermittently Connected Mobile Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIntermittently connected mobile networks are sparse wireless networks where most of the time there does not exist a complete path from the source to the destination. These networks fall into the general category of Delay Tolerant Networks. There are many real networks that follow this paradigm, for example, wildlife tracking sensor networks, military networks, inter-planetary networks, etc. In this context, conventional routing schemes would fail.To deal with such networks researchers have suggested to use flooding-based routing schemes. While flooding-based schemes have a high probability of delivery, they waste a lot of energy and suffer from severe contention, which can significantly degrade their performance. Furthermore, proposed efforts to significantly reduce the overhead of flooding-based schemes have often be plagued by large delays. With this in mind, we introduce a new routing scheme, called Spray and Wait, that \"sprays\" a number of copies into the network, and then \"waits\" till one of these nodes meets the destination.Using theory and simulations we show that Spray and Wait outperforms all existing schemes with respect to both average message delivery delay and number of transmissions per message delivered; its overall performance is close to the optimal scheme. Furthermore, it is highly scalable retaining good performance under a large range of scenarios, unlike other schemes. Finally, it is simple to implement and to optimize in order to achieve given performance goals in practice.\nTitle:\nSpray and wait: an efficient routing scheme for intermittently connected mobile networks\n\nAbstract:\nIntermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from the source to the destination. There are many real networks that follow this model, for example, wildlife tracking sensor networks, military networks, vehicular ad hoc networks (VANETs), etc. In this context, conventional routing schemes would fail, because they try to establish complete end-to-end paths, before any data is sent. To deal with such networks researchers have suggested to use flooding-based routing schemes. While flooding-based schemes have a high probability of delivery, they waste a lot of energy and suffer from severe contention which can significantly degrade their performance. With this in mind, we look into a number of \"single-copy\" routing schemes that use only one copy per message, and hence significantly reduce the resource requirements of flooding-based algorithms. We perform a detailed exploration of the single-copy routing space in order to identify efficient single-copy solutions that (i) can be employed when low resource usage is critical, and (ii) can help improve the design of general routing schemes that use multiple copies. We also propose a theoretical framework that we use to analyze the performance of all single-copy schemes presented, and to derive upper and lower bounds on the delay of any scheme.\nTitle:\nEfficient routing in intermittently connected mobile networks: the single-copy case\n\nAbstract:\nIntermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from the source to the destination. There are many real networks that follow this model, for example, wildlife tracking sensor networks, military networks, vehicular ad hoc networks (VANETs), etc. To deal with such networks researchers have suggested to use controlled replication or \"spraying\" methods that can reduce the overhead of flooding-based schemes by distributing a small number of copies to only a few relays. These relays then \"look\" for the destination in parallel as they move into the network. Although such schemes can perform well in scenarios with high mobility (e.g. VANETs), they struggle in situations were mobility is slow and correlated in space and/or time. To route messages efficiently in such networks, we propose a scheme that also distributes a small number of copies to few relays. However, each relay can then forward its copy further using a single-copy utility-based scheme, instead of naively waiting to deliver it to the destination itself. This scheme exploits all the advantages of controlled replication, but is also able to identify appropriate forwarding opportunities that could deliver the message faster. Simulation results for traditional mobility models, as well as for a more realistic \"community-based\" model, indicate that our scheme can reduce the delay of existing spraying techniques up to 20 times in some scenarios.\nTitle:\nSpray and Focus: Efficient Mobility-Assisted Routing for Heterogeneous and Correlated Mobility\n\nAbstract:\nIntermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from source to destination, or such a path is highly unstable and may break soon after it has been discovered. In this context, conventional routing schemes would fail. To deal with such networks we propose the use of an opportunistic hop-by-hop routing model. According to the model, a series of independent, local forwarding decisions are made, based on current connectivity and predictions of future connectivity information diffused through nodes' mobility. The important issue here is how to choose an appropriate next hop. To this end, we propose and analyze via theory and simulations a number of routing algorithms. The champion algorithm turns out to be one that combines the simplicity of a simple random policy, which is efficient in finding good leads towards the destination, with the sophistication of utility-based policies that efficiently follow good leads. We also state and analyze the performance of an oracle-based optimal algorithm, and compare it to the online approaches. The metrics used in the comparison are the average message delivery delay and the number of transmissions per message delivered. I. INTRODUCTION\nTitle:\nSingle-Copy Routing in Intermittently Connected Mobile Networks\n\nAbstract:\nA large body of work has theoretically analyzed the performance of mobility-assisted routing schemes for intermittently connected mobile networks. But the vast majority of these prior studies have ignored wireless contention. Recent papers have shown through simulations that ignoring contention leads to inaccurate and misleading results, even for sparse networks. In this paper, we analyze the performance of routing schemes under contention. First, we introduce a mathematical framework to model contention. This framework can be used to analyze any routing scheme with any mobility and channel model. Then, we use this framework to compute the expected delays for different representative mobility-assisted routing schemes under random direction, random waypoint and community-based mobility models. Finally, we use these delay expressions to optimize the design of routing schemes while demonstrating that designing and optimizing routing schemes using analytical expressions which ignore contention can lead to suboptimal or even erroneous behavior.\nTitle:\nContention-Aware Performance Analysis of Mobility-Assisted Routing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.698, mean value for all the writers is 8.648, which makes it 0.043 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIntermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from the source to the destination. There are many real networks that follow this model, for example, wildlife tracking sensor networks, military networks, vehicular ad hoc networks, etc. In this context, conventional routing schemes fail, because they try to establish complete end-to-end paths, before any data is sent. To deal with such networks researchers have suggested to use flooding-based routing schemes. While flooding-based schemes have a high probability of delivery, they waste a lot of energy and suffer from severe contention which can significantly degrade their performance. Furthermore, proposed efforts to reduce the overhead of flooding-based schemes have often been plagued by large delays. With this in mind, we introduce a new family of routing schemes that \"spray\" a few message copies into the network, and then route each copy independently towards the destination. We show that, if carefully designed, spray routing not only performs significantly fewer transmissions per message, but also has lower average delivery delays than existing schemes; furthermore, it is highly scalable and retains good performance under a large range of scenarios. Finally, we use our theoretical framework proposed in our 2004 paper to analyze the performance of spray routing. We also use this theory to show how to choose the number of copies to be sprayed and how to optimally distribute these copies to relays.\nTitle:", "model_inf_time": 1.52}, {"id": "41303", "output": "On the Intersection of Slender Languages with Primitive Words", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe introduce some classes of splicing languages generated with simple and semi-simple splicing rules, in both, the linear and circular cases. We investigate some of their properties.\nTitle:\nOn Some Classes of Splicing Languages\n\nAbstract:\nAbstract: In this paper we propose a Chomsky-Sch\u007futzenberger type characterization of k-poly-slender context-free languages, as the homomorphical image of an intersectionof a Dyck language and a (2k + 1)-poly-slender regular language. A stronger resultis provided, namely the homomorphism and the Dyck language are determined irrespectiveof the given poly-slender context-free language, when considering the familyof all poly-slender context-free languages. A similar characterization is obtained for...\nTitle:\nChomsky-Schu\"tzenberger Type Characterizations of Poly-Slender and Parikh Slender Context-Free Languages\n\nAbstract:\nWe define three operations on strings and languages suggested by the process of gene assembly in hypotrichous ciliates. This process is considered to be a prime example of DNA computing in vivo. This paper is devoted to some computational aspects of these operations from a formal language point of view. The closure of the classes of regular and context-free languages under these operations is settled. Then, we consider theld macronuclear language of a given language L, which consists of all ld- macronuclear strings obtained from the strings of L by iteratively applying the loop-direct repeat-excision. Finally, we discuss some open problems and further directions of research.\nTitle:\nOn some operations on strings suggested by gene assembly in ciliates\n\nAbstract:\nWe consider a few algorithmic problems regarding the hairpin completion. The first problem we consider is the membership problem of the hairpin and iterated hairpin completion of a language. We propose an O(nf(n)) and O(n^2f(n)) time recognition algorithm for the hairpin completion and iterated hairpin completion, respectively, of a language recognizable in O(f(n)) time. We show that the n factor is not needed in the case of hairpin completion of regular and context-free languages. The n^2 factor is not needed in the case of iterated hairpin completion of context-free languages, but it is reduced to n in the case of iterated hairpin completion of regular languages. We then define the hairpin completion distance between two words and present a cubic time algorithm for computing this distance. A linear time algorithm for deciding whether or not the hairpin completion distance with respect to a given word is connected is also discussed. Finally, we give a short list of open problems which appear attractive to us.\nTitle:\nOn some algorithmic problems regarding the hairpin completion\n\nAbstract:\nWe consider a new type of language defined by a word through iterative factor duplications, inspired by the process of tandem repeats production in the evolution of DNA. We investigate the effect of restricting the factor length to a constant. We prove that all these languages are regular, any word has a unique uniformly bounded duplication root, and show how this root can be computed in linear time and memory. We also address the problem of computing the uniformly bounded duplication distance between two words.\nTitle:\nUniformly bounded duplication languages\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.169, mean value for all the writers is 8.648, which makes it 1.262 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this note we investigate the languages obtained by intersecting slender regular or context-free languages with the set of all primitive Words over the common alphabet. We prove that. these languages axe also re gular, and, respectively, context-free. The statement does not hold anymore for either regular or context-free languages. Moreover, the set of all non-primitive words of a slender context-free language is still context-free. Some possible directions for further research are finally discussed.\nTitle:", "model_inf_time": 1.21}, {"id": "41304", "output": "Evolvability of Object-Oriented Software Architectures", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe ability to re-engineer object-oriented systems has become a vital matter in today\u2019s software industry. Early adopters\n of the object-oriented programming paradigm are now facing the problem of transforming their object-oriented \u201clegacy\u201d systems\n into full-fledged frameworks. As proclaimed by the industrial partners in the FAMOOS project,1 re-engineering object-oriented programs exceeding 10000 lines of poorly documented code requires support from tools as well\n as methodologies.\n \nTitle:\nDesign Pattern Restructuring\n\nAbstract:\nThis paper reports on the results of the Fifth International Workshop on Object-Oriented Reengineering in Oslo on June 15, 2004. It enumerates the presentations made, classifies the contributions and lists the main results of the discussions held at the workshop. As such it provides the context for future workshops around this topic.\nTitle:\nFifth international workshop on object-oriented reengineering\n\nAbstract:\n\n The ability to reengineer object-oriented legacy systems has become a vital matter in today\u2019s software industry. Early adopters\n of the object-oriented programming paradigm are now facing the problems of transforming their object-oriented \u201clegacy\u201d systems\n into full-fledged frameworks.\n \n \nTitle:\nObject-Oriented Reengineering\n\nAbstract:\nThe rapid growth of object-oriented development over the past twenty years has given rise to many object-oriented systems that are large, complex and hard to maintain. These systems exhibit a range of problems, effectively preventing them from satisfying the evolving requirements imposed by their customers. In our paper, we address problem of understanding and reengineering such object-oriented legacy systems. The material is presented as a set of \"reengineering patterns\" - recurring solutions that experts apply while reengineering and maintaining object-oriented systems. The patterns distill successful techniques in planning a reengineering project, reverse-engineering, problem detection, migration strategies and software redesign. The principles and techniques described have been observed and validated in a number of industrial projects, and reflect best practice in object-oriented reengineering.\nTitle:\nObject-oriented reengineering patterns\n\nAbstract:\nRefactorings are widely recognised as ways to improve the internal structure of object-oriented software while maintaining its external behaviour. Unfortunately, refactorings concentrate on the treatment of symptoms (the so called code-smells), thus improvements depend a lot on the skills of the maintainer. Coupling and cohesion on the other hand are quality attributes which are generally recognized as being among the most likely quantifiable indicators for software maintainability. Therefore, this paper analyzes how refactorings manipulate coupling/cohesion characteristics, and how to identify refactoring opportunities that improve these characteristics. As such we provide practical guidelines for the optimal usage of refactoring in a software maintenance process.\nTitle:\nRefactoring \" Improving Coupling and Cohesion of Existing Code\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.787, mean value for all the writers is 8.648, which makes it 0.119 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSoftware Architecture has become an established area of study within the software engineering community for a considerable time now. Recently, Software Architecture has become a topic of interest within the object-oriented community as well. The quality of an object-oriented architecture can be described by a set of characteristics, such as modularity, extensibility, flexibility adaptability, understandability, testability and reusability, which are recognized to facilitate the evolution and the maintenance of software systems. Architecture represents the highest level of design decisions about a system and evolution aspects have to be considered at this level. Moreover, the ever-changing world makes evolvability a strong quality requirement for the majority of software architectures. The main objective of this workshop was to establish a working dialogue about the effective use of techniques, formalisms and tools, as well as their combinations in order to address the architectural evolution of object oriented software systems, either in their initial development or in their later redesign. The workshop also did aim to highlight outstanding issues that should form a part of the forthcoming research agenda in the evolvability of object-oriented software architectures.\nTitle:", "model_inf_time": 1.22}, {"id": "41305", "output": "Predicting Immediacy from Human Poses", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nHuman eyes can recognize person identities based on small salient regions, i.e., person saliency is distinctive and reliable in pedestrian matching across disjoint camera views. However, such valuable information is often hidden when computing similarities of pedestrian images with existing approaches. Inspired by our user study result of human perception on person saliency, we propose a novel per...\nTitle:\nPerson Re-identification by Saliency Learning.\n\nAbstract:\nIn this paper, we propose a new face recognition approach combining a Bayesian probabilistic model and Gabor filter responses. Since both the Bayesian algorithm and the Gabor features can reduce intrapersonal variation through different mechanisms, we integrate the two methods to take full advantage of both approaches. The efficacy of the new method is demonstrated by the experiments on 1180 face images from the XM2VTS database and 1260 face images from the AR database.\nTitle:\nBayesian face recognition using Gabor features\n\nAbstract:\nPerson re-identification is an important task that requires learning discriminative visual features for distinguishing different person identities. Diverse auxiliary information has been utilized to improve the visual feature learning. In this paper, we propose to exploit natural language description as additional training supervisions for effective visual features. Compared with other auxiliary information, language can describe a specific person from more compact and semantic visual aspects, thus is complementary to the pixel-level image data. Our method not only learns better global visual feature with the supervision of the overall description but also enforces semantic consistencies between local visual and linguistic features, which is achieved by building global and local image-language associations. The global image-language association is established according to the identity labels, while the local association is based upon the implicit correspondences between image regions and noun phrases. Extensive experiments demonstrate the effectiveness of employing language as training supervisions with the two association schemes. Our method achieves state-of-the-art performance without utilizing any auxiliary information during testing and shows better performance than other joint embedding methods for the image-language association.\nTitle:\nImproving Deep Visual Representation For Person Re-Identification By Global And Local Image-Language Association\n\nAbstract:\nVisual appearance score, appearance mixture type and deformation are three important information sources for human pose estimation. This paper proposes to build a multi-source deep model in order to extract non-linear representation from these different aspects of information sources. With the deep model, the global, high-order human body articulation patterns in these information sources are extracted for pose estimation. The task for estimating body locations and the task for human detection are jointly learned using a unified deep model. The proposed approach can be viewed as a post-processing of pose estimation results and can flexibly integrate with existing methods by taking their information sources as input. By extracting the non-linear representation from multiple information sources, the deep model outperforms state-of-the-art by up to 8.6 percent on three public benchmark datasets.\nTitle:\nMulti-source Deep Learning for Human Pose Estimation\n\nAbstract:\nIn this paper, we propose a new approach for matching images observed in different camera views with complex cross-view transforms and apply it to person re-identification. It jointly partitions the image spaces of two camera views into different configurations according to the similarity of cross-view transforms. The visual features of an image pair from different views are first locally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized. The features optimal for recognizing identities are different from those for clustering cross-view transforms. They are jointly learned by utilizing sparsity-inducing norm and information theoretical regularization. This approach can be generalized to the settings where test images are from new camera views, not the same as those in the training set. Extensive experiments are conducted on public datasets and our own dataset. Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach.\nTitle:\nLocally Aligned Feature Transforms across Views\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.325, mean value for all the writers is 8.648, which makes it 0.276 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we propose to predict immediacy for interacting persons from still images. A complete immediacy set includes interactions, relative distance, body leaning direction and standing orientation. These measures are found to be related to the attitude, social relationship, social interaction, action, nationality, and religion of the communicators. A large-scale dataset with 10,000 images is constructed, in which all the immediacy measures and the human poses are annotated. We propose a rich set of immediacy representations that help to predict immediacy from imperfect 1-person and 2-person pose estimation results. A multi-task deep recurrent neural network is constructed to take the proposed rich immediacy representation as input and learn the complex relationship among immediacy predictions multiple steps of refinement. The effectiveness of the proposed approach is proved through extensive experiments on the large scale dataset.\nTitle:", "model_inf_time": 1.32}, {"id": "41306", "output": "Code Time Division Multiple Access (CTDMA) for Multicarrier Downlink Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe consider signal reception in multicarrier code-division multiple-access (MC-CDMA) systems. A blind adaptive algorithm is proposed to determine a weight vector which optimally combines the desired signal contributions from different carriers while suppressing noise and interference. No knowledge of the channel conditions (fading coefficients, signature sequences and timing of interferers, statis...\nTitle:\nBlind adaptive signal reception for MC-CDMA systems in Rayleigh fading channels\n\nAbstract:\nIn this paper, we study Pareto optimality for multiuser relay networks. We adopt single-stream transmission and amplify-and-forward relays. First, with fixed relay processing matrices and transmit and receive beamforming vectors, we study Pareto optimality with respect to the power of the transmitters. Based on the signal-to-noise-plus-interference ratio (SINR) balancing analysis, we give a necess...\nTitle:\nPareto Optimality for the Single-Stream Transmission in Multiuser Relay Networks.\n\nAbstract:\nWe investigate a system consisting of a single source, a single destination and multiple layers of parallel relays. The system can be considered as a combination of a broadcast channel, a multi-user multiple input multiple output (MIMO) interference channel and a multiple access channel. Interference alignment (IA) scheme is used throughout the whole system and perfect channel state information (CSI) is assumed available at all transmitters and receivers. In this paper, we consider two layers of parallel relays and our objective is to optimize the power allocation of each transmitter to maximize the total transmission rate of the whole system. Iterative algorithms are derived to find the optimal precode and decode vectors.\nTitle:\nOptimization of Interference Alignment in MIMO Channel with Multiple Layers of Relays\n\nAbstract:\nOptimal assignment of codes with multiple time slots is considered for multicarrier CDMA data networks. Our objective is to minimize the transmission time given the amount of each user's data to be transmitted. A user may experience different performance with different assigned codes in a fading channel. Therefore, with suitable time sharing of the codes among users, the transmission time can be minimized. For practical purpose, we also propose a suboptimal algorithm which can be readily implemented. Simulation results show that by applying the algorithm, the system performance can be very close to the optimal.\nTitle:\nMinimization of transmission time for multicarrier CDMA data networks\n\nAbstract:\nThis paper considers the phase control problem in multiple-input single-output antenna system (MISO) with finite number of control bits. We propose an optimal algorithm which requires only O(n(T) log n(T)) of computations, where n(T) is the number of transmit (input) antennas.\nTitle:\nAn Optimal Phase Control Algorithm For Miso Systems With Finite Feedback\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.949, mean value for all the writers is 8.648, which makes it 1.11 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we present a general 2-D multiple-access scheme, namely, code time division multiple access (CTDMA), on the downlink of multicarrier communication systems. CTDMA combines the idea of CDMA in the code domain and TDMA in the time domain. We consider suitable resource allocation strategies with our CTDMA scheme in the code domain and the time domain jointly. Under the total transmissio...\nTitle:", "model_inf_time": 1.46}, {"id": "41307", "output": "Ontology-Based Visualization Support System for Meaningful E-Book Learning", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we examined whether previous findings on educational big data consisting of e-book logs from a given academic course can be reproduced with different data from other academic courses. The previous findings showed that (1) students who attained consistently good achievement more frequently browsed different e-books and their pages than low achievers and that (2) this difference was found only for logs of preparation for course sessions (preview), not for reviewing material (review). Preliminarily, we analyzed e-book logs from four courses. The results were reproduced in only one course and only partially, that is, (1) high achievers more frequently changed e-books than low achievers (2) for preview. This finding suggests that to allow effective usage of learning and teaching analyses, we need to carefully construct an educational environment to ensure reproducibility.\nTitle:\nReproducibility of findings from educational big data: a preliminary study.\n\nAbstract:\nA real-time learning analytics system is proposed for in-classroom use. We used an e-learning system and an e-book system to collect real-time learning activities during lectures. The collected logs were analyzed and presented visually on a web-based system for the teacher. The teacher can monitor how many students are viewing the same page as the teacher, whether they are following the explanation, or if they are reading previous or subsequent pages. Through a case study, we confirmed the effectiveness of the real-time learning analytics system, in terms of high synchronization between the teacher and the students, i.e., that the majority of students followed the teacher's explanation and added more bookmarks, highlights, or notes on the e-book, compared with the control group where the teacher did not use our system.\nTitle:\nReal-Time Learning Analytics of e-Book Operation Logs for On-site Lecture Support\n\nAbstract:\nTo discover teaching knowledge efficiently, we must extract the various teaching activities from educational data. In this paper, through the use of e-book logs and techniques of time-series analysis, we describe a method of practicing teaching analytics in face-to-face classes, one which enable us to extract the teaching activity efficiently and accurately.\nTitle:\nFace-to-Face Teaching Analytics: Extracting Teaching Activities from E-Book Logs via Time-Series Analysis\n\nAbstract:\nThis paper discusses learning behavior analysis using a learning management system (LMS) and an e-textbook system. We collected a large number of operation logs from e-textbooks to analyze the process of learning. In addition, we conducted a quiz to check the level of understanding. In our study, we especially focus on an analysis of the relationship between learning behavior in informal learning and its effectiveness in the corresponding quiz. We apply a machine learning and classification methodology for behavior analysis. Our experimental results demonstrate that students who undertake good informal learning achieve better scores in quizzes.\nTitle:\nInformal Learning Behavior Analysis Using Action Logs and Slide Features in E-Textbooks\n\nAbstract:\nPurpose - The purpose of this study is to propose a real-time lecture supporting system. The target of this study is on-site classrooms where teachers give lectures and a lot of students listen to teachers' explanations, conduct exercises, etc.Design/methodology/approach - The proposed system uses an e-learning system and an e-book system to collect teaching and learning activities from a teacher and students in real time. The collected data are immediately analyzed to provide feedback to the teacher just before the lecture starts and during the lecture. For example, the teacher can check which pages were well previewed and which pages were not previewed by students using the preview achievement graph. During the lecture, real-time analytics graphs are shown on the teacher's PC. The teacher can easily grasp students' status and whether or not students are following the teacher's explanation.Findings - Through the case study, the authors first confirmed the effectiveness of each tool developed in this study. Then, the authors conducted a large-scale experiment using a real-time analytics graph and investigated whether the proposed system could improve the teaching and learning in on-site classrooms. The results indicated that teachers could adjust the speed of their lecture based on the real-time feedback system, which also resulted in encouraging students to put bookmarks and highlights on keywords and sentences.Originality/value - Real-time learning analytics enables teachers and students to enhance their teaching and learning during lectures. Teachers should start considering this new strategy to improve their lectures immediately.\nTitle:\nReal-Time Learning Analytics System For Improvement Of On-Site Lectures\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.531, mean value for all the writers is 8.648, which makes it 0.753 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we present a system framework making use of e-book logs for visualization learning support systems intended to provide meaningful learning environment for e-book learners. An ontology-based visualization support system, which supports not only meaningful reception learning but also meaningful discovery learning, is designed and developed to help e-book learners to effectively construct their knowledge frameworks. Three main functions are provided in this personalized visualization support system: (1) for any knowledge of E-books, the learner can get its e-book location information and a relation map including its relevance knowledge and their relations; (2) for any page range of any e-book, the learner can get a relation map including knowledge shown in those pages, their relevant KPs and their upper concepts; (3) for any learning period, the learner can check the knowledge map including the knowledge they had read, the relevance knowledge and their relations. Compared to the passive reception environment, in meaningful discovery learning environment learners are encouraged to actively locate new knowledge in their own knowledge framework and restructure existing knowledge by detecting hidden relations between relevant KPs through reflecting the attributes of acquired knowledge visually. Meanwhile the iterative procedure of confirmation and modification in their own relation map ensures that they check the logical consistency of their ideas and clear up misunderstandings.\nTitle:", "model_inf_time": 1.53}, {"id": "41308", "output": "A Dataflow-Based Massively Parallel Programming Language, V", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present a dataflow-based lenient implementation of a functional language, Valid, on conventional multi-processors. A data-flow execution scheme offers a good basis to execute in a highly concurrent way a large number of fine grain function instances, created during the execution of a functional program. The lenient execution and split-phase operation will overlap the idle time caused by remote memory access and remote calls. However, it is necessary to reduce the overhead to handle fine-grain parallelism on conventional multi-processors with no special hardware for fine grain data/message-flow processing. We discuss compilation issues of dataflow-based implementation and runtime systems to support fine-grain parallel execution on two different types of conventional multi-processor: a shared-memory multi-processor, Sequent Symmetry S2000, and a distributed-memory multi-processor, Fujitsu AP1000. We also show the preliminary evaluation of our implementation.\nTitle:\nDataflow-Based Lenient Implementation of a Functional Language, Valid, on Conventional Multi-processors\n\nAbstract:\n We propose a massively parallel programminglanguage, called &quot;V,&quot; which would minimize thedifficulties in writing massively parallel programs. Toabstract away the timing problem in writing parallelprograms, we based our work on a dataflow-based functionalprogramming language. Then, we extended thebase language with an object-based abstraction, called&quot;agent,&quot; to write parallel entities which contain theirown states and can communicate with each other. Inaddition to connecting agents... \nTitle:\nA dataflow language with object-based extension and its implementation on a commercially available parallel machine\n\nAbstract:\nIn this paper, we propose resource management schemes to realize an efficient multi-processing environment in dataflow architecture: to control the process level parallelism according to the hardware capacity, and to virtualize high speed memory in dynamic data-driven computation where context changes in every instruction execution. With a unified software and hardware mechanism, we can detect states of processes at run time, which are treated as control information. We apply these schemes to the Datarol architecture, an optimized version of dynamic data flow architecture, and evaluate them through software simulation.\nTitle:\nParallelism Control and Storage Management in Datarol PE\n\nAbstract:\nWe propose execution control scheme to realize an efficient multi-processing environment in dataflow architecture. With a unified software and hardware mechanism, we can detect states of processes at run time, which are treated as control information. We control the process level parallelism according to the hardware capacity, and virtualize high speed memory in dynamic data-driven computation where context changes in every instruction execution. We apply this scheme to the Datarol architecture, an optimized version of dynamic dataflow architecture, and evaluate them through software simulation.\nTitle:\nParallelism Control Scheme in a Dataflow Architecture\n\nAbstract:\nIn the execution of a functional program, a large number of function instances are dynamically created, and these created function instances are executed as fine grain concurrent processes. In order to implement massively parallel execution of such fine grain concurrent processes, ultra-multiprocessing mechanism must be designed in parallel machine architecture.\nTitle:\nAn ultra-multiprocessing machine architecture for efficient parallel execution of functional languages\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.044, mean value for all the writers is 8.648, which makes it 0.338 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we propose a dataflow-based massively parallel programming language, called V, which is loosely based on a data-flow oriented functional programming language. The language provides a programming unit, or agent, to write parallel entities communicating with each other. In addition, we can connect agents explicitly and abstract an ensemble of agents on a predefined topology description in order to write a massively parallel program that naturally reflects the structure of a problem. We also present some implementation issues and a preliminary evaluation of our compiler and runtime system developed for the Fujitsu AP1000, a distributed-memory parallel machine with conventional processors.\nTitle:", "model_inf_time": 1.32}, {"id": "41309", "output": "PoSIM: A Translucent Middleware for Synergic and Context-Aware Positioning System Management in LBSs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe diffusion of wireless terminals with multiple communication interfaces and the proliferation of heterogeneous positioning techniques opens new possibilities for Location Based Services (LBSs), even if potentially complicating their development In particular, LBSs can significantly benefit from middleware supports to uniformly access the set of positioning systems available at their wireless clients and to dynamically choose the most suitable one depending on application/execution context, also by fusing concurrent positioning data from multiple sources. The paper presents our PoSIM middleware for the integrated and synergic access/control of heterogeneous positioning systems in a highly flexible way. PoSIM provides LBSs with two differentiated levels of visibility for positioning management: a transparent way based on high-level context-based policies, and a fully-aware access to advanced functions/configurations, mediated and uniformed by the middleware independently of the underlying positioning solution. In particular, the paper mainly concentrates on the description of the PoSIM architecture and of its API, by pointing out our primary design and implementation choices.\nTitle:\nCoupling Transparency and Visibility: a Translucent Middleware Approach for Positioning System Integration and Management (PoSIM)\n\nAbstract:\nThe availability of heterogeneous wireless interfaces and of growing computing resources on widespread portable devices pushes for enabling innovative deployment scenarios where mobile nodes dynamically self-organize to offer Internet connectivity to their peers via dynamically established multi-hop multi-path opportunities. We claim the suitability of novel, mobility-aware, and application-layer middleware based on lightweight evaluation indicators to support the complexity of that scenario, involving heterogeneous wireless technologies over differentiated and statically unpredictable execution environments. To validate these claims, we have implemented an innovative middleware that manages the durability/throughput-aware formation and selection of different multi-hop paths simultaneously. This paper specifically focuses on how our middleware effectively exploits Bluetooth for multi-hop multi-path networking, by pointing out the crucial role of i) compliance with standard solutions to favor rapid deployment over off-the-shelf equipment and ii) the reduction of the usual overhead associated with some expensive Bluetooth operations, e.g., device inquiry. In particular, the paper shows how it is possible, on the one hand, to extend JSR-82 to portably access monitoring indicators for lightweight mobility/throughput estimations and, on the other hand, to reduce the time needed to update the set of available Bluetooth-based connectivity opportunities via approximated and lightweight forms of discovery.\nTitle:\nMiddleware Solutions for Self-organizing Multi-hop Multi-path Internet Connectivity Based on Bluetooth\n\nAbstract:\nUser/terminal mobility during service provisioning and high heterogeneity of wireless portable devices identify novel challenges for service delivery in ubiquitous pervasive environments. An emerging architecture solution in the wireless Internet is to have middleware components (mobile proxies) over the fixed network that follow the movements and act on behalf of the limited wireless clients. It is crucial that mobile proxies have full visibility of their context, i.e., the set of available and relevant resources depending on access control rules, client location, user preferences, privacy requirements, terminal characteristics, and current state of hosting environments. The paper presents the design and implementation of a context-centric access control middleware, called COSMOS, for the wireless Internet. COSMOS dynamically determines the contexts of mobile proxies, and effectively rules the access to them, by taking into account different types of metadata (user profiles and system/user-level authorization policies), expressed at a high level of abstraction and cleanly separated from the service logic. The paper also shows how COSMOS facilitates the development of articulated access control strategies in the case study of a context-dependent movie-info service deployed over IEEE 802.11 network localities.\nTitle:\nCOSMOS: A Context-Centric Access Control Middleware for Mobile Environments\n\nAbstract:\nThe rapid diffusion of heterogeneous forms of wireless connectivity is pushing the tremendous growth of the commercial interest in mobile services, i.e., distributed applications to portable wireless terminals that roam during service provisioning. In the case of both location-dependent mobile services and mobile services with session continuity requirements, there is a growing need for decentralized and lightweight solutions to predict cell handovers, in order to enable proactive service management operations that anticipate actual terminal reconnections at their newly visited cells. The paper discusses how to predict client handovers between IEEE 802.11 cells in a portable and completely decentralized way, only by exploiting RSSI monitoring and with no need of external global positioning systems. In particular, the paper focuses on proposing and comparing different filtering techniques for mitigating Received Signal Strength Indication abrupt fluctuations. Experimental results point out that i) filtering techniques can relevantly improve the efficiency and effectiveness of handover prediction, and ii) the choice of the most appropriate filtering solution to adopt should be made at provisioning time depending on specific service/system requirements, e.g., privileging minimum overhead vs. greater prediction proactivity.\nTitle:\nEvaluating Filtering Strategies for Decentralized Handover Prediction in the Wireless Internet\n\nAbstract:\nNew challenging service scenarios are integrating wireless portable devices with limited and heterogeneous capabilities. They are expected to access both traditional and novel (context-dependent) Internet services. This not only calls for novel infrastructures to support the integration of mobile devices with the fixed network, but also stresses the necessity of negotiation-time tailoring and provision-time adaptation of Quality of Service (QoS). The paper presents a flexible and dynamic middleware for the management of multimedia QoS to wireless devices that can roam during service provisioning. The middleware exploits mobile agents that act as device shadow proxies over the fixed network to transparently follow the device movements between wireless localities, to negotiate the proper QoS level, and to dynamically adapt multimedia flows depending on device profiles and user preferences. In particular, the paper focuses on how the middleware achieves the on-line visibility of the device change of locality in a portable way over different implementations of different wireless technologies, i.e., IEEE 802.11 and Bluetooth. The first experimental results show that, notwithstanding the application-level approach, the middleware reorganization time is compatible with the requirements imposed by the addressed multimedia scenario.\nTitle:\nMobile Middleware Solutions for the Adaptive Management of Multimedia QoS to Wireless Portable Devices\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.429, mean value for all the writers is 8.648, which makes it 1.52 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe widespread availability of devices with multiple wireless interfaces and of heterogeneous positioning techniques open new market potentials for Location Based Services (LBSs), even if complicating their development. The paper claims the need for novel middlewares to manage dynamically retrieved client-side positioning systems in a synergic way and depending on context, i.e., LBS requirements, user preferences, device characteristics, and overall system state. To that purpose, we have designed and implemented PoSIM, a context-aware middleware for the synergic exploitation and control of heterogeneous positioning systems that facilitates the development and portability of LBSs. PoSIM is translucent, i.e., can provide LBS developers with differentiated visibility of data characteristics and control possibilities, thus dynamically adapting to application-specific deployment requirements and enabling cross-layer management decisions. The paper describes the translucent PoSIM architecture, some primary implementation insights, and how to practically use it to simplify LBS development via events, filters, and policies.\nTitle:", "model_inf_time": 2.02}, {"id": "41310", "output": "Learning Image Similarity from Flickr Groups", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMeasuring image similarity is a central topic in computer vision. In this paper, we propose to measure image similarity by learning from the online Flickr image groups. We do so by: Choosing 103 Flickr groups, building a one-versus-all multiclass classifier to classify test images into a group, taking the set of responses of the classifiers as features, calculating the distance between feature vectors to measure image similarity. Experimental results on the Corel dataset and the PASCAL VOC 2007 dataset show that our approach performs better on image matching, retrieval, and classification than using conventional visual features. To build our similarity measure, we need one-versus-all classifiers that are accurate and can be trained quickly on very large quantities of data. We adopt an SVM classifier with a histogram intersection kernel. We describe a novel fast training algorithm for this classifier: the Stochastic Intersection Kernel MAchine (SIKMA) training algorithm. This method can produce a kernel classifier that is more accurate than a linear classifier on tens of thousands of examples in minutes.\nTitle:\nLearning image similarity from Flickr groups using fast kernel machines.\n\nAbstract:\nLearning models for recognizing objects with few or no training examples is important, due to the intrinsic long-tailed distribution of objects in the real world. In this paper, we propose an approach to use comparative object similarity. The key insight is that: given a set of object categories which are similar and a set of categories which are dissimilar, a good object model should respond more strongly to examples from similar categories than to examples from dissimilar categories. We develop a regularized kernel machine algorithm to use this category dependent similarity regularization. Our experiments on hundreds of categories show that our method can make significant improvement, especially for categories with no examples.\nTitle:\nComparative object similarity for improved recognition with few or no examples\n\nAbstract:\nDue to the intrinsic long-tailed distribution of objects in the real world, we are unlikely to be able to train an object recognizer/detector with many visual examples for each category. We have to share visual knowledge between object categories to enable learning with few or no training examples. In this paper, we show that local object similarity information--statements that pairs of categories are similar or dissimilar--is a very useful cue to tie different categories to each other for effective knowledge transfer. The key insight: Given a set of object categories which are similar and a set of categories which are dissimilar, a good object model should respond more strongly to examples from similar categories than to examples from dissimilar categories. To exploit this category-dependent similarity regularization, we develop a regularized kernel machine algorithm to train kernel classifiers for categories with few or no training examples. We also adapt the state-of-the-art object detector to encode object similarity constraints. Our experiments on hundreds of categories from the Labelme dataset show that our regularized kernel classifiers can make significant improvement on object categorization. We also evaluate the improved object detector on the PASCAL VOC 2007 benchmark dataset.\nTitle:\nImproved object categorization and detection using comparative object similarity.\n\nAbstract:\nMany computer vision algorithms limit their performance by ignoring the underlying 3D geometric structure in the image. We show that we can estimate the coarse geometric properties of a scene by learning appearance-based models of geometric classes, even in cluttered natural scenes. Geometric classes describe the 3D orientation of an image region with respect to the camera. We provide a multiple-hypothesis framework for robustly estimating scene structure from a single image and obtaining confidences for each geometric label. These confidences can then be used to improve the performance of many other applications. We provide a thorough quantitative evaluation of our algorithm on a set of outdoor images and demonstrate its usefulness in two applications: object detection and automatic single-view reconstruction.\nTitle:\nGeometric context from a single image\n\nAbstract:\nRecent work in machine learning has significantly benefited semantic extraction tasks in computer vision, particularly for object recognition and image retrieval. We argue that the computer vision techniques that have been successfully applied in those settings can effectively be translated to other domains, such as audio. This claim is supported by recent results in music vs. speech classification, structure from sound, robust music identification and sound object recognition. This paper focuses on two such audio applications and demonstrates how ideas from computer vision map naturally to these problems.\nTitle:\nSemantic Learning for Audio Applications: A Computer Vision Approach\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.135, mean value for all the writers is 8.648, which makes it 1.291 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMeasuring image similarity is a central topic in computer vision. In this paper, we learn similarity from Flickr groups and use it to organize photos. Two images are similar if they are likely to belong to the same Flickr groups. Our approach is enabled by a fast Stochastic Intersection Kernel MAchine (SIKMA) training algorithm, which we propose. This proposed training method will be useful for many vision problems, as it can produce a classifier that is more accurate than a linear classifier, trained on tens of thousands of examples in two minutes. The experimental results show our approach performs better on image matching, retrieval, and classification than using conventional visual features.\nTitle:", "model_inf_time": 1.18}, {"id": "41311", "output": "Object Pop-Out via Image Composition", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present a system for inserting new objects into existing photographs by querying a vast image-based object library, pre-computed using a publicly available Internet object database. The central goal is to shield the user from all of the arduous tasks typically involved in image compositing. The user is only asked to do two simple things: 1) pick a 3D location in the scene to place a new object; 2) select an object to insert using a hierarchical menu. We pose the problem of object insertion as a data-driven, 3D-based, context-sensitive object retrieval task. Instead of trying to manipulate the object to change its orientation, color distribution, etc. to fit the new image, we simply retrieve an object of a specified class that has all the required properties (camera pose, lighting, resolution, etc) from our large object library. We present new automatic algorithms for improving object segmentation and blending, estimating true 3D object size and orientation, and estimating scene lighting conditions. We also present an intuitive user interface that makes object insertion fast and simple even for the artistically challenged.\nTitle:\nPhoto clip art\n\nAbstract:\nWe address the problem of novel view synthesis: given an input image, synthesizing new images of the same object or scene observed from arbitrary viewpoints. We approach this as a learning task but, critically, instead of learning to synthesize pixels from scratch, we learn to copy them from the input image. Our approach exploits the observation that the visual appearance of different views of the same instance is highly correlated, and such correlation could be explicitly learned by training a convolutional neural network (CNN) to predict appearance flows - 2-D coordinate vectors specifying which pixels in the input view could be used to reconstruct the target view. Furthermore, the proposed framework easily generalizes to multiple input views by learning how to optimally combine single-view predictions. We show that for both objects and scenes, our approach is able to synthesize novel views of higher perceptual quality than previous CNN-based techniques.\nTitle:\nView Synthesis By Appearance Flow\n\nAbstract:\nIn this paper, we investigate how, given an image, similar images sharing the same global description can help with unsupervised scene segmentation. In contrast to recent work in semantic alignment of scenes, we allow an input image to be explainedbypartial matchesof similar scenes. This allows fora betterexplanation of the input scenes. We perform MRF-based segmentation that optimizes over matches,while respectingboundaryinformation. Therecoveredsegmentsarethen used to re-querya large database of images to retrieve better matches for the target regions. We show improved performance in detecting the principal occluding and contact boundaries for the scene over previous methods on data gathered from the LabelMe database.\nTitle:\nSegmenting Scenes by Matching Image Composites\n\nAbstract:\nThe goal of this work is to find visually similar images even if they appear quite different at the raw pixel level. This task is particularly important for matching images across visual domains, such as photos taken over different seasons or lighting conditions, paintings, hand-drawn sketches, etc. We propose a surprisingly simple method that estimates the relative importance of different features in a query image based on the notion of \"data-driven uniqueness\". We employ standard tools from discriminative object detection in a novel way, yielding a generic approach that does not depend on a particular image representation or a specific visual domain. Our approach shows good performance on a number of difficult cross-domain visual tasks e.g., matching paintings or sketches to real photographs. The method also allows us to demonstrate novel applications such as Internet re-photography, and painting2gps. While at present the technique is too computationally intensive to be practical for interactive image retrieval, we hope that some of the ideas will eventually become applicable to that domain as well.\nTitle:\nData-driven visual similarity for cross-domain image matching\n\nAbstract:\nThis paper addresses the well-established problem of unsupervised object discovery with a novel method inspired by weakly-supervised approaches. In particular, the ability of an object patch to predict the rest of the object (its context) is used as supervisory signal to help discover visually consistent object clusters. The main contributions of this work are: 1) framing unsupervised clustering as a leave-one-out context prediction task; 2) evaluating the quality of context prediction by statistical hypothesis testing between thing and stuff appearance models; and 3) an iterative region prediction and context alignment approach that gradually discovers a visual object cluster together with a segmentation mask and fine-grained correspondences. The proposed method outperforms previous unsupervised as well as weakly-supervised object discovery approaches, and is shown to provide correspondences detailed enough to transfer keypoint annotations.\nTitle:\nContext As Supervisory Signal: Discovering Objects With Predictable Context\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.639, mean value for all the writers is 8.648, which makes it 0.861 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose a new data-driven framework for novel object detection and segmentation, or \u00c2\u00bfobject pop-out\u00c2\u00bf. Traditionally, this task is approached via background subtraction, which requires continuous observation from a stationary camera. Instead, we consider this an image matching problem. We detect novel objects in the scene using an unordered, sparse database of previously captured images of the same general environment. The problem is formulated in a new image composition framework: 1) given an input image, we find a small set of similar matching images; 2) each of the matches is aligned with the input by proposing a set of homography transformations; 3) regions from different transformed matches are stitched together into a single composite image that best matches the input; 4) the difference between the input and the composite is used to \u00c2\u00bfpop-out\u00c2\u00bf new or changed objects.\nTitle:", "model_inf_time": 1.24}, {"id": "41312", "output": "Key Issues in Web Service Composition", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper investigates architectural properties required for supporting automatic service composition. First, composable service architecture will be described, based on modeling Web Services as abstract machines supported by formally defined composition operators. Based on the proposed infrastructure, we give several options for achieving automatic service composition by treating it as a search problem. Namely, basic heuristic, probabilistic, learning-based, decomposition and bidirectional automatic composition mechanisms will be presented and compared. Finally, it discusses the impact and outlook for automatic composition.\nTitle:\nSearch Strategies For Automatic Web Service Composition\n\nAbstract:\nWe propose architectural properties required for supporting automatic service composition. After defining composable service architecture, we proceed to examine the role of trust and reputation systems in such environment. Based on the proposed infrastructure we give several options for achieving automatic service composition, under the assumption that previously defined requirements are architecturally supported. Finally, we discuss the impact and outlook for automatic composition.\nTitle:\nArchitectural support for automatic service composition\n\nAbstract:\nThe operating system's role is often neglected in the availability analysis of modern, service-oriented applications. The usual argumentation is that the underlying OS seems to be irrelevant in the world of today's web-centric applications. We propose a framework for construction of \"service-oriented operating system\" and examine the role it plays in physical and user-perceived service availability by investigating potential abstractions and integration points between service-oriented applications and OS architecture, such as treating OS as a set of collaborating services, introducing standard middleware services as parts of an OS and including support for server consolidation through virtualization. We demonstrate how to address the following dependability attributes at the OS level: service availability (readiness for correct service), service reliability (continuity of correct service), integrity (absence of improper system alterations) and maintainability (ability to undergo modifications and repair). We further argue that availability at the OS level plays the key role in the availability of service-oriented applications and propose an orthogonal OS design methodology suited for that purpose.\nTitle:\nService-Oriented Operating System: A Key Element in Improving Service Availability\n\nAbstract:\nService-oriented architecture (SOA) is a popular design paradigm for distributed systems today but the high adaptivity and complexity of SOA implementations may also introduce additional sources of faults. We first describe typical steps in SOA to understand possible faults. Then, we provide a corresponding fault taxonomy according to the process of service invocation. Finally, we present possible benefits of our taxonomy for dependability enhancement in SOA-based systems.\nTitle:\nA Fault Taxonomy for Service-Oriented Architecture\n\nAbstract:\nThe Service-Oriented Computing (SOC) paradigm leads the innovation of today's computing system and software development, and it can transform the human society into a new republic of all-to-all connected world. There are a number of interesting and innovative research challenges involved in achieving this vision of Service-Oriented System Engineering (SOSE). This paper highlights some of these significant research opportunities and also the need for enhanced education in this emerging area.\nTitle:\nPerspectives on Service-Oriented Computing and Service-Oriented System Engineering\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.297, mean value for all the writers is 8.648, which makes it 0.299 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWeb service composition lets developers create applications on top of service-oriented computing's native description, discovery, and communication capabilities. Such applications are rapidly deployable and offer developers reuse possibilities and users seamless access to a variety of complex services. There are many existing approaches to service composition, ranging from abstract methods to those aiming to be industry standards. The authors describe four key issues for Web service composition.\nTitle:", "model_inf_time": 0.93}, {"id": "41313", "output": "Advanced Self-Correcting Time Synchronization with Kalman Filtering", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nL7-filter is a significant component in Linux's QoS framework that classifies network traffic based on application layer data. It enables subsequent distribution of network resources in respect to the priority of applications. Considerable research has been reported to deploy multi-core architectures for computationally intensive applications. Unfortunately, the proliferation of multi-core architectures has not helped fast packet processing due to: 1) the lack of efficient parallelism in legacy network programs, and 2) the non-trivial configuration for scalable utilization on multi-core servers. In this paper, we propose a highly scalable parallelized L7-filter system architecture with affinity-based scheduling on a multi-core server. We start with an analytical study of the system architecture based on an offline design. Similar to Receive Side Scaling (RSS) in the NIC, we develop a model to explore the connection level parallelism in L7-filter and propose an affinity-based scheduler to optimize system scalability. Performance results show that our optimized L7-filter has superior scalability over the naive multithreaded version. It improves system performance by about 50% when all the cores are deployed.\nTitle:\nA scalable multithreaded L7-filter design for multi-core servers\n\nAbstract:\nFlow statistics is a basic task of passive measurement and has been widely used to characterize the state of the network.Adaptive Non-Linear Sampling (ANLS)is one of the most accurate and memory-efficient flow statistics method proposed recently. This paper studies the parameter setting problem for ANLS. A parameter self-tuning algorithm is proposed in this paper, which enlarges the parameter to a equilibrium tuning point and renormalizes the counter when counter overflows. It is demonstrated that the estimation error of ANLS with parameter self-tuning algorithm is improved by about 89 times for real trace,70 times for Pareto traffic scenario and 370 times for exponential traffic, while giving the same memory size.\nTitle:\nSelf-Tuning the Parameter of Adaptive Non-linear Sampling Method for Flow Statistics\n\nAbstract:\nThis paper studies discrete-time impulsive hybrid systems. The comparison principle and uniform stability are established for discrete-time impulsive hybrid systems and this kind of systems with time delay. Moreover, the attractive regions of these kinds of systems are estimated. As theoretical applications, the comparison principle is used to study the stability problem for linear interval discrete-time impulsive hybrid systems, a class of nonlinear uncertain discrete-time impulsive hybrid systems, and affine discrete-time impulsive hybrid systems with time delay. Some examples are given to illustrate the results.\nTitle:\nComparison Principle and Stability of Discrete-Time Impulsive Hybrid Systems\n\nAbstract:\nParallel Packet Switch (PPS) is broadly used in designing large-capacity switching fabrics, since it resolves the bottlenecks of scheduling algorithm, memory bandwidth, and serial transmission technology simultaneously. Current packet-based load-balancing algorithms for PPS cannot preserve intra-flow packet orders without costly reordering mechanisms. In this paper, we study the feasibility of flow-based packet-mode load-balancing algorithm, which thoroughly avoids packet out-of-order. Through performance analysis based on traffic model from real-trace, we show that given moderate speedups, all the three proposed flow-based algorithms can provide throughput and delay guarantees under admissible traffic input. Trace-driven simulations are carried out to verify our results.\nTitle:\nFlow-based packet-mode load-balancing for parallel packet switches\n\nAbstract:\nAs a promising approach to improve network reliability, Proactive Failure Recovery (PFR) re-routes data traffic to backup paths without waiting for the completion of routing convergence after a local link failure. However, the diverted traffic may cause congestion on the backup paths if it is not carefully split over multiple paths according to their available capacity. Existing approach assigns new link weights based on links' load and re-calculates the routing paths, which incurs significant computation overhead and is susceptible to route oscillations. In this paper, we propose an efficient scheme for load balancing in PFR. We choose an adequate number of different types of loop-free backup paths for potential failures, and once a failure happens, the affected traffic is diverted to multiple paths in a well balanced manner. We formulate the traffic allocation problem as a tractable linear programming optimization problem, which can be solved iteratively and incrementally. As a result, only the flows affected by the failures are re-allocated to backup paths incrementally without disturbing flows not directly affected by the failures. Simulation results show that our scheme is computationally efficient, can effectively balance link utilization in the network, and can avoid route oscillations.\nTitle:\nLoad-Balanced IP Fast Failure Recovery\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.091, mean value for all the writers is 8.648, which makes it 0.378 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTo improve the two deficiencies of the Self- Correcting Time Synchronization (SCTS) mechanism, we propose a Kalman filter based Advanced SCTS (ASCTS) mechanism in this letter. We prove and explain the close relationship between the basic phase locked loop (PLL) employed by SCTS and the Kalman filter employed by ASCTS. An experimental platform with eight GAINS-3 nodes is used to evaluate the performance.\nTitle:", "model_inf_time": 1.37}, {"id": "41314", "output": "A Novel Signal Integrity Fault Model and Test Pattern Generation Method for Interconnects", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, considering the interconnection topology information, an abstract model and a new test pattern generation method of signal integrity problems on interconnects are proposed. In addition, previous SPICE-based pattern generation methods are too complex and time consuming to generate test patterns for signal integrity faults. To more accurately detect signal integrity defects on practical on-chip interconnection lines and avoid time consuming for interconnection analysis, in this paper, we propose a new high-level signal integrity fault model to estimate noise effects based on process variation and interconnect signal transition. Experimental results show that the proposed signal integrity fault model is more exact for long interconnects than previous approaches. In addition, the proposed method is much faster than the SPICE-based pattern generation method.\nTitle:\nA High-Level Signal Integrity Fault Model and Test Methodology for Long On-Chip Interconnections\n\nAbstract:\nInterconnect test for highly integrated environments becomes more important in terms of its test time and a complete diagnosis, as the complexity of the circuit increases. Since the board-level interconnect test is based on boundary scan technology, it takes a long test time to apply test vectors serially through a long scan chain. Complete diagnosis is another important issue. Since the board-level test is performed for repair, noticing the faulty position is an essential element of any interconnect test. Generally, the interconnect test algorithms that need a short test time cannot perform the complete diagnosis and the algorithms that perform the complete diagnosis need a lengthy test time. To overcome this problem, a new interconnect test algorithm is developed. The new algorithm can provide the complete diagnosis of all faults with a shorter test time compared to the previous algorithms.\nTitle:\nA new maximal diagnosis algorithm for interconnect test\n\nAbstract:\nAs an at-speed solution to board-level interconnect testing, an enhanced boundary-scan architecture utilizing a combination of slightly modified boundary-scan cells and a user-defined register is proposed. Test methods based on the new architecture can accomplish cost-effective at-speed testing and propagation delay measurements on board-level interconnects. Particularly when the board under test has multiple domains of interconnects controlled by different clock speeds, our at-speed solution. is much more efficient than other previous works.\nTitle:\nAt-speed boundary-scan interconnect testing in a board with multiple system clocks\n\nAbstract:\nIn this paper, we propose a new clustered reconfigurable interconnect network (CRIN) BIST to improve the embedding probabilities of random-pattern-resistant-patterns. The proposed method uses a scan-cell reordering technique based on the signal probabilities of given test cubes and specific hardware blocks that increases the embedding probabilities of care bit clustered scan chain test cubes. We have developed a simulated annealing based algorithm that maximizes the embedding probabilities of scan chain test cubes to reorder scan cells, and an iterative algorithm for synthesizing the CRIN hardware. Experimental results demonstrate that the proposed CRIN BIST technique achieves complete fault coverage with lower storage requirement and shorter testing time in comparison with a previous method.\nTitle:\nIncreasing embedding probabilities of RPRPs in RIN based BIST\n\nAbstract:\nTesting PLLs (phase-locked loops) is becoming an important issue that affects both time-to-market and production cost of electronic systems. Though a PLL is the most common mixed-signal building block, it is very difficult to test due to internal analog blocks and signals. In this paper, we propose a new PLL BIST (built-in self test) using the distorted frequency detector that uses only internal digital signals. The proposed BIST does not need to load any analog nodes of the PLL. Therefore, it provides an efficient defect-oriented structural test scheme, reduced area overhead, and improved test quality compared with previous approaches.\nTitle:\nA New Built-In Self Test Scheme For Phase-Locked Loops Using Internal Digital Signals\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.143, mean value for all the writers is 8.648, which makes it 2.129 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nUnacceptable loss of signal integrity may cause permanent or intermittent harm to the functionality and performance of SoCs. In this paper, we present an abstract model and a new test pattern generation method of signal integrity problems on interconnects. This approach is achieved by considering the effects for testing inputs and parasitic RLC elements of interconnects. We also develop a framework to deal with arbitrary interconnection topology. Experimental results show that the proposed signal integrity fault model is more exact and more powerful for long interconnects than previous approaches.\nTitle:", "model_inf_time": 1.46}, {"id": "41315", "output": "SNR-Incremental Stochastic Matching for Robust Speech Recognition", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIt is well-known that the performance of recognition systems is often largely degraded when there is a mismatch between the training and testing environment. It is desirable to compensate for the mismatch when the system is in operation without any supervised learning. Recently, a structural maximum a posteriori (SMAP) adaptation approach, in which a hierarchical structure in the parameter space is assumed, was proposed. In this paper, this SMAP method is applied to unsupervised adaptation. A novel normalization technique is also introduced as a front end for the adaptation process. The recognition results showed that the proposed method was effective even when only one utterance from a new speaker was used for adaptation. Furthermore, an effective way to combine the supervised adaptation and the unsupervised adaptation was investigated to reduce the need for a large amount of supervised learning data.\nTitle:\nUnsupervised Adaptation Using Structural Bayes Approach\n\nAbstract:\nWe address an over-smoothing issue of enhanced speech in deep neural network (DNN) based speech enhancement and propose a global variance equalization framework with two schemes, namely post-processing and post-training with modified object function for the equalization between the global variance of the estimated and the reference speech. Experimental results show that the quality of the estimated clean speech signal is improved both subjectively and objectively in terms of perceptual evaluation of speech quality (PESQ), especially in mismatch environments where the additive noise is not seen in the DNN training.\nTitle:\nGlobal variance equalization for improving deep neural network based speech enhancement\n\nAbstract:\nWe propose a unified speech enhancement framework to jointly handle both background noise and interfering speech in a speaker-dependent scenario based on deep neural networks (DNNs). We first explore speaker-dependent speech enhancement that can significantly improve system performance over speaker-independent systems. Next, we consider interfering speech as one noise type, thus a speaker-dependent DNN system can be adopted for both speech enhancement and separation. Experimental results demonstrate that the proposed unified system can achieve comparable performances to specific systems where only noise or speech interference is present. Furthermore, much better results can be obtained over individual enhancement or separation systems in mixed background noise and interfering speech scenarios. The training data for the two specific tasks are also found to be complementary. Finally, an ensemble learning-based framework is employed to further improve the system performance in low signal-to-noise ratio (SNR) environments. A voice activity detection (VAD) DNN and an ideal ratio mask (IRM) DNN are investigated to provide prior information to integrate two sub-modules at frame level and time-frequency level, respectively. The results demonstrate the effectiveness of the ensemble method in low SNR environments.\nTitle:\nA unified DNN approach to speaker-dependent simultaneous speech enhancement and speech separation in low SNR environments.\n\nAbstract:\nA new one-stage maximum confidence measure (MCM) based interaural phase difference estimation framework for noise masking is proposed to closely integrate the underline speech models into dual-microphone array noise filtering for robust speech recognition. The main ideas are: (1) utilizing both the speech and filler models of the recognizer to feedback confidence measures (CMs) that indicate the degree of separation between filtered speech and interference noises, and (2) automatically optimizing the parameters of the microphone array with an expectation maximization (EM) algorithm based on the proposed MCM criterion. Experimental results on a Mandarin voice command task show that the proposed approach significantly improves the final speech recognition rates. Moreover the observed performance degradation is usually graceful under low signal-to-noise ratios (SNRs) and close interference noises conditions.\nTitle:\nMaximum Confidence Measure Based Interaural Phase Difference Estimation For Noise Masking In Dual-Microphone Robust Speech Recognition\n\nAbstract:\nIn this paper, we derive upper and lower bounds on the mean of speech corrupted by additive noise. The bounds are derived in the log spectral domain. Also approximate bounds on the first and second order time derivatives are developed. It is also shown how to transform these bounds to the Mel frequency cepstral coefficient (MFCC) domain. The proposed bounds are used to define the mismatch neighborhood for minimax classification. It is shown that this parametric neighborhood works quite well for artificially added noise and for a real-life mismatch scenario (moving car environment) which does not fully conform with the theoretical conditions used to derive the bounds. In contrast to traditional neighborhood structure for minimax classification, no empirical tuning of the bounds is required. It is believed that the applicability of the derived bounds is not limited to a minimax setting and can be potentially used to develop various compensation scenarios in the log spectral domain.\nTitle:\nUpper And Lower Bounds On The Mean Of Noisy Speech: Application To Minimax Classification\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.86, mean value for all the writers is 8.648, which makes it 1.887 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, an signal-to-noise ratio (SNR)-incremental stochastic matching (SISM) algorithm is proposed for robust speech recognition in noisy environments. The SISM algorithm is an extension of Sankar and Lee's (1996) stochastic matching (SM) for dealing with the distortion due to additive noise. We address two issues concerning the original maximum likelihood-based SM techniques. One concern is that the initial condition of the expectation-maximization (EM) algorithm has to be set carefully if the mismatch between training and testing is large. The other is that the performance is often limited by the newly adapted model in noise compensation instead of reaching the higher level of accuracy often obtained in clean environments. Our proposed SISM algorithm attempts to improve the initial condition and to relax the performance bound. First, the SISM algorithm provides a good initial condition making use of a set of environment-matched models. The second is a recursive operation, i.e., the reference model in each recursion is changed along the direction of SNR increment in order to push the recognition performance to that obtained at higher SNR levels. Experimental results show that the SISM algorithm provides further improvement after the best environment-matched performance has been reached, and can therefore obtain an additional discriminative power through using the speech models with higher SNR instead of retraining process\nTitle:", "model_inf_time": 1.33}, {"id": "41316", "output": "Robust Acoustic Echo Cancellation via Error Enhancement and System Integration", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper examines the technique of using a memoryless noise-suppressing nonlinearity in the adaptive filter error feedback-loop of an acoustic echo canceler (AEC) based on normalized least-mean square (NLMS) when there is an additive noise at the near-end. It will be shown that introducing the nonlinearity to ldquoenhancerdquo the filter estimation error is well-founded in the information-theoretic sense and has a deep connection to the independent component analysis (ICA). The paradigm of AEC as a problem that can be approached by ICA leads to new algorithmic possibilities beyond the conventional LMS family of techniques. In particular, a right combination of the error enhancement procedure and a properly implemented regularization procedure enables the AEC to be performed recursively and continuously in the frequency domain when there are both ambient noise and double-talk even without the double-talk detection (DTD) or the voice activity detection (VAD) procedure.\nTitle:\nAcoustic echo cancellation based on independent component analysis and integrated residual echo enhancement\n\nAbstract:\nThis paper investigates the use of a signal enhancement technique, namely a noise suppressing nonlinearity, on the adaptive filter error in order to increase the stability and the performance of acoustic echo cancellation (AEC) when there is a continuous distortion to the acoustic echo signal. The algorithm presented here differs from others in that the enhancement of signal is done in the adaptation loop, rather than as a post-processing technique for further reduction of residual echo in the signal, and that the resulting nonlinearity for the cancellation error is formulated as a solution to the signal enhancement problem. Combining the nonlinear error suppression method with NLMS and other adaptive step-size algorithms based on NLMS shows an improvement of between 5 to 15 dB in the average ERLE for additive white noise and around 2 dB for speech coding distortion when a simulated acoustic echo is used. The reduction of the misalignment of 5 dB or more for both noise cases can be expected. The technique is shown to be beneficial also with a real acoustic echo. The new method is seen as a viable technique for improving the existing AEC algorithms when the acoustic echo is corrupted by linear distortion in the form of additive noise or by nonlinear distortion in the form of speech coding.\nTitle:\nEnhancement of residual echo for improved acoustic echo cancellation\n\nAbstract:\nThis paper examines the technique of using a noise suppressing nonlinearity in the adaptive filter error feedback loop of the acoustic echo canceler (AEC) based on the least mean square (LMS) algorithm when there are both double-talk and white background noise at the near-end. By combining the previously introduced noise suppressing technique with a compressive nonlinearity derived from the theory of robust statistics, consistently better results are obtained during double-talk as well as during single-talk when compared to the traditional approach of using only the compressive nonlinearity. It is shown that a compressive form of noise reducing nonlinearity can be derived also from the signal enhancement point of view when the noise probability density (pdf) is tailed more heavily and has a higher kurtosis than the Gaussian pdf. A combination of such a noise compressing nonlinearity and a noise suppressing nonlinearity is capable of producing results that are similar to that of the robust statistics approach during double-talk along with an added benefit of increased robustness during single-talk when there is only the background noise.\nTitle:\nTowards robust acoustic echo cancellation during double-talk and near-end background noise via enhancement of residual echo\n\nAbstract:\nSemi-blind source separation (SBSS) is a special case of the well-known blind source separation (BSS) when some partial knowledge of the source signals is available to the system. In particular, a batch adaptation in the frequency domain based on independent component analysis (ICA) can be effectively used to jointly perform source separation and multichannel acoustic echo cancellation (MCAEC) through SBSS without double-talk detection. Many issues related to the implementation of an SBSS system are discussed in this paper. After a deep analysis of the structure of the SBSS adaptation, we propose a constrained batch-online implementation that stabilizes the convergence behavior even in the worst case scenario of a single far-end talker along with the non-uniqueness condition on the far-end mixing system. Specifically, a matrix constraint is proposed to reduce the effect of the non-uniqueness problem caused by highly correlated far-end reference signals during MCAEC. Experimental results show that high echo cancellation can be achieved just as the misalignment remains relatively low without any preprocessing procedure to decorrelate the far-end signals even for the single far-end talker case.\nTitle:\nBatch-Online Semi-Blind Source Separation Applied to Multi-Channel Acoustic Echo Cancellation\n\nAbstract:\nIn this paper, we propose a new method based on a coherent source spectral estimation for solving the permutation problem of frequency-domain blind source separation (BSS). It combines the robustness of the State Coherence Transform (SCT) to recursively estimate a smooth phase spectrum associated with each source and the precision of the inter-frequency correlation to solve for a correct permutation. Namely, the TDOAs estimated by the SCT are used to constrain the permutation correction process in order to force the resulting filters to be coherent across frequency. This intrinsic interconnection between the TDOA information and the spectral correlation makes the new approach robust even when the signal is short in duration and spatial aliasing is substantial. Experimental results show that the proposed method is able to drastically reduce the number of permutation errors for three sources recorded in a short time block using microphones with large spacing.\nTitle:\nCoherent spectral estimation for a robust solution of the permutation problem\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.967, mean value for all the writers is 8.648, which makes it 1.125 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper examines the technique of using a noise-suppressing nonlinearity in the adaptive filter error feedback-loop of an acoustic echo canceler (AEC) based on the least mean square (LMS) algorithm when there is an interference at the near end. The source of distortion may be linear, such as local speech or background noise, or nonlinear due to speech coding used in the telecommunication networks. Detailed derivation of the error recovery nonlinearity (ERN), which \u201cenhances\u201d the filter estimation error prior to the adaptation in order to assist the linear adaptation process, will be provided. Connections to other existing AEC and signal enhancement techniques will be revealed. In particular, the error enhancement technique is well-founded in the information-theoretic sense and has strong ties to independent component analysis (ICA), which is the basis for blind source separation (BSS) that permits unsupervised adaptation in the presence of multiple interfering signals. The single-channel AEC problem can be viewed as a special case of semi-blind source separation (SBSS) where one of the source signals is partially known, i.e., the far-end microphone signal that generates the near-end acoustic echo. The system approach to robust AEC will be motivated, where a proper integration of the LMS algorithm with the ERN into the AEC \u201csystem\u201d allows for continuous and stable adaptation even during double talk without precise estimation of the signal statistics. The error enhancement paradigm encompasses many traditional signal enhancement techniques and opens up an entirely new avenue for solving the AEC problem in a real-world setting.\nTitle:", "model_inf_time": 1.54}, {"id": "41317", "output": "Fault-Tolerant Query Processing on Large Scientific Datasets", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAs development of high-throughput and low-cost sequencing technologies is leading to massive volumes of genomic data, new solutions for handling data-intensive applications on parallel platforms are urgently required. Particularly, the nature of processing leads to both load balancing and I/O contention challenges. In this paper, we have developed a novel middleware system, RE-PAGE, which allows parallelization of applications that process genomic data with a simple, high-level API. To address load balancing and I/O contention, the features of the middleware include: 1) use of domain-specific information in the formation of data chunks (which can be of non-uniform sizes), 2) replication and placement of each chunk on a small number of nodes, performed in an intelligent way, and 3) scheduling schemes for achieving load balance, when data movement costs out-weigh processing costs and the chunks are of non-uniform sizes. We have evaluated our framework using three genomic applications, which are VarScan, Unified Genotyper, and Coverage Analyzer. We show that our approach leads to better performance than conventional MapReduce scheduling approaches and systems that access data from a centralized store. We also compare against popular frameworks, Hadoop and GATK, and show that our middleware outperforms both, achieving high parallel efficiency and scalability.\nTitle:\nRE-PAGE: Domain-Specific REplication and PArallel Processing of GEnomic Data\n\nAbstract:\nThe deluge of available data for analysis demands the need to scale the performance of data mining implementations. With the current architectural trends, one of the major challenges today is achieving programmability and performance for data mining applications on multi-core machines and cluster of multi-core machines. To address this problem, we have been developing a runtime framework, FREERIDE, that enables parallel execution of data mining and data analysis tasks.The contributions of this paper are two-fold: 1) This paper describes and evaluates various shared-memory parallelization techniques developed in our run-time system on a cluster of multi-cores, and 2) We report on a detailed performance study to understand why certain parallelization techniques out-perform othertechniques for a particular application.\nTitle:\nPerformance Issues in Parallelizing Data-Intensive Applications on a Multi-core Cluster\n\nAbstract:\nWith an increasing use of data mining tools and techniques, we envision that a Knowledge Discovery and Data Mining System (KDDMS) will have to support and optimize for the following scenarios: 1) Sequence of Queries: A user may analyze one or more datasets by issuing a sequence of related complex mining queries, and 2) Multiple Simultaneous Queries: Several users may be analyzing a set of datasets concurrently, and may issue related complex queries.This paper presents a systematic mechanism to optimize for the above cases, targeting the class of mining queries involving frequent pattern mining on one or multiple datasets. We present a system architecture and propose new algorithms to simultaneously optimize multiple such queries and use a knowledgeable cache to store and utilize the past query results. We have implemented and evaluated our system with both real and synthetic datasets. Our experimental results show that our techniques can achieve a speedup of up to a factor of 9, compared with the systems which do not support caching or optimize for multiple queries.\nTitle:\nSimultaneous optimization of complex mining tasks with a knowledgeable cache\n\nAbstract:\nWith growing computational capabilities of parallel machines, scientific simulations are being performed at finer spatial and temporal scales, leading to a data explosion. Careful analysis of this data holds much promise for future scientific discoveries. Particularly, correlation analysis, which focuses on studying the potential relationships among multiple variables, is becoming a useful method for scientific analysis. This paper focuses on the problem of correlation analysis across large-scale simulation datasets, including 1) accelerating this analysis with the use of bitmap indexing as a representative summary of the data, 2) developing efficient algorithms for parallel execution, 3) performing analysis in distributed environments, i.e., for cases where different attributes are stored in geographically distributed repositories, and 4) combining sampling with correlation analysis. These algorithms have been implemented in a system that provides a high-level API for specification of the analyses, including allowing correlation analysis on specified value-based and dimension-based subsets of the data, and supports interactive and incremental analysis. We have extensively evaluated our framework for efficiency, and have also carried out case studies with domain scientists to establish how it can aid data-driven discovery process.\nTitle:\nSupporting correlation analysis on scientific datasets in parallel and distributed settings\n\nAbstract:\nWith increasing emphasis on analysis of large-scale scientific data, and with growing dataset sizes, a number of new challenges are arising. Particularly, novel data management solutions are needed, which can work together with the existing tools. This paper examines indexing support for supporting high-level queries (primarily those for sub setting) on array-based scientific datasets. This work is motivated by the limitations arising in visualizing climate datasets (stored in Net CDF), using tools like Para View. We have developed a new indexing strategy, which can help support a variety of sub setting queries over these datasets, including those requiring sub setting over dimensions/coordinates and those involving variable values. Our approach is based on bitmaps, but involves use of two-level indices and careful partitioning, based on query profiles. We also show how our indexing support can be used for sub setting operations executed in parallel. We compare our solutions against a number of other solutions, and demonstrate that our method is more effective.\nTitle:\nIndexing and Parallel Query Processing Support for Visualizing Climate Datasets\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.154, mean value for all the writers is 8.648, which makes it 0.432 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAs datasets are increasing in size, the data management and processing needs are being met with added parallelism, i.e, by involving more nodes and/or cores in the system. This, in turn, is increasing the chances of failures during processing. In this paper, we present the design and implementation of a fault-tolerant environment for processing queries on large scientific dataset. Our systems meet the following three requirements that we consider essential for any such environment: 1) high efficiency of execution of a particular data analysis task or query, when there are no failures, 2) ability to handle failure of up to a certain number of nodes, and 3) only a modest slowdown in processing times of data analysis task or a query when there are failures. We address these challenges by developing a new data replication scheme, which we refer to as subchunk or subpartition replication. Our system currently supports two types of queries: range queries on spatial data and aggregation queries on point datasets, but the underlying ideas can be extended to other query types as well. Our extensive evaluation shows that we can handle single and rack failures with only modest slowdowns, and particularly, clearly outperform the traditional (chunk or partition replication) schemes.\nTitle:", "model_inf_time": 1.52}, {"id": "41318", "output": "A constructive approach to the design of algorithms and their data structures", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWithout Abstract\nTitle:\nWanna Buy an Algorithm? Cheap! or: Algorithms for Text Searching Which Could Have Commercial Value (Abstract)\n\nAbstract:\nWithout Abstract\nTitle:\nEfficient Two-dimensional Searching\n\nAbstract:\nWe present algorithms for efficient searching of regular expressions on preprocessed text, using a Patricia tree as a logical model for the index. We obtain searching algorithms that run in logarithmic expected time in the size of the text for a wide subclass of regular expressions, and in sublinear expected time for any regular expression. This is the first such algorithm to be found with this complexity.\nTitle:\nFast text searching for regular expressions or automaton searching on tries\n\nAbstract:\nWe present algorithms for efficient searching of regular expressions on preprocessed text, using a Patricia tree as index. We obtain searching algorithms with logarithmic expected time in the size of the text for a wide subclass of regular expressions, and sublinear expected time for any regular expression. These are the first known algorithms to achieve these time complexities.\nTitle:\nEfficient Text Searching of Regular Expressions\n\nAbstract:\nA framework for\nTitle:\nA constructive approach to the design of algorithms and their data structures\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.648, mean value for all the writers is 8.648, which makes it 0.0 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nNo abstract available.\nTitle:", "model_inf_time": 1.0}, {"id": "41319", "output": "VADANA: A Comprehensive Dataset for Face Verification with Age Progression", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we study the task of face verification of age-separated images with the presence of various internal and external factors. We propose a hierarchical local binary pattern (HLBP) feature descriptor for robust face representation across age. The effective representation by HLBP across minimal age, illumination, and expression variations combined with its hierarchical computation provides a discriminative representation of the face image. The proposed face descriptor is combined with an AdaBoost classification framework to model the face verification task as a two-class problem. Experimental results on the FG-NET and MORPH aging datasets indicate that the performance of the proposed framework is robust with respect to images of both adults and children. A detailed empirical analysis on the effects of internal (age gap, gender, and ethnicity) and external (pose, expressions, facial hair, and glasses) factors in the face verification performance is also studied. The results indicate that the verification accuracy reduces as the age gap between the image pair increases. A quantitative comparison on the effects of gender on verification performance by both humans and the proposed machine learning approach is provided. The analysis indicate that the cues aid humans in verifying image pairs with large age gaps, while it aids machines for all age gaps. However, the cues mislead humans in the case of images of children and extra-personal pairs with large age gaps. Our analyses indicate that the pose and expression variations affect the performance, despite training with such variations, while facial hair and glasses act as discriminative cues. A study on the effects of ethnicity indicate that non-linear algorithms have insignificant effect in performance with the use of both generalized and individual ethnicity models when compared with linear algorithms.\nTitle:\nFace verification of age separated images under the influence of internal and external factors\n\nAbstract:\nHuman age provides key demographic information. It is also considered as an important soft biometric trait for human identification or search. Compared to other pattern recognition problems (e.g., object classification, scene categorization), age estimation is much more challenging since the difference between facial images with age variations can be more subtle and the process of aging varies greatly among different individuals. In this work, we investigate deep learning techniques for age estimation based on the convolutional neural network (CNN). A new framework for age feature extraction based on the deep learning model is built. Compared to previous models based on CNN, we use feature maps obtained in different layers for our estimation work instead of using the feature obtained at the top layer. Additionally, a manifold learning algorithm is incorporated in the proposed scheme and this improves the performance significantly. Furthermore, we also evaluate different classification and regression schemes in estimating age using the deep learned aging pattern (DLA). To the best of our knowledge, this is the first time that deep learning technique is introduced and applied to solve the age estimation problem. Experimental results on two datasets show that the proposed approach is significantly better than the state-of-the-art.\nTitle:\nDeeply-Learned Feature for Age Estimation\n\nAbstract:\nIn this paper, we study the face verification task across age by constructing a simple but powerful representation of the face which uses Local Binary Pattern (LBP) histograms. The spatial information is incorporated by constructing a hierarchical representation of the face image and computing the LBP histogram at each level. A set of most discriminative LBP features of the face are extracted using the AdaBoost learning algorithm. A strong classifier is built using a set of weak classifiers extracted and is used for classification purposes. Several experiments on the FGnet and the MORPH database were performed and the results indicate a significant improvement in the performance when compared with other discriminative approaches. Performance improvement is achieved with smaller age gaps between image pairs and it stabilizes as the age gap increases. Also, the facial hair, glasses, etc. provide discriminative cues to the system in face verification.\nTitle:\nFace verification with aging using AdaBoost and local binary patterns\n\nAbstract:\nIn this paper, we target a recent challenging problem raised in image processing area: face verification in the presence of facial makeup. To the best of our knowledge, very few works have been proposed to solve this problem. In this paper, we propose a novel face verification scheme. Random subspace is applied to get multiple correlation spaces. Similarity measures among face images are calculated based on correlation subspaces. In addition, performance on different facial parts is evaluated. Experiments on an unconstrained dataset demonstrate the effectiveness of the proposed approach.\nTitle:\nA new approach for face recognition under makeup changes\n\nAbstract:\nAutomatic human age estimation has attracted a great deal of interest in the past few years. Although many advancements have been made by researchers, there are still many challenges: such as age estimation across different image acquisition methods, different expressions, gender and races. The influence due to race and gender seems to be the most common issue, because collecting a large amount of face images with comprehensive racial diversities seems impractical. The performance will degrade when estimating face images of races that differ from the training set. In this work, we present a new scheme to mitigate the influences of race and gender in the problem of age estimation. Our system will contribute a robust solution to solve the problem of age estimation across races and genders. This study is essential for developing a practical age estimation system (with mixture of races and gender.) To evaluate the performance of the proposed algorithm, we run comprehensive experiments on one widely used big database - MORPH-II, which contains more than 55, 000 images. On an average, an improvement of more than 20% has been achieved using the proposed scheme.\nTitle:\nCan We Minimize the Influence Due to Gender and Race in Age Estimation?\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.029, mean value for all the writers is 8.648, which makes it 0.325 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAnalysis of face images has been the topic of in-depth research with wide spread applications. Face recognition, verification, age progression studies are some of the topics under study. In order to facilitate comparison and benchmarking of different approaches, various datasets have been released. For the specific topics of face verification with age progression, aging pattern extraction and age estimation, only two public datasets are currently available. The FGNET and MORPH datasets contain a large number of subjects, but only a few images are available for each subject. We present a new dataset, VADANA, which complements them by providing a large number of high quality digital images for each subject within and across ages (depth vs. breadth). It provides the largest number of intrapersonal pairs, essential for better training and testing. The images also offer a natural range of pose, expression and illumination variation. A parallel version with aligned faces is also created. Additionally, we provide relationships between subjects. We demonstrate the difference and difficulty of VADANA by testing with state-of-the-art algorithms. Our findings from experiments show how VADANA can aid further research on different types of verification algorithms. The variety of characteristics our data offers facilitate testing and benchmarking of other facial analysis algorithms.\nTitle:", "model_inf_time": 1.68}, {"id": "41320", "output": "A Process-Oriented Framework for Successful ECM Adoption", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nEnterprise content management (ECM) has emerged as a promising research area in the Information Systems (IS) discipline. According to recent works in the field, ECM may increase the consistency, availability, and traceability of content. We argue that one important aspect has dropped off the agenda-the role of creativity. While the above measures are, without any doubt, highly relevant in order to pursue important business imperatives, such as time and budget, organizations must also consider the creativity and innovativeness that employees can unfold in their work processes. In this conceptual paper, we thus advocate an approach to ECM research that not only considers traditional, rather control-oriented factors, but also the impact of ECM on an organization's creativity and vice versa. We propose a research framework for ECM and creativity that is grounded in diverse literatures relating to these two concepts.\nTitle:\nBridging the Gap between Enterprise Content Management and Creativity: A Research Framework\n\nAbstract:\nEnterprise Content Management (ECM) is an emerging concept in Information Systems (IS) research providing the means for an efficient administration of digital content. However, there are lots of obstacles which may face enterprises when adopting ECM. In particular the diligent analysis of an organisation's individual content situation often turns out to be a major success factor. However, adequate guidelines for performing content analyses can hardly be found in ECM literature. In this paper, we propose a process model for analysing content and present the first results that have been gained during its application within the Hilti Corporation.\nTitle:\nTowards A Process Model For Digital Content Analysis - The Case Of Hilti\n\nAbstract:\nThis article provides an analysis of the correlation between a competitive advantage derived from improved decision-making processes and knowledge management through enterprise content management (ECM) platforms. Therefore, it expands literature on knowledge management and explicates the relationships among knowledge management systems, ECM systems, and decision-making processes. In other words our research question is: are the ECM systems able to create value for organizations? If Yes, how? In the studied case, we have seen that decision makers achieve their best performance through improved quantity and quality of input to the decisional process, as well as better formalization of knowledge included through all phases of the process thank to the adoption of ECM systems.\nTitle:\nThe Shadow of ECM: The Hidden Side of Decision Processes\n\nAbstract:\nThe aim of this paper is to investigate the correlation between competitive advantage, associated with the improvement of the decision-making process, and knowledge management through Enterprise Content Management (ECM) platforms. As a result of our study we found that ECM platforms can be seen, as Decision Support Systems as they increase the quantity and the quality of the information needed by the decision makers and the decision processes. This study also analyses how organizations tend to modify their processes according to their capability in managing information.\nTitle:\nThe Role Of Ecm And Its Contribution In Decision-Making Processes\n\nAbstract:\nThe purpose of Business Process Management (BPM) is to increase the efficiency and effectiveness of organizational processes through improvement and innovation. Despite a common understanding that culture is an important element in BPM efforts, there is a dearth of theoretical and empirical research on culture as a facilitator of successful BPM. We develop the BPM culture construct and propose a validated instrument with which to measure organizational cultures' support of BPM. The operationalization of the BPM culture concept provides a theoretical foundation for future research and a tool to assist organizations in developing a cultural environment that supports successful BPM.\nTitle:\nDevelopment and validation of an instrument to measure organizational cultures' support of Business Process Management\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 12.725, mean value for all the writers is 8.648, which makes it 3.478 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn today's digital information age, companies are struggling with an immense overload of mainly unstructured data. Reducing search times, fulfilling compliance requirements and maintaining information quality represent only three of the challenges that organisations from all industry sectors are faced with. Enterprise content management (ECM) has emerged as a promising approach addressing these challenges. Yet, there are still numerous obstacles to the implementation of ECM technologies, particularly fostered by the fact that the key challenges of ECM adaptation processes are rather organisational than technological. In the present article we claim that the consideration of an organisation's business process structure is particularly crucial for ECM success. In response to this, we introduce a process-oriented conceptual framework that systematises the key steps of an ECM adoption. The paper suggests that ECM and business process management are two strongly related fields of research.\nTitle:", "model_inf_time": 1.21}, {"id": "41321", "output": "Evaluating Knowledge Repository Utility for Gene Expression Analysis", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe field of gene expression data analysis has grown in the past few years from being purely data-centric to integrative, aiming at complementing microarray analysis with data and knowledge from diverse available sources. In this review, we report on the plethora of gene expression data mining techniques and focus on their evolution toward knowledge-based data analysis approaches. In particular, we discuss recent developments in gene expression-based analysis methods used in association and classification studies, phenotyping and reverse engineering of gene networks.\nTitle:\nTowards knowledge-based gene expression data mining.\n\nAbstract:\nThis paper describes an information technology, infrastructure aimed at supporting translational bioinformatics studies that require joint management of phenotypic and genotypic data. In particular we integrated an electronic medical record with an open-source environment for data mining to create a flexible and easy to use query system aimed at supporting the discovery of the most frequent complex traits. We propose a logical formalization to define the phenotypes of interest; this is translated into a graphical interface that allows the user to combine different conditions relative to the electronic medical record data (e.g., the presence of a particular pathology). The phenotypes are then stored in a multidimensional database. Then, the data mining system engine reads the filtered data from the database and executes dynamic queries for analyzing phenotypic data, presenting the results in a multidimensional format through a simple web interface. The system has been applied in a study on genetically isolated individuals, the Val Borbera project.\nTitle:\nA dynamic query system for supporting phenotype mining in genetic studies.\n\nAbstract:\nGenome-scale transcription profile is known to be a good reporter of the state of the cell. Much of the early predictive modelling and cell-type clustering relied on this relation and has experimentally confirmed it. We have examined if this also holds for prediction of cell's staging, and focused on the inference of stage prediction models for stem cell development. We show that the problem relates to rank learning and, from the user's point of view, to projection of transcription profile data to a single dimension. Our comparison of several state-of-the-art algorithms on 10 data sets from Gene Expression Omnibus shows that rank-learning can be successfully applied to developmental cell staging, and that relatively simple techniques can perform surprisingly well.\nTitle:\nRanking and 1-dimensional projection of cell development transcription profiles\n\nAbstract:\nUnderstanding the key role that miRNAs play in the regulation of gene expression is one of the most important challenges in modern molecular biology. Standard gene set enrichment analysis (GSEA) is not appropriate in this context, due to the low specificity of the relation between miRNAs and their target genes. We developed alternative strategies to gain better insights in the differences in biological processes involved in different experimental conditions. We here describe a novel method to analyze and interpret miRNA expression data correctly, and demonstrate that annotating miRNA directly to biological processes through their target genes (which is nevertheless the only way possible) is a non-trivial task. We are currently employing the same strategy to relate miRNA expression patterns directly to pathway information, to generate new hypotheses, which may be relevant for the interpretation of their role in the gene expression regulatory processes.\nTitle:\nA data mining library for miRNA annotation and analysis\n\nAbstract:\nSpatial proteomic profiling of tissue sections provides in situ molecular analysis of proteins and peptides. Analysis and visualization of these high-dimensional data cubes is challenging. We present a methodology for this task based on a novel developed algorithm for the feature identification and reduction step. To show the validity of our approach, we analyzed prostate cancer tissue sections with an adapted kernel-density based clustering algorithm.\nTitle:\nAnalysis and Visualization of Spatial Proteomic Data for Tissue Characterization\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.444, mean value for all the writers is 8.648, which makes it 1.532 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMining of biomedical data increasingly relies on utility of knowledge repositories. In gene expression analysis, these are often used for gene labeling with an assumption that similarly annotated genes have similar expression profiles. In the paper we use this assumption to craft a method with which we scored six different annotation sources (e.g. , Gene Ontology, PubMed, and MeSH annotations) for their utility in gene expression data analysis. Experiments show that the sources that include manual curation perform well and, for instance, score better than automatic annotation from gene-related PubMed abstracts. We also show that there is no clear winner, pointing at the need for methods that could successfully integrate annotations from different sources.\nTitle:", "model_inf_time": 1.17}, {"id": "41322", "output": "Function Decomposition for Feature Hierarchy Induction", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nFunction decomposition is a recent machine learning method that develops a hierarchical structure from class-labeled data by discovering new aggregate attributes and their descriptions. Each new aggregate attribute is described by an example set whose complexity is lower than the complexity of the initial set. We show that function decomposition can be used to develop a hierarchical multi-attribute decision model from a given unstructured set of decision examples. The method implemented in a system called HINT is experimentally evaluated on a real-world housing loans allocation problem and on the rediscovery of three hierarchical decision models. The experimentation demonstrates that the decomposition can discover meaningful and transparent decision models of high classification accuracy. We specifically study the effects of human interaction through either assistance or provision of background knowledge for function decomposition, and show that this has a positive effect on both the comprehensibility and classification accuracy.\nTitle:\nA function-decomposition method for development of hierarchical multi-attribute decision models\n\nAbstract:\n We present a novel data mining approach basedon decomposition. In order to analyze a givendataset, the method decomposes it to a hierarchyof smaller and less complex datasets that canbe analyzed independently. The method is experimentallyevaluated on a real-world housingloans allocation dataset, showing that the decompositioncan (1) discover meaningful intermediateconcepts, (2) decompose a relatively complexdataset to datasets that are easy to analyze andcomprehend, and (3) derive a... \nTitle:\nA Dataset Decomposition Approach to Data Mining and Machine Discovery\n\nAbstract:\n Decision table decomposition is a machine learning approach that decomposes agiven decision table into an equivalent hierarchy of decision tables. The approachaims to discover decision tables that are overall less complex than the initial one, potentiallyeasier to interpret, and introduce new and meaningful intermediate concepts.Since an exhaustive search for an optimal hierarchy of decision tables is prohibitivelycomplex, the decomposition uses a suboptimal iterative algorithm that... \nTitle:\nExperimental Evaluation of Three Partition Selection Criteria for Decision Table Decomposition\n\nAbstract:\nWe present a new machine learning method that, given a set of training examples, induces a denition of the target concept in terms of a hierarchy of intermediate concepts and their denitions. This eectively decomposes the problem into smaller, less complex problems. The method is inspired by the Boolean func- tion decomposition approach to the design of digital circuits. To cope with high time complexity of nding an optimal decomposi- tion, we propose a suboptimal heuristic al- gorithm. The method, implemented in pro- gram HINT (HIerarchy Induction Tool), is ex- perimentally evaluated using a set of arti- cial and real-world learning problems. It is shown that the method performs well both in terms of classication accuracy and discovery of meaningful concept hierarchies.\nTitle:\nMachine Learning by Function Decomposition\n\nAbstract:\nTo solve a complex problem, one of the effective general approaches is to decompose it into smaller, less complex and more\n manageable subproblems. In machine learning, this principle is a foundation for structured induction [44]: instead of learning\n a single complex classification rule from examples, define a concept hierarchy and learn rules for each of the (sub)concepts.\n Shapiro [44] used structured induction for the classification of a fairly complex chess endgame and demonstrated that the\n complexity and comprehensiveness (\u201cbrain-compatibility\u201d) of the obtained solution was superior to the unstructured one. Shapiro\n was helped by a chess master to structure his problem domain. Typically, applications of structured induction involve a manual\n development of the hierarchy and a manual selection and classification of examples to induce the subconcept classification\n rules; usually this is a tiresome process that requires an active availability of a domain expert over long periods of time.\n Therefore, it would be very desirable to automate the problem decomposition task.\n \nTitle:\nFunction decomposition in machine learning\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.141, mean value for all the writers is 8.648, which makes it 0.421 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe function decomposition described can identify subsets of existing features and discover nongiven functions that map these subsets to a new feature, also, it can organize the existing and new features into a hierarchy. The authors demonstrate their Hierarchy Induction Tool (HINT) system on a housing loan-allocation application. Methods for switching circuit design often implicitly deal with feature transformation. Such methods construct a circuit to implement a given or partially given tabulated Boolean function. The authors' function-decomposition method can discover and construct a hierarchy of new features that one can add to the original dataset or transform into a hierarchy of less complex datasets. The method allows the decomposition to deal with nominal-feature (that is, not necessarily binary) functions\nTitle:", "model_inf_time": 1.07}, {"id": "41323", "output": "Research on Microscopic Traffic Simulation Platform", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWith the increasing of traffic complexity, traffic simulation becomes an important approach to deal with the complicated traffic problems; meanwhile, system modeling plays a more and more important role in the simulation systems. Cellular automata provide a simple discrete deterministic mathematical model for physical, biological, and computational systems and are shown to be capable of complicated behavior and generate complex and random patterns, which are very suitable for the description of complicate traffic environment [7]. A simulation model based on agent technology, HLA/RTI technology and expanded cellular automaton is presented in this paper. The simulation model makes the platform expandable and flexible, at the same time, it can provide high-capable computing resources to solve the complex traffic issues. In the traffic entity model aspects, the expanded cellular automata and agent technology were adopted to model the behaviors of passengers, vehicles, traffic signal lights and so on. The optimal scheme for evacuation of traffic disaster, superintendence of large scale activities and design of traffic environment will be obtained through the simulation model.\nTitle:\nResearch on modeling of complicate traffic simulation system\n\nAbstract:\nWith the improvement of the traffic complexity extent, the solution of traffic problems becomes more and more difficult in the reality, so traffic simulation is an effective method for analyzing the traffic states and problems. As the simulation area becomes larger and more complicate, which requires large computing and storage resources, it can't be resolved by the traditional computing technology. Due to grid technology's advantages on such aspects, a new simulation architecture named GHA, which implements the HLA 's component as grid service and combined with the agent technology is presented in this paper. Thus the simulation architecture can offer high-capable computing resources to solve the complex traffic issues with great expansibility and flexibility; moreover, it also makes solid foundation to the authenticity of traffic simulation by adopting agent model the character of traffic entity. At last, a traffic simulation platform is implemented based on GHA and the performance tests are given\nTitle:\nResearch on Grid-Based Traffic Simulation Platform\n\nAbstract:\nThe goal of ShanghaiGrid is providing information services to the people. It aims at constructing a metropolitan-area information service infrastructure and establishing an open standard for widespread upper-layer applications from both communities and the government. A typical application named as Traffic Information Grid is discussed in detail.\nTitle:\nShanghaiGrid: a grid prototype for metropolis information services\n\nAbstract:\nIn this paper, we propose a grid security infrastructure based on identity cryptography. We mainly discuss the grid security authentication and authorization architecture by using Tate Pairing. We propose a private key generator security infrastructure and secure group communication scheme by using non-interaction secret sharing protocol and one round tripartite Diffie-Hellman protocol. Finally, we present the advantages and disadvantages of our ID-based security infrastructure comparing with the public key infrastructure in grid circumstance.\nTitle:\nAn identity-based model for grid security infrastructure\n\nAbstract:\nGrid security is a wide topic, touching many of the core issues in information security. It is an area that has been overlooked by the established grid community. In this paper, We explore some roles of identity-based cryptography (IBC) in grid circumstance, and propose a grid security infrastructure model based on identity cryptography. We mainly discuss the grid security authentication and authorization architecture, public key infrastructure based on identity cryptography and security group communication scheme by using weil pairing. The security property of our scheme is discussed. Finally, we compare our ID-based security infrastructure with the public key infrastructure in grid circumstance.\nTitle:\nAn identity-based grid security infrastructure model\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.704, mean value for all the writers is 8.648, which makes it 0.048 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTraffic congestion has become a major concern for many cities throughout the world. Simulations provide useful tools for engineer to plan traffic systems and government to make decisions. The microscopic traffic simulation approach defines the behavior of each interactive object in the traffic network, such as a vehicle and a traffic light through an individual model so that we can observe the detail traffic information of a scenario. Since microscopic model requires large computational power and data storage power, new simulation system architecture is needed. In the paper, the background of this research work is introduced. The simulation platform which combines grid, agent and HLA is given. Some considerations about microscopic traffic simulation models and technical issues of this platform are also discussed.\nTitle:", "model_inf_time": 1.11}, {"id": "41324", "output": "Performance Comparison of Homeless and Home-Based Lazy Release Consistency Protocols in DSM", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents our novel lazy home-based protocol briefly and two implementation techniques. These techniques are developed by exploiting a relaxed memory consistency model such as scope consistency. First, we present a novel diff integration technique which can solve most diff accumulation problems that are prevalent in migratory DSM applications. Second, we propose a dynamic home migration protocol that solves the static home assignment problem in the original home-based protocol. To evaluate our protocol and techniques, we have done performance tests using well known DSM benchmark applications. The performance results proved our diff integration and dynamic home allocation solve the diff accumulation and wrong static home assignment problems respectively.\nTitle:\nPerformance improvement techniques for software distributed shared memory\n\nAbstract:\nThis paper presents our novel protocol design and implementation of an all-software page-based DSM system. The protocol combines the advantages of homeless and home-based protocols. During lock synchronization, it uses a homeless diff-based memory update using the update coherence protocol. The diff-based update during lock synchronization can reduce the time in a critical section since it reduces page faults and costly data fetching inside the critical section. Other than the update in lock synchronization, it uses a home-based page-based memory update using the invalidation coherence protocol. The protocol is called \u201clazy home-based\u201d since the home update is delayed until the next barrier time. The lazy home update has many advantages such as less interruption in home nodes as well as less data traffic and a smaller number of messages. We present an in-depth analysis of the effects of the protocol on DSM applications.\nTitle:\nLazy home-based protocol: combining homeless and home-based distributed shared memory protocols\n\nAbstract:\nSelf-organised maps (SOM) have been widely used for cluster analysis and visualisation purposes in exploratory data mining. In image retrieval applications, SOMs have been used to visualise high-dimensional feature space and build indexing structures. In this paper, we extend the use of SOMs for profiling and comparison of image collections, and present empirical results obtained in collection visualisation, visual and quantitative comparison of collections, and a prototype system implementation.\nTitle:\nVisualisation and comparison of image collections based on self-organised maps\n\nAbstract:\nThis paper describes efforts to facilitate collaborative work in a distributed environment by providing infrastructure that facilitates the understanding of inter-connected processes involved and how they interact. In this work we describe how our agent-based framework supports these. This distributed work environment makes use of both P2P and client-server architectures. Using an example of developing an open source software system, we explain how a collaborative work environment can be achieved. In particular, we address how the support for coordination, collaboration and communication are provided using our framework.\nTitle:\nFacilitating Collaboration in a Distributed Software Development Environment Using P2P Architecture\n\nAbstract:\nThis paper proposes an implementation of a Java networking application programmer's interface (API) for Asynchronous Transfer Mode (ATM) networks. ATM is considered as the network for the future. Since more and more Web-based applications are written in Java these years, it is of high importance to support Java network programming on ATM in order to support World Wide Web style applications on ATM networks. In this paper, we first give a brief introduction of ATM networks. Then, we give an overview of the architecture of applications on ATM networks. Next, we describe the implementation of our Java networking API on ATM. Finally, we point out the weakness of the current Java networking API in relation to ATM networking and discuss future research directions. Our implementation provides a platform for developing Java applications on native ATM protocols and demonstrates the feasibility of designing a new Java networking API on native ATM protocols.\nTitle:\nA Java Networking API for ATM Networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.727, mean value for all the writers is 8.648, which makes it 0.067 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper describes the comparison between homeless and home-based Lazy Release Consistency (LRC) protocols which are used to implement Distributed Shared Memory (DSM) in cluster computing. We present a performance evaluation of parallel applications running on homeless and home-based LRC protocols. We compared the performance between Tread-Marks, which uses homeless LRC protocol, and our home-based DSM system. We found that the home-based DSM system has shown better scalability than TreadMarks in parallel applications we tested. This poor scalability in the homeless protocol is caused by a hot spot and garbage collection, but we have shown that these factors do not affect the scalability of the home-based protocol.\nTitle:", "model_inf_time": 1.46}, {"id": "41325", "output": "Tag-Based Self-Organization in Agent Societies", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nNew mechanisms for group self-organization in agent societies are investigated and examined in the context of sharing digital goods. Specifically we illustrate how cooperative sharers and uncooperative free riders can be placed in different groups of an electronic society in a decentralized manner. We have simulated a decentralized, open P2P system which self-organizes itself to avoid cooperative sharers being exploited by uncooperative free riders. Inspired by human society, we use social mechanisms such as tags, gossip and ostracism. This approach encourages sharers to move to better groups and restricts free riders without necessitating centralized control, which makes the system appropriate for current open P2P systems.\nTitle:\nMechanisms for the self-organization of peer groups in agent societies\n\nAbstract:\nThe objective of this work is to demonstrate how cooperative sharers and uncooperative free riders can be placed in different groups of an electronic society in a decentralized manner. We have simulated an agent-based open and decentralized P2P system which self-organises itself into different groups to avoid cooperative sharers being exploited by uncooperative free riders. This approach encourages sharers to move to better groups and restricts free riders into those groups of sharers without needing centralized control. Our approach is suitable for current P2P systems that are open and distributed. Gossip is used as a social mechanism for information sharing which facilitates the formation of groups. Using multi-agent based simulations we demonstrate how the adaptive behaviour of agents lead to self-organization.\nTitle:\nGossip-Based self-organising open agent societies\n\nAbstract:\nOne of the problems in artificial agent societies is the problem of non-cooperation, where individuals have motivations for not cooperating with others. An example of non-cooperation is the issue of freeriding, where some agents do not contribute to the welfare of the society but do consume valuable resources. New mechanisms for group self-organisation and management in multi-agent societies are presented and examined in a multi-agent societies where nodes of a P2P system are modelled as interacting agents belonging to different groups. The context of interaction between agents is the sharing of digital goods in electronic societies. We have simulated a decentralised P2P system which self-organises itself to avoid cooperative sharers being exploited by uncooperative free riders. Specifically, we illustrate how cooperative sharers and uncooperative free riders can be placed in different groups of an electronic society in a decentralised manner. Inspired by human society, we use social mechanisms such as tags, gossip and ostracism. Our aim here is to restrict exploitation or in other words restrict uncooperative behaviour by separating groups based on performance since it reduces the likelihood of bad agents exploiting the good agents in the better groups. The developed system shows promising results by encouraging sharers to move to better groups and also by restricting free riders without any centralised control,\n which makes these mechanisms appropriate for distributed policy governance. Our work offers new insights into policy mechanisms for regulation of distributed societies.\nTitle:\nAn agent-based simulation for restricting exploitation in electronic societies through social mechanisms\n\nAbstract:\nThe objective of this work is to demonstrate how cooperative sharers and uncooperative free riders can be placed in different groups of an electronic society in a decentralised manner. We have simulated an agent-based open and decentralised P2P system which self-organises itself into different groups to avoid cooperative sharers being exploited by uncooperative free riders. This approach encourages sharers to move to better groups and restricts free riders into those groups of sharers without needing centralised control. Our approach is suitable for current P2P systems that are open and distributed. Gossip is used as a social mechanism for information sharing which facilitates the formation of groups. Using multi-agent based simulations we demonstrate how the adaptive behaviour of agents lead to self-organisation. We have tested with varying the gossip level and checked its impact in the system's behaviour. We have also investigated the impact of false gossip in this system where gossip is the medium for information sharing which leads to self-organisation.\nTitle:\nGossip-Based Self-Organising Agent Societies and the Impact of False Gossip\n\nAbstract:\nThis paper discusses altruistic sharing achieved by tags in an agent society where sharing information incurs a cost and non-sharing is thus the preferred option for selfish agents. We believe that the general features of our tagging mechanism can be used to facilitate altruism and increase the overall social welfare in artificial societies. We describe our findings based on experiments we have conducted through multi-agent-based simulation of artificial societies in the context of agents playing the knowledge-sharing game.\nTitle:\nAltruistic sharing using tags\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.288, mean value for all the writers is 8.648, which makes it 0.546 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper investigates how tags can be used to achieve self-organization of groups in a multi agent society where nodes of a P2P system are modeled as simple interacting agents located in different groups. The context of interaction between agents is the sharing of digital goods in electronic societies. This paper explains how cooperative sharers and uncooperative free riders can be placed in different groups of a P2P system in a decentralized manner. We have simulated a decentralized, distributed, P2P system which self-organizes itself to avoid cooperative sharers being exploited by uncooperative free riders. The developed system based on a gossip mechanism shows promising results by encouraging sharers to move to better groups and also by restricting free riders without any centralized control, which makes it favorable for current P2P systems. The results discussed in this paper were obtained using the simulation of artificial agent societies.\nTitle:", "model_inf_time": 1.34}, {"id": "41326", "output": "Monitoring Social Expectations in Virtual Worlds", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nOnline virtual worlds provide a rich platform for remote human interaction, and are increasingly being used as a simulation platform for multi-agent systems and as a way for software agents to interact with humans. It would therefore be beneficial to provide techniques allowing high-level agent development tools, especially cognitive agent platforms such as belief-desire-intention (BDI) programming frameworks, to be interfaced with virtual worlds. This is not a trivial task as it involves mapping potentially unreliable sensor readings from complex virtual environments to a domain-specific abstract logical model of observed properties and/or events. This paper investigates this problem in the context of agent interactions in a multi-agent system simulated in Second Life. We present a framework which facilitates the connection of any multi-agent platform with Second Life, and demonstrate it in conjunction with the Jason BDI interpreter.\nTitle:\nInterfacing a cognitive agent platform with a virtual world: a case study using Second Life\n\nAbstract:\nThe use of computers to mediate social interactions (e.g. blogs, chatting, facebook, second life) creates the possibility of providing software to support social awareness in a range of ways. In this paper we focus on monitoring expectations and consider how a user who is not a programmer or logician might specify expectations to be monitored. We propose a novel approach where the user provides a collection of scenarios, and then candidate formulae are induced from the scenarios. The approach is applied to examples and appears to be promising.\nTitle:\nEliciting expectations for monitoring social interactions\n\nAbstract:\nSecond Life is a popular multi-purpose online virtual world that provides a rich platform for remote human interaction. It is increasingly being used as a simulation platform to model complex human interactions in diverse areas, as well as to simulate multi-agent systems. It would therefore be beneficial to provide techniques allowing high-level agent development tools, especially cognitive agent platforms such as belief-desire-intention (BDI) programming frameworks, to be interfaced to Second Life. This is not a trivial task as it involves mapping potentially unreliable sensor readings from complex Second Life simulations to a domain-specific abstract logical model of observed properties and/or events. This paper investigates this problem in the context of agent interactions in a multi-agent system simulated in Second Life. We present a framework that facilitates the connection of any multi-agent platform with Second Life, and demonstrate it in conjunction with an extension of the Jason BDI interpreter.\nTitle:\nInterfacing a cognitive agent platform with second life\n\nAbstract:\nThis paper discusses the role that expectations have in agent reasoning, and focuses on the author's previous work on modelling and monitoring expectations with a complex temporal structure, and its application to expectation monitoring in virtual worlds. It also presents a proposal for a new extension of this work by integrating it with the event calculus to simplify the definition of institutions with actions that create expectations. It is shown how this \\\"expectation event calculus\\\" could provide a uniform basis for reasoning about various types of expectation, and commitments and norms in particular.\nTitle:\nAgents and Expectations.\n\nAbstract:\nThis paper proposes a rule language for defining social expectations based on a metric interval temporal logic with past and future modalities and a current-time binding operator. An algorithm for run-timemonitoring compliance of rules in this language based on formula progression is also outlined.\nTitle:\nA rule language for modelling and monitoring social expectations in multi-agent systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.987, mean value for all the writers is 8.648, which makes it 0.289 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOnline virtual worlds such as Second Life provide a rich medium for unstructured human interaction in a shared simulated 3D environment. However, many human interactions take place in a structured social context where participants are subject to expectations governing their behaviour, and current virtual worlds do not provide any support for this type of interaction. There is therefore an opportunity to adapt the tools developed in the multi-agent systems community for structured social interactions between software agents (inspired by human society) and adapt these for use with the computer-mediated human communication provided by virtual worlds. This paper describes the integration of one such tool with Second Life. A model checker for monitoring social expectations defined in temporal logic has been integrated with Second Life, allowing users to be notified when their expectations of others have been fulfilled or violated.\nTitle:", "model_inf_time": 1.06}, {"id": "41327", "output": "Network Error-Correcting Codes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCoherent network error correction is the error-control problem in network coding with the knowledge of the network codes at the source and sink nodes. With respect to a given set of local encoding kernels defining a linear network code, we obtain refined versions of the Hamming bound, the Singleton bound, and the Gilbert-Varshamov bound for coherent network error correction. Similar to its classical counterpart, this refined Singleton bound is tight for linear network codes. The tightness of this refined bound is shown by two construction algorithms of linear network codes achieving this bound. These two algorithms illustrate different design methods: one makes use of existing network coding algorithms for error-free transmission and the other makes use of classical error-correcting codes. The implication of the tightness of the refined Singleton bound is that the sink nodes with higher maximum flow values can have higher error correction capabilities.\nTitle:\nRefined Coding Bounds and Code Constructions for Coherent Network Error Correction\n\nAbstract:\nWith respect to a given set of local encoding kernels defining a linear network code, refined versions of the Hamming bound, the Singleton bound and the Gilbert-Varshamov bound for network error correction are proved by the weight properties of network codes. This refined Singleton bound is also proved to be tight for linear message sets.\nTitle:\nRefined Coding Bounds for Network Error Correction\n\nAbstract:\nIn this paper, we study the error correction and detection capabilities of block codes for a general transmission system inspired by network error correction. For a given weight measure on the error vectors, we define a corresponding minimum weight decoder. Then we obtain a complete characterization of the capabilities of a block code for error correction and error detection. Our results imply that for a linear network code with the Hamming weight being the weight measure on the error vectors, the capability of the code is fully characterized by a single minimum distance. By contrast, for a nonlinear network code, two different minimum distances are needed for characterizing the capabilities of the code for error correction and for error detection. This leads to the surprising discovery that for a nonlinear network code, the number of correctable errors can be more than half of the number of detectable errors. We further define equivalence classes of weight measures with respect to a channel. Specifically, for any given code, the minimum distance decoders for two different weight measures are equivalent if the two weight measures belong to the same equivalence class.\nTitle:\nCharacterization of error correction and detection in a general transmission system\n\nAbstract:\nWe introduce variable-rate linear network coding for single-source finite acyclic network. In this problem, the source of a network transmits messages at different rates in different time sessions and every nonsource node in the network decodes the messages if possible. We propose two efficient algorithms for implementing variable-rate linear network coding under different circumstances.\nTitle:\nVariable-Rate Linear Network Coding\n\nAbstract:\nIn this paper, we first study the error correction and detection capability of codes for a general transmission system inspired by network error correction. For a given weight measure on the error vectors, we define a corresponding minimum weight decoder. Then we obtain a complete characterisation of the capability of a code for (1) error correction; (2) error detection and (3) joint error correction and detection. Our results show that if the weight measure on the error vectors is the Hamming weight, the capability of a linear code is fully characterised by a single minimum distance. By contrast, for a nonlinear code, two different minimum distances are needed for characterising the capabilities of the code for error correction and for error detection. This leads to the surprising discovery that for a nonlinear code, the number of correctable errors can be more than half of the number of detectable errors. We also present a framework that captures joint error correction and detection. We further define equivalence classes of weight measures with respect to a channel. Specifically, for any given code, the minimum weight decoders for two different weight measures are equivalent if the two weight measures belong to the same equivalence class. In the special case of linear network coding, we study three weight measures, and show that they are in the same equivalence class of the Hamming weight and induce the same minimum distance as the Hamming weight. Copyright (C) 2008 John Wiley & Sons, Ltd.\nTitle:\nWeight properties of network codes\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.613, mean value for all the writers is 8.648, which makes it 0.883 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe introduce network error-correcting codes for error correction when a source message is transmitted to a set of receiving nodes on a network. The usual approach in existing networks, namely link-by-link error correction, is a special case of network error correction. The network generalizations of the Hamming bound and the Gilbert-Varshamov bound are derived.\nTitle:", "model_inf_time": 1.07}, {"id": "41328", "output": "Load-Balanced Multicast Switching Fabric Based on Self-Routing and Network Coding", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nScheduling algorithms are crucial for most existing switches to improve the throughput. However, the delay of the switching fabric cannot be guaranteed with such scheduling algorithms. This paper aims to design a novel load-balanced wire-speed multicast switching fabric along with the attractive merits of network coding. We adopt a two-phase self-routing switching fabric constructed by Boolean-mul...\nTitle:\nMulticast switching fabric based on network coding and algebraic switching theory\n\nAbstract:\nConcentration has been a popular technique for resolving contention in switching. In particular, self-route concentration is useful inside ATM switching fabrics in many ways. This paper presents an efficient algorithm for the construction of self-route concentrators from multi-stage cascades of 2x2 sorters, where the main criterion on the complexity is the number of stages in the cascade. The algorithm extensively generalizes the existing Fast Knockout algorithm and mixes in some k-sorting technique. The result includes a best known construction of m-to-n self-route concentrator for all m and n within the practical range.\nTitle:\nFast knockout algorithm for self-route concentration\n\nAbstract:\nThrough rigorous formalization of basic notions about interconnection networks, new and old constructions of interconnection networks are naturally organized into a systematic account. Meanwhile, generally known but vague facts about the theoretical properties of routing networks are stated precisely. \u201cReversible routing networks\u201d form the basis of a class of output-buffer self-route ATM switches. Such networks can be used in error-correcting routing, and some are better choices over others in terms of the modularity of exchange patterns between stages of routing cells. The \u201cRotary switch\u201d is designed over any reversible routing network. It corrects a misstep in routing without necessarily an exact reversion. This allows multiple paths for a deflected packet to proceed on and thereby reduces the probability of collision. Simulations have shown low packet loss rate vs. either the number of switching stages or the size of output multiplexers\nTitle:\nFormalization of self-route networks and the rotary switch\n\nAbstract:\nThe most compact pattern for interconnecting small switches into a large switch is conceivably the 2-stage interconnection network. A certain version of a 2-stage interconnection network is said to preserve switches with a certain attribute when the network constructs such a switch from smaller ones at nodes. Recursive application of this version of 2-stage interconnection then leads to the construction of indefinitely large switches with the same attribute. The present paper studies a few new classes of switches that are preserved under 2-stage interconnection and also identifies their applications. Different switch preservation theorems presented in the paper offer the advantages of both the algorithmic flexibility and the theoretic insight.\nTitle:\nTheory on Switch Preservation under 2-stage Interconnection\n\nAbstract:\nTo recover simultaneous multiple failures in erasure coded storage systems, Patrick Lee et al introduce concurrent repair based minimal storage regenerating codes to reduce repair traffic. The architecture of this approach is simpler and more practical than that of the cooperative mechanism in non-fully distributed environment, hence this paper unifies such class of regenerating codes as concurrent regenerating codes and further studies its characteristics by analyzing cut-based information flow graph in the multiple-node recovery model. We present a general storage-bandwidth tradeoff and give closed-form expressions for the points on the curve, including concurrent repair mechanism based on minimal bandwidth regenerating codes. We show that the general concurrent regenerating codes can be constructed by reforming the existing single-node regenerating codes or multiplenode cooperative regenerating codes. Moreover, a connection to strong-MDS is also analyzed. On the other respect, the application of RGC is hardly limited to repairing. It is of great significance for scaling, a scenario where we need to increase(decrease) nodes to upgrade(degrade) redundancy and reliability. Thus, by clarifying the similarities and differences, we integrate them into a unified model to adjust to the dynamic storage network.\nTitle:\nConcurrent Regenerating Codes and Scalable Application in Network Storage.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.982, mean value for all the writers is 8.648, which makes it 0.568 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA good switching fabric should be endowed with the properties of no internal buffers, delay guarantee, low component complexity and high-speed multicast, which are difficult for conventional switching fabrics to achieve, fueling the great interest in designing a new switching fabric that can support large-scale extension and high-speed multicast. Motivated by this, we reuse the self-routing Boolean concentrator network and embed a Multicast Packets Copy Separation (MPCS) in front to construct a load-balanced multicast switching fabric. Concretely, MPCS module replicates the multicast packets and forwards them according to the multicast addresses. The first phase of LB-MSNC is responsible for balancing the incoming traffic into uniform cells while the second phase is in charge of self-routing the cells to their final destinations. Differing from the existing fabrics, LB-MSNC is combined with the merits of network coding against the packet loss. Experimental results and analysis have verified that the proposed fabric is able to achieve high-speed multicast switching and suitable for building super large-scale switching fabric in Next Generation Network(NGN) with all the advantages mentioned above. \u00a9 2015 IEEE.\nTitle:", "model_inf_time": 1.62}, {"id": "41329", "output": "Multi-modal Situated Learning for Robot Grasping", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA major challenge for the realization of intelligent robots is to supply them with cognitive abilities in order to allow ordinary users to program them easily and intuitively. One approach to such programming is teaching work tasks by interactive demonstration. To make this effective and convenient for the user, the machine must be capable of establishing a common focus of attention and be able to use and integrate spoken instructions, visual perception, and non-verbal clues like gestural commands. We report progress in building a hybrid architecture that combines statistical methods, neural networks, and finite state machines into an integrated system for instructing grasping tasks by man-machine interaction. The system combines the GRAVIS-robot for visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation, and a modality fusion module to allow multi-modal task-oriented man-machine communication with respect to dextrous robot manipulation of objects.\nTitle:\nMulti-modal human-machine communication for instructing robot grasping tasks\n\nAbstract:\nWe argue that direct experimental approaches to elucidate the architecture of higher brains may benefit from insights gained from exploring the possibilities and limits of artificial control architectures for robot systems. We present some of our recent work that has been motivated by that view and that is centered around the study of various aspects of hand actions since these are intimately linked with many higher cognitive abilities. As examples, we report on the development of a modular system for the recognition of continuous hand postures based on neural nets, the use of vision and tactile sensing for guiding prehensile movements of a multifingered hand, and the recognition and use of hand gestures for robot teaching. Regarding the issue of learning, we propose to view real-world learning from the perspective of data-mining and to focus more strongly on the imitation of observed actions instead of purely reinforcement-based exploration. As a concrete example of such an effort we report on the status of an ongoing project in our laboratory in which a robot equipped with an attention system with a neurally inspired architecture is taught actions by using hand gestures in conjunction with speech commands. We point out some of the lessons learnt from this system, and discuss how systems of this kind can contribute to the study of issues at the junction between natural and artificial cognitive systems.\nTitle:\nNeural architectures for robot intelligence.\n\nAbstract:\nWe present a strategy for grasping of real world objects with two anthropomorphic hands, the three-fingered 9- DOF hydraulic TUM and the very dextrous 20-DOF pneumatic Bielefeld Shadow Hand. Our approach to grasping is based on a reach-pre-grasp-grasp scheme loosely motivated by human grasping. We comparatively describe the two robot setups, the control schemes, and the grasp type determination. We show that the grasp strategy can robustly cope with inaccurate control and object variation. We demonstrate that it can be ported among platforms with minor modifications. Grasping success is evaluated by comparative experiments performing a benchmark test on 21 everyday objects.\nTitle:\nPlatform portable anthropomorphic grasping with the bielefeld 20-DOF shadow and 9-DOF TUM hand\n\nAbstract:\nWe present a vision system for human-machine interaction based on a small wearable camera mounted on glasses. The camera views the area in front of the user, especially the hands. To evaluate hand movements for pointing gestures and to recognise object references, an approach to integrating bottom-up generated feature maps and top-down propagated recognition results is introduced. Modules for context-free focus of attention work in parallel with the hand gesture recognition. In contrast to other approaches, the fusion of the two branches is on the sub-symbolic level. This method facilitates both the integration of different modalities and the generation of auditory feedback.\nTitle:\nIntegrating context-free and context-dependent attentional mechanisms for gestural object reference\n\nAbstract:\nWe present a biologically motivated architecture for object recognition that is capable of online learning of several objects based on interaction with a human teacher. The system combines biological principles such as appearance-based representation in topographical feature detection hierarchies and context-driven transfer between different levels of object memory. Training can be performed in an unconstrained environment by presenting objects in front of a stereo camera system and labeling them by speech input. The learning is fully online and thus avoids an artificial separation of the interaction into training and test phases. We demonstrate the performance on a challenging ensemble of 50 objects.\nTitle:\nOnline Learning Of Objects In A Biologically Motivated Visual Architecture\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.201, mean value for all the writers is 8.648, which makes it 0.472 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA key prerequisite to make user instruction of work tasks by interactive demonstration effective and convenient is situated multi-modal interaction aiming at an enhancement of robot learning beyond simple low-level skill acquisition. We report the status of the Bielefeld GRAVIS-robot system that combines visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation to allow multi-modal task-oriented instructions. With respect to this platform, we discuss the essential role of learning for robust functioning of the robot and sketch the concept of an integrated architecture for situated learning on the system level. It has the long-term goal to demonstrate speech-supported imitation learning of robot actions. We describe the current state of its realization to enable imitation of human hand postures for flexible grasping and give quantitative results for grasping a broad range of everyday objects.\nTitle:", "model_inf_time": 1.38}, {"id": "41330", "output": "Autonomous Watercraft Fleets: A Promising Platform for Multi-Robot Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAs we advance the state of technology for robotic systems, there is a need for defining complex real-world challenge problems for the multi-agent/robot community to address. A well-defined challenge problem can motivate researchers to aggressively address and overcome core domain challenges that might otherwise take years to solve. As the focus of multi-agent research shifts from the mature domains of UGV and UAVs to USVs, there is a need for outlining well-defined and realistic challenge problems. In this position paper, we define one such problem, flood disaster mitigation. The ability to respond quickly and effectively to disasters is essential to saving lives and limiting the scope of damage. The nature of floods dictates the need for a fast-deployable fleet of low-cost and small autonomous boats that can provide situational awareness (SA), damage assessment and deliver supplies before more traditional emergency response assets can access affected areas. In addition to addressing an essential need, the outlined application provides an interesting challenge problem for advancing fundamental research in multi-agent systems (MAS) specific to the USV domain. In this paper, we define a technical statement of this MAS challenge problem based and outline MAS specific technical constraints based on the associated real-world issues. Core MAS sub-problems that must be solved for this application include coordination, control, human interaction, autonomy, task allocation, and communication. This problem provides a concrete and real-world MAS application that will bring together researchers with a diverse range of expertise to develop and implement the necessary algorithms and mechanisms.\nTitle:\nFlood disaster mitigation: a real-world challenge problem for multi-agent unmanned surface vehicles\n\nAbstract:\nThe present study investigates the effect of the number of controlled robots on performance of an urban search and rescue (USAR) task using a realistic simulation. Task performance increased in going from four to eight controlled robots but deteriorated in moving from eight to twelve. Workload increased monotonically with number of robots. Performance per robot decreased with increases in team size. Results are consistent with earlier studies suggesting a limit of between 8-12 robots for direct human control. This study demonstrates that these findings generalize to a more realistic setting and complex task.\nTitle:\nScaling effects in multi-robot control\n\nAbstract:\nTechnology for multirobot systems has advanced to the point where we can consider their use in a variety of important domains, including urban search and rescue. A key to the practical usefulness of multirobot systems is the ability to have a large number of robots effectively controlled by small numbers of operators. In this paper, two modalities for controlling a team of 24 robots in a foraging task in an urban search and rescue environment are compared. In both modalities, multiple operators must monitor video streams from the robots to detect and mark victims on a map as well as teleoperating robots that cannot get themselves out of difficult situations. In the first modality, the operators must also provide waypoints for the robots to explore, using both video and a partially completed map to choose appropriate waypoints. In the second modality, the robots autonomously plan their paths, allowing operators to focus on monitoring the video, but without being able to interpret video streams to guide exploration. Experimental results show that significantly better overall performance is achieved with autonomous path planning, although the reduction in operator workload is not significant.\nTitle:\nTowards an understanding of the impact of autonomous path planning on victim search in USAR\n\nAbstract:\nThe rapidly improving availability of small, unmanned aerial vehicles (UAVs) and their ever reducing cost is leading to considerable interest in multi-UAV applications. However, while UAVs have become smaller and cheaper, there is a lack of sensors that are light, small and power efficient enough to be used on a small UAV yet are capable of taking useful measurements of objects often several hundred metres below them. Static or video cameras are one option, however image processing normally requires human input or at least computationally intensive offboard processing, restricting their applicability to very small UAV teams. In this paper, we look at how teams of UAVs can use very small Relative Signal Strength Indicator (RSSI) sensors whose only capability is to detect the approximate strength of a Radio Frequency (RF) signal, to search for and accurately locate such sources. RSSI sensors give at most an approximate range to an RF emitter and will be misleading when signals overlap. Applications of such UAV teams range from finding lost hikers or skiers carrying small RF beacons to military reconnaissance operations. Moreover, the core techniques have a wider applicability to a range of robotic teams that rely on highly uncertain sensors, e.g., search and rescue in disaster environments.\nTitle:\nLocating RF emitters with large UAV teams\n\nAbstract:\nThe human role in sophisticated information gathering systems is usually conceived to be that of the consumer. Human sensory and perceptual capabilities, however, outstrip our abilities to process information by a substantial amount. The use of human operators as ldquoperceptual sensorsrdquo is standard practice for both UAVs and ground robotics where humans are called upon to ldquoprocessrdquo camera video to find targets and assist in navigation. In this paper we illustrate the human role as sensor referencing results of an earlier experiment investigating human performance of operator, navigator, or ldquoperceptual sensorrdquo tasks for teams of 4, 8, and 12 simulated pioneer P3AT robots. The experiment shows humans to be resource limited for the navigation/control task as the number of robots increases while the perceptual sensor function was less affected. We discuss the implications of using humans in a ldquoperceptual sensorrdquo role for information gathering from robotic teams and some of the difficulties including shifts in context and difficulties in developing situation awareness that are likely to arise.\nTitle:\nUsing humans as sensors in robotic search\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.86, mean value for all the writers is 8.648, which makes it 0.181 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMulti-robot systems (MRS) have received a great deal of attention recently due to their potential to address complex distributed tasks such as environmental monitoring, search and rescue, agriculture, and security [3, 4, 5, 1, 2]. One specific type of multi-robot system that has significant near term promise is fleets of autonomous watercraft for applications such as flood response, water monitoring and bathymetry. Small watercraft are an attractive option for real world multi-robot systems because some of the most critical robotic problems are minimized on water - movement is relatively simple and dangers are relatively low.\nTitle:", "model_inf_time": 1.68}, {"id": "41331", "output": "Parallelizing Heterogeneous Tasks Across Computational Nodes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nResearch on optimization in multi-agent systems (MASs) has contributed with a wealth of techniques to solve many of the challenges arising in a wide range of multi-agent application domains. Multi-agent optimization focuses on casting MAS problems into optimization problems. The solving of those problems could possibly involve the active participation of the agents in a MAS. Research on multi-agen...\nTitle:\nA Tutorial on Optimization for Multi-Agent Systems.\n\nAbstract:\nIn this paper we address efficient decentralised coordination for cooperative multi-agent systems (framed as DCOPs) by taking into account the communication and computational resources of the system. We focus on techniques that exploit structural independence among agents' actions to provide optimal solutions to the coordination problem, and, in particular, we use the Generalized Distributed Law (GDL) algorithm. In this settings, we propose a novel resource aware heuristic to build junction trees and to schedule GDL computations across the agents. Our approach aims at minimising directly the total running time of the coordination process, rather than the theoretical complexity of the computation, by considering computational and communication capabilities of agents.\nTitle:\nEfficient multi-agent coordination using resource-aware junction trees\n\nAbstract:\nIn this paper we present an asynchronous distributed mechanism for allocating tasks in a team of robots. Tasks to be allocated are dynamically perceived from the environment and can be tied by execution constraints. Conflicts among team mates arise when an uncontrolled number of robots execute the same task, resulting in waste of effort and spatial conflicts. The critical aspect of task allocation in Multi Robot Systems is related to conflicts generated by limited and noisy perception capabilities of real robots. This requires significant extensions to the task allocation techniques developed for software agents. The proposed approach is able to successfully allocate roles to robots avoiding conflicts among team mates and maintaining low communication overhead. We implemented our method on AIBO robots and performed quantitative analysis in a simulated environment.\nTitle:\nTask Assignment With Dynamic Perception And Constrained Tasks In A Multi-Robot System\n\nAbstract:\nWe propose a model for the allocation of agents to tasks when the tasks have a cost which grows over time. Our model accounts for both the natural growth of tasks and the effort of the agents at containing such growth. The objective is to produce solutions that minimize the growth of tasks (potentially stopping such growth) by efficiently coordinating the operations of the agents. This problem has strong spatial and temporal components, as the agents require time not only to work on the tasks but also to move between tasks and during that time the costs of completing the tasks continue to grow. We propose a novel distributed coordination algorithm, called Lazy max-sum, which works well even when the model of the environment has errors. The algorithm handles homogeneous as well as heterogeneous agents, which can do different amounts of work per time unit and have different travel speeds. We show experimentally that the algorithm outperforms other methods in both a simple simulation and the RoboCup Rescue agent simulation.\nTitle:\nLazy max-sum for allocation of tasks with growing costs.\n\nAbstract:\nBucket elimination (BE) is a framework that encompasses several algorithms, including belief propagation (BP) and variable elimination for constraint optimization problems (COPs). BE has significant computational requirements that can be addressed by using graphics processing units (GPUs) to parallelize its fundamental operations, i.e., composition and marginalization, which operate on functions r...\nTitle:\nAn Efficient Approach for Accelerating Bucket Elimination on GPUs.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.464, mean value for all the writers is 8.648, which makes it 0.157 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper tackles the problem of parallelizing heterogeneous computational tasks across a number of computational nodes (aka agents) where each agent may not be able to perform all the tasks and may have different computational speeds. An equivalent problem can be found in operations research, and it is known as scheduling tasks on unrelated parallel machines (also known as R\u2225Cmax). Given this eq...\nTitle:", "model_inf_time": 1.16}, {"id": "41332", "output": "Secure Partial Repair in Wireless Caching Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nConsider a distributed storage system where parts of the source file fragments in storage nodes are lost. We denote a storage node that lost a part of its fragments as a faulty storage node and a storage node that lost non of its fragment as a complete storage node. In a process, termed as partial repair, a set of storage nodes (among faulty and complete storage nodes) transmit repairing fragments to other faulty storage nodes to recover the lost fragments. We first investigate the optimal partial repair in which the required bandwidth for recovering the lost fragments is minimal. Next, we assume that an eavesdropper wiretaps a subset of links connecting storage nodes, and overhears a number of repairing fragments. We then study optimal secure partial-repair in which the partial-repair bandwidth is minimal and the eavesdropper obtains no information about the source file by overhearing the repairing fragments. We propose optimal secure codes for exact partial-repair in a special scenario.\nTitle:\nOptimal secure partial-repair in distributed storage systems\n\nAbstract:\nWe investigate the repair problem for wireless caching networks when parts of stored packets in cashing nodes are lost. We first develop theoretical lower bounds on the number of necessary transmission packets over error-free broadcast channels for repair. Then we discuss the impact of the distribution of the lost packets among caching nodes. Finally, we study the construction of repair codes and propose the optimal exact repair for some special scenarios.\nTitle:\nPartial Repair for Wireless Caching Networks With Broadcast Channels\n\nAbstract:\n  Reliability is essential for storing files in many applications of distributed storage systems. To maintain reliability, when a storage node fails, a new node should be regenerated by a repair process. Most of the previous results on the repair problem assume perfect (error-free) links in the networks. However, in practice, especially in a wireless network, the transmitted packets (for repair) may be lost due to, e.g., link failure or buffer overflow. We study the repair problem of distributed storage systems in packet erasure networks, where a packet loss is modeled as an erasure. The minimum repair-bandwidth, namely the amount of information sent from the surviving nodes to the new node, is established under the ideal assumption of infinite number of packet transmissions. We also study the bandwidth-storage tradeoffs in erasure networks. Then, the use of repairing storage nodes (nodes with smaller storage space) is proposed to reduce the repair-bandwidth. We study the minimal storage of repairing storage nodes. For the case of a finite number of packet transmissions, the probability of successful repairing is investigated. We show that the repair with a finite number of packet transmissions may use much larger bandwidth than the minimum repair-bandwidth. Finally, we propose a combinatorial optimization problem, which results in the optimal repair-bandwidth for the given packet erasure probability and finite packet transmissions. \nTitle:\nRepair for Distributed Storage Systems in Packet Erasure Networks.\n\nAbstract:\nWe study the transmission cost of repair in a distributed storage system, where storage nodes are connected together through an arbitrary network topology, and there is a cost in the use of the network link. Contrary to the classical model, where there exists a link between a pair of storage node, in our repair model there might not exist a link between some pairs of storage nodes or it might be expensive to use. For that, we propose surviving nodes cooperation in repair, meaning that the surviving nodes as the intermediate nodes combine their received packets with their own stored packets and then transmit coded packets towards the new node. We show that surviving node cooperation can reduce the repair-cost, the sum of the costs for transmitting repairing data between the surviving nodes and the new node. For the system that allows surviving node cooperation, we find the minimum-cost codes in repair by firstly deriving a lower bound of the repair-cost through an optimization problem and then proposing achievable codes. We show the gain of the proposed codes in reducing the repair-cost in some scenarios. Copyright \u00a9 2016 John Wiley & Sons, Ltd.\nTitle:\nOptimized-Cost Repair in Multi-hop Distributed Storage Systems with Network Coding\n\nAbstract:\nWe study the repair problem of distributed storage systems in erasure networks where the packets transmitted from surviving nodes to the new node might be lost. The fundamental storage-bandwidth tradeoff is calculated by multicasting analysis in erasure networks. The optimal tradeoff bound can be asymptotically achieved when the number of transmission (packets) goes to infinity. For a limited number of transmission, we study the probability of successful regenerating. Then, we investigate two approaches of increasing the probability of successful regenerating, namely, by connecting more surviving nodes or by increasing the storage space of nodes. Using more nodes may pose larger delay and in certain situation it might not be possible to connect to more nodes too. We show that in addition to reducing repair bandwidth, increasing storage space can also increase reliability for repair.\nTitle:\nRepair for distributed storage systems with erasure channels\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.557, mean value for all the writers is 8.648, which makes it 0.776 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe study security in partial repair in wireless caching networks where parts of the stored packets in the caching nodes are susceptible to be erased. Let us denote a caching node that has lost parts of its stored packets as a sick caching node and a caching node that has not lost any packet as a healthy caching node. In partial repair, a set of caching nodes (among sick and healthy caching nodes) broadcast information to other sick caching nodes to recover the erased packets. The broadcast information from a caching node is assumed to be received without any error by all other caching nodes. All the sick caching nodes then are able to recover their erased packets, while using the broadcast information and the nonerased packets in their storage as side information. In this setting, if an eavesdropper overhears the broadcast channels, it might obtain some information about the stored file. We thus study secure partial repair in the senses of information-theoretically strong and weak security. In both senses, we investigate the secrecy caching capacity, namely, the maximum amount of information which can be stored in the caching network such that there is no leakage of information during a partial repair process. We then deduce the strong and weak secrecy caching capacities, and also derive the sufficient finite field sizes for achieving the capacities. Finally, we propose optimal secure codes for exact partial repair, in which the recovered packets are exactly the same as erased packets.\nTitle:", "model_inf_time": 1.34}, {"id": "41333", "output": "Mobile Crowd Wireless Charging for Hybrid Energy-Supplied Sensors: A W3W Optimization Problem", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWith the rapid increasing of the number of mobile devices and their embedded sensing technologies, mobile crowd sensing (MCS) has become an emerging modern sensing paradigm for performing large-scale urban sensing. One of the key challenges of large-scale mobile crowd sensing systems is how to effectively select the minimum set of appropriate participants from the huge user pool to perform the sensing tasks. The capability of a particular user for certain task depends on many factors, such as her moving pattern/behavior, device capability, sensor quality, or even uploading bandwidth. Many of these information of participants are unknown by the selection mechanism. Therefore, self-learning based approaches have been proposed to learn the users' capability for certain tasks via multiple trials and their online performances. In this paper, we first model the cumulative participant selection problem as a combinational multi-armed bandit problem and present an online selection algorithm which leverages the historical performing records of participants to learn the different capabilities (both sensing probability and time delay) of participants. Further, to consider the cost of switching participant for particular tasks, we then introduce the cumulative participant selection problem with switch costs and propose a corresponding online learning method. For both proposed learning algorithms, we provide regret analysis. In addition, extensive simulations with real-world mobile datasets are conducted for the evaluations of the proposed methods. Our simulation results confirm the effeteness of them.\nTitle:\nCumulative Participant Selection with Switch Costs in Large-Scale Mobile Crowd Sensing\n\nAbstract:\nThe appearance of smart mobile devices with communication, computation and sensing capability and increasing popularity of various mobile applications have caused the explosion of mobile data recently. In the same time, mobile sensing has been emerging as a new sensing paradigm where vast numbers of mobile devices are used for sensing and collecting huge amounts of mobile data in cities. One of the challenges faced by mobile sensing is how to efficiently collect the huge amount of mobile data beyond the existing capacity of 4G networks. In this paper, we investigate the feasibility of collecting data packets from mobile devices through device-to-device communications by carefully selecting the subset of relaying (or/and sensing) devices. We formulate these problems as optimization problems and propose a set of solutions to solve them. Our experiments over a real-life mobile trace confirm the effectiveness of the proposed idea.\nTitle:\nParticipant selection for data collection through device-to-device communications in mobile sensing.\n\nAbstract:\nWith the rapid increasing of smart phones and their embedded sensing technologies, mobile crowd sensing (MCS) becomes an emerging sensing paradigm for performing large-scale sensing tasks. One of the key challenges of large-scale mobile crowd sensing systems is how to effectively select the minimum set of appropriate participants from the huge user pool to perform the tasks. However, the capabilities of individual participants are usually unknown by the selection mechanism, which leads to the most challenging issue of participant selection. While online learning techniques can be used to learn the participant's capability, the diverse expertise of each individual makes a single capability metric is not sufficient. To address the multi-expertise of participants, in this paper we introduce a new self-learning architecture which leverages the historical performing records of participants to learn the different capabilities (both sensing probability and time delay) of participants. Formulating the participant selection problem as a combinational multi-armed bandit problem, we present an online participant selection algorithm with both performance guarantee and bounded regret. Extensive simulations with a real-world mobile dataset demonstrate the efficiency of the proposed solution.\nTitle:\nMulti-expertise Aware Participant Selection in Mobile Crowd Sensing via Online Learning\n\nAbstract:\nWith the surging of smartphone sensing, wireless networking, and mobile social networking techniques, Mobile Crowd Sensing and Computing (MCSC) has become a promising paradigm for cross-space and large-scale sensing. MCSC extends the vision of participatory sensing by leveraging both participatory sensory data from mobile devices (offline) and user-contributed data from mobile social networking services (online). Further, it explores the complementary roles and presents the fusion/collaboration of machine and human intelligence in the crowd sensing and computing processes. This article characterizes the unique features and novel application areas of MCSC and proposes a reference framework for building human-in-the-loop MCSC systems. We further clarify the complementary nature of human and machine intelligence and envision the potential of deep-fused human--machine systems. We conclude by discussing the limitations, open issues, and research opportunities of MCSC.\nTitle:\nMobile Crowd Sensing and Computing: The Review of an Emerging Human-Powered Sensing Paradigm\n\nAbstract:\nMobile Crowd Sensing (MCS) enables the platform to offer data-based service by incentivizing mobile users to perform sensing task and collecting sensing data from them. Most of the existing works on MCS only consider designing incentive mechanisms for a single MCS platform. In this paper, we study the incentive mechanism in MCS with multiple platforms under two scenarios: competitive platform and ...\nTitle:\nMP-Coopetition: Competitive and Cooperative Mechanism for Multiple Platforms in Mobile Crowd Sensing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.667, mean value for all the writers is 8.648, which makes it 0.869 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe usage of hybrid energy supplied sensors in the Internet of Things has enabled longer lifetime of sensors and expanded scope of applications. These sensors can combine advantages of environmental energy harvesting techniques and wireless energy harvesting techniques. However, how to coordinate them is still a challenge and has not been studied extensively. In this article, we present a system based on mobile crowd wireless charging to manage energy of hybrid energy supplied sensors. When environmental energy is insufficient, the system will utilize smart devices carried by mobile users as chargers to provide wireless energy. We construct and study a W3W problem in the system: <underline>w</underline>hen to leverage mobile crowd wireless charging to support rechargeable sensors, <underline>w</underline>here to perform wireless energy transfer, and <underline>w</underline>hom to allocate and incentivize as chargers to maximize useful energy value over all sensors subject to a budget. In order to control the actual quality of wireless energy charging, we propose a design principle named task completion trustfulness. We consider offline and online conditions and design corresponding algorithms with incentive allocations. Extensive simulations are conducted to demonstrate the effectiveness of our algorithms, which also validates our theoretical results.\nTitle:", "model_inf_time": 1.77}, {"id": "41334", "output": "Nash Implementation for Algorithmic Mechanism Design", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we study the set cover games when the elements are selfish agents. In this case, each element has a privately known valuation of receiving the service from the sets, i.e., being covered by some set. Each set is assumed to have a fixed cost. We develop several approximately efficient truthful mechanisms, each of which decides, after soliciting the declared bids by all elements, which elements will be covered, which sets will provide the coverage to these selected elements, and how much each element will be charged. For set cover games when both sets and elements are selfish agents, we show that a cross-monotonic payment-sharing scheme does not necessarily induce a truthful mechanism.\nTitle:\nMechanism design for set cover games when elements are agents\n\nAbstract:\nFor multi-hop ad hoc networks formed by individually owned nodes, the endpoints can only observe whether or not the end-to-end transaction was successful or not, but not the individual actions of intermediate nodes. Consequently, in the absence of properly designed incentive schemes, rational (i.e., selfish) intermediate nodes may choose to forward data packets at a very low priority or simply drop the packets at all, and it could put the blame on the unreliable wireless channel. Using a principal-agent model, we propose several efficient methods that can eliminate the hidden actions under hidden information in multi-hop wireless networks with high probability. We design several algorithmic mechanisms for a number of routing scenarios such that each selfish agent will maximize its utility (i.e., profit) when it truthfully declares its type (i.e., cost and its actions) and it truthfully follows its declared actions. Our simulations show that the payment by our mechanisms is only slightly larger than the actual cost incurred by all intermediate nodes.\nTitle:\nHidden information and actions in multi-hop wireless ad hoc networks\n\nAbstract:\nThe family of Vickrey-Clarke-Groves (VCG) mechanisms is ar- guably the most celebrated achievement in truthful mechanism de- sign. However, VCG mechanisms have their limitations. They only apply to optimization problems with a utilitarian objective func- tion, and their output should optimize the objective function. For many optimization problems, finding the optimal output is compu- tationally intractable. If we apply VCG mechanisms to polynomial- time algorithms that approximate the optimal solution, the resulting mechanisms may no longer be truthful. In light of these limitations, it is useful to study whether we can design a truthful non-VCG payment scheme that is computation- ally tractable for a given output method O. In this paper, we fo- cus our attention on binary demand games in which the agents' only available actions are to take part in the a game or not to. For these problems, we prove that a truthful mechanism M = (O;P) exists (with proper payment method P) if and only if O satisfies a certain monotone property. We also provide several general al- gorithms to compute the payments efficiently for various types of output. In particular, we show how a truthful payment can be com- puted through \"or/and\" combinations, round-based combinations, and some more complex combinations of outputs from subgames.\nTitle:\nTowards Truthful Mechanisms for Binary Demand Games: A General Framework\n\nAbstract:\nWe propose novel solutions for unicast routing in wireless networks consisted of selfish terminals: in order to alleviate the inevitable over-payment problem (and thus economic inefficiency) of the VCG (Vickrey-Clark-Groves) mechanism, we design a mechanism that results in Nash equilibria rather than the traditional strate-gyproofness (using weakly dominant strategy). In addition, we systematically study the unicast routing system in which both the relay terminals and the service requestor (either the source or the destination nodes or both) could be selfish. To the best of our knowledge, this is the first paper that presents social efficient unicast routing systems with proved performance guarantee. Thus, we call the proposed systems: Optimal Unicast Routing Systems (OURS).Our main contributions of OURS are as follows. (1) For the principal model where the service requestor is not selfish, we propose a mechanism that provably creates incentives for intermediate terminals to cooperate in forwarding packets for others. Our mechanism substantially reduces the overpayment by using Nash equilibrium solutions as opposed to strategyproof solutions. We then study a more realistic case where the service requestor can act selfishly. (2) We first show that if we insist on the requirement of strategyproofness for the relay terminals, then no system can guarantee that the central authority can retrieve at least 1overn of the total payment. (3) We then present a strategyproof unicast system that collects 1over2n of the total payment, which is thus asymptotically optimum. (4) By only requiring Nash Equilibrium solutions, we propose a system that creates incentives for the service requestor and intermediate terminals to correctly follow the prescribed protocol. More importantly, the central authority can retrieve at least half the total payment. We verify the economic efficiency of our systems through simulations that are based on very realistic terminal distributions.\nTitle:\nOURS: optimal unicast routing systems in non-cooperative wireless networks\n\nAbstract:\nNumerous routing protocols have been proposed for wireless networks. A common assumption made by the majority of these protocols is that each wireless node will follow the prescribed protocol without any deviation. This may not be true in practice since wireless nodes could be owned by users who perform in their own interests. We then have to design routing protocols that still work properly even for networks composed of selfish nodes. In this paper, we propose a unicast routing protocol to address this issue under the assumption that all networking nodes are rational. Here, a node is rational if it always chooses a strategy that maximizes its benefit. We assume that each node has a privately known cost of relaying a unit of data for other nodes. In our protocol, each wireless node has to declare a cost for forwarding a unit of data. When a node wants to send data to the access point, it first computes the least cost path to the access point and then computes a payment to each node on this path. We present a pricing mechanism such that the profit of each relay node is maximized when it declares its true cost. We also give a time optimal method to compute the payment in a centralized manner. We then discuss in detail how to implement the routing protocol in the distributed manner. We conduct extensive simulations to study the ratio of the total payment over the total cost incurred by all relay nodes. We find that this ratio is small in practice. Our protocol works when the wireless nodes will not collude and we show that no truthful mechanism can avoid the collusion of any pair of two nodes. We also give a truthful mechanism when a node only colludes with its neighbors.\nTitle:\nLow-cost routing in selfish and rational wireless ad hoc networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.553, mean value for all the writers is 8.648, which makes it 0.772 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMost of the recent works on algorithmic mechanism design exploit the solution concept of dominant strategy equilibria. Such work designs a proper payment scheme so that selfish agents maximize their utility by truthfully revealing their types. It has been pointed out that these truthful mechanisms, the famous among them being the VCG mechanisms, often incur high payments and fruglity ratios. In this work, we exploit the solution concept of Nash implementation to overcome this problem. Our mechanisms induce a set of Nash equilibria so that selfish agents have incentive to act based on a Nash equilibrium. We prove that our mechanisms enjoy substantial advantages over the truthful mechanisms in terms of payment and frugality.\nTitle:", "model_inf_time": 1.42}, {"id": "41335", "output": "Flow-Level Traffic Matrix Generation for Data Center Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe number of data centers deployed by governments, enterprises, and universities has been increased affected by the development of cloud computing technologies to reduce CAPAX and OPEX. Many architectures or topologies for data center networks have been proposed to address the diverse purposes and requirements. However, the construction of data centers incurs significant costs. Moreover, there are many technologies that can affect the structure of the data center. Before building a data center, it must be confirmed that it possesses the characteristics necessary to satisfy requirements. Efficient ways to find and confirm network characteristics include simulation and tests using a traffic generation method. Our proposed method is designed to generate network traffic that address many characteristics of data center networks explored by several studies. The proposed method generates network traffic utilizing flow-level traffic matrix, not directly generates packets. We used Python programming language to create traffic matrix and iPerf to generate network packets. To evaluate it, we compared the generation results to real network traffic collected from a data center network. The result shows that the generated traffic is similar with the real network traffic.\nTitle:\nFlow-level traffic matrix generation for various data center networks\n\nAbstract:\nIn this paper we propose a data center traffic engineering method based on multi-objective optimization. We define a problem of traffic engineering in data center networks as a multi-objective optimization problem with two contrary objectives: load balancing and energy saving. Traffic engineering models for load balancing and energy saving are represented as a linear programming equation. Our simulation results show that the proposed method enables data center operators to change the relative importance between load balancing and energy saving. We also show the traffic engineering results with the predefined upper bound of maximum link utilization and energy cost in response to service requirements and energy OPEX. The contributions of proposed traffic engineering method are (a) to minimize both maximum link utilization and energy cost simultaneously; (b) to minimize maximum link utilization with upper bound of energy cost; and (c) to minimize energy cost with upper bound of maximum link utilization.\nTitle:\nMulti-objective optimization-based traffic engineering for data center networks\n\nAbstract:\nThe number of mobile devices and the amount of mobile traffic are increasing rapidly. The users of mobile device use various services such as web surfing, email, video and audio streaming, etc. Analyzing mobile traffic is necessary to provide high quality mobile network services such as QoS management, traffic engineering, etc. In fact, several multi-flow applications consume a lot of mobile network resources. In this paper, we performed measurement analysis to observe not only application-level traffic characteristics, but also device-level traffic characteristics in terms of traffic volume and flow characteristics. We present the relationship between application traffic and its origin device to understand the application traffic generation from the different types of devices.\nTitle:\nMeasurement analysis of mobile traffic in enterprise networks\n\nAbstract:\nWith the emergence of significant amounts of mobile and cloud services, the scale of Data Center Networks (DCNs) has been growing rapidly. A DCN has different network requirements compared to a traditional Internet Protocol (IP) network, and the existing Ethernet/IP style protocols constrain the DCNs scalability and its manageability. In this paper, we present a scalable failover method for large scale DCNs. Because most of the current DCNs are managed in a logically centralized manner with a specialized topology and growth model, we adopt Fat-Tree [1] as the reference DCN topology and design our failover method using an Open Flow-based approach. Further, to provide scalability, we design our failover algorithm in a local optimal manner, with which only three switches must be modified for handling a single fault, regardless of the size of the target network. We evaluate our failover method in terms of failover time by varying the network size and load balancing capability during failover. The experiment results show that our method scales well, even for a large-scale DCN with more than ten thousands hosts.\nTitle:\nScalable Failover Method For Data Center Networks Using Openflow\n\nAbstract:\nMost of network operation/management tasks such as traffic accounting, traffic engineering, and network design require an accurate and timely network Traffic Matrix (TM). A TM presents the network traffic volume between origin and destination (OD) nodes. Many network management methods assumed that an accurate traffic matrix is given by other network entities, but this is not true. To apply those methods in the practice, the first priority challenge is to obtain an accurate and timely TM. Another issue to obtaining TM is to estimate the change of TM in the near future, not just reporting historical traffic measurement results. Meanwhile, Software Defined Networking (SDN) paradigm has attracted a significant interest from industry and academia as a future network architecture. Within SDN environment, many advantages exist compared to traditional IP based network. To acquire an accurate and timely TM for SDN, we propose FLAME, a TM estimation method based on Poisson Shot Noise process. FLAME is designed to take advantages of the SDN paradigm, and it reflects network traffic characterics revealed by current measurement studies. To evaluate FLAME, we compared the estimation results with a real data center network traffic trace. The evaluation results show that the estimated TM is 66.36% similar compared to the measured traffic.\nTitle:\nFLAME: Flow level traffic matrix estimation using poisson shot-noise process for SDN\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.32, mean value for all the writers is 8.648, which makes it 0.573 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe number of data centers has been increased for various reasons such as cloud computing, big-data analysis, multimedia service, etc. With public interests on data center, many novel technologies for data center networks have been proposed and deployed to support data center operations more efficiently and effectively. However, the construction of data center network incurs significant costs. Moreover, various technologies interplay each other to achieve multiple objectives, and it makes difficult to validate and/or verify characteristics of data center network. In addition, it difficult to perform experiments with a number of hosts and switches. Therefore, it is necessary to observe the characteristics of target data center network before building it. A common approach to evaluate data center is to run simulations that should be similar with real-world data center environment. However, generating traffic with the characteristics of data center networks is not matured yet. People still employ a traffic generator based on the characteristics of Internet traffic. We design a traffic generator that shows more accurate characteristics of data center network traffic. Various traffic characteristics exploited explored by several studies are considered. The proposed method generates flow-level network traffic matrix based on Poisson Shot-Noise model. We implemented the traffic generator using Python programming language to create traffic matrix. To evaluate the proposed method, we compare the results with real data center network traffic. Our results show that the generated traffic owns similar characteristics with the real network traffic in terms of flow size, duration, and the mean and variance of total traffic rate.\nTitle:", "model_inf_time": 1.52}, {"id": "41336", "output": "Sentence Correction for English-Chinese Language Transfer Using a Relative Position Language Model", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSentence correction has been an important emerging issue in computer-assisted language learning. However, existing techniques based on grammar rules or statistical machine translation are still not robust enough to tackle the common errors in sentences produced by second language learners. In this paper, a relative position language model and a parse template language model are proposed to complement traditional language modeling techniques in addressing this problem. A corpus of erroneous English-Chinese language transfer sentences along with their corrected counterparts is created and manually judged by human annotators. Experimental results show that compared to a state-of-the-art phrase-based statistical machine translation system, the error correction performance of the proposed approach achieves a significant improvement using human evaluation.\nTitle:\nSentence Correction Incorporating Relative Position and Parse Template Language Models\n\nAbstract:\nError correction techniques have been proposed in the applications of language learning and spoken dialogue systems for spoken language understanding. These techniques include two consecutive stages: the generation of correction candidates and the selection of correction candidates. In this study, a Context-Dependent Syllable Cluster (CD-SC)-based Confusion Matrix is proposed for the generation of correction candidates. A Contextual Fitness Score, measuring the sequential relationship to the neighbors of the candidate, is proposed for corrected syllable sequence selection. Finally, the n-gram language model is used to determine the final word sequence output. Experiments show that the proposed method improved from 0.742 to 0.771 in terms of BLEU score as compared to the conventional speech recognition mechanism.\nTitle:\nCandidate Generation For Asr Output Error Correction Using A Context-Dependent Syllable Cluster-Based Confusion Matrix\n\nAbstract:\nIn this study, a novel approach to robust dialogue act detection for error-prone speech recognition in a spoken dialogue system is proposed. First, partial sentence trees are proposed to represent a speech recognition output sentence. Semantic information and the derivation rules of the partial sentence trees are extracted and used to model the relationship between the dialogue acts and the derivation rules. The constructed model is then used to generate a semantic score for dialogue act detection given an input speech utterance. The proposed approach is implemented and evaluated in a Mandarin spoken dialogue system for tour-guiding service. Combined with scores derived from the ASR recognition probability and the dialogue history, the proposed approach achieves 84.3% detection accuracy, an absolute improvement of 34.7% over the baseline of the semantic slot-based method with 49.6% detection accuracy.\nTitle:\nSemantic information and derivation rules for robust dialogue act detection in a spoken dialogue system\n\nAbstract:\nThis study presents a novel approach to error diagnosis of Chinese sentences for Chinese as second language (CSL) learners. A penalized probabilistic First-Order Inductive Learning (pFOIL) algorithm is presented for error diagnosis of Chinese sentences. The pFOIL algorithm integrates inductive logic programming (ILP), First-Order Inductive Learning (FOIL), and a penalized log-likelihood function for error diagnosis. This algorithm considers the uncertain, imperfect, and conflicting characteristics of Chinese sentences to infer error types and produce human-interpretable rules for further error correction. In a pFOIL algorithm, relation pattern background knowledge and quantized t-score background knowledge are proposed to characterize a sentence and then used for likelihood estimation. The relation pattern background knowledge captures the morphological, syntactic and semantic relations among the words in a sentence. One or two kinds of the extracted relations are then integrated into a pattern to characterize a sentence. The quantized t-score values are used to characterize various relations of a sentence for quantized t-score background knowledge representation. Afterwards, a decomposition-based testing mechanism which decomposes a sentence into background knowledge set needed for each error type is proposed to infer all potential error types and causes of the sentence. With the pFOIL method, not only the error types but also the error causes and positions can be provided for CSL learners. Experimental results reveal that the pFOIL method outperforms the C4.5, maximum entropy, and Naive Bayes classifiers in error classification.\nTitle:\nError Diagnosis of Chinese Sentences Using Inductive Learning Algorithm and Decomposition-Based Testing Mechanism\n\nAbstract:\nIn this paper, an approach for polyglot speech synthesis based on cross-lingual frame selection is proposed. This method requires only mono-lingual speech data of different speakers in different languages for building a polyglot synthesis system, thus reducing the burden of data collection. Essentially, a set of artificial utterances in the second language for a target speaker is constructed based on the proposed cross-lingual frame-selection process, and this data set is used to adapt a synthesis model in the second language to the speaker. In the cross-lingual frame-selection process, we propose to use auditory and articulatory features to improve the quality of the synthesized polyglot speech. For evaluation, a Mandarin-English polyglot system is implemented where the target speaker only speaks Mandarin. The results show that decent performance regarding voice identity and speech quality can be achieved with the proposed method.\nTitle:\nPolyglot speech synthesis based on cross-lingual frame selection using auditory and articulatory features\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.136, mean value for all the writers is 8.648, which makes it 2.123 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSentence correction has been an important and emerging issue in computer-assisted language learning. However, existing techniques based on grammar rules or statistical machine translation are still not robust enough to tackle the common incorrect word order errors in sentences produced by second language learners of Chinese. In this paper, a novel relative position. language model is proposed to address this problem, for which a corpus of erroneous English-Chinese language transfer sentences along with their corrected counterparts is created and manually judged by human annotators. Experimental results show that compared to a scoring approach based on an n-gram language model and a phrase-based machine translation system, the performance in terms of BLEU scores of the proposed approach achieved improvements of 20.3% and 26.5% for the correction of word order errors resulting from language transfer, respectively.\nTitle:", "model_inf_time": 1.58}, {"id": "41337", "output": "Practical Solution Methods for Combinatorial Optimization Problems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present an algorithm for the generation of all partitions of a graph G with positive edge weights into k mincuts. The algorithm is an enumeration procedure based on the cactus representation of the mincuts of G. We report computational results demonstrating the efficiency of the algorithm in practice and describe in more detail a specific application for generating cuts in branch-and-cut algorithms for the traveling salesman problem.\nTitle:\nGenerating partitions of a graph into a fixed number of minimum weight cuts\n\nAbstract:\nThis paper presents new algorithms for the maximum flow problem, the Hitchcock transportation problem and the general minimum-cost flow problem. Upper bounds on the number of steps in these algorithms are derived, and are shown to improve on the upper ...\nTitle:\nConstructing new facets of the consecutive ones polytope\n\nAbstract:\nThe target visitation problem (TVP) is concerned with finding a route to visit a set of targets starting from and returning to some base. In addition to the distance traveled a tour is evaluated by taking also preferences into account which address the sequence in which the targets are visited. The problem thus is a combination of two well-known combinatorial optimization problems: the traveling salesman and the linear ordering problem. In this paper we point out some polyhedral properties and develop a branch-and-cut algorithm for solving the TVP to optimality. Some computational results are presented.\nTitle:\nInter Programming Models For The Target Visitation Problem\n\nAbstract:\n. \u00a0\u00a0 Essential for the success of branch-and-cut algorithms for solving combinatorial optimization problems are the availability\n of reasonable tight relaxations and effective routines for solving the associated separation problems. In this paper we introduce\n the concept of small instance relaxations which can be particularly useful for problems with symmetric structure. Small instance\n relaxations are based on the facets of polytopes associated with small instances of the combinatorial optimization problem\n to be solved and can be generated automatically by facet enumeration. For a certain class of symmetric problems, we describe\n a general approach to the separation problem. Algorithmic aspects of using small instance relaxations effectively (parallel\n separation, facet selection, cutting plane selection) are discussed in detail. Extensive computational results are presented\n for the linear ordering problem and a certain betweenness problem.\nTitle:\nAlgorithmic Aspects of Using Small Instance Relaxations in Parallel Branch-and-Cut\n\nAbstract:\nOne approach for analyzing large networks is to partition its nodes into classes where the nodes in a class have similar characteristics with respect to their connections in the network. A class is represented as a blockmodel (or image matrix). In this context, a specific question is to test whether a presumed blockmodel is well reflected in the network or to select from a choice of possible blockmodels the one fitting best. In this paper, we formulate these problems as combinatorial optimization problems. We show that the evaluation of a blockmodel\u2019s quality is a generalization of well-known optimization problems such as quadratic assignment, minimum \\(k\\)-cut, traveling salesman, and minimum edge cover. A quadratic integer programming formulation is derived and linearized by making use of properties of these special cases. With a branch-and-cut approach, the resulting formulation is solved up to 10,000 times faster than a comparable formulation from the literature.\nTitle:\nEvaluating the quality of image matrices in blockmodeling\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.627, mean value for all the writers is 8.648, which makes it 0.835 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe determination of true optimum solutions of combinatorial optimization problems is seldomly required in practical applications. The majority of users of optimization software would be satisfied with solutions of guaranteed quality in the sense that it can be proven that the given solution is at most a few percent off an optimum solution. This paper presents a general framework for practical problem solving with emphasis on this aspect. A detailed discussion along with a report about extensive computational experiments is given for the traveling salesman problem.\nTitle:", "model_inf_time": 1.11}, {"id": "41338", "output": "Tracking Control of Switched Linear Time-Varying Delay Systems with Stabilizable and Unstabilizable Subsystems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper addresses stabilization problem for a class of switched stochastic systems with time delays by using piecewise Lyapunov-Krasovskii functional method. Asynchronous switching means the switching of the controller has a time delay to the switching of the system. We provide a set of Lyapunov-Krasovskii-type sufficient conditions for establishing the mean-square exponential stability. The mean-square exponential stability condition for the resulting closed-loop system is firstly derived by further allowing the Lyapunov-Krasovskii functional to increase during the running time of the active subsystem with the mismatched controller. Then, the corresponding solvability condition for stabilizing controllers is established. Finally, we present an example to show the effectiveness of the developed theory. \u00a9 2012 Springer Science+Business Media, LLC.\nTitle:\nStabilization of a class of switched stochastic systems with time delays under asynchronous switching\n\nAbstract:\nThis paper studies the stability issue for switched nonlinear systems with input delay and disturbance. It is assumed that for the nominal system an exponential stabilizing controller is predesigned such that the switched system is stable under a certain switching signal, and a piecewise Lyapunov function for the corresponding closed-loop system is known. However, in the presence of input delay and disturbance, the system may be unstable under the same switching signal. For this case, a new Lyapunov\u2013Krasovskii functional is firstly constructed based on the known Lyapunov function. Then, by employing this new functional, a new switching signal satisfying the new average dwell time conditions is constructed to guarantee the input-to-state stability of the system under a certain delay bound. The bound on the average dwell time is closely related to the bound on the input delay. Finally, numerical examples are given to illustrate the effectiveness of the proposed theory.\nTitle:\nConstruction of Lyapunov-Krasovskii functionals for switched nonlinear systems with input delay.\n\nAbstract:\nThis technical note concerns the stabilization problem for a class of switched linear neutral systems in which time delays appear in both the state and the state derivatives. In addition, the switching signal of the switched controller also involves time delays, which makes the switching between the controller and the system asynchronous. Based on a new integral inequality and the piecewise Lyapunov-Krasovskii functional technique, a condition for global uniform exponential stability of the switched neutral system under an average dwell time (ADT) scheme is proposed. Then, the corresponding solvability condition for the controller is established. Finally, a numerical example is given to illustrate the effectiveness of the proposed theory.\nTitle:\nStabilization of a Class of Switched Linear Neutral Systems Under Asynchronous Switching\n\nAbstract:\nThis article is concerned with the output tracking control problem for a class of nonlinear switched cascade systems under some average dwell-time-based switching laws. Firstly, stabilisation problem of this class of switched systems is considered. Then, the result is extended to the output tracking control problem. Based on the variable structure control technique and the characteristic of the system, the variable structure controllers and the average dwell-time are designed. The effectiveness of the proposed design approach is illustrated with simulation results.\nTitle:\nOutput Tracking Control Of Nonlinear Switched Cascade Systems Using A Variable Structure Control Method\n\nAbstract:\nThis paper studies the stabilization problem via the co-design of controllers and a switching policy for a new class of nonstrict feedback switched nonlinear systems whose subsystems consist of a chain of integrators, feedback paths, and first-order feedforward paths. Designing only smooth feedback controllers cannot deal with the unstabilizable factors caused by feedforward paths. By exploiting the single control Lyapunov function method, an effective switching policy is co-designed to compensate the controllers. In addition, we present a generalized backstepping process, based on which the solvability of virtual controllers is guaranteed, the algebraic condition for stabilizability is identified, and the transient response of the closed-loop systems is improved. Two examples demonstrate the effectiveness.\nTitle:\nCo-Design of Controllers and a Switching Policy for Nonstrict Feedback Switched Nonlinear Systems Including First-Order Feedforward Paths\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.69, mean value for all the writers is 8.648, which makes it 2.595 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe investigate the tracking control problem for switched linear time-varying delays systems with stabilizable and unstabilizable subsystems. Sufficient conditions for the solvability of the tracking control problem are developed. The tracking control problem of a switched time-varying delays system with stabilizable and unstabilizable subsystems is solvable if the stabilizable and unstabilizable subsystems satisfy certain conditions and admissible switching law among them. Average dwell time approach and piecewise Lyapunov functional methods are utilized to the stability analysis and controller design. A simulation example shows the effectiveness of the proposed method.\nTitle:", "model_inf_time": 1.9}, {"id": "41339", "output": "Generating a Socio-Cognitive Map of Urban Space from Crowd-Sourced Lifelogs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nGeospatial cognition to sophisticated urban space is an essential capability to make various location-based decisions for our daily urban lives. To adapt ourselves to an unfamiliar or ever-evolving city, we need to develop urban cognition which usually requires lots of experience taking time and efforts. Moreover, it must be a tiresome work to find and ask knowledgeable people who have enough experience to a local area to learn what we would like to know on the spot. In order to collect and utilize crowd's urban cognition probably obtained from living experience, we attempt to explore geospatial cognition of people through common experience from location-based social networks which can be regarded as a fruitful source of crowd-experienced local information. In particular, we propose a method to extract crowd's movements as a direct and useful hint to know common urban cognition and measure relative socio-cognitive distances between urban clusters. In order to intuitively and simply represent cognitive urban space, we generate a socio-cognitive map by projecting the cognitive relationship into a simplified two-dimensional Euclidean space by way of MDS (Multi-Dimensional Scaling). In the experiment, we show a socio-cognitive map significantly representing cognitive proximity among urban clusters in terms of crowd's movements from massive lifelogs over Twitter. We also provide a practical use case for nearest neighbor areas search on the cognitive map.\nTitle:\nExploring geospatial cognition based on location-based social network sites\n\nAbstract:\nRecent location-based social networking sites are attractively providing us with a novel capability of monitoring massive crowd lifelogs in the real-world space. In particular, they make it easier to collect publicly shared crowd lifelogs in a large scale of geographic area reflecting the crowd's daily lives and even more characterizing urban space through what they have in minds and how they behave in the space. In this paper, we challenge to analyze urban characteristics in terms of crowd behavior by utilizing crowd lifelogs in urban area over the social networking sites. In order to collect crowd behavioral data, we exploit the most famous microblogging site, Twitter, where a great deal of geo-tagged micro lifelogs emitted by massive crowds can be easily acquired. We first present a model to deal with crowds' behavioral logs on the social network sites as a representing feature of urban space's characteristics, which will be used to conduct crowd-based urban characterization. Based on this crowd behavioral feature, we will extract significant crowd behavioral patterns in a period of time. In the experiment, we conducted the urban characterization by extracting the crowd behavioral patterns and examined the relation between the regions of common crowd activity patterns and the major categories of local facilities.\nTitle:\nUrban area characterization based on crowd behavioral lifelogs over Twitter\n\nAbstract:\nThe advent of location-based social networking sites provides an open sharing space of crowd-sourced lifelogs that can be regarded as a novel source to monitor massive crowds' lifestyles in the real world. In this paper, we challenge to analyze urban characteristics in terms of crowd behavior by utilizing the crowd lifelogs in urban area. In order to collect crowd behavioral data, we utilize Twitter where enormous numbers of geo-tagged crowd's micro lifelogs can be easily acquired. We model the crowd behavior on the social network sites as a feature, which will be used to derive crowd-based urban characteristics. Based on this crowd behavior feature, we analyze significant crowd behavioral patterns for extracting urban characteristics. In the experiment, we actually conduct the urban characterization over the crowd behavioral patterns using a large number of geo-tagged tweets found in Japan from Twitter and report a comparison result with map-based observation of cities as an evaluation.\nTitle:\nCrowd-based urban characterization: extracting crowd behavioral patterns in urban areas from Twitter\n\nAbstract:\nDue to the remarkable growth of various social networks boosted by the pervasive mobile devices, massive crowds can become social sensors which can share microbolgs on a variety of social situations and natural phenomena in urban space in real-time. In order to take advantages of the novel realm of crowd-sourced lifelogs to characterize urban areas, we attempt to explore characteristics of complex and dynamic urban areas by monitoring crowd behavior via location-based social networks. In particular, we define social conditions consisting of crowd's experiential features extracted from the analysis of Twitter-based crowd's lifelogs. Then, we explore latent characteristic faces of urban areas in term of 5-dimensional social conditions by applying Non-negative Matrix Factorization (NMF). In the experiments with massive geo-tagged tweets, we classify urban areas into representative groups based on their latent patterns which enable to comprehensively understand images of the urban areas focusing on crowd's daily lives.\nTitle:\nTwitter-based Urban Area Characterization by Non-negative Matrix Factorization\n\nAbstract:\nLocation-based social network sites are recently attracting a great deal of attention by combing Web-based social network and the real-world location tagging in an integrated way, where people can publish their life logs about their real-world activities and share them with the public often looking for location-based information. Obviously, in terms of technological and social advance such as location sensing smartphones, experiences and thoughts by the unexpectedly growing number of the mobile users in urban area are conveniently being shared significantly impacting our ways of life experience sharing. In such context, we are able to monitor crowd's experiences through the location-based social network by collecting and analyzing crowd's numerous micro life logs to support a variety of decision makings. In this paper, we attempt to look into the crowd's urban lifestyles, which are characterizing urban areas, particularly utilizing Twitter. We provide a model to construct systems for a large-scale urban analytics with the location-based social network. We also describe our practical approach to describe urban characteristics represented by crowd's temporal behavioral patterns. In the experiment, we show an urban characterization by way of crowd's behavioral patterns, which are derived from temporal patterns of crowd behavior indirectly speculated from a massive number of collected Twitter messages. Finally, we discuss the importance of this kind of challenge amid the pervasive social network environment and some critical issues to be considered for the wide spectrum of sociological studies requiring technology-driven crowd life monitoring.\nTitle:\nCrowd-sourced urban life monitoring: urban area characterization based crowd behavioral patterns from Twitter\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.355, mean value for all the writers is 8.648, which makes it 2.31 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOn behalf of the rapid urbanization, urban areas are gradually becoming a sophisticated space where we often need to know ever evolving features to take the most of the space. Therefore, keeping up with the dynamic change of urban space would be necessary, while it usually requires lots of efforts to understand newly visiting and daily changing living spaces. In order to explore and exploit the urban complexity from crowd-sourced lifelogs, we focus on location-based social network sites. In fact, due to the proliferation of location-based social networks, we can easily acquire massive crowd-sourced lifelogs interestingly indicating their experiences in the real space. In particular, we can conduct various novel urban analytics by monitoring crowd's experiences in an unprecedented way. In this paper, we particularly attempt to exploit crowd-sourced location-based lifelogs for generating a socio-cognitive map, whose purpose is to deliver much simplified and intuitive perspective of urban space. For the purpose, we measure socio-cognitive distance among urban clusters based on human mobility to represent accessibility of urban areas based on crowd's movement. Finally, we generate a socio-cognitive map reflecting the proposed socio-cognitive distances which have computed with massive geo-tagged tweets from Twitter.\nTitle:", "model_inf_time": 1.91}, {"id": "41340", "output": "Multiscale Statistical Modeling of Wavelet Coefficients", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper investigates the statistical characterization of mul- tiscale wavelet coefficients corresponding to random signals and images. Virtually all approaches to wavelet shrinkage model the wavelet coefficients as independent; we challenge that assumption and demonstrate several cases where sub- stantial correlations may be present in the wavelet domain. In particular, the correlation between scales can be surpris- ingly substantial, even for pixels separated by several scales. Our goal, initiated in this paper, is to develop an efficient random field model describing these statistical correlations, and demonstrate its effectiveness in the context of Bayesian wavelet shrinkage for signal and image denoising.\nTitle:\nWavelet shrinkage with correlated wavelet coefficients\n\nAbstract:\nThis paper investigates the statistical characterization of sig- nals and images in the wavelet domain. In particular, in con- trast to common decorrelated-coefficient models, we find that the correlation between wavelet scales can be surpris- ingly substantial, even across several scales. In this pa- per we investigate possible choices of statistical-interaction models. One efficient and fast strategy which describes the wavelet-based statistical correlations is illustrated. Finally, the effectiveness of the proposed tool towards an efficient hierarchical MRF modeling of within-scale neighborhoods and across-scale dependencies will be demonstrated.\nTitle:\nTowards random field modeling of wavelet statistics\n\nAbstract:\nThis paper proposes a novel correlated shrinkage method based on wavelet joint statistics. Our objective is to demonstrate effectiveness of the wavelet correlation models [1] in estimating the original signal from a noising observation. Simulation results are given to show the advantage of the new correlated shrinkage function. In comparison with the popular nonlinear shrinkage algorithms, it improves the denoised results.\nTitle:\nCorrelated Wavelet Shrinkage: Models Of Local Random Fields Across Multiple Resolutions\n\nAbstract:\nWe estimate the Hurst parameter H of fractional Brownian motion (or, by extension, the fractal exponent \u03c6 of stochastic processes having 1/f\u03c6-like spectra) by applying a multiresolution framework. This framework admits an efficient likelihood function evaluation, allowing us to compute the maximum likelihood estimate of this fractal parameter with relative ease. In addition to yielding results that compare well with other proposed methods, and in contrast with other approaches, our method is directly applicable with, at most, very simple modification in a variety of other contexts including fractal estimation given irregularly sampled data or nonstationary measurement noise and the estimation of fractal parameters for 2-D random fields\nTitle:\nFractal estimation using models on multiscale trees\n\nAbstract:\nScientific image processing involves a variety of problems including image modeling, reconstruction, and synthesis. In this\n paper we develop a constrained sampling approach for porous media synthesis and reconstruction in order to generate artificial\n samples of porous media. Our approach is different from current porous media reconstruction methods in which the Gibbs probability\n distribution is maximized by simulated annealing. We show that the artificial images generated by those methods do not contain\n the variability that typical samples of random fields are required to have.\n \nTitle:\nConstrained Sampling Using Simulated Annealing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.333, mean value for all the writers is 8.648, which makes it 0.269 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents a multiscale-based analysis of the statistical dependencies between the wavelet coefficients of random fields. In particular, in contrast to common decorrelated-coefficient models, we find that the correlation between wavelet scales can be surprisingly substantial, even across several scales. In this paper we investigate eight possible choices of statistical-interaction models, from trivial models to wavelet-based hierarchical Markov stochastic processes. Finally, the importance of our statistical approach is examined in the context of Bayesian estimation.\nTitle:", "model_inf_time": 1.03}, {"id": "41341", "output": "Personalized Control for Occupant Comfort and Building Energy Savings", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nBuildings account for a large portion of the world's total delivered energy consumption. With smartphones becoming increasingly ubiquitous and sensor networks growing more mature, buildings can provide personalized and context-aware services to an occupant while minimizing energy consumption. This paper proposes the architecture of a Human-Building-Computer Interaction system that connects the building to its occupants by bridging the gap between the digital and physical worlds. We present our instantiation of an HBCI system, which is composed of an Android mobile application, a number of RESTful services in the cloud, and physical objects co-located with QR tags. We show that, with this system, a user can increase personal comfort within a building while reducing energy usage.\nTitle:\nHBCI: human-building-computer interaction.\n\nAbstract:\nThe design of energy-efficient commercial building Heating Ventilation and Air Conditioning (HVAC) systems has been in the forefront of energy conservation efforts over the past few decades. The HVAC systems traditionally run on a static schedule that does not take occupancy into account, wasting a lot of energy in conditioning empty or partially-occupied spaces. This paper investigates the application of non-intrusive techniques to obtain a rough estimate of occupancy from coarse-grained measurements of the sensors that are commonly available through the building management system. Various per-zone schedules can be developed based on this approximate knowledge of occupancy at the level of individual zones. Our experiments in three large commercial buildings confirm that the proposed techniques can uncover the occupancy pattern of the zones, and schedules that incorporate these occupancy patterns can achieve more than 38% reduction in reheat energy consumption while maintaining indoor thermal comfort.\nTitle:\nNon-Intrusive Techniques for Establishing Occupancy Related Energy Savings in Commercial Buildings.\n\nAbstract:\nThrough accurate and dynamic occupancy detection, building actuation systems can fine tune the targets of their actions to better fit the patterns of usage in modern buildings. We outline a method for achieving this through existing wireless infrastructure and present a demonstration of its viability in a corporate environment.\nTitle:\nZone-level occupancy counting with existing infrastructure.\n\nAbstract:\nExperimentally comparing the energy usage and comfort characteristics of different controllers in heating, ventilation, and air-conditioning (HVAC) systems is difficult because variations in weather and occupancy conditions preclude the possibility of establishing equivalent experimental conditions across the order of hours, days, and weeks. This paper is concerned with defining quantitative metrics of energy usage and occupant comfort, which can be computed and compared in a rigorous manner that is capable of determining whether differences between controllers are statistically significant in the presence of such environmental fluctuations. Experimental case studies are presented that compare two alternative controllers (a schedule controller and a hybrid system learning-based model predictive controller) to the default controller in a building-wide HVAC system. Lastly, we discuss how our proposed methodology may also be able to quantify the efficiency of other building automation systems.\nTitle:\nQuantitative methods for comparing different HVAC control schemes\n\nAbstract:\n  In this paper, we model energy use in commercial buildings using empirical data captured through sMAP, a campus building data portal at UC Berkeley. We conduct at-scale experiments in a newly constructed building on campus. By modulating the supply duct static pressure (SDSP) for the main supply air duct, we induce a response on the main supply fan and determine how much ancillary power flexibility can be provided by a typical commercial building. We show that the consequent intermittent fluctuations in the air mass flow into the building does not influence the building climate in a human-noticeable way. We estimate that at least 4 GW of regulation reserve is readily available only through commercial buildings in the US. Based on predictions this value will reach to 5.6 GW in 2035. We also show how thermal slack can be leveraged to provide an ancillary service to deal with transient frequency fluctuations in the grid. We consider a simplified model of the grid power system with time varying demand and generation and present a simple control scheme to direct the ancillary service power flow from buildings to improve on the classical automatic generation control (AGC)-based approach. Simulation results are provided to show the effectiveness of the proposed methodology for enhancing grid frequency regulation. \nTitle:\nFlexibility of Commercial Building HVAC Fan as Ancillary Service for Smart Grid.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.269, mean value for all the writers is 8.648, which makes it 0.323 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nBuildings are some of the largest energy consumers in the world and yet occupants are regularly dissatisfied with the interior environment in large part due to thermal discomfort [7]. Studies show that given personal control over their environment, occupants are comfortable in a much larger range of ambient temperatures [2]. We present a personalized control smartphone application designed with the dual goals of increasing occupant comfort and achieving building-wide energy savings. The application allows occupants to directly control the lighting and heating/cooling in their vicinity. Using wireless localization combined with data from existing sensors in the building, we estimate room occupancy and use this to dynamically adjust ventilation and air conditioning to save energy in the building.\nTitle:", "model_inf_time": 1.37}, {"id": "41342", "output": "Hybrid Additive-Multiplicative Watermarking for Robustness Enhancement", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe present paper proposes a blind multi-bit watermarking method for High Dynamic Range (HDR) images. The proposed approach is designed in order to guarantee the watermark imperceptibility both in the HDR marked image and in its Low Dynamic Range (LDR) counterpart, being thus robust against significant non-linear distortions such as those performed by tone-mapping operators (TMOs). In order to do so, the wavelet transform of the Just Noticeable Difference (JND)-scaled space of the original HDR image is employed as embedding domain Moreover, a visual mask taking into account specific aspects of the Human Visual System (HVS) is exploited to improve the quality of the resulting watermarked image. Specifically, bilateral filtering is used to locate information on the detail part of the HDR image, where the watermark should be preferably embedded. A contrast sensitivity function is also employed to modulate the watermark intensity in each wavelet decomposition subband according to its scale and orientation. An extensive set of experimental results testifies the effectiveness of the proposed scheme in embedding multi-bit watermarks into HDR images without affecting the visual quality of the original image, while being robust against TMOs.\nTitle:\nHdr Image Multi-Bit Watermarking Using Bilateral-Filtering-Based Masking\n\nAbstract:\nA novel video watermarking system operating in the three-dimensional discrete wavelet transform (3D DWT) is here presented. Specifically the video sequence is partitioned into spatio-temporal units and the single shots are projected onto the 3D DWT domain. This paper focuses on the definition of a perceptual mask, derived from the 3D wavelet coefficients, which allows to trade off between the mark robustness and its imperceptibility. The proposed mask takes into account the spatio-temporal frequency content, the variance, and the luminance of the 3D subbands where the mark is embedded. The embedding is additively performed in the 3D DWT domain by weighing the mark through the defined mask. Finally, the inverse 3D DWT (IDWT) is performed thus obtaining the marked video shot. The imperceptibility of the proposed technique along with its robustness has been observed through experimental results.\nTitle:\nVideo watermarking in the 3D-DWT domain using perceptual masking\n\nAbstract:\nA blind multi-bit watermarking method, specifically designed for high dynamic range (HDR) images, is introduced in this work. The presented embedding strategy takes into account several properties of the human visual system in order to guarantee proper imperceptibility of the embedded watermarks, as well as robustness against tone-mapping operators (TMOs) employed to derive low dynamic range (LDR) representations of the marked HDR content. Specifically, binary marks are embedded into the discrete wavelet transform (DWT) of the just-noticeable difference domain. Additionally, embedding locations are preferably selected in correspondence of the detail parts of an HDR image, determined by exploiting the properties of bilateral filtering. Furthermore, the watermark strength adopted in each DWT subband is modulated depending on a contrast sensitivity function, depending of the subband scale and orientation, to improve the visual quality of the watermarked image. In order to verify the effectiveness of the proposed approach, in terms of both mark imperceptibility and robustness, several experimental tests are carried out on a database comprising 15 HDR images. Specifically, robustness is evaluated by blindly extracting the embedded messages from both HDR marked images and from their LDR counterparts, thus investigating the effects of TMOs on message recovery. Copyright (c) 2015 John Wiley & Sons, Ltd.\nTitle:\nMulti\u2010bit watermarking of high dynamic range images based on perceptual models\n\nAbstract:\nThis paper proposes an approach for the combined image authentication and compression of color images by making use of a digital watermarking and data hiding framework. The digital watermark is comprised of two components: a soft-authenticator watermark for authentication and tamper assessment of the given image, and a chrominance watermark employed to improve the efficiency of compression. The multipurpose watermark is designed by exploiting the orthogonality of various domains used for authentication, color decomposition and watermark insertion. The approach is implemented as a DCT-DWT dual domain algorithm and is applied for the protection and compression of cultural heritage imagery. Analysis is provided to characterize the behavior of the scheme under ideal conditions. Simulations and comparisons of the proposed approach with state-of-the-art existing work demonstrate the potential of the overall scheme.\nTitle:\nDual domain watermarking for authentication and compression of cultural heritage images.\n\nAbstract:\nA video watermarking method operating in the three-dimensional discrete wavelet transform (3D DWT) relaying on the use of a novel video perceptual mask, applied in the 3D DWT domain, is here proposed. Specifically the method consists in partitioning the video sequence into spatio-temporal units of fixed length. Then the video shots undergo a one level 3D DWT. The mark is embedded by means of a multiplicative approach using perceptual masking on the 3D DWT coefficients in order to trade off between the mark robustness and its imperceptibility. The mask we propose takes into account the spatio-temporal frequency content by means of the spatio-temporal contrast sensitivity function, the luminance, and the variance of the 3D subbands which host the mark. The effectiveness of the proposed mask is verified experimentally, thus guaranteeing a high imperceptibility of the mark. Moreover, experimental results show the robustness of the proposed approach against MPEG2 compression, MPEG4 compression, gain attack, collusion, and transcoding.\nTitle:\nPerceptual video watermarking in the 3D-DWT domain using a multiplicative approach\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.0, mean value for all the writers is 8.648, which makes it 0.3 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents a hybrid watermarking technique which mixes additive and multiplicative watermark embedding with emphasis on its robustness versus the imperceptibility of the watermark. The embedding is performed in six wavelet sub-bands using independently three embedding equations and two parameters to modulate the embedding strength for multiplicative and additive embedding. The watermark strength is independently modulated into distinct image areas. Specifically, when a multiplicative embedding is used, the visibility threshold is first reached near the image edges, whereas using an additive embedding technique the visibility threshold is first reached into the smooth areas. A subjective experiment has been used to provide the optimal watermark strength for three distinct embedding equations. Observers were asked to tune the watermark amplitude and to set the strength at the visibility threshold. The experimental results showed that using an hybrid watermarking technique significantly improves the robustness performance. This work is a preliminary study for the design of an optimal wavelet domain Just Noticeable Difference (JND) mask.\nTitle:", "model_inf_time": 1.54}, {"id": "41343", "output": "Secure Watermarking and Encryption of Color Images Based on a Key-Dependent Wavelet Transform", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this contribution a novel reversible data hiding scheme for digital images is presented. The proposed technique allows the exact recovery of the original image upon extraction of the embedded information. Lossless recovery of the original is achieved by adopting the histogram shifting technique in a novel wavelet domain: the Integer Fibonacci-Haar Transform, which is based on a parameterized subband decomposition of the image. In particular, the parametrization depends on a selected Fibonacci sequence. The use of this transform increases the security of the proposed method. Experimental results show the effectiveness of the proposed scheme.\nTitle:\nReversible Data Hiding In The Fibonacci-Haar Transform Domain\n\nAbstract:\nThe increasing use of digital image-based applications is resulting in huge databases that are often difficult to use and prone to misuse and privacy concerns. These issues are especially crucial in medical applications. The most commonly adopted solution is the encryption of both the image and the patient data in separate files that are then linked. This practice results to be inefficient since, in order to retrieve patient data or analysis details, it is necessary to decrypt both files.In this contribution, an alternative solution for secure medical image annotation is presented. The proposed framework is based on the joint use of a key-dependent wavelet transform, the Integer Fibonacci-Haar transform, of a secure cryptographic scheme, and of a reversible watermarking scheme.The system allows: i) the insertion of the patient data into the encrypted image without requiring the knowledge of the original image, ii) the encryption of annotated images without causing loss in the embedded information, and iii) due to the complete reversibility of the process, it allows recovering the original image after the mark removal. Experimental results show the effectiveness of the proposed scheme.\nTitle:\nSecure Annotation For Medical Images Based On Reversible Watermarking In The Integer Fibonacci-Haar Transform Domain\n\nAbstract:\nIn this paper a watermarking scheme based on the human visual system (HVS) is presented. A quantization index modulation - dither modulation scheme is used in the discrete cosine transform domain for the embedding. The watermark quantization step is selected according to the inter-frequency masking effect computed by using a HVS-inspired objective image quality metric. Experimental results demonstrate the effectiveness and the robustness of the proposed solution.\nTitle:\nQIM-DM watermarking optimization based on inter-frequency contrast masking in the DCT domain\n\nAbstract:\nIn this contribution a Multiple Description Coding scheme for video transmission over unreliable channel is presented. The method is based on an integer wavelet transform and on a data hiding scheme for exploiting the spatial redundancy and for reducing the scheme overhead. Experimental results show the effectiveness of the proposed scheme.\nTitle:\nMultiple Description Video Coding Technique Based On Data Hiding In The Tree Structured Haar Transform Domain\n\nAbstract:\nMultimedia data hiding by digital watermarking is usually employed for copyright protection purposes. In this contribution, a new application of watermarking is presented. Specifically, watermarking is here employed as a technique for testing the quality of service in multimedia mobile communications. A fragile known watermark is embedded in a MPEG-like host data video transport stream using a spread-spectrum technique to avoid visual interference. Like a tracing signal, a (known) tracing watermark tracks the (unknown) information stream that follows the same communication link. The detection of the tracing watermark allows dynamically evaluating the effective quality of the provided video services, depending on the whole physical layer (including the employed image co/decoder). The performed method is based on the mean-square-error between estimated and actual watermarks. The devised technique has been usefully applied to typical scenarios of mobile wireless multimedia communication systems, in presence of multi-path channel and interfering users.\nTitle:\nTracing Watermarldng For Multimedia Communication Quality Assessment\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.432, mean value for all the writers is 8.648, which makes it 0.669 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA novel method for watermarking and ciphering color images, based on the joint use of a key-dependent wavelet transform with a secure cryptographic scheme, is presented. The system allows to watermark encrypted data without requiring the knowledge of the original data and also to cipher watermarked data without damaging the embedded signal. Since different areas of the proposed transform domain are used for encryption and watermarking, the extraction of the hidden information can be performed without deciphering the cover data and it is also possible to decipher watermarked data without removing the watermark. Experimental results show the effectiveness of the proposed scheme.\nTitle:", "model_inf_time": 1.6}, {"id": "41344", "output": "Multimodal Feedback for Proactive Information Retrieval in Human-Robot Interaction", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents a first theoretical framework for a dialog strategy handling miscommunication in natural language Human-Robot Interaction (HRI). On the one hand the dialog strategy is deduced from findings about human-human communication patterns and coping strategies for miscommunication. On the other hand, relevant cognitive theories concerning human perception serve as a conceptual basis for the dialog strategy. The novel approach is firstly to combine these communication patterns with coping strategies and cognitive theories from human-human interaction (HHI) and secondly transfer them to HRI as a general dialog strategy for handling miscommunication. The presented approach is applicable to any task-oriented dialog. In a first step the conversational context is confined to route descriptions, given that asking for directions is an restricted but nevertheless challenging example for task-oriented dialog between humans and a robot.\nTitle:\nTowards a dialog strategy for handling miscommunication in human-robot dialog\n\nAbstract:\nA robot controller is developed for human-robot handshaking. The focus of the work is to provide realistic experiences for the human participant in haptic interactions with a robot. To achieve this goal, a position-based admittance controller is implemented. By using haptic data as inputs, a hidden Markov model-based high-level controller is used to estimate human intentions and modify the reference trajectory accordingly. The overall control framework is implemented onto a robot with validation experiments carried out with human participants.\nTitle:\nAn HMM approach to realistic haptic human-robot interaction\n\nAbstract:\nThis article describes an emotional adaption approach to proactively trigger increased helpfulness towards a robot in task-related human-robot interaction (HRI). Based on social-psychological predictions of human behavior, the approach aims at inducing empathy, paired with a feeling of similarity in human users towards the robot. This is achieved by two differently expressed emotional control variables: by an explicit statement of similarity before task-related interaction, and implicitly expressed by adapting the emotional state of the robot to the mood of the human user, such that the current values of the human mood in the dimensions of pleasure, arousal, and dominance (PAD) are matched. The thereby shifted emotional state of the robot serves as a basis for the generation of task-driven emotional facial- and verbal expressions, employed to induce and sustain high empathy towards the robot throughout the interaction. The approach is evaluated in a user study utilizing an expressive robot head. The effectiveness of the approach is confirmed by significant experimental results. An analysis of the individual components of the approach reveals significant effects of explicit emotional adaption on helpfulness, as well as on the HRI-key concepts anthropomorphism and animacy.\nTitle:\nIncreasing Helpfulness towards a Robot by Emotional Adaption to the User.\n\nAbstract:\nIn the past, working spaces of humans and robots were strictly separated, but recent developments have sought to bring robots into closer interaction with humans. In this context, physical human-robot interaction represents a major challenge, as it is based on continuous bilateral information and energy exchanges which result in a mutual adaptation of the partners. To address the challenge of designing robot collaboration partners, making them as human-like as possible is an approach often adopted. In order to compare different implementations with each other, their degree of human-likeness on a continuous scale is required. So far, the human-likeness of haptic interaction partners has only been studied in the form of binary choices. In this paper, we first introduce methods that allow measuring the human-likeness of haptic interaction partners on a continuous scale. In doing so, two subjective rating methods are proposed and correlated with a task performance measure. To demonstrate the applicability and validity of the proposed measures, they are applied to a joint kinesthetic manipulation task and used to compare two different implementations of a haptic interaction partner: a feedforward model based on force replay, and a feedback model. This experiment demonstrates the use of the proposed measures in building a continuous human-likeness scale and the interpretation of the scale values achieved for formulating guidelines for future robot implementations.\nTitle:\nHaptic human-robot collaboration: Comparison of robot partner implementations in terms of human-likeness and task performance\n\nAbstract:\nThe creation of a robot capable of navigating in unknown urban environments without the use of GPS data or prior map knowledge is envisioned in the Autonomous City Explorer (ACE) project. The robot has to retrieve direction information solely by interacting with humans. This work presents a human-robot communication system that enables the robot to ask for directions and store the retrieved route information as internal knowledge. The system incorporates theories from linguistics in a mixed-modalities communication interface. It stores acquired information into a topological route graph which is used to give feedback to the human and to navigate in unknown environments.\nTitle:\nInformation retrieval system for human-robot communication: asking for directions\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.007, mean value for all the writers is 8.648, which makes it 1.159 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis work is a first step towards an integration of multimodality with the aim to make efficient use of both human-like, and non-human-like feedback modalities in order to optimize proactive information retrieval from task-related Human-Robot Interaction (HRI) in human environments. The presented approach combines the human-like modalities speech and emotional facial mimicry with non-human-like modalities. The proposed non-human-like modalities are a screen displaying retrieved knowledge of the robot to the human and a pointer mounted above the robot head for pointing directions and referring to objects in shared visual space as an equivalent for arm and hand gestures. Initially, pre-interaction feedback is explored in an experiment investigating different approach behaviors in order to find socially acceptable trajectories to increase the success of interactions and thus efficiency of information retrieval. Secondly, pre-evaluated human-like modalities are introduced. First results of a multimodal feedback study are presented in the context of the IURO project,(1) where a robot asks for its way to a predefined goal location.\nTitle:", "model_inf_time": 1.59}, {"id": "41345", "output": "Automatic Color Image Segmentation via Adaptive Region Information and Mixture Modeling", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose a novel automatic framework for variational color image segmentation based on unifying adaptive region information and mixture modelling. We consider a formulation of the region information by using the posterior probability of a mixture of General Gaussian (GG) pdfs, where each region is represented by a pdf. The segmentation is formulated by the minimization of an energy functional according to the region contours and all the mixture parameters respectively. Two main objectives are achieved by the approach. A scheme is provided to extend easily the adaptive segmentation to an arbitrary number of regions and to perform it in a fully automatic fashion. Moreover, the segmentation recovers an accurate and representative mixture of pdfs. In the approach, we couple the boundary and region information of the image to steer the segmentation. We validate the method on the segmentation of real world color images.\nTitle:\nAn Automatic Segmentation Combining Mixture Analysis and Adaptive Region Information: A Level Set Approach\n\nAbstract:\nIn this paper we propose a fully automatic segmentation method for colour/texture images. By fully automatic, we mean that the steps of region initialization and calculation of the number of regions are performed automatically by the method. The region information is formulated using a mixture of pdfs for the combination of colour and texture features. The segmentation is obtained by minimizing an energy functional combining boundary and region information, which evolves the initial region contours towards the real region boundaries and adapts the mixture parameters to the region data. The method is implemented using the level sets that permit automatic handling of topology changes and stable numerical schemes. We validate the approach using examples of synthetic and natural colour-texture image segmentation.\nTitle:\nAutomatic colour-texture image segmentation using active contours\n\nAbstract:\nIn this paper, we propose a novel method for unsupervised color-texture segmentation. The approach aims at combining color and texture features and active contours to build a fully automatic segmentation algorithm. By fully automatic, we mean the steps of region initialization and calculation of the number of regions are performed automatically by the algorithm. Furthermore, the approach combines boundary and region information for accurate region boundary localization. We validate the approach by examples of synthetic and natural color-texture image segmentation.\nTitle:\nAutomatic color-texture image segmentation by using active contours\n\nAbstract:\nIn this paper, we propose an automatic segmentation of color-texture images with arbitrary numbers of regions. The approach combines region and boundary information and uses active contours to build a partition of the image. The segmentation algorithm is initialized automatically by using homogeneous region seeds on the image domain. The partition of the image is formed by evolving the region contours and adaptively updating the region information formulated using a mixture of pdfs. We show the performance of the proposed method on examples of color-texture image segmentation, with comparison to two state-of-the-art methods.\nTitle:\nGlobally adaptive region information for automatic color-texture image segmentation\n\nAbstract:\nVariational image segmentation combining boundary and region information was and still is the subject of many recent works. This combination is usually subject to arbitrary weighting parameters that control the boundary and region features contribution during the segmentation. However, since the objective functions of the boundary and the region features is different in nature, their arbitrary combination may conduct to local conflicts that stem principally from abrupt illumination changes or the presence of texture inside the regions. In the present paper, we investigate an adaptive estimation of the weighting parameters (hyper-parameters) on the regions data during the segmentation by using a Bayesian method. This permits to give adequate contributions of the boundary and region features to segmentation decision making for pixels and, therefore, improving the accuracy of region boundary localization. We validated the approach on examples of real world images.\nTitle:\nA bayesian approach for weighting boundary and region information for segmentation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.566, mean value for all the writers is 8.648, which makes it 0.783 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we propose a novel automatic framework for variational color image segmentation based on unifying adaptive region information and mixture modelling. We consider a formulation of the region information based on the posterior probability of a mixture of general Gaussian (GG) pdfs where each region is represented by a pdf. The segmentation is formulated by the minimization of an energy functional according to the region contours and all the mixture parameters respectively. Two main objectives are achieved by the approach. A scheme is provided to extend easily the adaptive segmentation to an arbitrary number of regions and to perform it in a fully automatic fashion. Moreover, the segmentation recovers an accurate and representative mixture of pdfs. In the approach, we couple the boundary and region information of the image to steer the segmentation. We validate the method on the segmentation of real world color images.\nTitle:", "model_inf_time": 1.31}, {"id": "41346", "output": "Design and Implementation of a Reuse Repository for Brazilian Software Factories", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSoftware reuse processes have been under continuous attention in the software engineering and software reuse research communities during past years. Although several processes have been investigated to develop reusable software, there are not available studies that compare them. In this way, this paper presents a detailed survey on software reuse processes.\nTitle:\nA survey on software reuse processes\n\nAbstract:\nSoftware reuse is a critical aspect for companies interested in the improvement of software development quality and productivity, and in costs reduction. However, achieving it is a nontrivial task. In this paper, we present a robust framework for software reuse, based on previous success factors, in order to guide organizations in the effective reuse. Nontechnical and technical aspects compose the framework.\nTitle:\nRiSE project: towards a robust framework for software reuse\n\nAbstract:\nA fundamental principle for reusing software assets is providing means to access them. Information retrieval mechanisms assisted by semantic initiatives, play a very important role in finding relevant reusable assets. In this context, this paper presents a semantic search tool in order to improve the precision of search returns. Furthermore, the requirements, the decomposition of architectural module and aspects of implementation are presented.\nTitle:\nApplying a semantic layer in a source code search tool\n\nAbstract:\nThe software reuse initiatives are better implemented when there is an efficient way to find the reusable assets. However, the search and retrieval of such information is considered a big deal in literature, once there is a gap between what the software engineer would like to retrieve and what is stored in the repository. Thus, this paper presents an efficient way to reduce this problem and aid search engines applying data mining techniques on log mechanisms to extract knowledge about the historic data. In order to evaluate the results, an initial experiment is also discussed.\nTitle:\nSuggesting Software Components for Reuse in Search Engines Using Discovered Knowledge Techniques\n\nAbstract:\nSoftware engineering and reuse-oriented tools have been studied along the last years, aiming to provide help in the software development. With the importance of reuse growing significantly, effective software reuse tools and environments started to be needed. This paper presents and discusses some works that comprise many issues related to source code search tools, covered by university and industry since 90's until today. In the end of the paper, a set of requirements is presented, integrating the features that can be found in most works of the area, serving as a basis for future work toward an effective source code search tool.\nTitle:\nToward a Code Search Engine Based on the State-of-Art and Practice\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.525, mean value for all the writers is 8.648, which makes it 0.748 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA repository is a necessary prerequisite to support software engineers and other users in the process of developing software for and with reuse. In the literature, there are several works that explore reuse repositories, however their focus is mostly on reusable component search and retrieval issues, while important aspects of reuse repositories have not been properly explored. On the other hand, some questions raised by companies that desire to adopt or build a reuse repository remain unanswered. Motivated by these questions, this paper presents the specification, design and implementation of a reuse repository that was successfully constructed and deployed in real Brazilian software factories.\nTitle:", "model_inf_time": 1.16}, {"id": "41347", "output": "A Global Asymptotically Stable Controller for Target Tracking of Unicycle Type Mobile Robots", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents the design of a compact/open network-based controller incorporating modular software architecture for various kinds of robot applications. Within the proposed controller scheme, a standardized real-time network like CAN connects the central motion control part and the servo control part. Thus, the size of the servo controller becomes small enough to be attached inside the robot body and the control software can be designed with an open and modular concept. The open/compact controller incorporating a modular software architecture offers benefits of reduced engineering costs. The proposed architecture has been implemented on a KIST humanoid robot controller platform and its performance has been verified through experimental tests.\nTitle:\nA Compact/Open Network-Based Controller Incorporating Modular Software Architecture for a Humanoid Robot\n\nAbstract:\nThis paper proposes the indirect zero momentum position (ZMP) controller for biped robot systems and proves its disturbance input-to-state stability (ISS). The ZMP control has been used as a standard method for stable walking control of biped robot systems. Since the ZMP information consists of position and acceleration of the center of gravity (COG) for a biped robot system, the ZMP can be indirectly controlled by the motion of COG. In this paper, the reference COG planner is developed by solving the reference ZMP differential equation. The indirect ZMP controller is proposed to derive the desired motion of COG from the reference ZMP trajectory and the COG error (the difference between the reference and real COG). The ISS of the proposed indirect ZMP controller is proved for the simplified biped robot model. The robustness of the proposed indirect ZMP controller is shown in simulation.\nTitle:\nOn the stability of indirect ZMP controller for biped robot systems\n\nAbstract:\nThis paper presents a real-time motion control system using EtherCAT protocol and its application on a differential drive mobile robot. The motion controller is designed from open-source components consisting of dual kernel approach using standard Linux and real-time extension of Xenomai, and the EtherCAT Master stack, IgH. In order to validate feasibility of the real-time system, timing analysis between the master and the slaves is performed in terms of periodicity of the cyclic task, jitter, and in-control execution time as test metrics. Furthermore, we conducted a convolution based trajectory planning algorithm that considers the physical limits of the mobile robot to generate periodic velocity commands following a curved path. Encoder data from each wheels is evaluated to guarantee the accuracy of the motion control system in Cartesian space.\nTitle:\nAn Ethercat-Based Real-Time Motion Control System In Mobile Robot Application\n\nAbstract:\nThis paper proposes a new walking pattern generation method for humanoid robots. The proposed method consists of feedforward control and feedback control for walking pattern generation. The pole placement method as a feedback controller changes the poles of system in order to generate more stable and smoother walking pattern. The advanced pole-zero cancellation by series approximation (PZCSA) as a feedforward controller plays a role of reducing the inherent property of linear inverted pendulum model (LIPM), that is, non-minimum phase property due to an unstable zero of LIPM and tracking efficiently the desired zero moment point (ZMP). The efficiency of the proposed method is verified by three simulations such as arbitrary walking step length, arbitrary walking phase time and sudden change of walking path.\nTitle:\nA walking pattern generation method with feedback and feedforward control for humanoid robots\n\nAbstract:\nDespite that omni-directional mobile robots have been employed popularly in several application areas, effort on optimal design of such mobile robots has been few in literature. Thus, this paper investigates the optimal design of omni-directional mobile robots. Particularly, optimal design parameters such as one or double offset distance of wheel mechanism and the wheel radius are identifed with respect to isotropic characteristic of mobile robots. In addition, the force transmission characteristics and actuator-sizing problem of mobile robots are investigated Analysis has been performed for three actuation sets. It is shown that the redundantly acruated mobile robot with three active caster wheels represents the best performance among them.\nTitle:\nOptimal design and actuator sizing of redundantly actuated omni-directional mobile robots\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.455, mean value for all the writers is 8.648, which makes it 1.542 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper deals with target tracking control of unicycle type mobile robots. Target tracking function is essential for autonomous robots such as guide robots, security guard robots, etc. In the field of mobile robot control, many control schemes for posture stabilization and trajectory tracking problem have been proposed. Target tracking control, however, cannot be achieved using these kinds of control laws. Therefore, a new global asymptotic stable controller for this problem is designed using the backstepping method. The stability of the system is proved using the Lyapunov function. Various simulation results validate the performance and theoretical analysis\nTitle:", "model_inf_time": 1.51}, {"id": "41348", "output": "Iterative Learning Control for Linear Periodic Systems with Application to Robot Path Control", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAn iterative learning control method is proposed for a class of non-linear dynamic systems with uncertain parameters. The method, in which non-linear system model is used, employs the model algorithmic control concept in the iteration sequence. A sufficient condition for convergency is provided. Then the method is shown to be applicable to continuous-path control of a robot manipulator.\nTitle:\nA nonlinear iterative learning method for robot path control.\n\nAbstract:\nThis paper addresses a torque control method in a high friction robot manipulator. A stiction feed-forward compensator is proposed to eliminate the control problem caused by the nonlinear friction and disturbance. In order to control a robot manipulator with unknown effects, a time-delay control method is used to control the torque. One degree of freedom flexible joint robot manipulator with a joint torque sensor is used to show the performance of the proposed control method.\nTitle:\nJoint torque servo of a high friction robot manipulator based on time-delay control with feed-forward friction compensation\n\nAbstract:\nA force/position control method is proposed for robot manipulators equipped with a wrist force/torque sensor and position servo drives. Especially, fuzzy PI gain tuning rules are suggested to improve the transient force response. To show the validity of the proposed method, a numerical example employing a two-link manipulator is given\nTitle:\nFuzzy rule based position/force control of industrial manipulators\n\nAbstract:\nThis paper deals with target tracking control of unicycle type mobile robots. Target tracking function is essential for autonomous robots such as guide robots, security guard robots, etc. In the field of mobile robot control, many control schemes for posture stabilization and trajectory tracking problem have been proposed. Target tracking control, however, cannot be achieved using these kinds of control laws. Therefore, a new global asymptotic stable controller for this problem is designed using the backstepping method. The stability of the system is proved using the Lyapunov function. Various simulation results validate the performance and theoretical analysis\nTitle:\nA stable target-tracking control for unicycle mobile robots\n\nAbstract:\nIn this paper, we describe an online trajectory control scheme for robotic manipulators with dynamic change of the task priority. First, given tasks are reconstructed using a geometric projection on the given index function. Then, the reconstructed tasks are analyzed in the framework of task priority based method. From this analysis, the proposed algorithm is shown to have the property that the task priority is assigned dynamically. Using this algorithm, we easily set the criteria of changing task priority and estimate the performance of the given index function. Considering the measure of manipulability as a index function, the approach is suitable for avoiding kinematic singularities for autonomous operation of, for example, an underwater robot. The result shows a good performance near the singular configurations, as shown by simulation results.\nTitle:\nDynamic task priority approach to avoid kinematic singularity for autonomous manipulation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.304, mean value for all the writers is 8.648, which makes it 2.266 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAn iterative learning control algorithm for a class of linear periodic systems is proposed in which parameter estimation is performed in the domain of an iterative sequence of operations with the time frozen. A sufficient condition for the convergence of the proposed algorithm is given. It is also shown that the proposed learning controller can be applied to the continuous path control of robot manipulators. It is noted that the proposed algorithm works reasonably well only for the case of small perturbations with respect to a nominal trajectory\nTitle:", "model_inf_time": 1.32}, {"id": "41349", "output": "Performance Evaluation of DRR with Fragmentation in L-DACS 1 for Aeronautical Communications", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe aviation community is currently working on the standardization of data communication systems for the future air traffic management. In this context, International Civil Aviation Organization (ICAO) has initiated a work on standardization of an IPv6-based aeronautical telecommunications network and on future radio access technologies, respectively. In this paper, we integrate L-Band Digital Aeronautical Communications System Option 1 (L-DACS 1), which is one candidate for future radio access technologies, with realistic IPv6-based network layer functionality and analyze the effect of handover delay to the TCP performance. Realistic Frame Error Rate (FER) values obtained from an L-DACS 1 physical layer simulator, which uses a realistic aeronautical channel model, are used in the simulation experiments. In the first stage, we decreased layer 3 handover latency by removing the Duplicate Address Detection (DAD) procedure for address configuration. In the second stage, we introduced a Home Agent (HA)-buffering method, which is used to buffer the traffic (destined to the mobile node) during handover. Transmission completion time is the primary performance metric in our analysis. With the HA-buffering method, the transmission completion time is reduced by at least 10% for the transmission of 110kB of information over a wireless link with 31.5kbits^-^1 data rate.\nTitle:\nPerformance evaluation of network mobility handover over future aeronautical data link\n\nAbstract:\nModern cars comprise a multitude of electronic features which are implemented in tens of communicating control units. To connect these in-car embedded systems, the CAN bus offers a sustainable performance, hence it is used as a widespread communication infrastructure, even for safety critical applications. However, CAN media access is priority based and performed competitive and non-preemptive. Thus, assessing the worst case end-to-end delay is inevitable in order to provide safe and efficient operation of functions with hard real-time properties. In this paper, we use the analytical method of Network Calculus to determine guaranteed upper bounds for transmission delays of all CAN priorities. We demonstrate the applicability of our approach by investigating current real-life CAN communication data from the German car manufacturer Audi.\nTitle:\nReal-Time Guarantees for CAN Traffic\n\nAbstract:\nThe idea of wirelessly connected vehicles has long ceased to be a vision as researchers from both academic and industrial institutions and field operational tests all over the world are contributing to bringing this technology to life. Both ETSI and IEEE have been working on respective standards (ETSI ITS G5 in Europe, IEEE WAVE in North America) to enable this new application of wireless communication. In this paper we compare medium access in these systems by means of an extensive simulation study while focusing on the transmission of periodic safety messages on the control channel.We observe that for different reasons high node density scenarios appear to be critical for the overall performance of both systems. This includes end-to-end delay, packet error rates and a non-optimal channel utilization that leaves room for improvement. We find that the approach proposed by ETSI ITS G5 with Decentralized Congestion Control (DCC) may access the channel rather conservatively but still outperforms IEEE WAVE in most of the scenarios.\nTitle:\nA performance study of cooperative awareness in ETSI ITS G5 and IEEE WAVE.\n\nAbstract:\nThe performance of communication systems can be evaluated using various distinct techniques and paradigms, e.g. queuing theory, simulation or worst case analysis. Mean values for performance measures like transmission delay, queue length or system utilization are valuable information for network dimensioning. However, in many cases, quantile-based approaches or deterministic upper bounds are indispensable, especially for systems that need real-time guarantees. A typical application area are safety-critical functions in automotive environments, where hard real-time transmission deadlines have to be met to assure safe operation of the vehicle. In this paper, we investigate a contemporary automotive in-car communication system, the Controller Area Network (CAN). A simulation study of the system yields stochastic quantile-related use case performance measures for non-time-critical communication. It is complemented by a deterministic evaluation using Network Calculus, which allows to determine worst case transmission times and provides closed and easily applicable formulas for delay bounds of messages on all priority levels. Comprising the outcomes from this dual evaluation approach supports the design, dimensioning and parameterization of the overall CAN bus system with respect to both hard real-time demands and performance characteristics in typical use case scenarios.\nTitle:\nStochastic and deterministic performance evaluation of automotive CAN communication\n\nAbstract:\nIEEE has recently standardized the physical layer and the medium access mechanism for wireless local area networks. This paper presents a performance study of the Distributed coordination function, the fundamental contention based access mechanism. Stochastic Petri nets are used as a modeling formalism.A detailed model captures all relevant system aspects in a concise way. The detailed model is evaluated by simulation and also used to derive two more compact models which are analytically tractable. All models are used to investigate different physical layer options and the influence of several system parameters.\nTitle:\nPerformance Evaluation of IEEE 802.11 Wireless LANs with Stochastic Petri Nets\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.247, mean value for all the writers is 8.648, which makes it 1.364 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe aviation community is currently working on the standardization of data communication systems for the future air traffic management. In this context, the International Civil Aviation Organization (ICAO) has initiated a work on standardization of an IPv6-based aeronautical telecommunications network and on future radio access technologies, respectively. In this paper, we integrate L-Band Digital Aeronautical Communications System Option 1 (L-DACS 1), which is one candidate for future radio access technologies, with realistic IPv6-based network layer functionality and analyze Deficit Round Robin (DRR) with fragmentation algorithm for the forward and return link in L-DACS 1. Our analysis mainly covers two application domains: file transfer and real-time services. We show that DRR with fragmentation scheduler provides good performance results in terms of throughput, delay, and bandwidth fairness.\nTitle:", "model_inf_time": 1.77}, {"id": "41350", "output": "ForumReader: Visualizing and Navigating Flash Forums", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper describes a new collaboration technology that is based on the support of lightweight, informally structured, opportunistic activities featuring heterogeneous threads of shared items with dynamic membership. We introduce our design concepts, and we provide a detailed analysis of user behavior during a five month field study. We present the patterns of media use that we observed, using a variety of analytical methods including thread clustering and analysis. Major findings include four patterns of media use: communicating, exchanging mixed objects, coordinating, (e.g., of status reports), and semi-archival filing. We observed differential use of various media including highly variable use of chats and surprisingly informal uses of files. We discuss the implications for the design of mixed media collaborative tools to support the work activities of small to medium sized work teams.\nTitle:\nPatterns of media use in an activity-centric collaborative environment\n\nAbstract:\nThis paper investigates using social tags for the purpose of making personalized content recommendations. Our tag-based recommender creates a personalized bookmark recommendation model for each user based on \"current\" and \"general interest\" tags, defined by different time intervals.\nTitle:\nTag-based filtering for personalized bookmark recommendations\n\nAbstract:\nOnline communities are important in enterprises, helping workers to build skills and collaborate. Despite their unique and critical role fostering successful communities, community leaders have little direct support in existing technologies. We introduce CommunityCompare, an interactive visual analytic system to enable leaders to make sense of their community's activity with comparisons. Composed of a parallel coordinates plot, various control widgets, and a preview of example posts from communities, the system supports comparisons with hundreds of related communities on multiple metrics and the ability to learn by example. We motivate and inform the system design with formative interviews of community leaders. From additional interviews, a field deployment, and surveys of leaders, we show how the system enabled leaders to assess community performance in the context of other comparable communities, learn about community dynamics through data exploration, and identify examples of top performing communities from which to learn. We conclude by discussing how our system and design lessons generalize.\nTitle:\nCommunityCompare: visually comparing communities for online community leaders in the enterprise\n\nAbstract:\nThis paper studies people recommendations designed to help users find known, offline contacts and discover new friends on social networking sites. We evaluated four recommender algorithms in an enterprise social networking site using a personalized survey of 500 users and a field study of 3,000 users. We found all algorithms effective in expanding users' friend lists. Algorithms based on social network information were able to produce better-received recommendations and find more known contacts for users, while algorithms using similarity of user-created content were stronger in discovering new friends. We also collected qualitative feedback from our survey users and draw several meaningful design implications.\nTitle:\nMake new friends, but keep the old: recommending people on social networking sites\n\nAbstract:\nSocial networking sites support a variety of shared content types such as photos, videos, or music. More structured or form-based social content types are not mainstream but we have started seeing sites evolve that support them. This paper describes the design and use of structured lists in an enterprise social networking system. As a major feature of our shared lists, we introduced the ability to reuse someone else's list. We report the results on the use and reuse of shared lists based on three months of usage data from 285 users and interviews with 9 users. Our findings suggest that despite the structured nature of lists, our users socialize more around lists than photos, and use lists as a medium for self-representation.\nTitle:\nUse and reuse of shared lists as a social content type\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.01, mean value for all the writers is 8.648, which makes it 0.309 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe describe a popular kind of large, topic-centered, transient discussion, which we term a flash forum. These occur in settings ranging from web-based bulletin boards to corporate intranets, and they display a conversational style distinct from Usenet and other online discussion. Notably, authorship is more diffuse, and threads are less deep and distinct. To help orient users and guide them to areas of interest within flash forums, we designed ForumReader, a tool combining data visualization with automatic topic extraction. We describe lessons learned from deployment to thousands of users in a real world setting. We also report a laboratory experiment to investigate how interface components affect behavior, comprehension, and information retrieval. The ForumReader interface is well-liked by users, and our results suggest it can lead to new navigation patterns. We also find that, while both visualization and text analytics are helpful individually, combining them may be counterproductive.\nTitle:", "model_inf_time": 1.26}, {"id": "41351", "output": "Object-Centric Sharing: Bridging the Gap Between Ad Hoc and Formal Collaboration", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis demonstration presents a new hybrid collaboration technology that partakes of selected qualities of informal, ad hoc, easy-to-initiate collaborative tools, and more formal, structured, and disciplined collaborative applications. Our approach focuses on the support of lightweight, informally structured, opportunistic activities featuring heterogeneous threads of shared objects with dynamic membership as well as blended synchronous and asynchronous collaboration. We will introduce the system, and then invite audience members to use it in several exercises.\nTitle:\nExplorations in an activity-centric collaboration environment\n\nAbstract:\nThis paper describes a new collaboration technology that is carefully poised between informal, ad hoc, easy-to-initiate collaborative tools, vs. more formal, structured, and high-overhead collaborative applications. Our approach focuses on the support of lightweight, informally structured, opportunistic activities featuring heterogeneous threads of shared objects with dynamic membership. We introduce our design concepts, and we provide a detailed first look at data from the first 100 days of usage by 20 researchers and 13 interns, who both confirmed our hypotheses and surprised us by reinventing the technology in several ways.\nTitle:\nOne-hundred days in an activity-centric collaboration environment based on shared objects\n\nAbstract:\nIn this paper, we address the design issues of a collaborative workspace system, called TeamSpace, that supports geographically distributed teams by managing shared work processes and maintaining shared artifacts in a project. TeamSpace attempts to integrate both synchronous and asynchronous types of team interaction into a task-oriented environment. Since meetings are an integral part of teamwork, our current work focuses on supporting virtual meetings as part of a larger collaborative work process. We present an initial TeamSpace prototype that supports asynchronous meeting management seamlessly integrated with capture and access of synchronous distributed meetings. The captured synchronous data is integrated with other related information in TeamSpace, enabling users to efficiently gain knowledge of both current and past team activities.\nTitle:\nA team collaboration space supporting capture and access of virtual meetings\n\nAbstract:\nThis paper describes a new collaboration technology that is based on the support of lightweight, informally structured, opportunistic activities featuring heterogeneous threads of shared items with dynamic membership. We introduce our design concepts, and we provide a detailed analysis of user behavior during a five month field study. We present the patterns of media use that we observed, using a variety of analytical methods including thread clustering and analysis. Major findings include four patterns of media use: communicating, exchanging mixed objects, coordinating, (e.g., of status reports), and semi-archival filing. We observed differential use of various media including highly variable use of chats and surprisingly informal uses of files. We discuss the implications for the design of mixed media collaborative tools to support the work activities of small to medium sized work teams.\nTitle:\nPatterns of media use in an activity-centric collaborative environment\n\nAbstract:\nContextual collaboration seamlessly integrates existing groupware technologies into a uniform user experience that combines synchronous and asynchronous interactions. This user experience is usually supported by a contextual collaboration infrastructure that needs to efficiently cope with the fast switching and integration of different modes of interaction. This paper experiments with a new model for contextual collaboration based on the notion of generic shared objects. We describe a native implementation of this model and evaluate its behavior under different media traffic conditions. We compare the native implementation with an alternative implementation that integrates existing notification and meeting servers to deliver the same model behavior. We discuss trade-offs and limitations of those two implementations.\nTitle:\nUnderstanding the trade-offs of blending collaboration services in support of contextual collaboration\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.653, mean value for all the writers is 8.648, which makes it 0.857 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe describe a new collaborative technology that is mid-way between the informality of email and the formality of shared workspaces. Email and other ad hoc collaboration systems are typically lightweight and flexible, but build up an unmanageable clutter of copied objects. At the other extreme, shared workspaces provide formal, structured collaboration, but are too heavyweight for users to set up. To bridge this gap between the ad hoc and formal, this paper introduces the notion of \"object-centric sharing\", where users collaborate in a lightweight manner but aggregate and organize different types of shared artifacts into semi-structured activities with dynamic membership, hierarchical object relationships, as well as real-time and asynchronous collaboration. We present a working prototype implemented with a replicated peer-to-peer architecture, which we describe in detail, and demonstrate its performance in synchronous and asynchronous modes.\nTitle:", "model_inf_time": 1.55}, {"id": "41352", "output": "Smart Social Collaboration: Research Projects at IBM Research", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we explore how a social bookmarking service is used to support knowledge sharing in a large enterprise. While there has been considerable interest in social bookmarking and collaborative tagging systems in recent years, very little is known about their actual usage. In this paper, we present the results of a\nTitle:\nInformation Sharing and Patterns of Social Interaction in an Enterprise Social Bookmarking Service\n\nAbstract:\nThis paper describes a social network site designed to support employees within an enterprise in connecting and learning about each other through personal and professional sharing. We introduce the design concepts and provide a detailed account of the first three months of usage, involving nearly 300 users. Our findings suggest that employees find the site particularly useful as a way to perform people sensemaking of individuals and to connect and maintain relationships with others on the site.\nTitle:\nPeople Sensemaking and Relationship Building on an Enterprise Social Network Site\n\nAbstract:\nThis paper describes a new collaboration technology that is carefully poised between informal, ad hoc, easy-to-initiate collaborative tools, vs. more formal, structured, and high-overhead collaborative applications. Our approach focuses on the support of lightweight, informally structured, opportunistic activities featuring heterogeneous threads of shared objects with dynamic membership. We introduce our design concepts, and we provide a detailed first look at data from the first 100 days of usage by 20 researchers and 13 interns, who both confirmed our hypotheses and surprised us by reinventing the technology in several ways.\nTitle:\nOne-hundred days in an activity-centric collaboration environment based on shared objects\n\nAbstract:\nThis paper describes a new collaboration technology that is based on the support of lightweight, informally structured, opportunistic activities featuring heterogeneous threads of shared items with dynamic membership. We introduce our design concepts, and we provide a detailed analysis of user behavior during a five month field study. We present the patterns of media use that we observed, using a variety of analytical methods including thread clustering and analysis. Major findings include four patterns of media use: communicating, exchanging mixed objects, coordinating, (e.g., of status reports), and semi-archival filing. We observed differential use of various media including highly variable use of chats and surprisingly informal uses of files. We discuss the implications for the design of mixed media collaborative tools to support the work activities of small to medium sized work teams.\nTitle:\nPatterns of media use in an activity-centric collaborative environment\n\nAbstract:\nThe value of enterprise social media applications, components, and users is difficult to quantify in formal economic terms such as Return On Investment. In this work we propose a different approach, based on human service to other humans. We describe a family of metrics, Return On Contribution (ROC), to assist in managing social software systems. ROC focuses on human collaboration, namely the creation and consumption of information and knowledge among employees. We show how ROC can be used to track the performance of several types of social media applications, and how ROC can help to understand the usage patterns of items within those applications, and the performance of employees who use those applications. Design implications include the importance of \"lurkers\" in organizational knowledge exchange, and specific types of measurements that may be of value to employees, managers, and system administrators.\nTitle:\nReturn On Contribution (ROC): A Metric for Enterprise Social Software.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.484, mean value for all the writers is 8.648, which makes it 0.14 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper we feature a set of research projects done at several IBM Research laboratories across the world. The work featured here focuses on the topic of smart social collaboration, which studies, designs, and develops social collaboration principles and technologies that can help customize and enhance existing social collaboration tools to suit specific user needs, including cultural, business, and personal needs.\nTitle:", "model_inf_time": 1.16}, {"id": "41353", "output": "VideoCLEF 2009: Exploring Multimedia Access and Enrichment in a Multilingual Context", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe VideoCLEF track, introduced in 2008, aims to develop and evaluate tasks related to analysis of and access to multilingual multimedia content. In its first year, VideoCLEF piloted the Vid2RSS task, whose main subtask was the classification of dual language video (Dutch-language television content featuring English-speaking experts and studio guests). The task offered two additional discretionary subtasks: feed translation and automatic keyframe extraction. Task participants were supplied with Dutch archival metadata, Dutch speech transcripts, English speech transcripts and ten thematic category labels, which they were required to assign to the test set videos. The videos were grouped by class label into topic-based RSS-feeds, displaying title, description and keyframe for each video. Five groups participated in the 2008 VideoCLEF track. Participants were required to collect their own training data; both Wikipedia and general web content were used. Groups deployed various classifiers (SVM, Naive Bayes and k-NN) or treated the problem as an information retrieval task. Both the Dutch speech transcripts and the archival metadata performed well as sources of indexing features, but no group succeeded in exploiting combinations of feature sources to significantly enhance performance. A small scale fluency/adequacy evaluation of the translation task output revealed the translation to be of sufficient quality to make it valuable to a non-Dutch speaking English speaker. For keyframe extraction, the strategy chosen was to select the keyframe from the shot with the most representative speech transcript content. The automatically selected shots were shown, with a small user study, to be competitive with manually selected shots. Future years of VideoCLEF will aim to expand the corpus and the class label list, as well as to extend the track to additional tasks.\nTitle:\nOverview of VideoCLEF 2008: Automatic Generation of Topic-based Feeds for Dual Language Audio-Visual Content.\n\nAbstract:\nWe describe Dublin City University (DCU)'s participation in the VideoCLEF 2009 Linking Task. Two approaches were implemented using the Lemur information retrieval toolkit. Both approaches first extracted a search query from the transcriptions of the Dutch TV broadcasts. One method first performed search on a Dutch Wikipedia archive, then followed links to corresponding pages in the English Wikipedia. The other method first translated the extracted query using machine translation and then searched the English Wikipedia collection directly. We found that using the original Dutch transcription query for searching the Dutch Wikipedia yielded better results.\nTitle:\nWhen to cross over? cross-language linking using wikipedia for VideoCLEF 2009\n\nAbstract:\nDCU participated in the VideoCLEF 2009 Linking Task. Our approach was based on identifying relevant related content using the Lemur information retrieval toolkit. We implemented two distinctive variants of our approach. One version performs the search in the Dutch Wikipedia with the exact words (either stemmed or not) of the search query extracted from the ASR transcription, and returns the corresponding links pointing to the English Wikipedia. The other variant first performs an automatic machine translation of the Dutch query into English, and then the translated query is used to search the English Wikipedia directly. Among our four runs, we achieved the best results with the first approach, when the base of retrieval was the stemmed and stopped Dutch Wikipedia. Unfortunately for us, there is no one-to-one relation between the pages of the Dutch and the English Wikipedias, hence some hits from the Dutch Wikipedia have been lost as results due to lack of equivalent English article. In extreme cases, our system might return no output at all if none of the hits for a given anchor are linked to a page in the English Wikipedia. Although we included a preprocessing phase before indexing the article collections, some unuseful, but frequently occurring types of page escaped and had a significant negative impact of our second basic approach implemented in Run 3.\nTitle:\nDCU at VideoCLEF 2009\n\nAbstract:\nContent-based video retrieval systems (CBVR) are creating new search and browse capabilities using metadata describing significant features of the data. An often overlooked aspect of human interpretation of multimedia data is the affective dimension. Incorporating affective information into multimedia metadata can potentially enable search using this alternative interpretation of multimedia content. Recent work has described methods to automatically assign affective labels to multimedia data using various approaches. However, the subjective and imprecise nature of affective labels makes it difficult to bridge the semantic gap between system-detected labels and user expression of information requirements in multimedia retrieval. We present a novel affect-based video retrieval system incorporating an open-vocabulary query stage based on WordNet enabling search using an unrestricted query vocabulary. The system performs automatic annotation of video data with labels of well defined affective terms. In retrieval annotated documents are ranked using the standard Okapi retrieval model based on open-vocabulary text queries. We present experimental results examining the behaviour of the system for retrieval of a collection of automatically annotated feature films of different genres. Our results indicate that affective annotation can potentially provide useful augmentation to more traditional objective content description in multimedia retrieval.\nTitle:\nAn affect-based video retrieval system with open vocabulary querying\n\nAbstract:\nUnderstanding audience engagement levels for presentations has the potential to enable richer and more focused interaction with audio-visual recordings. We describe an investigation into automated analysis of multimodal recordings of scientific talks where the use of modalities most typically associated with engagement such as eye-gaze is not feasible. We first study visual and acoustic features to identify those most commonly associated with good speaking techniques. To understand audience interpretation of good speaking techniques, we angaged human annotators to rate the qualities of the speaker for a series of 30-second video segments taken from a corpus of 9 hours of presentations from an academic conference. Our annotators also watched corresponding video recordings of the audience to presentations to estimate the level of audience engagement for each talk. We then explored the effectiveness of multimodal features extracted from the presentation video against Likert-scale ratings of each speaker as assigned by the annotators. and on manually labelled audience engagement levels. These features were used to build a classifier to rate the qualities of a new speaker. This was able classify a rating for a presenter over an 8-class range with an accuracy of 52%. By combining these classes to a 4-class range accuracy increases to 73%. We analyse linear correlations with individual speaker-based modalities and actual audience engagement levels to understand the corresponding effect on audience engagement. A further classifier was then built to predict the level of audience engagement to a presentation by analysing the speaker's use of acoustic and visual cues. Using these speaker based modalities pre-fused with speaker ratings only, we are able to predict actual audience engagement levels with an accuracy of 68%. By combining with basic visual features from the audience as whole, we are able to improve this to an accuracy of 70%.\nTitle:\nEffects of Good Speaking Techniques on Audience Engagement.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.702, mean value for all the writers is 8.648, which makes it 0.899 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nVideoCLEF 2009 offered three tasks related to enriching video content for improved multimedia access in a multilingual environment. For each task, video data (Dutch-language television, predominantly documentaries) accompanied by speech recognition transcripts were provided. The Subject Classification Task involved automatic tagging of videos with subject theme labels. The best performance was achieved by approaching subject tagging as an information retrieval task and using both speech recognition transcripts and archival metadata. Alternatively, classifiers were trained using either the training data provided or data collected from Wikipedia or via general Web search. The Affect Task involved detecting narrative peaks, defined as points where viewers perceive heightened dramatic tension. The task was carried out on the \"Beeldenstorm\" collection containing 45 short-form documentaries on the visual arts. The best runs exploited affective vocabulary and audience directed speech. Other approaches included using topic changes, elevated speaking pitch, increased speaking intensity and radical visual changes. The Linking Task, also called \"Finding Related Resources Across Languages,\" involved linking video to material on the same subject in a different language. Participants were provided with a list of multimedia anchors (short video segments) in the Dutch-language \"Beeldenstorm\" collection and were expected to return target pages drawn from English-language Wikipedia. The best performing methods used the transcript of the speech spoken during the multimedia anchor to build a query to search an index of the Dutch-language Wikipedia. The Dutch Wikipedia pages returned were used to identify related English pages. Participants also experimented with pseudo-relevance feedback, query translation and methods that targeted proper names.\nTitle:", "model_inf_time": 2.11}, {"id": "41354", "output": "Pic-A-Topic: Enabling Efficient Viewing of Travel TV Shows through Topic Segmentation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nPic-A-Topic is a prototype system designed for enabling the user to view topical segments of recorded TV shows selectively. By analysing closed captions and eletronic program guide texts, it performs topic segmentation and topic sentence selection, and presents a clickable table of contents to the user. Our previous work handled TV shows on travel, and included a user study which suggested that Pic-A-Topic's average segmentation accuracy at that point was possibly indistinguishable from that of manual segmentation. This paper shows that the latest version of Pic-A-Topic is capable of effectively segmenting several TV genres related to travel, cooking, food and talk/variety shows, by means of genre-specific strategies. According to an experiment using 26.5 hours of real Japanese TV shows (25 clips) which subsumes the travel test collection we used earlier (10 clips), Pic-A-Topic's topic segmentation results for non-travel genres are as accurate as those for travel. We adopt an evaluation method that is more demanding than the one we used in our previous work, but even in terms of this strict measurement, Pic-A-Topic's accuracy is around 82% of manual performance on average. Moreover, the fusion of cue phrase detection and vocabulary shift detection is very successful for all the genres that we have targeted.\n\n\nTitle:\nPic-A-Topic: efficient viewing of informative TV contents on travel, cooking, food and more\n\nAbstract:\nIn our previous work, we developed a prototype of a speech-input help system for home appliances such as digital cameras and microwave ovens. Given a factoid question, the system performs textual question answering using the manuals as the knowledge source. Whereas, given a HOW question, it retrieves and plays a demonstration video. However, our first prototype suffered from speech recognition errors, especially when the Japanese interrogative phrases in factoid questions were misrecognized. We therefore propose a method for solving this problem, which complements a speech query transcript with an interrogative phrase selected from a pre-determined list. The selection process first narrows down candidate phrases based on co-occurrences within the manual text, and then computes the similarity between each candidate and the query transcript in terms of pronunciation. Our method improves the Mean Reciprocal Rank of top three answers from 0.429 to 0.597 for factoid questions.\nTitle:\nImproving the robustness to recognition errors in speech input question answering\n\nAbstract:\nThe application of relevance feedback techniques has been shown to improve retrieval performance for a number of information retrieval tasks. This paper explores incremental relevance feedback for ad hoc Japanese text retrieval&semi; examining, separately and in combination, the utility of term reweighting and query expansion using a probabilistic retrieval model. Retrieval performance is evaluated in terms of standard precision-recall measures, and also using \u201cnumber-to-view\u201d graphs. Experimental results, on the standard BMIR-J2 Japanese language retrieval collection, show that both term reweighting and query expansion improve retrieval performance. This is reflected in improvements in both precision and recall, but also a reduction in the average number of documents which must be viewed to find a selected number of relevant items. In particular, using a simple simulation of user searching, incremental application of relevance information is shown to lead to progressively improved retrieval performance and an overall reduction in the number of documents that a user must view to find relevant ones.\nTitle:\nIncremental Relevance Feedback in Japanese Text Retrieval\n\nAbstract:\nIn this paper we report results of an investigation into EnglishJapanese Cross-Language Information Retrieval (CLIR) comparing a number of query translation methods. Results from experiments using the standard BMIR-J2 Japanese collection suggest that full machine translation (MT) can outperform popular dictionary-based query translation methods and further that in this context MT is largely robust to queries with little linguistic structure.\nTitle:\nA comparison of query translation methods for English-Japanese cross-language information retrieval (poster abstract)\n\nAbstract:\nWe tackle Attitude Detection, which we define as the task of extracting the replier's attitude, i.e., a target-polarity pair, from a given one-round conversation. While previous studies considered Target Extraction and Polarity Classification separately, we regard them as subtasks of Attitude Detection. Our experimental results show that treating the two subtasks independently is not the optimal solution for Attitude Detection, as achieving high performance in each subtask is not sufficient for obtaining correct target-polarity pairs. Our jointly trained model AD-NET substantially outperforms the separately trained models by alleviating the target-polarity mismatch problem. Moreover, we proposed a method utilising the attitude detection model to improve retrieval-based chatbots by re-ranking the response candidates with attitude features. Human evaluation indicates that with attitude detection integrated, the new responses to the sampled queries from are statistically significantly more consistent, coherent, engaging and informative than the original ones obtained from a commercial chatbot.\n\n\nTitle:\nAttitude Detection for One-Round Conversation - Jointly Extracting Target-Polarity Pairs.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.506, mean value for all the writers is 8.648, which makes it 0.732 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe introduce a system called Pic-A-Topic, which analyses closed captions of Japanese TV shows on travel to perform topic segmentation and topic sentence selection. Our objective is to provide a table-of-contents interface that enables efficient viewing of desired topical segments within recorded TV shows to users of appliances such as hard disk recorders and digital TVs. According to our experiments using 14.5 hours of recorded travel TV shows, Pic-A-Topic\u2019s F1-measure for the topic segmentation task is 82% of manual performance on average. Moreover, a preliminary user evaluation experiment suggests that this level of performance may be indistinguishable from manual performance.\nTitle:", "model_inf_time": 1.68}, {"id": "41355", "output": "Optimal Coding Design for Energy Minimization in On-Off Keying Wireless Nanosensor Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWireless nanosensor networks (WNSNs), which consist of a lot of nanosensors with size of just a few hundred nanometers and are able to detect and sense new types of events at the nanoscale, are promising for a lot of unique applications like intrabody drug delivery systems, air pollution surveillance, etc. One important feature of WNSNs is that the nanosensors are highly energy-constrained, which makes it essential to develop energy efficient protocols for different layers of such networks. This paper focuses on a WNSN with on-off keying (OOK) modulation and explores the problem of transmission energy minimization in it. We first propose a general minimum transmission energy (MTE) coding scheme, which maps m-bit symbols into n-bit codewords with the least number of high-bits and thus results in the lowest energy consumption per symbol for any given m and n. We further determine the optimal setting of symbol length m and codeword length n in the MTE coding scheme so as to achieve the minimum energy consumption per data bit, which serves as the lower bound of transmission energy consumption in such WNSNs. Numerical results are provided to demonstrate the efficiency of the MTE coding scheme.\nTitle:\nEnergy optimal coding for wireless nanosensor networks\n\nAbstract:\nWireless nano-sensor networks (WNSNs), which consist of nano-sensors a few hundred nanometers in size with the capability to detect and sense new types of events in nano-scale, are promising for many unique applications such as air pollution surveillance. The nano-sensors of WNSNs are highly energy-constrained, which makes it essential to develop energy-efficient communication techniques in such networks. In this paper, we focus on WNSNs employing on-off keying (OOK) modulation, whereby transmission energy minimization corresponds to the minimization of average codeword weight (ACW). We formulate an integer nonlinear programming problem to construct prefix-free codes with minimum ACW under the constraint of average codeword length (ACL) so as to minimize the transmission energy consumption while guaranteeing the throughput larger than a preset desired value. In addition, two efficient algorithms, called binary tree based weight decreasing (BT-WD) algorithm and binary tree based length decreasing (BT-LD) algorithm, are presented for constructing low-ACW prefix-free codes. The effectiveness of the proposed algorithms is verified through simulations and comparisons with the exhaustive search method. Compared with the available fixed-length low-weight codes, the designed prefix-free variable-length codes allow us to not only control the throughput more flexibly but also achieve lower transmission energy consumption in the scenarios with low or medium bit error rates.\nTitle:\nEnergy-Efficient Prefix-Free Codes for Wireless Nano-Sensor Networks Using OOK Modulation\n\nAbstract:\nWireless nanosensor networks (WNSNs) represent a class of emerging and important network model for perceiving information in the nanoscale. The extremely limited energy storage in nanosensors necessitates the design of energy-efficient protocols (e.g., data collection schemes) for such networks. This paper considers the design of energy-efficient data collection scheme in a WNSN operating in the human body, i.e., Body Area Nanonetwork (BANN). More specifically, this scheme involves a hierarchical (i.e., nanointerface-nanorouter-nanonode) collection structure with nanointerface receiving external requests and returning the requested data, nanorouter collecting data from nanonodes and nanonodes gathering data from human body. In the data collection process, each nanorouter first activates the nanonodes in a rectangle region centered at itself with length l and width W (i.e., wake-up region) and obtains the available energy level of each nanonode in this region. Each nanorouter then selects the nanonode with the largest available energy in the wake-up region to return the requested data. Finally, simulation results are provided to illustrate the performances of our scheme in terms of average available energy and path loss. These results show that better performances can be achieved when a smaller wake-up region is specified.\nTitle:\nAn Energy-Efficient Data Collection Scheme in Body Area Nanonetworks.\n\nAbstract:\nNetwork coding has the potential to greatly improve the throughput of wireless networks. In the current proposals for wireless network coding, network nodes transmit packets at a fixed transmission rate. It is notable, however, that by dynamically selecting the rate, we can effectively improve the node transmission efficiency. In this paper, we study the application of a rate-adaptive transmission mechanism in network-coding-based multihop wireless networks. In such networks, whether a coding solution is satisfactory or not depends not only on the number of involved native packets but on the packet loss probabilities of its intended next hops and its transmission time as well, both of which depend on the transmission rate. Therefore, we aim to jointly design the coding operation and rate selection to maximize the transmission efficiency. Specifically, we first describe and mathematically formulate the optimal packet coding and rate-selection problem. Then, we prove the NP-completeness of this optimization problem. Finally, we propose an efficient algorithm for finding good combinations of the coding solution and the transmission rate. Simulation results demonstrate that compared with the rate-fixed transmission, the rate-adaptive transmission based on our algorithm can significantly improve the node transmission efficiency.\nTitle:\nJoint Design of Network Coding and Transmission Rate Selection for Multihop Wireless Networks\n\nAbstract:\nThis paper studies the performance of a general two-hop relay (2HR)-(x, w, f) packet delivery scheme that combines both erasure coding and packet replication techniques in mobile ad hoc networks (MANETs). Under this packet delivery scheme, a group of x packets is first encoded into w(w \u2265 x) coded packets using erasure coding, and each coded packet is then replicated to at most f distinct relay nodes that can help to forward the coded packets to its destination node. The original packets can be recovered when x distinct coded packets reach the destination node. To understand the packet delivery process under the 2HR-(x, w, f) scheme, we develop a multidimensional Markov chain framework, and based on this analytical expressions on the packet delivery ratio and corresponding expected packet delivery cost are further derived. Finally, extensive simulation and numerical studies are conducted to illustrate the efficiency of the developed theoretical models and to illustrate our findings. Our results indicate that the replication parameter f should be carefully selected in order to obtain a high packet-delivery-ratio performance while maintaining a relatively low delivery cost.\nTitle:\nPacket Delivery Ratio/Cost in MANETs With Erasure Coding and Packet Replication\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.15, mean value for all the writers is 8.648, which makes it 1.281 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWireless nanosensor networks (WNSNs), which consist of nanosensors with the size of just a few hundred nanometers, can sense new types of events at nanoscale and thus are promising for lots of important applications, like intrabody drug delivery, air pollution surveillance, etc. Since nanosensors are highly energy-constrained, it is critical to adopt energy-efficient protocols for communications in WNSNs. This paper focuses on WNSNs adopting the on\u2013off keying modulator which transmits a signal for a high bit and keeps silence for a low bit and explores the optimal coding design for transmission energy minimization in such networks. Specifically, we focus on the design of low-weight code which maps m-bit symbols into n-bit codewords with fewer high bits. First, for given symbol length m and codeword length n, we determine the optimal codebook with the least average number of high bits per codeword and give the corresponding average codeword weight as a function of m and n. Then, we study the minimum transmission energy (MTE) code design problem with codeword length and codeword rate constraints and investigate how the transmission energy changes when m and n are varied. Finally, the corresponding optimal settings of m and n are determined for the MTE code design problem to achieve the minimum transmission energy consumption per data bit.\nTitle:", "model_inf_time": 2.01}, {"id": "41356", "output": "Enhancing Discriminative Sequential Learning via Data-Driven Association Mining", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDiscriminative sequential learning models like Conditional Random Fields (CRFs) have achieved significant success in several areas such as natural language processing or information extraction. Their key advantage is the ability to capture various nonindependent and overlapping features of inputs. However, several unexpected pitfalls have a negative influence on the model's performance; these mainly come from a high imbalance among classes, irregular phenomena, and potential ambiguity in the training data. This article presents a data-driven approach that can deal with such difficult data instances by discovering and emphasizing important conjunctions or associations of statistics hidden in the training data. Discovered associations are then incorporated into these models to deal with difficult data instances. Experimental results of phrase-chunking and named entity recognition using CRFs show a significant improvement in accuracy. In addition to the technical perspective, our approach also highlights a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large datasets.\nTitle:\nImproving discriminative sequential learning by discovering important association of statistics\n\nAbstract:\nConditional random fields (CRFs) have been successfully applied to various applications of predicting and labeling structured data, such as natural language tagging & parsing, image segmentation & object recognition, and protein secondary structure prediction. The key advantages of CRFs are the ability to encode a variety of overlapping, non-independent features from empirical data as well as the capability of reaching the global normalization and optimization. However, estimating parameters for CRFs is very time-consuming due to an intensive forward-backward computation needed to estimate the likelihood function and its gradient during training. This paper presents a high-performance training of CRFs on massively parallel processing systems that allows us to handle huge datasets with hundreds of thousand data sequences and millions of features. We performed the experiments on an important natural language processing task (text chunking) on large-scale corpora and achieved significant results in terms of both the reduction of computational time and the improvement of prediction accuracy.\nTitle:\nHigh-Performance Training of Conditional Random Fields for Large-Scale Applications of Labeling Sequence Data\n\nAbstract:\n\n This paper presents a new classification model in which a classifier is built upon predictive association rules (PARs) and\n the maximum entropy principle (maxent). In this model, PARs can be seen as confident statistical patterns discovered from\n training data with strong dependencies and correlations among data items. Maxent, on the other hand, is an approach to build\n an estimated distribution having maximum entropy while obeying a potentially large number of useful features observed in empirical\n data. The underlying idea of our model is that PARs have suitable characteristics to serve as features for maxent. As a result,\n our classifier can take advantage of both the useful correlation and confidence of PARs as well as the strong statistical\n modeling capability of maxent. The experimental results show that our model can achieve significantly higher accuracy in comparison\n with the previous methods.\n \n \nTitle:\nClassification with Maximum Entropy Modeling of Predictive Association Rules\n\nAbstract:\nThis paper investigates a novel application of support vector machines (SVMs) for sentence reduction. We also propose a new probabilistic sentence reduction method based on support vector machine learning. Experimental results show that the proposed methods outperform earlier methods in term of sentence reduction performance.\nTitle:\nProbabilistic sentence reduction using support vector machines\n\nAbstract:\nExtracting data on the Web is an important information extraction task. Most existing approaches rely on wrappers which require human knowledge and user interaction during extraction. This paper proposes the use of conditional models as an alternative solution to this task. Deriving the strength of conditional models like maximum entropy and maximum entropy Markov models, our method offers three major advantages: the full automation, the ability to incorporate various non-independent, overlapping features of different hypertext representations, and the ability to deal with missing and disordered data fields. The experimental results on a wide range of e-commercial websites with different layouts show that our method can achieve a satisfactory trade-off between automation and accuracy, and also provide a practical application of automated data extraction from the Web.\nTitle:\nAutomated data extraction from the web with conditional models\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.231, mean value for all the writers is 8.648, which makes it 1.351 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDiscriminative sequential learning models like Conditional Random Fields (CRFs) have achieved significant success in several areas such as natural language processing or information extraction. Their key advantage is the ability to capture various non--independent and overlapping features of inputs. However, several unexpected pitfalls have a negative influence on the model's performance; these mainly come from an imbalance among classes/labels, irregular phenomena, and potential ambiguity in the training data. This paper presents a data--driven approach that can deal with such hard--to--predict data instances by discovering and emphasizing rare--but--important associations of statistics hidden in the training data. Mined associations are then incorporated into these models to deal with difficult examples. Experimental results of English phrase chunking and named entity recognition using CRFs show a significant improvement in accuracy. In addition to the technical perspective, our approach also highlights a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large dataset.\nTitle:", "model_inf_time": 1.48}, {"id": "41357", "output": "Predicting Negotiation Outcomes: A Multimodal Approach", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nNegotiation is a component deeply ingrained in our daily lives, and it can be challenging for a person to predict the respondent\u2019s reaction (acceptance or rejection) to a negotiation offer. In this work, we focus on finding acoustic and visual behavioral cues that are predictive of the respondent\u2019s immediate reactions using a face-to-face negotiation dataset, which consists of 42 dyadic interactions in a simulated negotiation setting. We show our results of exploring 4 different sources of information, namely nonverbal behavior of the proposer, that of the respondent, mutual behavior between the interactants related to behavioral symmetry and asymmetry, and past negotiation history between the interactants. Firstly, we show that considering other sources of information (other than the nonverbal behavior of the respondent) can also have comparable performance in predicting respondent reactions. Secondly, we show that automatically extracted mutual behavioral cues of symmetry and asymmetry are predictive partially due to their capturing information of the nature of the interaction itself, whether it is cooperative or competitive. Lastly, we identify audio-visual behavioral cues that are most predictive of the respondent\u2019s immediate reactions.\nTitle:\nI Can Already Guess Your Answer: Predicting Respondent Reactions during Dyadic Negotiation\n\nAbstract:\nIn this paper, we analyze face-to-face negotiation interactions with the goal of predicting the respondent's immediate reaction (i.e., accept or reject) to a negotiation offer. Supported by the theory of social rapport, we focus on mutual behaviors which are defined as nonverbal characteristics that occur due to interactional influence. These patterns include behavioral symmetry (e.g., synchronized smiles) as well as asymmetry (e.g., opposite postures) between the two negotiators. In addition, we put emphasis on finding audio-visual mutual behaviors that can be extracted automatically, with the vision of a real-time decision support tool. We introduce a dyadic negotiation dataset consisting of 42 face-to-face interactions and show experiments confirming the importance of multimodal and mutual behaviors.\nTitle:\nMutual Behaviors during Dyadic Negotiation: Automatic Prediction of Respondent Reactions\n\nAbstract:\nDuring face-to-face conversation, people use visual feedback such as head nods to communicate relevant information and to synchronize rhythm between participants. In this paper we describe how contextual information from other participants can be used to predict visual feedback and improve recognition of head gestures in human-human interactions. For example, in a dyadic interaction, the speaker contextual cues such as gaze shifts or changes in prosody will influence listener backchannel feedback (e.g., head nod). To automatically learn how to integrate this contextual information into the listener gesture recognition framework, this paper addresses two main challenges: optimal feature representation using an encoding dictionary and automatic selection of optimal feature-encoding pairs. Multimodal integration between context and visual observations is performed using a discriminative sequential model (Latent-Dynamic Conditional Random Fields) trained on previous interactions. In our experiments involving 38 storytelling dyads, our context-based recognizer significantly improved head gesture recognition performance over a vision-only recognizer.\nTitle:\nContext-based recognition during human interactions: automatic feature selection and encoding dictionary\n\nAbstract:\nAutomated negotiation between two agents has been the subject of much research focused on optimization and efficiency. However, human-agent negotiation represents a field in which real-world considerations can be more fully explored. Furthermore, teaching negotiation and other interpersonal skills requires long periods of practice with open-ended dialogues and partners. The API presented in this paper represents a novel platform on which to conduct human-agent research and facilitate teaching negotiation tactics in a longitudinal way. We present a prototype demonstration that is real-time, rapidly distributable, and allows more actions than current platforms of negotiation.\nTitle:\nIAGO: Interactive Arbitration Guide Online (Demonstration).\n\nAbstract:\nEmotional signaling plays an important role in negotiations and other social decision-making tasks as it can signal intention and shape joint decisions. Specifically it has been shown to influence cooperation or competition. This has been shown in previous studies for scripted interactions that control emotion signaling and rely on manual coding of affect. In this work we examine face-to-face interactions in an iterative social dilemma task (prisoner's dilemma) via an automatic framework for facial expression analysis. We explore if automatic analysis of emotion can give insight into the social function of emotion in face-to-face interactions. Our analysis suggests that positive and negative displays of emotion are associated with more prosocial and proself game acts respectively. Moreover signaling cooperative intentions to the opponent via positivity can leave participants more open to exploitation, whereas signaling a more tough stance via negativity seems to discourage exploitation. However, the benefit of negative affect is short-term and both players do worse over time if they show negative emotions.\nTitle:\nEmotional signaling in a social dilemma: An automatic analysis\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.124, mean value for all the writers is 8.648, which makes it 1.259 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nBe it in our workplace or with our family or friends, negotiation comprises a fundamental fabric of our everyday life, and it is apparent that a system that can automatically predict negotiation outcomes will have substantial implications. In this paper, we focus on finding nonverbal behaviors that are predictive of immediate outcomes (acceptances or rejections of proposals) in a dyadic negotiation. Looking at the nonverbal behaviors of the respondent alone would be inadequate since ample predictive information could also reside in the behaviors of the proposer, as well as the past history between the two parties. With this intuition in mind, we show that a more accurate prediction can be achieved by considering all the three sources (multimodal) of information together. We evaluate our approach on a face-to-face negotiation dataset consisting of 42 dyadic interactions and show that integrating all three sources of information outperforms each individual predictor.\nTitle:", "model_inf_time": 1.34}, {"id": "41358", "output": "Convergent Lagrangian Method for Nonlinear Integer Programming in Series-Parallel Reliability Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we present an efficient exact solution method for solving nonlinear separable integer programming problems with a quadratic objective function. The proposed method combines the Lagrangian dual method with a duality reduction scheme using contour cut. At each iteration of the algorithm, lower and upper bounds of the problem are determined by the Lagrangian dual search. To eliminate the duality gap, a novel cut-and-partition scheme is derived by exploring the special structure of the quadratic contour. The method finds an exact solution of the problem in a finite number of iterations. Computational results are reported for problems with up to 2000 integer variables. Comparison results with other methods are also presented.\nTitle:\nConvergent Lagrangian and Contour Cut Method for Nonlinear Integer Programming with a Quadratic Objective Function\n\nAbstract:\nWe consider in this paper the Lagrangian dual method for solving general integer programming. New properties of Lagrangian duality are derived by a means of perturbation analysis. In particular, a necessary and sufficient condition for a primal optimal solution to be generated by the Lagrangian relaxation is obtained. The solution properties of Lagrangian relaxation problem are studied systematically. To overcome the difficulties caused by duality gap between the primal problem and the dual problem, we introduce an equivalent reformulation for the primal problem via applying a pth power to the constraints. We prove that this reformulation possesses an asymptotic strong duality property. Primal feasibility and primal optimality of the Lagrangian relaxation problems can be achieved in this reformulation when the parameter p is larger than a threshold value, thus ensuring the existence of an optimal primal-dual pair. We further show that duality gap for this partial pth power reformulation is a strictly decreasing function of p in the case of a single constraint.\nTitle:\nTowards Strong Duality in Integer Programming\n\nAbstract:\nNonlinear Lagrangian theory offers a success guarantee for the dual search via construction of a nonlinear support of the perturbation function at the optimal point. In this paper, a new nonlinear dual formulation of an exponential form is proposed for bounded integer programming. This new formulation possesses an asymptotic strong duality property and guarantees a success in identifying a primal optimum solution. No actual dual search is needed in the solution process when the parameter of the nonlinear Lagrangian formulation is set to be large enough.\nTitle:\nA nonlinear Lagrangian dual for integer programming\n\nAbstract:\nA logarithmic-exponential dual formulation is proposed in this paper for bounded integer programming problems. This new dual formulation possesses an asymptotic strong duality property and guarantees the identification of an optimal solution of the primal problem. These prominent features are achieved by exploring a novel nonlinear Lagrangian function, deriving an asymptotic zero duality gap, investigating the unimodality of the associated dual function and ensuring the primal feasibility of optimal solutions in the dual formulation. One other feature of the logarithmicexponential dual formulation is that no actual dual search is needed when parameters are set above certain threshold-values.\nTitle:\nAsymptotic Strong Duality for Bounded Integer Programming: A Logarithmic-Exponential Dual Formulation\n\nAbstract:\nWe investigate in this paper the Lagrangian duality properties of linear equality constrained binary quadratic programming. We derive an underestimation of the duality gap between the primal problem and its Lagrangian dual or SDP relaxation, using the distance from the set of binary integer points to certain affine subspace, while the computation of this distance can be achieved by the cell enumeration of hyperplane arrangement. Alternative Lagrangian dual schemes via the exact penalty and the squared norm constraint reformulations are also discussed.\nTitle:\nDuality Gap Estimation of Linear Equality Constrained Binary Quadratic Programming\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.768, mean value for all the writers is 8.648, which makes it 0.956 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe consider two related nonlinear integer programming problems arising in series-parallel reliability systems: the constrained redundancy problem and the cost minimization problem. We propose in this paper an efficient method for solving these two types of nonlinear integer programming problems. The proposed convergent Lagrangian method combines Lagrangian relaxation with a duality reduction technique. An outer approximation method is used to search for the optimal dual solution and to generate Lagrangian bounds of the primal problem. To reduce the duality gap, we derive a special partition scheme by exploiting the inherent monotonicity and separability of the problem. Furthermore, a special optimality criterion is adopted to improve feasible solutions and to fathom integer subboxes. Computational results show that the algorithm is capable of solving large-scale optimization problems in series-parallel reliability systems. Comparison numerical results with other existing methods are also reported.\nTitle:", "model_inf_time": 1.45}, {"id": "41359", "output": "Pinball Loss for One-Bit Compressive Sensing: Models and Algorithms", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe one-bit quantization is implemented by one single comparator that operates at low power and a high rate. Hence one-bit compressive sensing (1bit-CS) becomes attractive in signal processing. When measurements are corrupted by noise during signal acquisition and transmission, 1bit-CS is usually modeled as minimizing a loss function with a sparsity constraint. The one-sided \u21131 loss and the linear loss are two popular loss functions for 1bit-CS. To improve the decoding performance on noisy data, we consider the pinball loss, which provides a bridge between the one-sided \u21131 loss and the linear loss. Using the pinball loss, two convex models, an elastic-net pinball model and its modification with the \u21131-norm constraint, are proposed. To efficiently solve them, the corresponding dual coordinate ascent algorithms are designed and their convergence is proved. The numerical experiments confirm the effectiveness of the proposed algorithms and the performance of the pinball loss minimization for 1bit-CS.\nTitle:\nPinball loss minimization for one-bit compressive sensing: Convex models and algorithms.\n\nAbstract:\nThe ramp loss is a robust but non-convex loss for classification. Compared with other non-convex losses, a local minimum of the ramp loss can be effectively found. The effectiveness of local search comes from the piecewise linearity of the ramp loss. Motivated by the fact that the l1-penalty is piecewise linear as well, the l1-penalty is applied for the ramp loss, resulting in a ramp loss linear programming support vector machine (ramp-LPSVM). The proposed ramp-LPSVM is a piecewise linear minimization problem and the related optimization techniques are applicable. Moreover, the l1-penalty can enhance the sparsity. In this paper, the corresponding misclassification error and convergence behavior are discussed. Generally, the ramp loss is a truncated hinge loss. Therefore ramp-LPSVM possesses some similar properties as hinge loss SVMs. A local minimization algorithm and a global search strategy are discussed. The good optimization capability of the proposed algorithms makes ramp-LPSVM perform well in numerical experiments: the result of ramp-LPSVM is more robust than that of hinge SVMs and is sparser than that of ramp-SVM, which consists of the || \u010b || k-penalty and the ramp loss.\nTitle:\nRamp loss linear programming support vector machine\n\nAbstract:\nTraditionally, the hinge loss is used to construct support vector machine (SVM) classifiers. The hinge loss is related to the shortest distance between sets and the corresponding classifier is hence sensitive to noise and unstable for re-sampling. In contrast, the pinball loss is related to the quantile distance and the result is less sensitive. The pinball loss has been deeply studied and widely applied in regression but it has not been used for classification. In this paper, we propose a SVM classifier with the pinball loss, called pin-SVM, and investigate its properties, including noise insensitivity, robustness, and misclassification error. Besides, insensitive zone is applied to the pin-SVM and a sparse model is obtained. Compared to the SVM with the hinge loss, the proposed pin-SVM has the same computational complexity and enjoys noise insensitivity and re-sampling stability.\nTitle:\nSupport Vector Machine Classifier with Pinball Loss.\n\nAbstract:\nThis letter addresses the robustness problem when learning a large margin classifier in the presence of label noise. In our study, we achieve this purpose by proposing robustified large margin support vector machines. The robustness of the proposed robust support vector classifiers (RSVC), which is interpreted from a weighted viewpoint in this work, is due to the use of nonconvex classification losses. Besides the robustness, we also show that the proposed RSCV is simultaneously smooth, which again benefits from using smooth classification losses. The idea of proposing RSVC comes from M-estimation in statistics since the proposed robust and smooth classification losses can be taken as one-sided cost functions in robust statistics. Its Fisher consistency property and generalization ability are also investigated. Besides the robustness and smoothness, another nice property of RSVC lies in the fact that its solution can be obtained by solving weighted squared hinge loss-based support vector machine problems iteratively. We further show that in each iteration, it is a quadratic programming problem in its dual space and can be solved by using state-of-the-art methods. We thus propose an iteratively reweighted type algorithm and provide a constructive proof of its convergence to a stationary point. Effectiveness of the proposed classifiers is verified on both artificial and real data sets.\nTitle:\nRobust Support Vector Machines for Classification with Nonconvex and Smooth Losses.\n\nAbstract:\nThis paper introduces a general framework of non-parallel support vector machines, which involves a regularization term, a scatter loss and a misclassification loss. When dealing with binary problems, the framework with proper losses covers some existing non-parallel classifiers, such as multisurface proximal support vector machine via generalized eigenvalues, twin support vector machines, and its least squares version. The possibility of incorporating different existing scatter and misclassification loss functions into the general framework is discussed. Moreover, in contrast with the mentioned methods, which applies kernel-generated surface, we directly apply the kernel trick in the dual and then obtain nonparametric models. Therefore, one does not need to formulate two different primal problems for the linear and nonlinear kernel respectively. In addition, experimental results are given to illustrate the performance of different loss functions.\nTitle:\nNon-parallel support vector classifiers with different loss functions\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.333, mean value for all the writers is 8.648, which makes it 0.584 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe one-bit quantization can be implemented by one single comparator, which operates at low power and a high rate. Hence one-bit compressive sensing (\\emph{1bit-CS}) becomes very attractive in signal processing. When the measurements are corrupted by noise during signal acquisition and transmission, 1bit-CS is usually modeled as minimizing a loss function with a sparsity constraint. The existing loss functions include the hinge loss and the linear loss. Though 1bit-CS can be regarded as a binary classification problem because a one-bit measurement only provides the sign information, the choice of the hinge loss over the linear loss in binary classification is not true for 1bit-CS. Many experiments show that the linear loss performs better than the hinge loss for 1bit-CS. Motivated by this observation, we consider the pinball loss, which provides a bridge between the hinge loss and the linear loss. Using this bridge, two 1bit-CS models and two corresponding algorithms are proposed. Pinball loss iterative hard thresholding improves the performance of the binary iterative hard theresholding proposed in [6] and is suitable for the case when the sparsity of the true signal is given. Elastic-net pinball support vector machine generalizes the passive model proposed in [11] and is suitable for the case when the sparsity of the true signal is not given. A fast dual coordinate ascent algorithm is proposed to solve the elastic-net pinball support vector machine problem, and its convergence is proved. The numerical experiments demonstrate that the pinball loss, as a trade-off between the hinge loss and the linear loss, improves the existing 1bit-CS models with better performances.\nTitle:", "model_inf_time": 1.74}, {"id": "41360", "output": "Glance: Efficient Distance-Sensitive Querying in Wireless Sensor Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDistance sensitivity is a locality concept that is useful for designing scalable wireless sensor network applications. In this paper, we formally define distance sensitivity and we highlight its different forms such as distance sensitive latency, error, rate, membership, and healing. We show how distance sensitivity allows the application requirements and the network specification to be stated (and reasoned about) purely in geometric terms. This paper also examines key aspects of the concept, namely sufficiency, decomposability, and robustness. Specifically, sufficiency involves consideration of whether distance sensitive properties are enough for meeting application requirements. Decomposability involves choosing properties of network layers/components so that together the distance sensitive network abstraction holds. And, robustness implies preservation of distance sensitivity in the presence of failures in the network, including both permanent and transient failures and even those that violate the density or geometric assumptions of the network. We illustrate these aspects via examples from our previous work, all in the context of a common case study, regarding the design of a distributed pursuer evader tracking application.\nTitle:\nAspects of Distance Sensitive Design of Wireless Sensor Networks\n\nAbstract:\nDistributed observation and control of mobile objects via static wireless sensors demands timely information in a distance sensitive manner: information about closer objects is required more often and more quickly than that of farther objects. In this paper, we present a wireless sensor network protocol, Trail, that supports distance sensitive tracking of mobile object by in-network subscribers upon demand. Trail achieves a find time that is linear in the distance from the subscriber to the object, via a distributed data structure that is updated only locally when objects move. Trail seeks to minimize the size of the data structure. Moreover, Trail is reliable, fault-tolerant and energy-efficient, despite the network dynamics that are typical of wireless sensor networks. We evaluate the performance of Trail by simulations in a 90-by-90 sensor network and report on 105 node experiments in the context of a pursuer-evader control application.\nTitle:\nTrail: a distance sensitive WSN service for distributed object tracking\n\nAbstract:\nWe present a fast local clustering service, FLOC, that partitions a multi-hop wireless network into nonoverlapping and approximately equal-sized clusters. Each cluster has a clusterhead such that all nodes within unit distance of the clusterhead belong to the cluster but no node beyond distance m from the clusterhead belongs to the cluster. By asserting m \u9a74 2, FLOC achieves locality: effects of cluster formation and faults/changes at any part of the network are contained within at most m units. By taking unit distance to be the reliable communication radius and m to be the maximum communication radius, FLOC exploits the double-band nature of wireless radio-model and achieves clustering in constant time regardless of the network size. Through simulations and experiments with actual deployments, we analyze the tradeoffs between clustering time and the quality of clustering, and suggest suitable parameters for FLOC to achieve a fast completion time without compromising the quality of the resulting clustering.\nTitle:\nDesign and Analysis of a Fast Local Clustering Service for Wireless Sensor Networks\n\nAbstract:\nIn this technical note, we focus on a control based surveillance application using a wireless sensor network in which information from the network is used to actively guide a mobile agent leading to eventual pursuit of one or more evaders in a large region. We exploit distance sensitivity as a locality concept in designing a scalable pursuit control system that accounts for network constraints. Specifically, we show that eventual pursuit is satisfied if information about an evader is available to the pursuing agent with error, latency and frequency that decrease linearly with distance from the evader. Then, we design network algorithms for delivering snapshots of the system that satisfy these distance sensitivity properties.\nTitle:\nPursuit Control Over Wireless Sensor Networks Using Distance Sensitivity Properties\n\nAbstract:\nDistributed observation and control of mobile objects via static wireless sensors demands timely information in a distance-sensitive manner: Information about closer objects is required more often and more quickly than that of farther objects. In this article, we present a wireless sensor network protocol, Trail, that supports distance-sensitive tracking of mobile objects for in-network subscribers upon demand. Trail achieves a find time that is linear in the distance from a subscriber to an object, via a distributed data structure that is updated only locally when the object moves. Notably, Trail does not partition the network into a hierarchy of clusters and clusterheads, and as a result Trail has lower maintenance costs, is more locally fault tolerant, and it better utilizes the network in terms of load balancing and minimizing the size of the data structure needed for tracking. Moreover, Trail is reliable and energy efficient, despite the network dynamics that are typical of wireless sensor networks. Trail can be refined by tuning certain parameters, thereby yielding a family of protocols that are suited for different application settings such as rate of queries, rate of updates, and network size. We evaluate the performance of Trail by analysis, simulations in a 90 \u00d7 90 sensor network, and experiments on 105 Mica2 nodes in the context of a pursuer-evader control application.\nTitle:\nTrail: A distance-sensitive sensor network service for distributed object tracking\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.837, mean value for all the writers is 8.648, which makes it 0.692 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDistance-sensitivity guarantee in querying is a highly desirable property in wireless sensor networks as it limits the cost of executing a \u201cquery\u201d operation to be within a constant factor of the distance to the nearest node that contains an answer. However, such a tight guarantee may require building an infrastructure for efficient resolution of queries, and the cost of maintaining this infrastructure may be prohibitive. Here we show that it is possible to implement distance-sensitive querying in an efficient way by exploiting the geometry of the network. Our querying service Glance ensures that a \u201cquery\u201d operation invoked within d distance of an event intercepts the event's \u201cadvertise\u201d operation within d*s distance, where s is a \u201cstretch-factor\u201d tunable by the user.\nTitle:", "model_inf_time": 1.53}, {"id": "41361", "output": "Hybrid PCNN, Fuzzy Sets, and Wavelet-Based Classification for Breast Cancer Detection in MRI Images", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe aim of the article is to present a novel method for fuzzy medical image retrieval (FMIR) using vector quantization (VQ) with fuzzy signatures in conjunction with fuzzy S-trees. In past times, a task of similar pictures searching was not based on searching for similar content (e.g. shapes, colour) of the pictures but on the picture name. There exist some methods for the same purpose, but there is still some space for development of more efficient methods. The proposed image retrieval system is used for finding similar images, in our case in the medical area --- in mammography, in addition to the creation of the list of similar images --- cases. The created list is used for assessing the nature of the finding --- whether the medical finding is malignant or benign. The suggested method is compared to the method using Normalized Compression Distance (NCD) instead of fuzzy signatures and fuzzy S-tree. The method with NCD is useful for the creation of the list of similar cases for malignancy assessment, but it is not able to capture the area of interest in the image. The proposed method is going to be added to the complex decision support system to help to determine appropriate healthcare according to the experiences of similar, previous cases.\nTitle:\nMedical Image Retrieval Using Vector Quantization and Fuzzy S-tree.\n\nAbstract:\nMethods based on fuzzy sets and fuzzy logic have proved to be efficient data classifiers and value estimators. This study presents an application of evolutionary evolved fuzzy rules based on the concept of extended Boolean queries to a multi-class data mining problem. Fuzzy rules are used as symbolic classifiers machine-learned from the data and used to label data samples and predict the value of an output variable. The output variable can be both a label (category) and a continuous value. This study presents an application of evolutionary fuzzy rules to the prediction of multi-class quality attributes in an industrial data set and compares the prediction obtained by fuzzy rules to the prediction achieved by support vector machines.\nTitle:\nMining multi-class industrial data with evolutionary fuzzy rules\n\nAbstract:\nThis paper proposes an automatic localization and boundary detection of retina images using basic filters to support ophthalmologists for detection and diagnoses eyes harmful diseases such as glaucoma and diabetic retinopathy accurately and diligently. The proposed system comprising three main phases including preprocessing, segmentation and detection phase. The preprocessing phase is used to enhance retinal image and to remove the noise of the retina image. The second phase is the segmentation for main parts of retinal image including optic disc, blood vessels, and fovea to extract their features. Optic disc is segmented using color intensity, the region of interest (ROT) is detected and morphological operations are applied to reduce search complexity. Also, fovea feature is extracted and the blood vessels tree is extracted from retinal image using line detection techniques. The third phase is the detection, in which identification and classifying whether the input image is left or right eye, to support ophthalmologists in identifying which eye is infected by the disease and to check it periodically. Basic image processing filters including average filter, median filter, spatial filter and morphological filter are used in all system phases. Moreover, a simple approach were used to detect left and right retinal fundus images. The proposed system is tested and evaluated using a subset of ophthalmologic images of the publically available DRIVE database.\nTitle:\nAutomatic Localization and Boundary Detection of Retina in Images Using Basic Image Processing Filters.\n\nAbstract:\nIn this paper, a region-based image fusion approach were proposed based on the stationary wavelet transform (SWT) in conjunction with marker-controlled watershed segmentation technique. The SWT is redundant, linear and shift invariant and these properties allow SWT to be realized exploiting a recursive algorithm and gives a better approximation than the DWT. The performance of the fusion approach is illustrated via experimental results obtained with a broad series of images and the experimental results used the MODIS multi-spectral bands and Spot panchromatic band to validate the proposed image fusion technique. Moreover, the visual presentation and different evaluation criteria including the standard deviation, the entropy information, the correlation coefficient, the root mean square error, the peak signal to noise ratio and the structural similarity index was used to evaluate the obtained results. The proposed approach achieves superior results compared with the existing work.\nTitle:\nRegion-based Image Fusion Approach of Panchromatic and Multi-spectral Images.\n\nAbstract:\nThis article presents a classification approach based on random forests algorithm for estimating and classifying the different maturity/ripeness stages of two types of crops; namely tomato and bell pepper (sweet pepper). The proposed approach consists of three phases that are pre-processing, feature extraction, and classification phases. Surface color of tomato and bell pepper is the most important characteristic to observe ripeness. So, the proposed classification system uses color features for classifying ripeness stages. It implements principal components analysis (PCA) along with support vector machine (SVM) algorithms and random forests (RF) classifier for features extraction and classification of ripeness stages, respectively. The datasets used for experiments were constructed based on real sample images for both tomatoes and bell pepper at different stages, which were collected from farms in Minya city, Upper Egypt. Datasets of total 250 and 175 images for tomato and bell pepper, respectively were used for both training and testing datasets. Training dataset is divided into five classes representing the different stages of tomato and bell pepper ripeness. Experimental results showed that SVM with Linear Kernel function achieved accuracy better than RF.\nTitle:\nRandom Forests Based Classification for Crops Ripeness Stages.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.042, mean value for all the writers is 8.648, which makes it 0.336 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis article introduces a hybrid scheme that combines the advantages of pulse coupled neural networks (PCNNs) and support vector machine, in conjunction with type-II fuzzy sets and wavelet to enhance the contrast of the original images and feature extraction. An application of MRI breast cancer imaging has been chosen and hybridization scheme have been applied to see their ability and accuracy to classify the breast cancer images into two outcomes: cancer or non-cancer. In order to enhance the contrast of the input image, identify the region of interest and detect the boundary of the breast pattern, a type-II fuzzy-based enhancement and PCNN-based segmentation were applied. Finally, wavelet-based features are extracted and normalized and a support vector machine classifier were employed to evaluate the ability of the lesion descriptors for discrimination of different regions of interest to determine whether they represent cancer or not. To evaluate the performance of presented approach, we present tests on different breast MRI images.\nTitle:", "model_inf_time": 1.97}, {"id": "41362", "output": "Classification and Computation of Pareto Optimal Robot Coordination", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nGiven a collection of robots sharing a common environment, assume that each possesses a graph (a one-dimensional complex also known as a roadmap) approximating its configuration space and, furthermore, that each robot wishes to travel to a goal while optimizing elapsed time. We consider vector-valued (or Pareto) optima for collision-free coordination on the product of these roadmaps with collision-type obstacles. Such optima are by no means unique: in fact, continua of Pareto optimal coordinations are possible. We prove a finite bound on the number of optimal coordinations in the physically relevant case where all obstacles are cylindrical (i.e., defined by pairwise collisions). The proofs rely crucially on perspectives from geometric group theory and CAT(0) geometry. In particular, the finiteness bound depends on the fact that the associated coordination space is devoid of positive curvature. We also demonstrate that the finiteness bound holds for systems with moving obstacles following known trajectories.\nTitle:\nNonpositive Curvature and Pareto Optimal Coordination of Robots\n\nAbstract:\nGiven a collection of robots sharing a common environment, assume that each possesses an individual roadmap for its C-space and a cost function for attaining a goal. Vector-valued (or PARETO) optima for collision-free coordination are by no means unique: in fact, continua of optimal coordinations are possible. However, for CYLINDRICAL obstacles (those defined by pairwise interactions), we prove a finite bound on the number of optimal coordinations. For such systems, we present an exact algorithm for reducing a coordination scheme to its Pareto optimal representative.\nTitle:\nPareto Optimal Coordination on Roadmaps.\n\nAbstract:\nWe present a global vector field computation algorithm in configuration spaces for smooth feedback motion planning. Our algorithm performs approximate cell decomposition in the configuration space and approximates the free space using rectanguloid cells. We compute a smooth local vector field for each cell in the free space and address the issue of the smooth composition of the local vector fields between the non-uniform adjacent cells. We show that the integral curve over the computed vector field is guaranteed to converge to the goal configuration, be collision-free, and maintain Cinfin smoothness. As compared to prior approaches, our algorithm works well on non-convex robots and obstacles.We demonstrate its performance on planar robots with 2 or 3 DOFs, articulated robots composed of 3 serial links and multi-robot systems with 6 DOFs.\nTitle:\nGlobal vector field computation for feedback motion planning\n\nAbstract:\nThis paper examines the problem of determining the distribution of a number of indistinguishable moving bodies located in regions separated by sensor beams that can detect whether a body moves across them. We characterize the conditions under which an exact distribution of bodies can be determined, and compute bounds on the expected number of sensor observations required to determine this exact distribution for a certain movement model of the bodies.\nTitle:\nCounting Moving Bodies Using Sparse Sensor Beams.\n\nAbstract:\nWe present a framework based on graph search for navigation in the plane with a variety of topological constraints. The method is based on modifying a standard graph-based navigation approach to keep an additional state variable that encodes topological information about the path. The topological information is represented by a sequence of virtual sensor beam crossings. By considering classes of beam crossing sequences to be equivalent under certain equivalence relations, we obtain a general method for planning with topological constraints that subsumes existing approaches while admitting more favorable representational characteristics. We provide experimental results that validate the approach and show how the planner can be used to find loop paths for autonomous surveillance problems, simultaneously satisfying minimum-cost objectives and in dynamic environments. As an additional application, we demonstrate the use of our planner on the PR2 robot for automated building of 3D object models.\nTitle:\nPlanning under topological constraints using beam-graphs.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.148, mean value for all the writers is 8.648, which makes it 0.427 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe consider the coordination of multiple robots in a common environment, each robot having its own (distinct) roadmap. Our primary contribution is a classification of and exact algorithm for computing vector-valued (or Pareto) optima for collision-free coordination. We indicate the utility of new geometric techniques from CAT(0) geometry and give an argument that curvature bounds are the key distinguishing feature between systems for which the classification is finite and for those in which it is not.\nTitle:", "model_inf_time": 1.16}, {"id": "41363", "output": "Print-and-Fold Hexapod: A Miniature Origami-Inspired Robot", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper describes a truss climbing robot we designed and prototyped. The robot has a minimalist design with three motive degrees of freedom that enable movement along three-dimensional truss structures. This robot can form a six-degree-of-freedom structure by connecting to another identical module using a passive bar as a medium. We present the design and implementation of this robot, control algorithms for moving the robot in a 3D truss structure, and hardware experiments\nTitle:\nShady3D: A Robot that Climbs 3D Trusses\n\nAbstract:\nThis paper considers a robot in the form of a self-folding sheet that is capable of origami-style autonomous folding. The sheet is composed of triangular tiles, folding actuators and an integrated electronic substrate, and is formed as an n\u00d7m box-pleated crease pattern. The design of the sheet is generated by an automated sheet design algorithm. We control the sheet with a programming method including a hardware model and supporting algorithms. In this paper we present the programming method. We describe and analyze the algorithms that generate designs and programs for the sheet. We finally demonstrate and analyze experiments with 4\u00d74 and 8\u00d78 self-folding sheet devices.\nTitle:\nDesigning and programming self-folding sheets.\n\nAbstract:\nThis work presents a technique which allows the application of 2-D fabrication methods to build 3-D robotic systems. The ability to print robots introduces a fast and low-cost fabrication method to modern, real-world robotic applications. To this end, we employ laser-engraved origami patterns to build a new class of robotic systems for mobility and manipulation. Origami is suitable for printable robotics as it uses only a flat sheet as the base structure for building complicated functional shapes, which can be utilized as robot bodies. An arbitrarily complex folding pattern can be used to yield an array of functionalities, in the form of actuated hinges or active spring elements. For actuation, we use compact NiTi coil actuators placed on the body to move parts of the structure on-demand. We demonstrate, as a proof-of-concept case study, the end-to-end fabrication and assembly of a simple mobile robot that can undergo worm-like peristaltic locomotion.\nTitle:\nTowards printable robotics: Origami-inspired planar fabrication of three-dimensional mechanisms\n\nAbstract:\nThis paper describes a robot in the form of a self-folding sheet that is capable of origami-style autonomous folding. We describe the hardware device we designed and fabricated. The device is a sheet with a box-pleated pattern and an integrated electronic substrate and actuators. The sheet is programmed and controlled to achieve different shapes using an idea called sticker programming. We describe the sticker controller and its instantiation. We also describe the algorithms for programming and controlling a given sheet to self-fold into a desired shape. Finally we present experiments with a 4 x 4 hardware device and an 8 x 8 hardware device.\nTitle:\nProgramming And Controlling Self-Folding Robots\n\nAbstract:\nPrinting and folding are fast and inexpensive methods for prototyping complex machines. Self-assembly of the folding step would expand the possibilities of this method to include applications where external manipulation is costly, such as micro-assembly, mass production, and space applications. This paper presents a method for self-folding of printed robots from two-dimensional materials based on shape memory polymers actuated by joule heating using embedded circuits. This method was shown to be capable of sequential folding, angle-controlled folds, slot-and-tab assembly, and mountain and valley folds. An inchworm robot was designed to demonstrate the merits of this technique. Upon the application of sufficient current, the robot was able to fold into its functional form with fold angle deviations within six degrees. This printed robot demonstrated locomotion at a speed of two millimeters per second.\nTitle:\nRobot self-assembly by folding: A printed inchworm robot\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.516, mean value for all the writers is 8.648, which makes it 0.113 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents the design, fabrication and operation of a hexapod fabricated using a combination of printing and folding flat sheets of polyester. The polyester sheets are cut and engraved with crease patterns, which are then manually folded to create 3D functional modules, inspired by the Japanese art of Origami. These modules, when connected, form a hexapod with two degrees of freedom per leg. All custom mechanical parts are manufactured in a planar fashion using a laser cutter. We created this print-and-fold hexapod as a miniature version of a commercially available platform, to which we compare several metrics, such as weight, walking speed, and cost of transportation. Our print-and-fold hexapod has a mass of 195 g, can walk at speeds of up to 38.1 cm/sec (two body lengths per second), and can be manufactured and assembled from scratch by a single person in approximately seven hours. Experimental results of gait control and trajectory tracking are provided.\nTitle:", "model_inf_time": 1.66}, {"id": "41364", "output": "Robust Distributed MIMO Cognitive Radio Networks Against Channel Uncertainties", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn cognitive radio (CR) systems, the primary users (PU) are protected by temperature interference constraints imposed on secondary users (SU). However, such limitations may be easily violated by SUs if perfect SU-to-PU channel state information (CSI) is not available at the secondary transmitters. In this paper, we propose a novel and distributed design of MIMO CR networks that is robust against imperfect SU-to-PU CSI. Specifically, we formulate the system design as a noncooperative game and robust global interference constraints are enforced via pricing; the prices are thus additional variables to be optimized. Building on the advanced and new theory of finite-dimensional variational inequalities (VI) in the complex domain, we analyze the proposed NE problem and devise alternative distributed algorithms along with their convergence properties.\nTitle:\nRobust MIMO cognitive radio systems under temperature interference constraints\n\nAbstract:\nCognitive radio (CR) systems improve the spectral efficiency by allowing the coexistence in harmony of primary users (PUs), the legacy users, with secondary users (SUs). This coexistence is built on the premises that no SU can generate interference higher than some prescribed limits against PUs. The system design based on perfect channel state information (CSI) can easily end up violating the interference limits in a realistic situation where CSI may be imperfect. In this paper, we propose a robust design of CR systems, composed of multiple PUs and multiple noncooperative SUs, in either single-input single-output (SISO) frequency-selective channels or more general multiple-input multiple-output (MIMO) channels. We formulate the design of the SU network as a noncooperative game, where the SUs compete with each other over the resources made available by the PUs, by maximizing their own information rates subject to the transmit power and robust interference constraints. Following the philosophy of the worst-case robustness, we take explicitly into account the imperfectness of SU-to-PU CSI by adopting proper interference constraints that are robust with respect to the worst channel errors. Relying on the variational inequality theory, we study the existence and uniqueness properties of the Nash equilibria of the resulting robust games, and devise totally asynchronous and distributed algorithms along with their convergency properties. We also propose efficient numerical methods, based on decomposition techniques, to compute the robust transmit strategy for each SU.\nTitle:\nRobust MIMO Cognitive Radio Via Game Theory\n\nAbstract:\nUsing imperfect channel state information (CSI) may cause severe violations of the interference restriction in cognitive radio (CR). We consider designing a robust CR system, over either SISO frequency-selective or MIMO channels, with multiple primary users (PUs) and multiple noncooperative secondary users (SUs), who form an ad-hoc network that is naturally modeled as a noncooperative game. The imperfectness of PU CSI is taken into account through the worst-case robustness philosophy. We study the existence and uniqueness properties of the Nash equilibria (NE) of the robust games, and devise distributed algorithms with their convergency properties to achieve the competitive optimality for the SU network. As special cases, our framework also provides, through convex optimization, the robust power allocation and precoding for each SU.\nTitle:\nRobust cognitive radio via game theory\n\nAbstract:\nIn this paper, we propose a novel class of Nash problems for cognitive radio (CR) networks, modeled as Gaussian frequency-selective interference channels, wherein each secondary user (SU) competes against the others to maximize his own opportunistic throughput by choosing jointly the sensing duration, the detection thresholds, and the vector power allocation. The proposed general formulation allows us to accommodate several (transmit) power and (deterministic/probabilistic) interference constraints, such as constraints on the maximum individual and/or aggregate (probabilistic) interference tolerable at the primary receivers. To keep the optimization as decentralized as possible, global (coupling) interference constraints are imposed by penalizing each SU with a set of time-varying prices based upon his contribution to the total interference; the prices are thus additional variable to optimize. The resulting players' optimization problems are nonconvex; moreover, there are possibly price clearing conditions associated with the global constraints to be satisfied by the solution. All this makes the analysis of the proposed games a challenging task; none of classical results in the game theory literature can be successfully applied. The main contribution of this paper is to develop a novel optimization-based theory for studying the proposed nonconvex games; we provide a comprehensive analysis of the existence and uniqueness of a standard Nash equilibrium, devise alternative best-response based algorithms, and establish their convergence. Some of the proposed algorithms are totally distributed and asynchronous, whereas some others require limited signaling among the SUs (in the form of consensus algorithms) in favor of better performance; overall, they are thus applicable to a variety of CR scenarios, either cooperative or noncooperative, which allows the SUs to explore the existing tradeoff between signaling and performance.\nTitle:\nJoint Sensing and Power Allocation in Nonconvex Cognitive Radio Games: Nash Equilibria and Distributed Algorithms\n\nAbstract:\nThe concept of cognitive radio (CR) has recently received great attention from the researchers' community as a promising paradigm to achieve efficient use of the frequency resource by allowing the coexistence of licensed (primary) and unlicensed (secondary) users in the same bandwidth. In this paper, we propose a distributed approach based on game theory to design cognitive MIMO transceiver in hierarchical CR networks, where primary users establish null and/or soft shaping constraints on the transmit covariance matrix of secondary users, so that the interference generated by secondary users be confined within the interference-temperature limits. We formulate the resource allocation problem among secondary users as a strategic noncooperative game, where each transmit/receive pair competes against the others to maximize the information rate over his own MIMO channel, under transmit power and/or null/soft shaping constraints. We provide a unified set of conditions that guarantee the uniqueness and global asymptotic stability of the Nash equilibrium of all the proposed games through totally distributed and asynchronous algorithms. Interestingly, the proposed algorithms overcome the main drawback of classical waterfilling based algorithms--the violation of the interference-temperature limits--and they have many of the desired features required for cognitive radio applications, such as low-complexity, distributed nature, robustness against missing or outdated updates of the users, and fast convergence behavior.\nTitle:\nCompetitive optimization of cognitive radio MIMO systems via game theory.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.48, mean value for all the writers is 8.648, which makes it 0.71 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCognitive Radio (CR) systems are built on the coexistence of primary users (PUs) and secondary users (SUs), the latter being allowed to share spectral resources with the PUs but under strict interference limitations. However, such limitations may easily be violated by SUs if perfect SU-to-PU channel state information (CSI) is not available at the secondary transmitters, which always happens in practice. In this paper, we propose a distributed design of MIMO CR networks under global interference temperature constraints that is robust (in the worst-case sense) against SU-to-PU channel uncertainties. More specifically, we consider two alternative formulations that are complementary to each other in terms of signaling and system performance, namely: a game-theoretical design and a social-oriented optimization. To study and solve the proposed formulations we hinge on the new theory of finite-dimensional variational inequalities (VI) in the complex domain and a novel parallel decomposition technique for nonconvex sum-utility problems with coupling constraints, respectively. A major contribution of this paper is to devise a new class of distributed best-response algorithms with provable convergence. The algorithms differ in computational complexity, convergence speed, communication overhead, and achievable performance; they are thus applicable to a variety of CR scenarios, either cooperative or non-cooperative, which allow the SUs to explore the trade-off between signaling and performance.\nTitle:", "model_inf_time": 1.57}, {"id": "41365", "output": "Zombie Chasing: Efficient Flash Management via Zombie Data Awareness", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSolid state disks (SSDs) have many advantages over hard disk drives, including better reliability, performance, durability, and power efficiency. However, the characteristics of SSDs are completely different from those of hard disk drives with rotating disks. To achieve the full potential performance improvement with SSDs, operating systems or applications must understand the critical performance parameters of SSDs to fine-tune their accesses. However, the internal hardware and software organizations vary significantly among SSDs and, thus, each SSD exhibits different parameters which influence the overall performance. In this paper, we propose a methodology which can extract several essential parameters affecting the performance of SSDs, and apply the extracted parameters to SSD systems for performance improvement. The target parameters of SSDs considered in this paper are 1) the size of read/write unit, 2) the size of erase unit, 3) the size of read buffer, and 4) the size of write buffer. We modify two operating system components to optimize their operations with the SSD parameters. The experimental results show that such parameter-aware management leads to significant performance improvements for large file accesses by performing SSD-specific optimizations.\nTitle:\nParameter-Aware I/O Management for Solid State Disks (SSDs)\n\nAbstract:\nRecently, NAND flash-based Solid State Drives (SSDs) have been rapidly adopted in laptops, desktops, and server storage systems because their performance is superior to that of traditional magnetic disks. However, NAND flash memory has some limitations such as out-of-place updates, bulk erase operations, and a limited number of write operations. To alleviate these unfavorable characteristics, various techniques for improving internal software and hardware components have been devised. In particular, the internal device cache of SSDs has a significant impact on the performance. The device cache is used for two main purposes: to absorb frequent read/write requests and to store logical-to-physical address mapping information. In the device cache, we observed that the optimal ratio of the data buffering and the address mapping space changes according to workload characteristics. To achieve optimal performance in SSDs, the device cache should be appropriately partitioned between the two main purposes. In this paper, we propose an adaptive partitioning scheme, which is based on a ghost caching mechanism, to adaptively tune the ratio of the buffering and the mapping space in the device cache according to the workload characteristics. The simulation results demonstrate that the performance of the proposed scheme approximates the best performance.\nTitle:\nAn adaptive partitioning scheme for DRAM-based cache in Solid State Drives\n\nAbstract:\nSeparating hot data from cold data is known to allow for efficient management of NAND flash memory in Solid State Drives (SSDs). However, most of previous work has been evaluated with the trace-driven simulations under different workloads and testing conditions. The goal of this paper is to empirically study the performance, computation overhead, and memory consumption of the existing hot/cold data separation policies on a real SSD platform. After devising a general framework where a different policy can be easily plugged in, we have evaluated three hot/cold data separation policies: 2-level LRU (LRU), Multiple Bloom Filter (MBF), and Dynamic dAta Clustering (DAC). Our evaluation results show that DAC performs best, improving the performance by up to 58% in real workloads with a reasonable computation and memory overhead.\nTitle:\nAn empirical study of hot/cold data separation policies in solid state drives (SSDs)\n\nAbstract:\nRecently, NAND flash-based solid state drives (SSDs) have emerged as revolutionary storage media. Numerous studies have been carried out to employ SSDs in database systems and storage systems, motivated by SSD's attractive features such as decreased drive weight, increased shock resistance, low power consumption, and no seek latency. However, low-end SSDs targeting desktop and notebook environments show problematic random write performance which is only comparable to or lower than that of HDDs This paper proposes a novel software layer called ReSSD whose purpose is to improve the small random write performance of low-end SSDs with low memory usage. ReSSD works as a virtual block device on top of SSD which requires no modification neither in the operating systems nor in the applications. By inspecting all incoming requests, ReSSD identifies small random writes which have potential to degrade SSD's performance significantly and transforms them into sequential and ordered-sequential writes which are more favorable to SSDs. Our evaluation results through Postmark and OLTP benchmarks show that the proposed approach accomplishes noticeable performance improvement on low-end SSDs under all workloads.\nTitle:\nReSSD: A Software Layer for Improving the Small Random Write Performance of SSDs.\n\nAbstract:\nNAND flash memory becomes one of the most popular storage for portable embedded systems. Although many flash-aware file systems, such as JFFS2 and YAFFS2, were proposed, the large memory consumption and the longmount delay have been serious obstacles for large-capacity NAND flash memory. In this paper, we present a new flash-aware file system called DFFS (Direct Flash File System) which fetches only the needed metadata on demand from flash memory. In addition, DFFS employs two novel metadata management schemes, inode embedding scheme and hybrid inode indexing scheme, to improve the performance of metadata operations. Comprehensive evaluation results using microbenchmark, postmark, and Linux kernel compilation trace, show that DFFS has comparable performance to JFFS2 and YAFFS2, while achieving a small memory footprint and instant mount time.\nTitle:\nEfficient Metadata Management for Flash File Systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.895, mean value for all the writers is 8.648, which makes it 1.064 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents a novel technique, called Zombie Chasing, for efficient flash management in solid state drives (SSDs). Due to the unique characteristics of NAND flash memory, SSDs need to accurately understand the liveness of the data stored in themselves. Recently, the TRIM command has been introduced to notify SSDs of dead data caused by file deletions, which otherwise could not be tracked by SSDs. This paper goes one step further and proposes a new liveness state, called the zombie state, to denote live data that will be dead shortly due to the corresponding dirty data in the buffer cache. We also devise new zombie-aware garbage collection algorithms which utilize the information about such zombie data inside SSDs. To evaluate Zombie Chasing, we implement zombie-aware garbage collection algorithms in the prototype SSD and modify the Linux kernel and the Oracle DBMS to deliver the information on the zombie data to the prototype SSD. Through comprehensive evaluations using our in-house micro-benchmark and the TPC-C benchmark, we observe that Zombie Chasing improves SSD performance effectively by reducing garbage collection overhead. Especially, our evaluation with the TPC-C benchmark on the Oracle DBMS shows that Zombie Chasing enhances the Transactions Per Second (TPS) value by up to 22% with negligible overhead.\nTitle:", "model_inf_time": 1.57}, {"id": "41366", "output": "Optimizing Xen I/O for Hadoop Performance", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nConsolidated environments are progressively accommodating diverse and unpredictable workloads in conjunction with virtual desktop infrastructure and cloud computing. Unpredictable workloads, however, aggravate the semantic gap between the virtual machine monitor and guest operating systems, leading to inefficient resource management. In particular, CPU management for virtual machines has a critical impact on I/O performance in cases where the virtual machine monitor is agnostic about the internal workloads of each virtual machine. This paper presents virtual machine scheduling techniques for transparently bridging the semantic gap that is a result of consolidated workloads. To enable us to achieve this goal, we ensure that the virtual machine monitor is aware of task-level I/O-boundedness inside a virtual machine using inference techniques, thereby improving I/O performance without compromising CPU fairness. In addition, we address performance anomalies arising from the indirect use of I/O devices via a driver virtual machine at the scheduling level. The proposed techniques are implemented on the Xen virtual machine monitor and evaluated with micro-benchmarks and real workloads on Linux and Windows guest operating systems.\nTitle:\nTransparently bridging semantic gap in CPU management for virtualized environments\n\nAbstract:\nThe use of virtualization is progressively accommodating diverse and unpredictable workloads as being adopted in virtual desktop and cloud computing environments. Since a virtual machine monitor lacks knowledge of each virtual machine, the unpredictableness of workloads makes resource allocation difficult. Particularly, virtual machine scheduling has a critical impact on I/O performance in cases where the virtual machine monitor is agnostic about the internal workloads of virtual machines. This paper presents a task-aware virtual machine scheduling mechanism based on inference techniques using gray-box knowledge. The proposed mechanism infers the I/O-boundness of guest-level tasks and correlates incoming events with I/O-bound tasks. With this information, we introduce partial boosting, which is a priority boosting mechanism with task-level granularity, so that an I/O-bound task is selectively scheduled to handle its incoming events promptly. Our technique focuses on improving the performance of I/O-bound tasks within heterogeneous workloads by lightweight mechanisms with complete CPU fairness among virtual machines. All implementation is confined to the virtualization layer based on the Xen virtual machine monitor and the credit scheduler. We evaluate our prototype in terms of I/O performance and CPU fairness over synthetic mixed workloads and realistic applications.\nTitle:\nTask-aware virtual machine scheduling for I/O performance.\n\nAbstract:\nThe use of virtualization is rapidly expanding from server consolidation to various computing systems including PC, multimedia set-top box and gaming console. However, different from the server environment, timeliness response for the external input is an essential property for the user-interactive applications. To provide timeliness scheduling of virtual machine this paper presents a priority-based scheduling scheme for virtual machine monitors. The suggested scheduling scheme selects the next task to be scheduled based on the task priorities and the I/O usage stats of the virtual machines. The suggested algorithm was implemented and evaluated on Xen virtual machine monitor. The results showed that the average response time to I/O events is improved by 5~22% for highly consolidated environment.\nTitle:\nGuest-Aware Priority-Based Virtual Machine Scheduling for Highly Consolidated Server\n\nAbstract:\nSince a virtual machine independently uses its own caching policy, redundant disk operations exacerbate the I/O virtualization overhead when virtual machines access large amounts of data on shared storage. This paper presents XHive, an efficient cooperative caching system that is implemented at the virtualization layer, for consolidated environments. Our proposed scheme globally manages buffer caches of consolidated virtual machines in order to accommodate a shared working set in machine memory. A singlet, which is a block cached solely by a virtual machine, is preferentially given more chances to be cached in machine memory by XHive, when it is evicted by a guest operating system. For efficient use of limited memory, singlets are cached in memory that is collaboratively donated from idle memory of virtual machines. Our evaluation shows that XHive significantly reduces disk I/O operations for shared working sets, thereby achieving high read performance and scalability. Improved scalability enables a high degree of workload consolidation with respect to virtual machines that have shared working sets.\nTitle:\nXHive: Efficient Cooperative Caching for Virtual Machines\n\nAbstract:\nThe memory size limits the scalability of virtual machine systems. There have been some researches about sharing identical pages among guest systems to reduce memory usage. However, they require memory overcommitment feature through swap mechanism which some virtual machines including Xen do not have. In this paper a new approach is proposed to share identical pages with designated sharing area. This approach reduces the memory usage as well as redundant I/O operations. Moreover, understanding the characteristics of certain shared pages becomes easier. The conceptional design was evaluated by simulation based on real-world applications.\nTitle:\nDomain level page sharing in Xen virtual machine systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.029, mean value for all the writers is 8.648, which makes it 0.325 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nVirtualization offers many advantages by providing server consolidation, isolation, and live migration. In cloud computing environments, virtual machine monitor is an essential part for efficient management of physical resources. Recently, processing big data is important to both most cloud service providers and users. In this paper, the performance of Xen based Hadoop cluster is analyzed, especially related to block device I/O. In addition, the overhead transferring I/O requests are introduced with Xen I/O ring mechanism, and the experimental result of our optimization for Hadoop workloads is presented. The proposed approach reduces CPU utilization by a third during I/O operations and improve throughput of Hadoop applications.\nTitle:", "model_inf_time": 1.34}, {"id": "41367", "output": "Subset Sum with Limited Additive Structure", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe SUBSET SUM problem asks whether a given set of n positive integers contains a subset of elements that sum up to a given target t. It is an outstanding open question whether the O*(2(n)(/2))-time algorithm for SUBSET SUM by Horowitz and Sahni [J. ACM 1974] can be beaten in the worst-case setting by a \"truly faster\", O*(2((0.5-delta)n))-time algorithm, with some constant delta > 0. Continuing an earlier work [STACS 2015], we study SUBSET SUM parameterized by the maximum bin size beta, defined as the largest number of subsets of the n input integers that yield the same sum. For every epsilon > 0 we give a truly faster algorithm for instances with beta <= 2((0.5- epsilon)n) as well as instances with beta >= 2(0)(.661n) Consequently, we also obtain a characterization in terms of the popular density parameter n/log(2)t: if all instances of density at least 1.003 admit a truly faster algorithm, then so does every instance. This goes against the current intuition that instances of density 1 are the hardest, and therefore is a step toward answering the open question in the affirmative. Our results stem from a novel combinatorial analysis of mixings of earlier algorithms for SUBSET SUM and a study of an extremal question in additive combinatorics connected to the problem of Uniquely Decodable Code Pairs in information theory.\nTitle:\nDense Subset Sum May Be the Hardest.\n\nAbstract:\nWe study the problem of computing an ensemble of multiple sums where the summands in each sum are indexed by subsets of size p of an n-element ground set. More precisely, the task is to compute, for each subset of size q of the ground set, the sum over the values of all subsets of size p that are disjoint from the subset of size q. We present an arithmetic circuit that, without subtraction, solves the problem using O((np+nq)logn) arithmetic gates, all monotone; for constant p, q this is within the factor logn of the optimal. The circuit design is based on viewing the summation as a \"set nucleation\" task and using a tree-projection approach to implement the nucleation. Applications include improved algorithms for counting heaviest k-paths in a weighted graph, computing permanents of rectangular matrices, and dynamic feature selection in machine learning.\nTitle:\nFast monotone summation over disjoint sets\n\nAbstract:\n\u2022We present faster algorithms for classic problems on paths and packings.\u2022The algorithms are randomized, fixed-parameter tractable, and take polynomial space.\u2022We also present a faster algorithm for the edge-coloring problem in regular graphs.\u2022We generalize a recently studied algebraic approach.\nTitle:\nNarrow sieves for parameterized paths and packings\n\nAbstract:\nWe study classes of Dynamic Programming (DP) algorithms which, due to their algebraic definitions, are closely related to coefficient extraction methods. DP algorithms can easily be modified to exploit sparseness in the DP table through memorization. Coefficient extraction techniques on the other hand are both space-efficient and parallelisable, but no tools have been available to exploit sparseness. We investigate the systematic use of homomorphic hash functions to combine the best of these methods and obtain improved space-efficient algorithms for problems including LINEAR SAT, SET PARTITION and SUBSET SUM. Our algorithms run in time proportional to the number of nonzero entries of the last segment of the DP table, which presents a strict improvement over sparse DP. The last property also gives an improved algorithm for CNF SAT and SET COVER with sparse projections.\nTitle:\nHomomorphic hashing for sparse coefficient extraction\n\nAbstract:\nIn the Steiner tree problem, we are given as input a connected n-vertex graph with edge weights in {1, 2, ..., W}, and a subset of k terminal vertices. Our task is to compute a minimum-weight tree that contains all the terminals. We give an algorithm for this problem with running time O(7.97(k) . n(4) . log W) using O(n(3) . log nW . log k) space. This is the first single-exponential time, polynomial-space FPT algorithm for the weighted Steiner Tree problem.\nTitle:\nParameterized Single-Exponential Time Polynomial Space Algorithm for Steiner Tree\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.136, mean value for all the writers is 8.648, which makes it 0.437 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe study the exact time complexity of the Subset Sum problem. Our focus is on instances that lack additive structure in the sense that the sums one can form from the subsets of the given integers are not strongly concentrated on any particular integer value. We present a randomized algorithm that runs in O(2(0.3399n) B-4) time on instances with the property that no value can arise as a sum of more than B different subsets of the n given integers.\nTitle:", "model_inf_time": 1.17}, {"id": "41368", "output": "Elliptic Curve Cryptosystem Coprocessor Design Using FPGAs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA compact fast elliptic curve scalar multiplier with variable key size is implemented as a coprocessor with a Xilinx FPGA. This implementation utilizes the internal SRAM/registers of the FPGA and has the whole scalar multiplier implemented within a single FPGA chip. The compact design helps reduce the overhead and limitations associated with data transfer between FPGA and host, and thus leads to high performance. The experimental data from the mappings over small fields shows that the carefully constructed hardware architecture is regular and has high CLB utilization.\nTitle:\nElliptic Curve Scalar Multiplier Design Using FPGAs\n\nAbstract:\nThis paper presents timing and area results for an FPGA implementation of a CDMA-based switch for networks-on-chip. The design was mapped onto the Xilinx Virtex4 XC4VLX200 device using Synplify Pro for a range of pay-load sizes. The synthesis results give the area and maximum frequency obtained. Simulation verifies the desired functionality and provides throughput and latency values as functions of payload size.\nTitle:\nFPGA-based CDMA switch for networks-on-chip\n\nAbstract:\nAn enhancement to the MB-OFDM system, known as pulsed-OFDM, has been proposed to reduce the complexity and power consumption of the transceiver without sacrificing performance. This paper describes the detailed FPGA implementation of a complete pulsed-OFDM transceiver. The resource requirements are given for each of the major blocks for an implementation using a Xilinx Virtextrade-4 device. The entire system can be mapped onto a single FPGA chip\nTitle:\nFPGA-Based Design of a Pulsed-OFDM System\n\nAbstract:\nThis paper presents a novel application-specific field-programmable gate array (FPGA) architecture that satisfies efficient implementation of digit-serial DSP architectures on a digit wide basis. Digit-serial DSP designs have been an effective implementation method for FPGAs. To efficiently realize a digit-serial DSP design on FPGAs, one must create an FPGA architecture optimized for those types of systems. We examine the various circuits used in digit-serial DSP designs to extract their key features that should be reflected in the new FPGA architecture. We explain the design methodology, layout and implementation of the new digit-serial FPGA architecture. Digit-serial DSP designs using the digit-serial FPGA (DS-FPGA) are compared to those implemented on Xilinx FPGAs. We have estimated that the DS-FPGA are about 2.5similar to3 times more efficient in area and faster than the equivalent digit-serial DSP architectures implemented using Xilinx FPGAs.\nTitle:\nVLSI Design Of Digit-Serial FPGA Architecture\n\nAbstract:\nWe present a MIMO joint transceiver design that can run at 350 MHz on a Xilinx Virtex-4 xc4vlx200ff1513-12 FPGA. The implementation is an 8 x 8 MIMO transceiver with a 16-QAM symbol constellation. This system can provide data throughput of 11.2 Gbps. The design is based on a modified Geometric Mean Decomposition (GMD) for a flat fading MIMO channel using VBLAST MIMO detection. The design flow uses Matlab Simulink as the model builder followed by the Xilinx System Generator to transform the Simulink model into a VHDL description which can be synthesized and mapped onto the FPGA device. Speed and area results are given for the synthesized designs.\nTitle:\nMimo Transceiver Design Based, On A Modified Geometric Mean Decomposition\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.278, mean value for all the writers is 8.648, which makes it 0.316 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA compact fast elliptic curve cryptosystem coprocessor with variable key size is implemented with a Xilinx FPGA. This implementation utilizes the internal SRAM/registers of the FPGA and has the whole system implemented within a single FPGA chip. The compact design helps reduce the overhead and limitations associated with data transfer between FPGA and host, and thus leads to high performance. The experimental data shows that the carefully constructed hardware architecture is regular and has high CLB utilization.\nTitle:", "model_inf_time": 1.41}, {"id": "41369", "output": "Decentralized Task Execution for Heterogeneous Robotic Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose a decentralized model and control framework for the assignment and execution of tasks, i.e. the dynamic task planning, for a network of heterogeneous robots. The proposed modeling framework allows the design of missions, defined as sets of tasks, in order to achieve global objectives regardless of the actual characteristics of the robotic network. The concept of skills, defined by the mission designer and considered as constraints for the mission execution, is exploited to distribute tasks across the robotic network. In addition, we develop a decentralized control algorithm, based on the concept of skills for decoupling the mission design from its deployment, which combines task assignment and execution through a consensus-based approach. Finally, conditions upon which the proposed decentralized formulation is equivalent to a centralized one are discussed. Experimental results are provided to validate the effectiveness of the proposed framework in a real-world scenario.\nTitle:\nDecentralized dynamic task planning for heterogeneous robotic networks.\n\nAbstract:\nIn this paper a novel approach to the problem of decentralized agreement toward a common point in space in a multi-agent system is proposed. Our method allows the agents to agree on the relative location of the network centroid respect to themselves, on a common reference frame and therefore on a common heading. Using this information a global positioning system for the agents using only local measurements can be achieved. In the proposed scenario, an agent is able to sense the distance between itself and its neighbors and the direction in which it sees its neighbors with respect to its local reference frame. Furthermore only point-to-point asynchronous communications between neighboring agents are allowed thus achieving robustness against random communication failures. The proposed algorithms can be thought as general tools to locally retrieve global information usually not available to the agents.\nTitle:\nOn agreement problems with gossip algorithms in absence of common reference frames\n\nAbstract:\nIn this paper, a novel distributed algorithm to deal with the problem of estimating the network centroid in a multi-agent system is proposed. In this scenario, agents are assumed to be lacking any global reference frame or absolute position information. The proposed algorithm can be thought as a general tool to retrieve information about the centroid of a network of agents. Indeed, this allows to release several simplifying assumptions for a significant family of algorithms dealing with decentralized motion coordination. The convergence properties of the algorithm are carefully investigated in the case of a fully connected network for which a proof of convergence is provided. Successively, simulations to show the effectiveness of the algorithm also for arbitrary undirected connected graphs are given.\nTitle:\nDecentralized centroid estimation for multi-agent systems in absence of any common reference frame\n\nAbstract:\nIn this paper, a novel decentralized swarm aggregation algorithm for multi-robot systems with an integrated obstacle avoidance is proposed. In this framework, the interaction among robots is limited to their visibility neighborhood, i.e., robots that are within the visibility range of each other. Furthermore, to better comply with the hardware/software limitations of mobile robotic platforms, robots actuators are assumed to be saturated. A theoretical characterization of the main properties of the proposed swarm aggregation algorithm is provided. Simulations have been carried out to validate the theoretical results and experiments have been performed with a team of low-cost mobile robots to demonstrate the effectiveness of the proposed approach in real scenario.\nTitle:\nA swarm aggregation algorithm based on local interaction with actuator saturations and integrated obstacle avoidance\n\nAbstract:\nIn this work we focus on the topology control problem for robotic networks. In particular, we assume agents to be equipped with limited field of view sensors. As a consequence, directed graphs are required to model the robot-to-robot interaction. This significantly limits the applicability of algorithms developed for undirected graphs. In that view, we propose an auction-based solution for the decentralized estimation of an approximated minimum (in terms of number of links and in terms of a global cost function) strongly connected directed graph. This represents the first step towards the development of a connectivity maintenance framework for directed graphs. A theoretical analysis along with numerical simulations are provided to show the effectiveness of the proposed approach.\nTitle:\nDecentralized estimation of the minimum strongly connected subdigraph for robotic networks with limited field of view\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.02, mean value for all the writers is 8.648, which makes it 1.171 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper describes a decentralized approach to control the execution of a set of tasks for a network of heterogeneous robotic agents, i.e., agents with different sets of sensors and actuators. The proposed approach is based on a discrete-event modeling and control framework. The novelty is the full decoupling between the tasks modeling and their deployment. This implies an higher robustness to faults in the network of agents and a more systematic way for the task modeling regardless of the actual capability of the agents. A consensus-based mechanism is exploited in the control system to achieve coherence among the agents state and synchronize the agents action.\nTitle:", "model_inf_time": 1.26}, {"id": "41370", "output": "Motivating and Inhibiting Factors Influencing Consumer Intention to Contribute to Online Feedback Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe online feedback system (OFS) has been touted to be an effective artifact for electronic word-of-mouth (EWOM). Accumulating sufficient detailed consumption information in the OFS is essential to the success of OFS. Yet, past research has focused on the effects of OFS on building trust and promoting sales and little knowledge about information provision to OFS has been developed. This study attempts to fill this gap by developing and testing a theoretical model to identify the possible antecedents that lead to the intention of consumers' information contribution to OFS. The model employs social exchange theory to identify benefit and cost factors influencing consumer intention, and motivation crowding theory to explore the moderating effects from environmental interventions that are embodied in OFS. Our preliminary results in general provide empirical support for the model. Practical implications are offered to OFS designers for system customization.\nTitle:\nUnderstanding the Intention of Information Contribution to Online Feedback Systems from Social Exchange and Motivation Crowding Perspectives\n\nAbstract:\nAnecdotal evidence indicates that an online discussion forum may not be utilized to its full potential in enhancing the effectiveness and efficiency of teaching due to a lower than expected student participation rate. This paper seeks to identify the motivational behavioral factors influencing students' intention to participate in an online discussion forums (ODF). Drawing on the literature on social psychology and applying the theory of reasoned action, we develop a conceptual model of intention to participate in an online discussion forum and empirically test the hypotheses in a cross-sectional quantitative survey. The findings indicate that expectancy on hedonic outcome and utilitarian outcome and peer pressure positively influence the participation intention of students. Also, the perceived importance of learning positively moderates the relationship between utilitarian outcome expectancy and participation intention. Theoretical and practical implications of the findings are discussed.\nTitle:\nStudents' participation intention in an online discussion forum: Why is computer-mediated interaction attractive?\n\nAbstract:\nOpen Source Software (OSS) is generally developed by interested professionals who have decided to participate in the process. The presence of effective leaders who both steer the development and motivate the developers is crucial to ensure a successful product. Using path-goal theory and built on leadership and motivation theories, we proposed and tested a model that can be used to assess the relationship between an OSS project leader's leadership style and a developer's motivation to contribute to the software development. We specifically decomposed the leadership and motivation construct to understand the hidden mechanisms by which leadership impacts motivation. A set of survey data collected from 118 OSS developers on Sourceforge.net was used to test our hypotheses. Our results indicate that leaders' transformational leadership is positively related to developers' intrinsic motivation and that leaders' active management style is positively related to the developers' extrinsic motivation.\nTitle:\nLeadership characteristics and developers' motivation in open source software development\n\nAbstract:\nThis research explores how consumers use online decision aids with screening and evaluation support functionalities under varying product attribute-load conditions. Drawing on resource-matching theory, we conducted a 3 \u00d7 2 factorial experiment to test the interaction between decision aid features (i.e., low versus high-screening support, and aids with weight assignment and computation decision tools) and attribute load (i.e., large versus small number of product attributes) on decision performance. The findings reveal that: (1) where the decision aids render cognitive resources that match those demanded for the task environment, consumers will process more information and decision performance will be enhanced; (2) where the decision aids render cognitive resources that exceed those demanded for the task environment, consumers will engage in less task-related elaboration of decision-making issues to the detriment of decision performance; and (3) where the decision aids render cognitive resources that fall short of those demanded for the task environment, consumers will use simplistic heuristic decision strategies to the detriment of decision performance or invest additional effort in information processing to attain a better decision performance if they perceive the additional investments in effort to be manageable.\nTitle:\nAssessing Screening and Evaluation Decision Support Systems: A Resource-Matching Approach\n\nAbstract:\nProviding explanation to justify product recommendations is critical in the online purchase decision process. Bulk of the extant literature has focused on the provision of decision aids facilitating screening of product alternatives and presenting of filtered alternatives. In comparison, few studies are conducted to examine decision aids that support the assessment and evaluation of the presented product alternatives prior to actual purchase, i.e., explanation-featured decision aid. This article conceptualizes three implementations of explanation aid differed by the forms of explanation elaboration. Experimental results indicate that a more elaborated explanation aid could heighten a consumer's decision confidence leading to lesser cognitive effort expended and inferior product choice made.\nTitle:\nConsumer-based decision aid that explains which to buy: Decision confirmation or overconfidence bias?\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.686, mean value for all the writers is 8.648, which makes it 2.592 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOnline feedback systems (OFSs) are increasingly available on online shopping websites; they allow consumers to post their ratings and consumption reviews for products. We employed motivation theory and a goal attainment perspective to model a set of motivating and inhibiting factors that could influence a consumer's intention to contribute to an OFS. Our experiment, which involved 168 university students, showed that a consumer's intention to contribute product reviews is influenced by perceived satisfaction gained in helping other consumers, perceived satisfaction gained in influencing the merchant, perceived probability of enhancing self-image, and perceived executional costs. In addition, the presence of an economic rewarding mechanism was found to promote a contribution when a consumer's perceived probability of enhancing self-image was relatively high or when perceived cognitive cost was relatively low. Implications of our findings are discussed.\nTitle:", "model_inf_time": 1.68}, {"id": "41371", "output": "Parametric Representation of Cerebral Sulci from Magnetic Resonance Images", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe propose a methodology for extracting parametric representations of the cerebral sulci from magnetic resonance images, and we consider its application to two medical imaging problems: quantitative morphological analysis and spatial normalization and registration of brain images. Our methodology is based on deformable models utilizing characteristics of the cortical shape. Specifically, a parametric representation of a sulcus is determined by the motion of an active contour along the medial surface of the corresponding cortical fold. The active contour is initialized along the outer boundary of the brain, and deforms toward the deep edge of a sulcus under the influence of an external force field restricting it to lie along the medial surface of the particular cortical fold. A parametric representation of the surface is obtained as the active contour traverses the sulcus. In this paper we present results of this methodology and its applications.\nTitle:\nMapping the Cerebral Sulci: Application to Morphological Analysis of the Cortex and to Non-rigid Registration\n\nAbstract:\nA method for automated segmentation of major cortical sulci on the outer brain boundary is presented, with emphasis on automatically determining point correspondence and on labeling cortical regions. The method is formulated in a general optimization framework defined on the unit sphere, which serves as parametric domain for convoluted surfaces of spherical topology. A statistical shape model, which includes a network of deformable curves on the unit sphere, seeks geometric features such as high curvature regions and labels such features via a deformation process that is confined within a spherical map of the outer brain boundary. The limitations of the customary spherical coordinate system, which include discontinuities at the poles and nonuniform sampling, are overcome by defining the statistical prior of shape variation in terms of projections of landmark points onto corresponding tangent planes of the sphere. The method is tested against and shown to be as accurate as manually defined segmentations.\nTitle:\nUsing a statistical shape model to extract sulcal curves on the outer cortex of the human brain.\n\nAbstract:\nThe development of algorithms for the spatial transformation and registration of tomographic brain images is a key issue in several clinical and basic science medical applications, including computer-aided neurosurgery, functional image analysis, and morphometrics. This paper describes a technique for the spatial transformation of brain images, which is based on elastically deformable models. A deformable surface algorithm is used to find a parametric representation of the outer cortical surface and then to define a map between corresponding cortical regions in two brain images. Based on the resulting map, a three-dimensional elastic warping transformation is then determined, which brings two images into register. This transformation models images as inhomogeneous elastic objects which are deformed into registration with each other by external force fields. The elastic properties of the images can vary from one region to the other, allowing more variable brain regions, such as the ventricles, to deform more freely than less variable ones. Finally, the framework of prestrained elasticity is used to model structural irregularities, and in particular the ventricular expansion occurring with aging or diseases, and the growth of tumors. Performance measurements are obtained using magnetic resonance images.\nTitle:\nSpatial transformation and registration of brain images using elastically deformable models.\n\nAbstract:\nA method for building a statistical shape model of sulci of the human brain cortex is described. The model includes sulcal fundi that are defined on a spherical map of the cortex. The sulcal fundi are first extracted in a semi-automatic way using an extension of the fast marching method. They are then transformed to curves on the unit sphere via a conformal mapping method that maps each cortical point to a point on the unit sphere. The curves that represent sulcal fundi are parameterized with piecewise constant-speed parameterizations. Intermediate points on these curves correspond to sulcal landmarks, which are used to build a point distribution model on the unit sphere. Statistical information of local properties of the sulci, such as curvature and depth, are embedded in the model. Experimental results are presented to show how the models are built.\nTitle:\nStatistical Study on Cortical Sulci of Human Brains\n\nAbstract:\nParcellation of the cortex has received a great deal of attention in magnetic resonance (MR) image analysis, but its usefulness has been limited by time-consuming algorithms that require manual labeling. An automatic labeling scheme is necessary to accurately and consistently parcellate a large number of brains. The large variation of cortical folding patterns makes automatic labeling a challenging problem, which cannot be solved by deformable atlas registration alone. In this work, an automated classification scheme that consists of a mix of both atlas driven and data driven methods is proposed to label the sulcal regions, which are defined as the gray matter regions of the cortical surface surrounding each sulcus. The premise for this algorithm is that sulcal regions can be classified according to the pattern of anatomical features (e.g. supramarginal gyrus, cuneus, etc.) associated with each region. Using a nearest-neighbor approach, a sulcal region is classified as being in the same class as the sulcus from a set of training data which has the nearest pattern of anatomical features. Using just one subject as training data, the algorithm correctly labeled 83% of the regions that make up the main sulci of the cortex.\nTitle:\nAutomatic classification of sulcal regions of the human brain cortex using pattern recognition\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.611, mean value for all the writers is 8.648, which makes it 1.675 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe cortical sulci are brain structures resembling thin convoluted ribbons embedded in three dimensions. The importance of the sulci lies primarily in their relation to the cytoarchitectonic and functional organization of the underlying cortex and in their utilization as features in non-rigid registration methods. This paper presents a methodology for extracting parametric representations of the cerebral sulci from magnetic resonance images. The proposed methodology is based on deformable models utilizing characteristics of the cortical shape. Specifically, a parametric representation of a sulcus is determined by the motion of an active contour along the medial surface of the corresponding cortical fold. The active contour is initialized along the outer boundary of the brain and deforms toward the deep root of a sulcus under the influence of an external force field, restricting it to lie along the medial surface of the particular cortical fold. A parametric representation of the medial surface of the sulcus is obtained as the active contour traverses the sulcus. Based on the first fundamental form of this representation, the location and degree of an interruption of a sulcus can be readily quantified; based on its second fundamental form, shape properties of the sulcus can be determined. This methodology is tested on magnetic resonance images and it is applied to three medical imaging problems: quantitative morphological analysis of the central sulcus; mapping of functional activation along the primary motor cortex and non-rigid registration of brain images.\nTitle:", "model_inf_time": 1.54}, {"id": "41372", "output": "A Specialisation Calculus for Enhanced Communication in Intelligent Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe motivation of this work is the im- provement of the classical input/output expert sys- tems behaviour. In an uncertain reasoning context this behaviour consists of just getting certainty values for propositions. Instead, the answer of an expert sys- tem will be a set of formulas: a set of propositions and a set of specialised rules containing unknown propo- sitions in their left part. This type of behaviour is much more informative than the classical one because gives to users not only the answer to a query but all the relevant information to improve the solution. A family of propositional rule-based languages founded on multiple-valued logics is presented and formalised. The deductive system dened on top of it is based on a Specialisation Inference Rule (SIR): (A1^A2:::^An ! P; V ); (A1; V 0) ' (A2 ^ : : : ^ An ! P; V 00), where V , V 0 and V 00 are uncertainty intervals. This inference rule provides a way of obtaining rules containing un- known conditions in their premise as the result of the deductive process. The soundness and literal com- pleteness of the deductive system are proved. The implementation of this deductive calculus is based on techniques of partial evaluation. Moreover, the spe- cialisation mechanism provides an interesting way of validating knowledge bases. Keywords: Partial Eval- uation, Expert Systems, Multiple-valued Logic.\nTitle:\nA specialisation calculus to improve expert systems communications\n\nAbstract:\nIn this paper we consider the expansions of logics of a left-continuous t-norm with truth-constants from a subalgebra of the rational unit interval. From known results on standard semantics, we study completeness for these propositional logics with respect to chains defined over the rational unit interval with a special attention to the completeness with respect to the canonical chain, i.e. the algebra over $$[0,1] \\\\cap {{\\\\mathbb{Q}}}$$ where each truth-constant is interpreted in its corresponding rational truth-value. Finally, we study rational completeness results when we restrict ourselves to deductions between the so-called evaluated formulae.\nTitle:\nExpanding the propositional logic of a t-norm with truth-constants: completeness results for rational semantics\n\nAbstract:\nIn this paper we define a framework to introduce gradedness in Deontic logics through the use of fuzzy modalities. By way of example, we instantiate the framework to Standard Deontic logic (SDL) formulas. Given a deontic formula \u9a74\u9a74 SDL, our language contains formulas of the form $\\overline{r} \\to N\\Phi$ or $\\overline{r} \\to P\\Phi$, where r\u9a74 [0, 1], expressing that the preference or probability degree respectively of a norm \u9a74is at least r. We present sound and complete axiomatisations for these logics.\nTitle:\nIntroducing Grades in Deontic Logics\n\nAbstract:\nThis paper aims at being a systematic investigation of different completeness properties of first-order predicate logics with truth-constants based on a large class of left-continuous t-norms (mainly continuous and weak nilpotent minimum t-norms). We consider standard semantics over the real unit interval but also we explore alternative semantics based on the rational unit interval and on finite chains. We prove that expansions with truth-constants are conservative and we study their real, rational and finite chain completeness properties. Particularly interesting is the case of considering canonical real and rational semantics provided by the algebras where the truth-constants are interpreted as the numbers they actually name. Finally, we study completeness properties restricted to evaluated formulae of the kind r\u00af\u2192\u03c6, where \u03c6 has no additional truth-constants.\nTitle:\nFirst-order t-norm based fuzzy logics with truth-constants: Distinguished semantics and completeness properties\n\nAbstract:\nThe aim of this paper is to propose dynamic logic as a common logical framework to describe and identify the most relevant formal characteristics of multi-language logical architectures (MLA) in order to investigate the expressive power of the knowledge bases that can be built upon them. In general, a MLA allows to build knowledge bases as a set of units with initial local theories written in possible different logical languages. Each unit is also usually allowed to have its own intra-unit deductive system. Moreover, the whole knowledge base is equipped with an additional set of deductive rules, called bridge rules , to control the information flow among the different units of the knowledge base. The set of bridge rules act as an inter-unit deductive system. The reasoning dynamics of a knowledge base on top of a MLA can therefore be described by how the local theories of the units evolve during execution.\nTitle:\nDescriptive dynamic logic and its application to reflective architectures.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.019, mean value for all the writers is 8.648, which makes it 0.317 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper we propose a deductive calculus aiming at improving the query/simple-answer communication behaviour of many intelligent systems. In an uncertain reasoning context this behaviour consists of getting certainty values for propositions as answers to queries. Instead, with our calculus, answers to queries will become sets of formulas: a set of propositions and a set of specialised rules containing propositions for which the truth value is unknown in their left part. This type of behaviour is much more informative because it returns to users not only the answer to a query but all the relevant information, related to the answer, necessary to, possibly, improve the solution. To exemplify the general approach a family of propositional rule-based languages founded on multiple-valued logics is presented and formalised. The deductive system defined on top of these languages is based on a Specialisation Inference Rule (SIR): (A1 \u2227 A2 \u2227 \u2026 \u2227 An \u2192 P, V), (A1, V\u2032) \u2252 (A2 \u2227 \u2026 \u2227 An \u2192 P, V\u2033), where V, V\u2032 and V\u2033 are truth intervals. This inference rule provides a way of generating rules containing less conditions in their premise by eliminating the conditions for which a definitive truth value already exists. The soundness and atom completeness of the deductive system are proved. The implementation of this deductive calculus is based on partial deduction techniques. Finally, an example of the application of the specialisation calculus to a multi-agent system is provided.\nTitle:", "model_inf_time": 1.43}, {"id": "41373", "output": "A Comparative Study of Underwater Acoustic Modem Ranging Functions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents the design, implementation and measurement of Aqua-Lab, an underwater acoustic sensor network lab testbed. Aqua-Lab consists of a water tank, a set of acoustic communication hardware, and a set of software. One important component of the software is an emulator that we developed to provide user-friendly programming interfaces and emulate realistic network settings. Using Aqua-Lab, we explore basic characteristics of data transmission using Micro-Modems and conduct a set of experiments in both field and lab environments. Our results from lab testbed are consistent with those from the field experiments, and thus demonstrate that Aqua-Lab can be used to experimentally evaluate algorithms and protocols designed for underwater sensor networks.\nTitle:\nAn underwater network testbed: design, implementation and measurement\n\nAbstract:\nWe investigate the problem of localizing an underwater sensor node based on message broadcasting from multiple surface nodes. With the time-of-arrivalmeasurements from a DSP-basedmulticarrier modem, each sensor node localizes itself based on the travel time differences amongmultiple senders to the receiver. Using one-waymessage passing, such a solution can scale to accommodate a large number of nodes in a network. We consider the issue from not only the physical layer, but also at the node processing layer by incorporating a tracking solution. We present simulation results, testing results in a swimming pool featuring both stationary and moving receivers, and results from a lake test with a mobile receiver.\nTitle:\nUnderwater localization and tracking of physical systems\n\nAbstract:\nTime synchronization and localization are basic services in a sensor network system. Although they often depend on each other, they are usually tackled independently. In this work, we investigate the time synchronization and localization problems in underwater sensor networks, where more challenges are introduced because of the unique characteristics of the water environment. These challenges include long propagation delay and transmission delay, low bandwidth, energy constraint, mobility, etc.We propose a joint solution for localization and time synchronization, in which the stratification effect of underwater medium is considered, so that the bias in the range estimates caused by assuming sound waves travel in straight lines in water environments is compensated. By combining time synchronization and localization, the accuracy of both are improved jointly. Additionally, an advanced tracking algorithm IMM (interactive multiple model) is adopted to improve the accuracy of localization in the mobile case. Furthermore, by combining both services, the number of required exchanged messages is significantly reduced, which saves on energy consumption. Simulation results show that both services are improved and benefit from this scheme.\nTitle:\nA Joint Time Synchronization and Localization Design For Mobile Underwater Sensor Networks\n\nAbstract:\nIn this paper, we investigate the problem of localizing an underwater sensor node based on message broadcasting from multiple surface nodes. With the time-of-arrival measurements from a DSP-based multicarrier modem, each sensor node localizes itself based on the travel time differences among multiple senders to the receiver. Using one-way message passing, such a solution can scale to accommodate a large number of nodes in a network. Continuing on our preliminary work in [1], we in this paper focus on the tracking solutions at the node processing layer. We present simulation results as well as testing results in a swimming pool with both stationary and moving receivers.\nTitle:\nLocalization and tracking of underwater physical systems\n\nAbstract:\nUnderwater Acoustic Networks (UANs) have become a focus of interest for emerging scientific research and military applications. Recent work has shown that performance of existing security attacks are sensitive to network topology. In this paper, we utilize the mobility of Autonomous Underwater Vehicles (AUVs) to discover the topology of UANs by monitoring the broadcast patterns of geographic routing protocols. In this way, a mobile attacker can take advantage of the geographic information used in UANs to improve attack performance. We evaluate our approach in Aqua-Sim and results show that attack performance of jamming is significantly improved.\nTitle:\nUncooperative localization improves attack performance in Underwater Acoustic Networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.038, mean value for all the writers is 8.648, which makes it 0.333 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nFinding the position of a device within an underwater network, known as localization, has been critical for the advancement of autonomous underwater vehicle (AUV) research. Many of the current methods of underwater localization rely on a modem's ability to accurately estimate the time of flight for a given message transmission. This ability is also utilized by a modem's built-in ranging function. To advance the research in underwater localization, a comparison study was done on the ranging function of three commercially available underwater acoustic modems, all of which utilize different modulation schemes. A pair of AquaSeNT (OFDM), Benthos (MFSK), and LinkQuest (DSSS) acoustic modems were tested at several positions in both a pool and lake environment. It was found that the AquaSeNT was the most robust, as in it always calculated a distance, at the expense of precision. The precision of the AquaSeNT was effected by a consistent 8-m bias. The LinkQuest was found to be the most accurate and precise, but had issues communicating in short channels (large delay spread), and would occasionally produce erroneous measurements. The Benthos had a balance between communication success as well as precision and accuracy.\nTitle:", "model_inf_time": 1.38}, {"id": "41374", "output": "Wireless Digital Music Delivery to Mobile Users", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe use of handheld devices, such as smart phones for personal entertainment, has become commonplace in today's lifestyle. Virtually all of these devices are equipped with Bluetooth technology, which can be used to distribute entertainment content, such as music and movie clips. Mobile users can download content from opportunistically available infrastructure (e.g., digital billboards) and direct peer-to-peer (P2P) collaboration, which significantly increases content availability/coverage. P2P content distribution protocol design is heavily influenced by the characteristics of Bluetooth, which is a main departure from Internet-based content distribution. However, little has been done to understand the performance of overall Bluetooth operations, ranging from peer discovery to data downloading, in dynamic environments with mobility, interference, and different Bluetooth versions/chipsets. In this paper, we perform an extensive measurement study and find that Bluetooth-based content distribution suffers from time/energy-consuming resource discovery and limited bandwidth, even with the enhanced features of the latest Bluetooth version. Given this, we discuss strategies that can effectively improve the performance of the resource-discovery and downloading phases.\nTitle:\nP2P Content Distribution to Mobile Bluetooth Users\n\nAbstract:\nFuture digital entertainment services available to home users will share several characteristics: i) they will be deployed and delivered through the Internet, ii) a single media center will be exploited to orchestrate all parallel services, and iii) wireless technologies integrated within the home entertainment system will be massively utilized for the transmission of various data streams to networked devices. In this scenario, new effective strategies are needed to regulate the concurrent access to the wireless network when parallel applications generate different but simultaneous UDP/TCP-based flows. In this work, we present a novel technique aimed at guaranteeing a fast and smooth data delivery for real-time streams while maintaining a high throughput for TCP-based applications. Our approach is based on the utilization of a smart Access Point able to exploit available information about the ongoing traffic and existing features of the regular TCP. We compare the performance of our solution with an alternative one that makes use of an optimal setting of the 802-11 MAC layer parameters. Simulation results confirm that our smart Access Point represents an optimal candidate to be exploited in complex wireless scenarios for in-home entertainment.\nTitle:\nWireless home entertainment center: reducing last hop delays for real-time applications\n\nAbstract:\nThe great success of P2P systems for the purpose of file-sharing set the path to the next killer application on the Internet, P2P video streaming. Although it solves scalability issues, P2P technology experiences problems of a long start time and churn-induced instability that can greatly affect the user experience. Moreover, technical and business solutions for digital rights management are still under investigation. Great efforts are underway in both academia and industry to solve these problems, whose solution will offer a scalable, affordable, and legal TV-quality-like broadcast of content. In this article, we analyze what is available to the end user in terms of P2P video-streaming products and determine which of these are the most promising for IPTV and content distribution companies. In the following, we offer: (1) A survey of the available architectures. (2) A set of experiments on a popular peer-to-peer system, SopCast. (3) Guidelines for large scale deployment.\nTitle:\nWill IPTV ride the peer-to-peer stream? [Peer-to-Peer Multimedia Streaming]\n\nAbstract:\nContent distribution in vehicular networks, such as multimedia file sharing and software updates, poses a great challenge due to network dynamics and high-speed mobility. In recent years, network coding has been shown to efficiently support distribution of content in such dynamic environments, thereby considerably enhancing the performance. However, the related work in the literature has mostly focused on theoretic or algorithmic aspects of network coding so far. In this paper, we provide an in-depth analysis on the implementation issues of network coding in wireless networks. In particular, we study the impact of resource constraints (namely CPU, disk, memory, and bandwidth) on the performance of network coding in the content distribution application. The contribution of this paper is twofold. First, we develop an abstract model of a general network coding process and evaluate the validity of the model via several experiments on real systems. This model enables us to find the key resource constraints that influence the network coding strategy and thus to efficiently configure network coding parameters in wireless networks. Second, we propose schemes that considerably improve the performance of network coding under resource constrained environments. We implement our overhead model in the QualNet network simulator and evaluate these schemes in a large-scale vehicular network. Our results show that the proposed schemes can significantly improve the network coding performance by reducing the coding overhead.\nTitle:\nUnderstanding Processing Overheads of Network Coding-Based Content Distribution in VANETs\n\nAbstract:\nIncreasing need for people to be \"connected\"; while at the same time remain as mobile as ever poses several interesting issues in wireless networks. It is conceivable in the near-future that wireless \"hotspots\" experience flash crowds-like traffic arrival pattern. A common phenomena in the Internet today characterized by sudden and unpredicted increase in popularity of on-line content. In this paper, we propose SPAWN, a cooperative strategy for content delivery and sharing in future vehicular networks. We study the issues involved in using such a strategy from the standpoint of Vehicular Ad-Hoc networks. In particular, we show that not only content server but also wireless access network load reduction is critical. We propose a \"communication efficient\" swarming protocol which uses a gossip mechanism that leverages the inherent broadcast nature of the wireless medium, and a piece-selection strategy that takes proximity into account in decisions to exchange pieces. We show through simulation that gossip incorporates location-awareness into peer selection, while incurring low messaging overhead, and consequently enhancing the swarming protocol performance. We develop an analytical model to characterize the performance of SPAWN.\nTitle:\nCo-operative Downloading in Vehicular Ad-Hoc Wireless Networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.115, mean value for all the writers is 8.648, which makes it 0.398 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe maturing distributed file sharing technology implemented by Napster has first enabled the dissemination of musical content in digital form, permitting to customers to retrieve stored music files from around the world. In the post-Napster era, the Apple iTunes online music service has hit a record share of 16.7% in the MP3 player market [J. Mc Hugh, Why Wi-Fi is a good business? Wired (2003) 25 26]. This is only the most prominent example of the success of digital music distribution based on packet network technologies. However, to the best of our knowledge, the most noteworthy aspect of the success of digital music distribution is that little about this music delivery technology is really new. To drastically change the nature of this business, we claim that wireless delivery technology must come into the picture. This way, the digital music delivery model will benefit from the integration of the wired Internet with a broad gamut of wireless access technologies, such as WiFi, WPAN and 3G. In this paper we address the problem of the \"customer on the move\"; we present a wireless Internet application designed to support the distribution of digital music to handheld devices. The main novelty of our software is the ability to provide a seamless music delivery service even in the presence of handoffs within the same radio medium (horizontal) and across media (vertical). Actual measurements from a deployed application show that our system can deliver a smooth, ultra reliable, low latency music service to mobile users.\nTitle:", "model_inf_time": 1.42}, {"id": "41375", "output": "Optimal Partial Spectrum Reuse for Energy-Efficient Two-Tier Heterogeneous Cellular Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we consider a Partial Spectrum Reuse (PSR) scheme, namely \"\u03b2-PSR\", to improve spectrum efficiency of two-tier heterogeneous cellular networks, in which each micro Base Station (BS) occupies a uniform portion \u03b2 of the whole system spectrum randomly and independently. We analyze the optimal PSR factor \u03b2 to minimize service outage probability, which is not in an explicit form. Then a closed-form limit of the PSR factor is derived when the ratio of the user data rate requirement over the system bandwidth is approaching zero. This limit is an explicit function of the traffic intensity, the macro/micro BS density and their transmit power. Our numerical results also show that for most current rate requirements, the optimal PSR factor is well approximated by our derived limit. This motivates the design of an adaptive PSR scheme with near-optimal performance, which only depends on statistical network information. \u00a9 2012 IEEE.\nTitle:\n\u03b2-PSR: A Partial Spectrum Reuse scheme for two-tier heterogeneous cellular networks\n\nAbstract:\nIn this paper, we adopt stochastic geometry theory to analyze the optimal macro/micro BS (base station) density for energy-efficient heterogeneous cellular networks with QoS constraints. We first derive the upper and lower bounds of the optimal BS density for homogeneous scenarios and, based on these, we analyze the optimal BS density for heterogeneous networks. The optimal macro/micro BS density can be calculated numerically through our analysis, and the closed-form approximation is also derived. Our results reveal the best type of BSs to be deployed for capacity extension, or to be switched off for energy saving. Specifically, if the ratio between the micro BS cost and the macro BS cost is lower than a threshold, which is a function of path loss and their transmit power, the micro BSs are preferred, i.e., deploy more micro BSs for capacity extension or switch off certain macro BSs for energy saving. Otherwise, the optimal choice is the opposite. Our work provides guidance for energy efficient cellular network planning and dynamic operation control.\nTitle:\nOptimal base station density for energy-efficient heterogeneous cellular networks\n\nAbstract:\nIn this paper, the optimal BS (Base Station) density for both homogeneous and heterogeneous cellular networks to minimize network energy cost is analyzed with stochastic geometry theory. For homogeneous cellular networks, both upper and lower bounds of the optimal BS density are derived. For heterogeneous cellular networks, our analysis reveals the best type of BSs to be deployed for capacity extension, or to be switched off for energy saving. Specifically, if the ratio between the micro BS cost and the macro BS cost is lower than a threshold, which is a function of path loss and their transmit power, then the optimal strategy is to deploy micro BSs for capacity extension or to switch off macro BSs (if possible) for energy saving with higher priority. Otherwise, the optimal strategy is the opposite. The optimal combination of macro and micro BS densities can be calculated numerically through our analysis, or alternatively be conservatively approximated with a closed-form solution. Based on the parameters from EARTH, numerical results show that in the dense urban scenario, compared to the traditional macro-only homogeneous cellular network with no BS sleeping, deploying micro BSs can reduce about 40% of the total energy cost, and further reduce up to 35% with BS sleeping capability.\nTitle:\nOptimal Combination of Base Station Densities for Energy-Efficient Two-Tier Heterogeneous Cellular Networks\n\nAbstract:\nHeterogeneous Networks (HetNets), where Low Power Nodes (LPN) are deployed under the coverage of Macro Base Stations (MBS), are promising to boost the spectrum efficiency per unit area. However, this hierarchical architecture also brings new problems, like severe inter-cell interference. The randomly deployed LPNs, and the co-existence of the cross-tier and intra-tier interference make it challenging to design the effective frequency reuse schemes, which can have significant influences on the user experience and the network capacity. In this paper, we explore the frequency reuse problem in HetNets for interference mitigation. Firstly, Partial Spectrum Reuse (PSR) scheme is adopted to mitigate the cross-tier interference, where MBSs can use all available spectrums while LPNs can only use part of the channels based on their traffic load. Then, the channel allocation scheme is further optimized to mitigate the intra-tier interference. As the channel allocation problem is NP-hard, we propose a greedy channel allocation scheme based on a weighted conflict graph, where cross-tier and intra-tier interference are both considered. In addition, our method is practical for channel allocation in real systems. Simulation results show the SINR of cell edge users and the ergodic SINR can be both improved by 3dB as much with our greedy method compared with the random channel allocation scheme.\nTitle:\nA practical channel allocation scheme based on the weighted conflict graph in heterogeneous networks\n\nAbstract:\nThe base station (BS) turning off scheme has been considered as a feasible solution to save energy of wireless networks. At the same time, it is also important to maintain quality of service (QoS) of the cell whose BS is turned off. In this paper, we consider multi-hop relay (MR) and cooperative transmission (CT) in cellular network and evaluate performances focusing on the tradeoff between QoS (i.e., user throughput and outage probability) and energy consumption when a BS is turned off. The result shows that combining cellular network with MR (MR network) is more robust in maintaining QoS level when a BS is turned off. Moreover, the MR network can also reduce overall energy consumption depending on the energy consumption level of a relay station (RS). Finally, a cooperative multi-hop relay (CMR) network which supports both intra-sector and inter-sector cooperative relaying is proposed to fill up the coverage holes.\nTitle:\nMulti-Hop Relay Network for Base Station Energy Saving and Its Performance Evaluation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.117, mean value for all the writers is 8.648, which makes it 2.106 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nPartial Spectrum Reuse (PSR) in the second tier of two-tier heterogeneous cellular networks has a potential to improve spectrum efficiency by reducing inter-cell interference, and thus energy efficiency as well by deploying less or switching off more Base Stations (BSs). In this paper, we analyze the optimal PSR factor, defined as the portion of spectrum reused by micro cells in two-tier heterogeneous networks, which is not in an explicit form generally. Then, a closed-form limit of the optimal PSR factor is derived as the ratio of the user rate requirement over the whole system spectrum bandwidth is approaching zero, based on which a threshold of the micro-BS energy cost is also derived to determine which type of BSs is preferable. Specifically, one should deploy more micro BSs or switch off more macro BSs if the micro-BS energy cost is lower than the threshold. Otherwise, the optimal choice is the opposite. This threshold with the PSR scheme is higher than that without PSR scheme, i.e., PSR can improve both spectrum efficiency and energy efficiency. Numerical results show that adopting PSR can reduce the network energy consumption by up to 50% when the transmit power of macro BSs is 10dB higher than that of micro BSs.\nTitle:", "model_inf_time": 1.78}, {"id": "41376", "output": "Equimatchable Graphs and 3-Connected Planar Graphs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we study the relationship between forbidden subgraphs and the existence of a matching. Let H be a set of connected graphs, each of which has three or more vertices. A graph G is said to be H-free if no graph in H is an induced subgraph of G. We completely characterize the set H such that every connected H-free graph of sufficiently large even order has a perfect matching in the following cases.(1) Every graph in H is triangle-free. (2) H consists of two graphs (i.e. a pair of forbidden subgraphs).A matching M in a graph of odd order is said to be a near-perfect matching if every vertex of G but one is incident with an edge of M. We also characterize H such that every H-free graph of sufficiently large odd order has a near-perfect matching in the above cases.\nTitle:\nA pair of forbidden subgraphs and perfect matchings\n\nAbstract:\nLet G be a bipartite graph with bipartition (X,Y) which has a perfect matching. It is proved that G is n-extendable if and only if for any perfect matching M of G and for each pair of vertices x in X and y in Y there are n internally disjoint M-alternating paths connecting x and y. Furthermore, these n paths start and end with edges in E(G)\\M. This theorem is then generalized.\nTitle:\nM-alternating paths in n-extendable bipartite graphs\n\nAbstract:\nLet G be a balanced bipartite graph with partite sets X and Y, which has a perfect matching, and let x\u2208X and y\u2208Y. Let k be a positive integer. Then we prove that if G has k internally disjoint alternating paths between x and y with respect to some perfect matching, then G has k internally disjoint alternating paths between x and y with respect to every perfect matching.\nTitle:\nA note on internally disjoint alternating paths in bipartite graphs\n\nAbstract:\nBy Tutte's constructive characterization of 3-connected graphs ( Indag. Math. 23 (1961) , 441\u2013455), we see that every 3-connected graph of order at least five has an edge whose contraction results in a 3-connected graph. We call such an edge a contractible edge and study the distribution of contractible edges in 3-connected graphs. As a consequence, we prove that every 3-connected graph of order at least five has [ |G| 2 ] or more contractible edges and determine the graphs which attain the equality.\nTitle:\nContractible edges in 3-connected graphs\n\nAbstract:\nAn edgee in a 3-connected graphG is contractible if the contraction ofe inG results in a 3-connected graph; otherwisee is non-contractible. In this paper, we prove that the number of non-contractible edges in a 3-connected graph of orderp\u22655 is at most\nTitle:\nNon-Contractible Edges in A 3-Connected Graph\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.88, mean value for all the writers is 8.648, which makes it 0.655 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA graph G is said to be equimatchable if every matching in G extends to (i.e., is a subset of) a maximum matching. In this paper, we use the Gallai-Edmonds decomposition theory for matchings to determine the equimatchable members of two important graph classes. We find that there are precisely 23 3-connected planar graphs (i.e., 3-polytopes) which are equimatchable and that there are only two cubic equimatchable graphs.\nTitle:", "model_inf_time": 1.3}, {"id": "41377", "output": "Linear-Time Approximation Algorithms for Metric Space Problems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n  In the first part of the paper, we present an (1+\\mu)-approximation algorithm to the minimum-spanning tree of points in a planar arrangement of lines, where the metric is the number of crossings between the spanning tree and the lines. The expected running time is O((n/\\mu^5) alpha^3(n) log^5 n), where \\mu > 0 is a prescribed constant.   In the second part of our paper, we show how to embed such a crossing metric, into high-dimensions, so that the distances are preserved. As a result, we can deploy a large collection of subquadratic approximations algorithms \\cite im-anntr-98,giv-rahdg-99 for problems involving points with the crossing metric as a distance function. Applications include matching, clustering, nearest-neighbor, and furthest-neighbor. \nTitle:\nWhen crossings count \u2014 approximating the minimum spanning tree\n\nAbstract:\nIn this paper we present a randomized constant factor approximation algorithm for the problem of computing the optimal cost of the metric Minimum Facility Location problem, in the case of uniform costs and uniform demands, and in which every point can open a facility. By exploiting the fact that we are approximating the optimal cost without computing an actual solution, we give the first algorithm for this problem with running time O(n log2n), where n is the number of metric space points. Since the size of the representation of an n-point metric space is \u0398(n2), the complexity of our algorithm is sublinear with respect to the input size. We consider also the general version of the metric Minimum Facility Location problem and we show that there is no o(n2)-time algorithm, even a randomized one, that approximates the optimal solution to within any factor. This result can be generalized to some related problems, and in particular, the cost of minimum-cost matching, the cost of bi-chromatic matching, or the cost of n/2-median cannot be approximated in o(n2)-time.\nTitle:\nFacility location in sublinear time\n\nAbstract:\nWe present improved running times for a wide range of approximate high dimensional proximity problems. We obtain subquadratic running time for each of these problems. These improved running times are obtained by reduction to Nearest Neighbour queries. The problems we consider in this paper are Approximate Diameter, Approximate Furthest Neighbours, Approximate Discrete Center, Approximate Metric Facility Location, Approximate Bottleneck Matching, and Approximate Minimum Weight Matching.\nTitle:\nReductions among high dimensional proximity problems\n\nAbstract:\nWe give an NlogO(1) N-time randomized O(1)-approximation algorithm for computing the cost of minimum bichromatic matching between two planar point-sets of size N.\nTitle:\nA near linear time constant factor approximation for Euclidean bichromatic matching (cost)\n\nAbstract:\nIn this paper we give approximation algorithms for several proximity problems in high dimensional spaces. In particular, we give the first Las Vegas data structure for (1+ epsilon)-nearest neighbor with polynomial space and query time polynomial in dimension d and log n, where n is the database size. We also give a deterministic 3-approximation algorithm with similar bounds; this is the first deterministic constant factor approximation algorithm (with polynomial space) for any norm. For the closest pair problem we give a roughly n(1+rho) time Las Vegas algorithm with approximation factor O(1/rho log 1/rho); this is the first Las Vegas algorithm for this problem. Finally, we show a general reduction from the furthest point problem to the nearest neighbor problem. As a corollary, we improve the running time for the (1 + epsilon)-approximate diameter problem from n(2-O(epsilon 2)) to n(2-O(epsilon)) Our results are unified by the fact that their key component is a dimensionality reduction technique for Hamming spaces.\nTitle:\nDimensionality reduction techniques for proximity problems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.435, mean value for all the writers is 8.648, which makes it 1.035 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper we give approximation algorithms for the following problems on metric spaces:Furthest Pair, k-median, Minimum Routing Cost Spanning Tree, Multiple Sequence Alignment,Maximum Traveling Salesman Problem, Maximum Spanning Tree and Average Distance. Thekey property of our algorithms is that their running times is linear in the number of points. Asthe full specification of an n-point metric space is of size \\Theta(n2), the complexity of our algorithmsis sublinear with respect...\nTitle:", "model_inf_time": 1.21}, {"id": "41378", "output": "Hierarchical Self-Organizing Neural Networks for Multimodal Action Recognition", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nLifelong learning is fundamental in autonomous robotics for the acquisition and fine-tuning of knowledge through experience. However, conventional deep neural models for action recognition from videos do not account for lifelong learning but rather learn a batch of training data with a predefined number of action classes and samples. Thus, there is the need to develop learning systems with the ability to incrementally process available perceptual cues and to adapt their responses over time. We propose a self-organizing neural architecture for incrementally learning to classify human actions from video sequences. The architecture comprises growing self-organizing networks equipped with recurrent neurons for processing time-varying patterns. We use a set of hierarchically arranged recurrent networks for the unsupervised learning of action representations with increasingly large spatiotemporal receptive fields. Lifelong learning is achieved in terms of prediction-driven neural dynamics in which the growth and the adaptation of the recurrent networks are driven by their capability to reconstruct temporally ordered input sequences. Experimental results on a classification task using two action benchmark datasets show that our model is competitive with state-of-the-art methods for batch learning also when a significant number of sample labels are missing or corrupted during training sessions. Additional experiments show the ability of our model to adapt to non-stationary input avoiding catastrophic interference.\nTitle:\nLifelong learning of human actions with deep neural network self-organization.\n\nAbstract:\nThe visual recognition of transitive actions comprising human-object interactions is a key component for artificial systems operating in natural environments. This challenging task requires jointly the recognition of articulated body actions as well as the extraction of semantic elements from the scene such as the identity of the manipulated objects. In this paper, we present a self-organizing neural network for the recognition of human-object interactions from RGB-D videos. Our model consists of a hierarchy of Grow-When-Required (GWR) networks that learn prototypical representations of body motion patterns and objects, accounting for the development of action-object mappings in an unsupervised fashion. We report experimental results on a dataset of daily activities collected for the purpose of this study as well as on a publicly available benchmark dataset. In line with neurophysiological studies, our self-organizing architecture exhibits higher neural activation for congruent action-object pairs learned during training sessions with respect to synthetically created incongruent ones. We show that our unsupervised model shows competitive classification results on the benchmark dataset with respect to strictly supervised approaches.\nTitle:\nA self-organizing neural network architecture for learning human-object interactions.\n\nAbstract:\nThe visual recognition of complex, articulated human movements is fundamental for a wide range of artificial systems oriented toward human-robot communication, action classification, and action-driven perception. These challenging tasks may generally involve the processing of a huge amount of visual information and learning-based mechanisms for generalizing a set of training actions and classifying new samples. To operate in natural environments, a crucial property is the efficient and robust recognition of actions, also under noisy conditions caused by, for instance, systematic sensor errors and temporarily occluded persons. Studies of the mammalian visual system and its outperforming ability to process biological motion information suggest separate neural pathways for the distinct processing of pose and motion features at multiple levels and the subsequent integration of these visual cues for action perception. We present a neurobiologically-motivated approach to achieve noise-tolerant action recognition in real time. Our model consists of self-organizing Growing When Required (GWR) networks that obtain progressively generalized representations of sensory inputs and learn inherent spatio-temporal dependencies. During the training, the GWR networks dynamically change their topological structure to better match the input space. We first extract pose and motion features from video sequences and then cluster actions in terms of prototypical pose-motion trajectories. Multi-cue trajectories from matching action frames are subsequently combined to provide action dynamics in the joint feature space. Reported experiments show that our approach outperforms previous results on a dataset of full-body actions captured with a depth sensor, and ranks among the best results for a public benchmark of domestic daily actions.\nTitle:\nSelf-organizing neural integration of pose-motion features for human action recognition\n\nAbstract:\nCrossmodal conflict resolution is crucial for robot sensorimotor coupling through the interaction with the environment, yielding swift and robust behaviour also in noisy conditions. In this paper, we propose a neurorobotic experiment in which an iCub robot exhibits human-like responses in a complex crossmodal environment. To better understand how humans deal with multisensory conflicts, we conducted a behavioural study exposing 33 subjects to congruent and incongruent dynamic audio-visual cues. In contrast to previous studies using simplified stimuli, we designed a scenario with four animated avatars and observed that the magnitude and extension of the visual bias are related to the semantics embedded in the scene, i.e., visual cues that are congruent with environmental statistics (moving lips and vocalization) induce the strongest bias. We implement a deep learning model that processes stereophonic sound, facial features, and body motion to trigger a discrete behavioural response. After training the model, we exposed the iCub to the same experimental conditions as the human subjects, showing that the robot can replicate similar responses in real time. Our interdisciplinary work provides important insights into how crossmodal conflict resolution can be modelled in robots and introduces future research directions for the efficient combination of sensory observations with internally generated knowledge and expectations.\nTitle:\nA Neurorobotic Experiment For Crossmodal Conflict Resolution In Complex Environments\n\nAbstract:\nFor the complex human brain that enables us to communicate in natural language, we gathered good understandings of principles underlying language acquisition and processing, knowledge about sociocultural conditions, and insights into activity patterns in the brain. However, we were not yet able to understand the behavioural and mechanistic characteristics for natural language and how mechanisms in the brain allow to acquire and process language. In bridging the insights from behavioural psychology and neuroscience, the goal of this paper is to contribute a computational understanding of appropriate characteristics that favour language acquisition. Accordingly, we provide concepts and refinements in cognitive modelling regarding principles and mechanisms in the brain and propose a neurocognitively plausible model for embodied language acquisition from real-world interaction of a humanoid robot with its environment. In particular, the architecture consists of a continuous time recurrent neural network, where parts have different leakage characteristics and thus operate on multiple timescales for every modality and the association of the higher level nodes of all modalities into cell assemblies. The model is capable of learning language production grounded in both, temporal dynamic somatosensation and vision, and features hierarchical concept abstraction, concept decomposition, multi-modal integration, and self-organisation of latent representations.\nTitle:\nInteractive Natural Language Acquisition in a Multi-modal Recurrent Neural Architecture.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.407, mean value for all the writers is 8.648, which makes it 0.648 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe integration of multisensory information plays a crucial role in autonomous robotics to forming robust and meaningful representations of the environment. In this work, we investigate how robust multimodal representations can naturally develop in a self-organizing manner from co-occurring multisensory inputs. We propose a hierarchical architecture with growing self-organizing neural networks for learning human actions from audiovisual inputs. The hierarchical processing of visual inputs allows to obtain progressively specialized neurons encoding latent spatiotemporal dynamics of the input, consistent with neurophysiological evidence for increasingly large temporal receptive windows in the human cortex. Associative links to bind unimodal representations are incrementally learned by a semi-supervised algorithm with bidirectional connectivity. Multimodal representations of actions are obtained using the co-activation of action features from video sequences and labels from automatic speech recognition. Experimental results on a dataset of 10 full-body actions show that our system achieves state-of-the-art classification performance without requiring the manual segmentation of training samples, and that congruent visual representations can be retrieved from recognized speech in the absence of visual stimuli. Together, these results show that our hierarchical neural architecture accounts for the development of robust multimodal representations from dynamic audiovisual inputs.\nTitle:", "model_inf_time": 1.62}, {"id": "41379", "output": "Food Image Recognition with Cloud Computing and SIFT Features", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nFood image recognition is increasingly important for e-health applications. But this is a challenging topic due to the diversity of food, and color, light, view angles' effect on food image. Based on empirical and experimental explorations, we propose to use SIFT(Scale Invariant Feature Transform) and Gabor descriptors as food image features and KMeans algorithm for feature clustering. We also propose to use pervasive cloud computing paradigm to improve the performance of food image recognition due to the heavy computing requirement for large number of concurrent recognition requests. Evaluations show that the proposed approach can give acceptable recognition rate, and MapReduce programming can provide promising performance advantage compared to traditional client server approach.\nTitle:\nFood Image Recognition Using Pervasive Cloud Computing.\n\nAbstract:\nIntelligent video surveillance is a challenging issue due to complicated scenes. Based on empirical and experimental explorations, we propose a multi-person tracking-by-detection framework to achieve pedestrian counting at run time. This framework is integrated with a stream based cloud computing paradigm to improve tracking performance. We evaluated our approach which shows improved time performance compared with those classical approaches.\nTitle:\nOnline Multiperson Tracking and Counting with Cloud Computing\n\nAbstract:\nPervasive cloud computing heavily depends on task migrations in order to mitigate resource scarceness in some cloud nodes, especially the light weight nodes. In order to make decisions on task migrations, a number of possibly conflicting objectives should be considered, such as less energy consumption, quick response, in order to find an optimal migration path and optimal configurations. In this paper, we conduct initial exploration on using a Genetic algorithm (GA) based approach which is effective in solving multi-objective optimization problems. The preliminary evaluations we have done shows that the proposed approach is promising.\nTitle:\nTowards a Genetic Algorithm Based Approach for Task Migrations\n\nAbstract:\nAbstractPervasive computing is converging with cloud computing which becomes pervasive cloud computing as an emerging computing paradigm. Users can run their applications or tasks in pervasive cloud environment in order to gain better execution efficiency and performance leveraging powerful computing and storage capacities of pervasive clouds through task migration. During task migration, there are possibly a number of conflicting objectives to be considered when making migration decisions, such as less energy consumption and quick response, in order to find an optimal migration path. In this paper, we propose a genetic algorithms- (GAs-) based approach which is effective in addressing multiobjective optimization problems. We have performed some preliminary evaluations of the proposed approach which shows quite promising results, using one of the classical genetic algorithms.The conclusion is that GAs can be used for decision making in task migrations in pervasive clouds.\nTitle:\nA genetic-algorithm-based approach for task migration in pervasive clouds\n\nAbstract:\nReal-time response is a challenging issue for video object detection, especially when the number of cameras is large and correspondingly the video data are big. The existing solutions for object detection fall short in addressing the real-time performance aspect, and can not handle fast response requirements such as fleeing vehicle tracking at run time. Therefore, in this paper we propose a Storm-based real-time framework for video object detection that can scale to handle large number of cameras. To evaluate its performance, we implement the framework in a Storm cluster environment where we test the detection rate and real-time performance of the framework. The results show that the detection rate is relatively acceptable and real-time response is achieved.\nTitle:\nA Realtime Framework for Video Object Detection with Storm\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.392, mean value for all the writers is 8.648, which makes it 0.635 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nFood image recognition is challenging due to the diversity of food, and color, light, view angles' effect on food image. The recognition process is also a computation heavy process. Therefore, We also propose to use pervasive cloud computing paradigm to improve the performance of food image recognition. Based on empirical and experimental explorations, we propose to use SIFT(Scale Invariant Feature Transform) and Gabor descriptors as food image features and KMeans algorithm for feature clustering. Evaluations show that the proposed approach can give acceptable recognition rate with good performance gains.\nTitle:", "model_inf_time": 1.22}, {"id": "41380", "output": "Extended Mixture Models for Big Data in Healthcare Research", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n . Normal mixture models are being increasingly used as a wayof clustering sets of continuous multivariate data. They provide a probabilistic(soft) clustering of the data in terms of their fitted posterior probabilitiesof membership of the mixture components corresponding to theclusters. An outright (hard) clustering can be subsequently obtained byassigning each observation to the component to which it has the highestfitted posterior probability of belonging. However, outliers in the data... \nTitle:\nRobust Cluster Analysis via Mixtures of Multivariate t-Distributions\n\nAbstract:\nIn recent years, finite mixtures of skew distributions are gaining popularity as a flexible tool for modelling data with asymmetric distributional features. Parameter estimation for these mixture models via the traditional EM algorithm requires the number of components to be specified a priori. In this paper, we consider unsupervised learning of skew mixture models where the optimal number of components is estimated during the parameter estimation process. We adopt a component-wise EM algorithm and use the minimum message length (MML) criterion. For illustrative purposes, we focus on the case of a finite mixture of multivariate skew t distributions. The performance of the approach is demonstrated on a real dataset from flow cytometry, where our mixture model was used to provide an automated segmentation of cell populations.\nTitle:\nUnsupervised Component-Wise EM Learning for Finite Mixtures of Skew t-distributions.\n\nAbstract:\nWith mixed feature data, problems are induced in modeling the gating network of normalized Gaussian (NG) networks as the assumption of multivariate Gaussian becomes invalid. In this paper, we propose an independence model to handle mixed feature data within the framework of NG networks. The method is illustrated using a real example of breast cancer data.\nTitle:\nNormalized gaussian networks with mixed feature data\n\nAbstract:\nIn many applied problems in the context of pattern recognition, the data often involve highly asymmetric observations. Normal mixture models tend to overfit when additional components are included to capture the skewness of the data. Increased number of pseudo-components could lead to difficulties and inefficiencies in computations. Also, the contours of the fitted mixture components may be distorted. In this paper, we propose to adopt mixtures of multivariate skew t distributions to handle highly asymmetric data. The EM algorithm is used to compute the maximum likelihood estimates of model parameters. The method is illustrated using a flurorescence-activated cell sorting data.\nTitle:\nMultivariate Skew t Mixture Models: Applications to Fluorescence-Activated Cell Sorting Data\n\nAbstract:\nFinite mixture models have been widely used for the modelling and analysis of data from heterogeneous populations. Maximum likelihood estimation of the parameters is typically carried out via the Expectation-Maximization (EM) algorithm. The complexity of the implementation of the algorithm depends on the parametric distribution that is adopted as the component densities of the mixture model. In the case of the skew normal and skew t-distributions, for example, the E-step would involve complicated expressions that are computationally expensive to evaluate. This can become quite time-consuming for large and/or high-dimensional datasets. In this paper, we develop a multithreaded version of the EM algorithm for the fitting of finite mixture models. Due to the structure of the algorithm for these models, the E- and M-steps can be easily reformulated to be executed in parallel across multiple threads to take advantage of the processing power available in modern-day multicore machines. Our approach is simple and easy to implement, requiring only small changes to standard code. To illustrate the approach, we focus on a fairly general mixture model that includes as special or limiting cases some of the most commonly used mixture models including the normal, t-, skew normal, and skew t-mixture models. The performance gain with our approach is illustrated using two real datasets.\nTitle:\nA Simple Parallel EM Algorithm for Statistical Learning via Mixture Models\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.077, mean value for all the writers is 8.648, which makes it 1.219 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nBig data in healthcare research is now commonplace. The extraction of useful information on group structures from these data can contribute to improve the quality and effectiveness of care for a sustainable health system. It is not only the sheer size of the data that imposes difficulty in direct application of conventional clustering methods, big data in healthcare often exhibit a multilevel structure with complex correlation among observations and/or a mix of variable types. This paper considers two aspects of extension of mixture models in random effects modelling and multitask clustering of big data. The applicability of these extended mixture models is illustrated using simulated data and real data sets.\nTitle:", "model_inf_time": 1.21}, {"id": "41381", "output": "Supporting Older Adults' Learning of Mobile Devices: Insights from Survey and Field Studies", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMobile computing devices can offer older adults (ages 65+) support in their daily lives, but older adults often find such devices difficult to learn and use. One potential design approach to improve the learnability of mobile devices is a Multi-Layered (ML) interface, where novice users start with a reduced-functionality interface layer that only allows them to perform basic tasks, before progressing to a more complex interface layer when they are comfortable. We studied the effects of a ML interface on older adults\u2019 performance in learning tasks on a mobile device. We conducted a controlled experiment with 16 older (ages 65--81) and 16 younger participants (age 21--36), who performed tasks on either a 2-layer or a nonlayered (control) address book application, implemented on a commercial smart phone. We found that the ML interface\u2019s Reduced-Functionality layer, compared to the control\u2019s Full-Functionality layer, better helped users to master a set of basic tasks and to retain that ability 30 minutes later. When users transitioned from the Reduced-Functionality to the Full-Functionality interface layer, their performance on the previously learned tasks was negatively affected, but no negative impact was found on learning new, advanced tasks. Overall, the ML interface provided greater benefit for older participants than for younger participants in terms of task completion time during initial learning, perceived complexity, and preference. We discuss how the ML interface approach is suitable for improving the learnability of mobile applications, particularly for older adults.\nTitle:\nMulti-Layered Interfaces to Improve Older Adults\u2019 Initial Learnability of Mobile Applications\n\nAbstract:\nMobile devices offer much potential to support older adults (age 65+). However, older adults have been relatively slow to adopt mobile devices. Although much ongoing HCI research has examined usability problems to address this issue, little work has looked at whether existing graphical icons are harder to use for this population compared with younger adults. We conducted a qualitative exploratory study and a follow-up experimental study to determine which icon characteristics help initial icon usability for older adults. We found that our older participants did have more problems using existing mobile device icons, but that particular icon characteristics\u00a0-\u00a0semantically close meaning (i.e. natural, close link between depicted objects and associated function), familiar, labelled and concrete (i.e. those depicting real-world objects)\u00a0-\u00a0improved icon usability for them. We discuss how these findings can help icon designers to create mobile device icons that are more suited to the abilities and technology experience of older adults.\nTitle:\nAge-related differences in the initial usability of mobile device icons\n\nAbstract:\nThis work reports on the design and evaluation of culturally appropriate technology for older adults. Our design context was Cognitive Testing on a Computer (C-TOC): a self-administered computerized test under development, intended to screen older adults for cognitive impairments. Using theory triangulation of cultural attitudes toward uncertainty, we designed two interfaces (one minimal and one rich) for one C-TOC subtest and hypothesized they would be culturally appropriate for older adult Caucasians and East Asians respectively. We ran an experiment with 36 participants to investigate cultural differences in performance, preference and anxiety. We found that Caucasians preferred the interface with minimal elements (i.e. those essential for the primary task) or had no preference. By contrast, East Asians preferred the rich interface augmented with security and learning support and felt less anxious with it than the minimal.\nTitle:\nInterface design for older adults with varying cultural attitudes toward uncertainty\n\nAbstract:\nInterruptions in the home pose a threat to the validity of self-administered computerised cognitive testing. We report the findings of a laboratory experiment investigating the effects of increased interruption workload demand on older adults' computerised cognitive test performance. Related work has reported interruptions having a range of inhibitory and facilitatory effects on primary task performance. Cognitive ageing literature suggests that increased interruption workload demand should have greater detrimental effects on older adults' performance, when compared to younger adults. With 36 participants from 3 age groups (20-54, 55-69, 70+), we found divergent effects of increased interruption demand on two primary tasks. Results suggest that older and younger adults experience interruptions differently, but at no age is test performance compromised by demanding interruptions. This finding is reassuring with respect to the success of a self-administered computerised cognitive assessment test, and is likely to be useful for other applications used by older adults.\nTitle:\nInvestigating interruptions in the context of computerised cognitive testing for older adults\n\nAbstract:\nMany applications provide personalization mechanisms through which users can make changes to adapt a system to better fit their needs or preferences. However, advanced personalization, such as extending system functionality, is often only available to programmers. Building on ideas from end-user programming and personalization literature, we developed an adaptable task management tool that allows advanced personalization using a self-disclosing mechanism and a guided scripting mechanism, ScriPer. We present our design process, its outcome, and the results of a user study (n=24). Participants, even those with no to some background in programming, were able to use ScriPer to perform advanced personalization (in 142 of 144 trials). We also found error patterns differed across programming expertise.\nTitle:\nDesigning for Advanced Personalization in Personal Task Management.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.571, mean value for all the writers is 8.648, which makes it 1.641 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMobile computing devices, such as smart phones, offer benefits that may be especially valuable to older adults (age 65+). Yet, older adults have been shown to have difficulty learning to use these devices. In the research presented in this article, we sought to better understand how older adults learn to use mobile devices, their preferences and barriers, in order to find new ways to support them in their learning process. We conducted two complementary studies: a survey study with 131 respondents from three age groups (20--49, 50--64, 65+) and an in-depth field study with 6 older adults aged 50+. The results showed, among other things, that the preference for trial-and-error decreases with age, and while over half of older respondents and participants preferred using the instruction manual, many reported difficulties using it. We discuss implications for design and illustrate these implications with an example help system, Help Kiosk, designed to support older adults\u2019 learning to use mobile devices.\nTitle:", "model_inf_time": 1.73}, {"id": "41382", "output": "Designing Interactive Multimedia for Individuals with Alzheimer's Disease", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present a framework for technological aids for cognition intended primarily for individuals with cognitive impairments and seniors experiencing cognitive decline. We illustrate the framework with concrete research projects and near-term challenges.\nTitle:\nDesigning technology to aid cognition\n\nAbstract:\nWe present experiences and insights into participatory design with individuals who have anterograde amnesia and therefore have extreme difficulty storing new memories. We discuss our design of the design process, and present a set of techniques used to support memory during and between design sessions. From this experience, we identify cognitive assumptions of participatory design that break down when working with amnestics. We generalize these ideas into an analytical framework for researchers and practitioners who intend to use participatory design with persons having various kinds of cognitive impairments. We illustrate the framework by analyzing a cognitive deficit unrelated to memory that we encountered, and an unanticipated benefit from what at first appeared to be a liability in working with this design team.\nTitle:\nParticipatory design with individuals who have amnesia\n\nAbstract:\nIndividuals with cognitive deficits and their families are prime examples of collaborative \"systems\" that seek to perform everyday tasks together. Yet there has been little investigation into how these families communicate and coordinate in basic tasks like remembering appointments. In this paper we take a distributed cognition approach to studying ten families struggling with amnesia through nonparticipant observation and interviews. Our data show that the families work closely together as cognitive systems that must compensate for memory volatility in one of the members. We explore our participants' strategies for overcoming these difficulties and present lessons for the design of assistive technologies, highlighting the need for redundancy, easy and frequent synchronization, and awareness of updates. We conclude with implications for distributed cognition theory.\nTitle:\nCollaborating to remember: a distributed cognition account of families coping with memory impairments\n\nAbstract:\nOur aim is to introduce techniques that allow for active involvement of users throughout the design process, starting with the very early stages of ideation and exploration. The approach discussed in this study augments conventional usability testing with a user sketching component. We found that enabling users to sketch their ideas facilitated reflection, and provided a rich medium for discovery and communication of design ideas. We believe that this technique has the potential to complement usability testing in general, in order to generate \"reflective\" as opposed to purely \"reactive\" user feedback.\nTitle:\nUser sketches: a quick, inexpensive, and effective way to elicit more reflective user feedback\n\nAbstract:\nCognitive impairments, assistive technologies, design and evaluation methodologies, accessibility, inclusive design, universal usability.\nTitle:\nDesigning technology for people with cognitive impairments\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.125, mean value for all the writers is 8.648, which makes it 1.26 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper we present a design project involving primary end users who have declining cognitive abilities such as memory, communication, and problem solving. We are designing interactive multimedia with personalized life stories for individuals with Alzheimer's disease. We conducted a case study to discover and address the design challenges for this project. A particular challenge is a limited ability to communicate with the primary end users. In this paper, we present design methods that take this challenge into consideration. Our goal is to contribute insight into designing for users with cognitive disabilities, and to present methodologies that are useful for designers who have a limited ability to interact or communicate with end users.\nTitle:", "model_inf_time": 1.12}, {"id": "41383", "output": "Testing for Isomorphism of Interval Graphs Using PQ-Trees", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA data structure called a PQ-tree is introduced. PQ-trees can be used to represent the permutations of a set U in which various subsets of U occur consecutively. Efficient algorithms are presented for manipulating PQ-trees. Algorithms using PQ-trees are then given which test for the consecutive ones property in matrices and for graph planarity. The consecutive ones test is extended to a test for interval graphs using a recently discovered fast recognition algorithm for chordal graphs. All of these algorithms require a number of steps linear in the size of their input.\nTitle:\nTesting for the consecutive ones property, interval graphs, and graph planarity using PQ-tree algorithms\n\nAbstract:\nA matrix of zeroes and ones is said to have the consecutive ones property if there is a permutation of its rows such that the ones in each column appear consecutively. This paper develops a data structure which may be used to test a matrix for the consecutive ones property, and produce the desired permutation of the rows, in linear time. One application of the consecutive ones property is in recognizing interval graphs. A graph is an interval graph if there exists a 1-1 correspondence between its vertices and a set of intervals on the real line such that two vertices are adjacent if and only if the corresponding intervals have a nonempty intersection. Fulkerson and Gross have characterized interval graphs as those for which the clique versus vertex incidence matrix has the consecutive ones property. In testing this particular matrix for the consecutive ones property we may process the columns in a special order to simplify the algorithm. This yields the interval graph recognition algorithm which is presented in section 2; section 3 indicates how this algorithm may be extended to the general consecutive ones problem.\nTitle:\nLinear algorithms to recognize interval graphs and test for the consecutive ones property\n\nAbstract:\nUsing binary search and a Strassen-like matrix multiplication algorithm we obtain efficient algorithms for computing the diameter, the radius, and other distance-related quantities associated with undirected and directed graphs having unit cost (unweighted) edges. Similar methods are used to find approximate values for the distances between all pairs of vertices, and if the graph satisfies certain regularity conditions to find the exact distances.\nTitle:\nComputing extremal and approximate distances in graphs having unit cost edges\n\nAbstract:\nUsing modular arithmetic we obtain the following improved bounds on the time and space complexities for n \u00d7 n Boolean matrix multiplication: O(nlog27 lognlogloglognloglogloglogn) bit operations and O(n2loglog n) bits of storage on a logarithmic cost RAM having no multiply or divide instruction; O(nlog27(logn)2-1/2log27(loglog n)1/2log27-1) bit operations and O(n2log n) bits of storage on a RAM which can use indirect addressing for table lookups. The first algorithm can be realized as a Boolean circuit with O(nlog27lognlogloglognloglogloglogn) gates. Whenever n\u00d7n arithmetic matrix multiplication can be performed in less than O(nlog27) arithmetic operations, our results have corresponding improvements.\nTitle:\nImproved Time and Space Bounds for Boolean Matrix Multiplication.\n\nAbstract:\nEnabling group collaboration is important incomputer graphics today. We have developed aframework that supports multiple pointing devices toexplore the collaborative utility of multiple mice andlaser pointer interaction in graphical environments.Because most pointing device comparisons are done inthe context of single user performance, very little isknown about the affordances of collaborating withmultiple pointing devices. We present an experimentalcomparison of mouse pointer to laser pointerinteraction in a problem-solving task involving groupsof one, two, and three people. We show thatcollaborative performance is largely orthogonal tomotor performance and that the interaction patternsare dependent on the task and on the group size. Thissuggests that the collaborative characteristics of apointing device are just as important as the physicalcharacteristics that are usually given the mostattention, such as precision and accuracy.\nTitle:\nExploring Collaboration with Group Pointer Interaction\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.208, mean value for all the writers is 8.648, which makes it 0.478 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA graph is an interval graph tf and only if each of Its verttces can be associated with an interval on the real hne m such a way that two vertices are adjacent m the graph exactly when the corresponding mtervals have a nonempty mtersectmn An effictent algonthrn for testing tsomorpinsm of interval graphs ts unplemented using a data structure called a PQ-tree. The algorithm runs m O(n + e) steps for graphs having n vemces and e edges It is shown that for a somewhat larger class of graphs, namely the chordal graphs, lsomorpinsm is as hard as for general graphs\nTitle:", "model_inf_time": 1.32}, {"id": "41384", "output": "Ordinary Preserving Manifold Analysis for Age and Head Pose Estimation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose a cost-sensitive local binary feature learning (CS-LBFL) method for facial age estimation. Unlike the conventional facial age estimation methods that employ hand-crafted descriptors or holistically learned descriptors for feature representation, our CS-LBFL method learns discriminative local features directly from raw pixels for face representation. Motivated by the fact ...\nTitle:\nCost-Sensitive Local Binary Feature Learning for Facial Age Estimation\n\nAbstract:\nIn this paper, we present a label-sensitive deep metric learning (LSDML) approach for facial age estimation. Motivated by the fact that human age labels are chronologically correlated, our proposed LSDML aims to seek a series of hierarchical nonlinear transformations by deep residual network to project face samples to a latent common space, where the similarity of face pairs is equivalently isoton...\nTitle:\nLabel-Sensitive Deep Metric Learning for Facial Age Estimation.\n\nAbstract:\nIn this paper, we propose a simultaneous feature and dictionary learning (SFDL) method for image set-based face recognition, where each training and testing example contains a set of face images, which were captured from different variations of pose, illumination, expression, resolution, and motion. While a variety of feature learning and dictionary learning methods have been proposed in recent ye...\nTitle:\nSimultaneous Feature and Dictionary Learning for Image Set Based Face Recognition.\n\nAbstract:\nWe propose in this paper a novel ordinary preserving manifold analysis approach for human age estimation using face and gait features. Motivated by the fact that high-dimensional human facial images and gait sequences may reside in low-dimensional aging manifolds and two samples of face images or gait sequences with distinct age difference can provide different discriminative information for devising the low-dimensional aging manifold, we project the high-dimensional face or gait samples into a low-dimensional submanifold such that the samples with similar age values (i.e., smaller age difference) are projected to be as close as possible while those with dissimilar age values (i.e., larger age difference), as far as possible. To uncover the relation of the projected features and the ground-truth age values, we learn a multiple linear regression function with a quadratic model for age estimation. Experimental results on the MORPH face database and the USF gait database are presented to demonstrate the efficacy of our proposed approach.\nTitle:\nOrdinary preserving manifold analysis for human age estimation\n\nAbstract:\nMetric learning has attracted wide attention in face and kinship verification, and a number of such algorithms have been presented over the past few years. However, most existing metric learning methods learn only one Mahalanobis distance metric from a single feature representation for each face image and cannot make use of multiple feature representations directly. In many face-related tasks, we ...\nTitle:\nLocal Large-Margin Multi-Metric Learning for Face and Kinship Verification.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.311, mean value for all the writers is 8.648, which makes it 0.288 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose in this paper an ordinary preserving manifold analysis approach for human age and head pose estimation. While a large number of manifold learning algorithms have been proposed in the literature and some of them have been successfully applied to age/pose estimation, the ordinary characteristics of the age/pose information of samples have not been fully exploited to learn the low-dimensio...\nTitle:", "model_inf_time": 1.21}, {"id": "41385", "output": "RGMP-ROS: A Hybrid Real-Time ROS Architecture on Multi-Core Processors", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nROS, an open-source robot operating system, is widely used and rapidly developed in the robotics community. However, running on Linux, ROS does not provide real-time guarantees, while real-time tasks are required in many robot applications such as robot motion control. This paper for the first time presents a real-time ROS architecture called RT-RTOS on multi-core processors. RT-ROS provides an integrated real-time/non-real-time task execution environment so real-time and non-real-time ROS nodes can be separately run on a real-time OS and Linux, respectively, with different processor cores. In such a way, real-time tasks can be supported by real-time ROS nodes on a real-time OS, while non-real-time ROS nodes on Linux can provide other functions of ROS. Furthermore, high performance is achieved by executing real-time ROS nodes and non-real-time ROS nodes on different processor cores. We have implemented RT-ROS on a dual-core processor and conducted various experiments with real robot applications. The experimental results show that RT-ROS can effectively provide real-time support for the ROS platform with high performance by exploring the multi-core architecture.\nTitle:\nRT-ROS: A real-time ROS architecture on multi-core processors\n\nAbstract:\nRobot control systems are complex cyber-physical systems which are difficult to develop. In this paper, we present a formal model-based automatic code synthesis method which can generate executable C++ code running on the world-wide used Robot Operating System (ROS). The internal interaction behaviors of robot systems are modeled as a network of timed automata. The safety requirements and specifications related to the model are formalized as CTL formulas and verified by Uppaal. We design a code synthesis method to generate the executable C++ code from the verified model. Compared to existing code generators based on timed automata, our method supports more complex structures and advanced features such as timer and committed location, and provides the important abstraction and mapping of ROS instructions, which realize the seamless connection between the generated code and ROS. A case study of grasping a cup by a robot with seven degrees of freedom manipulator is conducted and the generated codes are successfully applied to a ROS development environment.\nTitle:\nFormal Modeling and Automatic Code Synthesis for Robot System\n\nAbstract:\nRobot Operating System(ROS) is widely used in the development of various service robots, which have high requirements in safety and reliability. Data distribution service(DDS) is deployed in ROS2 to implement the control of nodes and data distribution, which is very important for the operation of application on ROS2. In this paper, we focus on model abstraction, formal modeling and automatic verification for DDS in ROS2. A distributed model with interface parameters was established in PRISM. Properties such as security, liveness and priority of DDS in ROS2 were verified. The results show that the design satisfies these properties and high-priority messages can be sent and transmitted preferentially by the system. Due to the experiment result, we come up with a suggestion that the system will be better if its buffer can store six messages or more.\n\n\nTitle:\nFormal Analysis and Verification of DDS in ROS2.\n\nAbstract:\nSambot is a module robot system, with the advantages of self-assembly. A target robotic configuration can be organized by a group of Sambots. A novel motion planning method for Sambot configuration using model checking is presented in this paper. This hierarchical method contains two layers. The abstract logic layer is responsible for the discrete planning of Sambots configuration. The robot and the environment are all modeled as timed automata. System requirements are formalized as Computational Tree Logic (CTL) formulas. Model checking is applied on the system model. The verification result gives the optimal discrete plans for the configuration of Sambot. In physical layer, a sample-based planner generates the trajectory trace considering the dynamics of Sambot and the suggested high level plans. The experiment results illustrate the effectiveness of our approach.\nTitle:\nTimed automata based motion planning for a self-assembly robot system\n\nAbstract:\n\u2022Path planning control is a key technique for the self-assembly of swarm robots.\u2022A CVT based intelligent control algorithm is proposed for self-assembly path planning.\u2022Matlab simulations are performed to verify the effectiveness of the algorithm.\u2022The algorithm provides a promising way for self-assembly path planning of swarm robots.\nTitle:\nA centroidal Voronoi tessellation based intelligent control algorithm for the self-assembly path planning of swarm robots.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.4, mean value for all the writers is 8.648, which makes it 0.642 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nRecently, the open-source robot operating system (ROS) has been growing rapidly in the robotics community. However, the ROS runs on Linux, which does not provide timing guarantees for robot motion. This paper present a hybrid real-time ROS architecture on multi-core processor \u201cRGMP-ROS\u201d, which consists of two parts including the non-real-time subsystem \u201cGPOS (General Operating system)\u201d and the real-time one \u201cRTOS (Real-time Operating system)\u201d. The GPOS is comprised of non-real-time ROS nodes running in Linux, while the RTOS only contains real-time ROS nodes running in Nuttx. To get higher operational efficiency, the RGMP-ROS system is executed by a dual-core processor, one CPU for GPOS and the other for RTOS. The RGMP-ROS has used in the controller of a 6-DOF modular manipulator, and its effectiveness and efficiency are demonstrated by software testing and experiments. The main contributions of the present work lie in the realization of real-time ROS architecture and the application of multi-core processor in the hybrid control of an industrial robot.\nTitle:", "model_inf_time": 1.73}, {"id": "41386", "output": "Ordinal Regression for Improved Musical Concept Detection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nTo exploit the co-occurrence patterns of semantic concepts while keeping the simplicity of context fusion, a novel reranking approach is proposed in this paper. The approach, called ordinal reranking, adjusts the ranking of an initial search (or detection) list based on the co-occurrence patterns obtained by using ranking functions such as ListNet. Ranking functions are by nature more effective than classification-based reranking methods in mining ordinal relationships. In addition, the ordinal reranking is free of the ad hoc thresholding for noisy binary labels and requires no extra offline learning or training data. To select informative concepts for reranking, we also propose a new concept selection measurement, wc-tf-idf, which considers the underlying ordinal information of ranking lists and is thus more effective than the feature selection algorithms for classification. Being largely unsupervised, the reranking approach to context fusion can be applied equally well to concept detection and video search. While being extremely efficient, ordinal reranking outperforms existing methods by up to 40% in mean average precision (MAP) for the baseline text-based search and 12% for the baseline concept detection over TRECVID 2005 video search and concept detection benchmark.\nTitle:\nOnline reranking via ordinal informative concepts for context fusion in concept detection and video search\n\nAbstract:\nMusic auto-tagging refers to automatically assigning semantic labels (tags) such as genre, mood and instrument to music so as to facilitate text-based music retrieval. Although significant progress has been made in recent years, relatively little research has focused on semantic labels that are time-varying within a track. Existing approaches and datasets usually assume that different fragments of a track share the same tag labels, disregarding the tags that are time-varying (e.g., mood) or local in time (e.g., instrument solo). In this paper, we present a new dataset dedicated to time-varying music auto-tagging. The dataset, called CAL500exp, is an enriched version of the well-known CAL500 dataset used for conventional track-level tagging. Given the tag set of CAL500, eleven subjects with strong music background were recruited to annotate the time-varying tag labels. A new user interface for annotation is developed to reduce the subject's annotation effort yet increase the quality of labels. Moreover, we present an empirical evaluation that demonstrates the performance improvement CAL500exp brings about for time-varying music auto-tagging. By providing more accurate and consistent descriptions of music content in a finer granularity, CAL500exp may open new opportunities to understand and to model the temporal context of musical semantics.\nTitle:\nTowards time-varying music auto-tagging based on CAL500 expansion\n\nAbstract:\nIn light of the strong demands for semantic search over large-scale consumer photos, which generally lack reliable user-provided annotations, we investigate the feasibility and challenges entailed by the new paradigm, concept search - retrieving visual objects by large-scale automatic concept detectors with keywords. We investigate the problem in three folds: (1) the effective concept mapping and selection methods over large-scale concept ontology; (2) the quality and feasibility of the pre-trained concept detectors applying on cross-domain consumer data (i.e., Flickr photos); (3) the search quality by fusing automatic concepts and user-annotated data (tags). Through experiments over large-scale benchmarks, TRECVID and Flickr550, we confirm the effectiveness of concept search in the proposed framework, where the semantic mapping by web-based kernel function over Google snippets significantly outperforms conventional WordNet-like methods both in accuracy and efficiency.\nTitle:\nKeyword-based concept search on consumer photos by web-based kernel function\n\nAbstract:\nUnsupervised feature learning algorithms such as sparse coding and deep belief networks have been shown a viable alternative to hand-crafted feature design for music information retrieval. Nevertheless, such algorithms are usually computationally expensive. This paper investigates techniques to accelerate sparse feature extraction and music classification. To study the trade-off between computational efficiency and accuracy, we compare state-of-the-art, dense audio features with sparse features computed using 1) sparse coding with a random dictionary, 2) randomized clustering forest, and 3) an extension of randomized clustering forest to temporal signals. For classifier training and prediction, we compare support vector machines with linear or non-linear kernel functions. We conduct evaluation on music auto-tagging for 140 genre/style tags using a subset of 7,799 songs of the CAL10k data set. Our result leads to an 11-fold speed increase with 3.45% accuracy loss comparing to dense features. With the proposed sparse features, the feature extraction and auto-tagging operations can be finished in 1 second per song, with 0.1302 tagging accuracy in mean average precision.\nTitle:\nTowards real-time music auto-tagging using sparse features\n\nAbstract:\nSinging voice plays an important role in the listening experience of music. In this paper, we propose to classify popular music by the timbre quality of the singing voice. Specifically, we adopt six singing voice timbre classes as the taxonomy and build a new data set, KKTIC, that contains the expert annotations of 387 Chinese popular songs. To build an automatic classifier, we resort to signal processing and machine learning techniques and extract a number of singing voice-related features such as vibrato and harmonic-to-noise ratio. We also propose the use of vocal segment detection and singing voice separation as preprocessing steps. Our evaluation identifies the relevant acoustic features and validates the importance of these preprocessing steps. The accuracy in timbre classification reaches 79.84% in a five-fold stratified cross validation.\nTitle:\nSinging voice timbre classification of Chinese popular music\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.418, mean value for all the writers is 8.648, which makes it 0.657 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nTofacilitateinformationretrievaloflarge-scalemusicdata- bases, the detection of musical concepts, or auto-tagging, has been an active research topic. This paper concerns the use of concept correlations to improve musical concept de- tection. We propose to formulate concept detection as an ordinal regression problem to explicitly take advantage of the ordinal relationship between concepts and avoid the data imbalance problem of conventional multi-label clas- sification methods. To further improve the detection ac- curacy, we propose to leverage the co-occurrence patterns of concepts for context fusion and employ concept selec- tion to remove irrelevant or noisy concepts. Evaluation on the cal500 dataset shows that we are able to improve the detection accuracy of 174 concepts from 0.2513 to 0.2924.\nTitle:", "model_inf_time": 1.38}, {"id": "41387", "output": "Spatio-temporal Dynamics of Epileptic Brain Activity", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThere is evidence that the mechanisms leading to epileptic seizures can be understood by continuously tracking the ongoing spatio-temporal mappings in the brain. We propose to quantify the spatio-temporal dynamics by using a SOM-based similarity index (SI) measure. While it is shown that this measure is statistically as accurate as the original SI measure, it is also computationally faster and therefore applicable for real-time analyses. In order to quantify the changes of SI in electrode space and along time, a spectral clustering approach is employed by interpreting the SOM-SI values as affinity matrices. Preliminary analyses on spatial mappings of multivariate epileptic ECOG data are presented using the modified spectral clustering approach. Results involving two pairs of seizures of an epileptic patient suggest that patterns associated with channels' spatio-temporal dynamics during the inter-ictal to pre-post ictal transition vary from seizure to seizure.\nTitle:\nQuantifying spatio-temporal dependencies in epileptic ECOG\n\nAbstract:\nWe introduce adversarial neural networks for representation learning as a novel approach to transfer learning in brain-computer interfaces (BCIs). The proposed approach aims to learn subject-invariant representations by simultaneously training a conditional variational autoencoder (cVAE) and an adversarial network. We use shallow convolutional architectures to realize the cVAE, and the learned encoder is transferred to extract subject-invariant features from unseen BCI users\u2019 data for decoding. We demonstrate a proof-of-concept of our approach based on analyses of electroencephalographic (EEG) data recorded during a motor imagery BCI experiment.\nTitle:\nTransfer Learning in Brain-Computer Interfaces with Adversarial Variational Autoencoders\n\nAbstract:\nWe proposed a novel algorithm to extract connectivity information of neuronal arbors from 3D confocal images. The method is based on the use of ridge sampling and approximation using piecewise linear segments that conform to an inequality constraint that ensures shape accuracy. An automatic neuron structure reconstruction algorithm based on kernel interpolation of intensities using multiscale local Gaussian kernels has been utilized on a sample Olfactory neuron reconstruction problem successfully.\nTitle:\nA ridge scorewith application to piecewise linear neural reconstruction\n\nAbstract:\nPower spectrum density (PSD) of electroencephalogram (EEG) signals is a widely used feature for Brain Computer Interfaces (BCI). Usually, PSD features are integrated over different frequency bands, such as delta, theta, alpha, beta, gamma, which are based on well-established interpretations of EEG signals in prior experimental and clinical contexts. However, these predefined frequency bands do not necessarily relate to the optimal features for various BCI applications. In this paper, we propose an alternative feature dimensionality reduction method, which automatically determines the optimal number and the range of frequency bands. We applied the proposed method on EEG classification in the context of Augmented Cognition (AugCog) using BCI. The experimental results show that the proposed method can extract more robust features than features manually extracted from predefined frequency bands.\nTitle:\nAutomatic Frequency Bands Segmentation Using Statistical Similarity For Power Spectrum Density Based Brain Computer Interfaces\n\nAbstract:\nWe propose the use of nonnegative matrix factorization (NMF) as a model-independent methodology to analyze neural activity. We demonstrate that, using this technique, it is possible to identify local spatiotemporal patterns of neural activity in the form of sparse basis vectors. In addition, the sparseness of these bases can help infer correlations between cortical firing patterns and behavior. We demonstrate the utility of this approach using neural recordings collected in a brain-machine interface (BMI) setting. The results indicate that, using the NMF analysis, it is possible to improve the performance of BMI models through appropriate pruning of inputs.\nTitle:\nDetermining patterns in neural activity for reaching movements using nonnegative matrix factorization\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.855, mean value for all the writers is 8.648, which makes it 1.03 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAbnormal dynamical coupling between brain structures is believed to be primarily responsible for the generation of epileptic seizures and their propagation. In this study, we attempt to identify the spatio-temporal interactions of an epileptic brain using a previously proposed nonlinear dependency measure. Using a clustering model, we determine the average spatial mappings in an epileptic brain at different stages of a complex partial seizure. Results involving 8 seizures from 2 epileptic patients suggest that there may be a fixed pattern associated with regional spatio-temporal dynamics during the interictal to pre-post-ictal transition.\nTitle:", "model_inf_time": 1.25}, {"id": "41388", "output": "Quantum Query Complexity of Total Boolean Functions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present a number of results related to quantum algorithms with small error probability and quantum algorithms that are zero-error. First, we give a tight analysis of the trade-offs between the number of queries of quantum search algorithms, their error probability, the size of the search space, and the number of solutions in this space. Using this, we deduce new lower and upper bounds for quantum versions of amplification problems. Next, we establish nearly optimal quantum-classical separations for the query complexity of monotone functions in the zero-error model (where our quantum zero-error model is defined so as to be robust when the quantum gates are noisy). Also, we present a communication complexity problem related to a total function for which there is a quantum-classical communication complexity gap in the zero-error model. Finally, we prove separations for monotone graph properties in the zero-error and other error models which imply that the evasiveness conjecture for such properties does not hold for quantum computers\nTitle:\nBounds for Small-Error and Zero-Error Quantum Algorithms\n\nAbstract:\nWe define and study the complexity of robust polynomials for Boolean functions and the related fault-tolerant quantum decision trees, where input bits are perturbed by noise. We compare several different possible definitions. Our main results are: *For every n-bit Boolean function f there is an n-variate polynomial p of degree O(n) that robustly approximates it, in the sense that p(x) remains close to f(x) if we slightly vary each of the n inputs of the polynomial. *There is an O(n)-query quantum algorithm that robustly recovers n noisy input bits. Hence every n-bit function can be quantum computed with O(n) queries in the presence of noise. This contrasts with the classical model of Feige et al., where functions such as parity need \u0398(n log n) queries. We give several extensions and applications of these results.\nTitle:\nRobust Polynomials and Quantum Algorithms\n\nAbstract:\nWe compare classical and quantum query complexities of total Boolean functions. It is known that for worst-case complexity, the gap between quantum and classical can be at most polynomial [3]. We show that for average-case complexity under the uniform distribution, quantum algorithms can be exponentially faster than classical algorithms. Under non-uniform distributions the gap can even be super-exponential. We also prove some general bounds for average-case complexity and show that the average-case quantum complexity of MAJORITY under the uniform distribution is nearly quadratically better than the classical complexity.\nTitle:\nAverage-Case Quantum Query Complexity\n\nAbstract:\nWe study the complexity of quantum query algorithms that make  queries in parallel in each timestep. This model is in part motivated by the fact that decoherence times of qubits are typically small, so it makes sense to parallelize quantum algorithms as much as possible. We show tight bounds for a number of problems, specifically  -parallel queries for element distinctness and  for -sum. Our upper bounds are obtained by parallelized quantum walk algorithms, and our lower bounds are based on a relatively small modification of the adversary lower bound method, combined with recent results of Belovs et al. on learning graphs. We also prove some general bounds, in particular that quantum and classical -parallel query complexity are polynomially related for all total functions\u00a0 when  is small compared to \u2019s block sensitivity.\nTitle:\nOptimal Parallel Quantum Query Algorithms.\n\nAbstract:\nWe study the query complexity of computing a function f : {0, 1}(n) -> R+ in expectation. This requires the algorithm on input x to output a nonnegative random variable whose expectation equals f(x), using as few queries to the input x as possible. We exactly characterize both the randomized and the quantum query complexity by two polynomial degrees, the nonnegative literal degree and the sum-of-squares degree, respectively. We observe that the quantum complexity can be unboundedly smaller than the classical complexity for some functions, but can be at most polynomially smaller for Boolean functions. These query complexities relate to (and are motivated by) the extension complexity of polytopes. The linear extension complexity of a polytope is characterized by the randomized communication complexity of computing its slack matrix in expectation, and the semidefinite (psd) extension complexity is characterized by the analogous quantum model. Since query complexity can be used to upper bound communication complexity of related functions, we can derive some upper bounds on psd extension complexity by constructing efficient quantum query algorithms. As an example we give an exponentially-close entrywise approximation of the slack matrix of the perfect matching polytope with psd-rank only 2n(1/2+epsilon). Finally, we show randomized and quantum query complexity in expectation corresponds to the Sherali-Adams and Lasserre hierarchies, respectively.\nTitle:\nQuery Complexity in Expectation.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.38, mean value for all the writers is 8.648, which makes it 1.082 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe examine the number of queries to input variables that a quantum algorithm requires to compute Boolean functions on {0,1}N in the black-box model. We show that the exponential quantum speed-up obtained for partial functions (i.e., problems involving a promise on the input) by Deutsch and Jozsa, Simon, and Shor cannot be obtained for any total function: if a quantum algorithm computes some total Boolean function f with small error probability using T black-box queries, then there is a classical deterministic algorithm that computes f exactly with O(Ts6) queries. We also give asymptotically tight characterizations of T for all symmetric f in the exact, zero-error, and bounded-error settings. Finally, we give new precise bounds for AND, OR, and PARITY. Our results are a quantum extension of the so-called polynomial method, which has been successfully applied in classical complexity theory, and also a quantum extension of results by Nisan about a polynomial relationship between randomized and deterministic decision tree complexity.\nTitle:", "model_inf_time": 1.24}, {"id": "41389", "output": "Efficient Algorithms for One-String Mass Finding", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe investigate a problem which arises in computational biology: Given a constant-size alphabet A with a weight function \u03bc:A\u2192N, find an efficient data structure and query algorithm solving the following problem: For a string \u03c3 over A and a weight M\u2208N, decide whether \u03c3 contains a substring with weight M, where the weight of a string is the sum of the weights of its letters (ONE-STRING MASS FINDING PROBLEM). If the answer is yes, then we may in addition require a witness, i.e., indices i\u2a7dj such that the substring beginning at position i and ending at position j has weight M. We allow preprocessing of the string and measure efficiency in two parameters: storage space required for the preprocessed data and running time of the query algorithm for given M. We are interested in data structures and algorithms requiring subquadratic storage space and sublinear query time, where we measure the input size as the length n of the input string \u03c3. Among others, we present two non-trivial efficient algorithms: LOOKUP solves the problem with O(n) storage space and O(n/logn) time; INTERVAL solves the problem for binary alphabets with O(n) storage space in O(logn) query time. We introduce other variants of the problem and sketch how our algorithms may be extended for these variants. Finally, we discuss combinatorial properties of weighted strings.\nTitle:\nAlgorithmic complexity of protein identification: combinatorics of weighted strings\n\nAbstract:\nAn arrangement of n lines (or line segments) in the plane is the partition of the plane defined by these objects. Such an arrangement consists of &Ogr;(n2) regions, called faces. In this paper we study the problem of calculating and storing arrangements implicitly, using subquadratic space and preprocessing, so that, given any query point p, we can calculate efficiently the face containing p. First, we consider the case of lines and show that with &Lgr;(n) space1 and &Lgr;(n3/2) preprocessing time, we can answer face queries in &Lgr;(\u221an) + &Ogr;(K) time, where K is the output size. (The query time is achieved with high probability.) In the process, we solve three interesting subproblems: 1) given a set of n points, find a straight-edge spanning tree of these points such that any line intersects only a few edges of the tree, 2) given a simple polygonal path &Ggr;, form a data structure from which we can find the convex hull of any subpath of &Ggr; quickly, and 3) given a set of points, organize them so that the convex hull of their subset lying above a query line can be found quickly. Second, using random sampling, we give a trade-off between increasing space and decreasing query time. Third, we extend our structure to report faces in an arrangement of line segments in &Lgr;(n1/3) time, given &Lgr;(n4/3) space and &Lgr;(n5/3) preprocessing time. Lastly, we note that our techniques allow us to compute m faces in an arrangement of n lines in time &Lgr;(m2/3n2/3 + n), which is nearly optimal.\nTitle:\nImplicitly representing arrangements of lines or segments\n\nAbstract:\nThis paper presents quasi-optimal upper bounds for simplex range searching. The problem is to preprocess a setP ofn points in \u211cd so that, given any query simplexq, the points inP \u2229q can be counted or reported efficiently. Ifm units of storage are available (n m n\n \n d\n ), then we show that it is possible to answer any query inO(n\n 1+\u025b/m\n 1/d\n ) query time afterO(m\n 1+\u025b) preprocessing. This bound, which holds on a RAM or a pointer machine, is almost tight. We also show how to achieveO(logn) query time at the expense ofO(n\n \n d+\u025b) storage for any fixed \u025b > 0. To fine-tune our results in the reporting case we also establish new zone theorems for arrangements\n and merged arrangements of planes in 3-space, which are of independent interest.\nTitle:\nQuasi-Optimal Upper Bounds for Simplex Range Searching and New Zone Theorems\n\nAbstract:\nWe present a simple randomized algorithm which solves linear programs with n constraints and d variables in expected O(d32dn) time. The expectation is over the internal randomizations performed by the algorithm, and holds for any input.\nTitle:\nA Combinatorial Bound for Linear Programming and Related Problems\n\nAbstract:\nThe range-searching problems that allow efficient partition trees are characterized as those defined by range spaces of finite Vapnik-Chervonenkis dimension. More generally, these problems are shown to be the only ones that admit linear-size solutions with sublinear query time in the arithmetic model. The proof rests on a characterization of spanning trees with a low stabbing number. We use probabilistic arguments to treat the general case, but we are able to use geometric techniques to handle the most common range-searching problems, such as simplex and spherical range search. We prove that any set ofn points inEd admits a spanning tree which cannot be cut by any hyperplane (or hypersphere) through more than roughlyn1\u22121/d edges. This result yields quasi-optimal solutions to simplex range searching in the arithmetic model of computation. We also look at polygon, disk, and tetrahedron range searching on a random access machine. Givenn points inE2, we derive a data structure of sizeO(n logn) for counting how many points fall inside a query convexk-gon (for arbitrary values ofk). The query time isO(\u221akn logn). Ifk is fixed once and for all (as in triangular range searching), then the storage requirement drops toO(n). We also describe anO(n logn)-size data structure for counting how many points fall inside a query circle inO(\u221an log2n) query time. Finally, we present anO(n logn)-size data structure for counting how many points fall inside a query tetrahedron in 3-space inO(n2/3 log2n) query time. All the algorithms are optimal within polylogarithmic factors. In all cases, the preprocessing can be done in polynomial time. Furthermore, the algorithms can also handle reporting within the same complexity (adding the size of the output as a linear term to the query time).\nTitle:\nQuasi-optimal range searching in spaces of finite VC-dimension\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.767, mean value for all the writers is 8.648, which makes it 0.752 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe investigate a problem which arises in computational biology: Given a constant{size alphabet A with a weight function : A ! , nd an ecien t data structure and query algorithm solving the following problem: For a string over A and a weight M 2 , decide whether contains a substring with weight M (One{String Mass Finding Problem). If the answer is yes, then we may in addition require a witness, i.e., indices i j such that the substring beginning at position i and ending at position j has weight M. We allow preprocessing of the string, and measure eciency in two parameters: storage space required for the preprocessed data, and running time of the query algorithm for given M. We are interested in data structures and algorithms requiring subquadratic storage space and sublinear query time, where we mea- sure the input size as the length of the input string. Among others, we present two non{trivial ecien t algorithms: Lookup solves the prob- lem with O(n) space and O( n log n log log n) time; Interval solves the problem for binary alphabets with O(n) storage space in O(log n) query time. Finally, we introduce other variants of the problem and sketch how our algorithms may be extended for these variants.\nTitle:", "model_inf_time": 1.57}, {"id": "41390", "output": "Primal-Dual Approximation Algorithms for Facility Location with Service Installation Costs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn the capacitated facility location problem with hard capacities, we are given a set of facilities, $${\\mathcal{F}}$$, and a set of clients $${\\mathcal{D}}$$ in a common metric space. Each facility i has a facility opening cost f i and capacity u i that specifies the maximum number of clients that may be assigned to this facility. We want to open some facilities from the set $${\\mathcal{F}}$$ and assign each client to an open facility so that at most u i clients are assigned to any open facility i. The cost of assigning client j to facility i is given by the distance c ij , and our goal is to minimize the sum of the facility opening costs and the client assignment costs. The only known approximation algorithms that deliver solutions within a constant factor of optimal for this NP-hard problem are based on local search techniques. It is an open problem to devise an approximation algorithm for this problem based on a linear programming lower bound (or indeed, to prove a constant integrality gap for any LP relaxation). We make progress on this question by giving a 5-approximation algorithm for the special case in which all of the facility costs are equal, by rounding the optimal solution to the standard LP relaxation. One notable aspect of our algorithm is that it relies on partitioning the input into a collection of single-demand capacitated facility location problems, approximately solving them, and then combining these solutions in a natural way.\nTitle:\nLP-based Approximation Algorithms for Capacitated Facility Location\n\nAbstract:\nWe consider the uncapacitated facility location problem. In this problem, there is a set of locations at which facilities can be built; a fixed cost fi is incurred if a facility is opened at location i. Furthermore, there is a set of demand locations to be serviced by the opened facilities; if the demand location j is assigned to a facility at location i, then there is an associated service cost proportional to the distance between i and j, cij. The objective is to determine which facilities to open and an assignment of demand points to the opened facilities, so as to minimize the total cost. We assume that the distance function c is symmetric and satisfies the triangle inequality. For this problem we obtain a (1+2/e)-approximation algorithm, where $1+2/e \\approx 1.736$, which is a significant improvement on the previously known approximation guarantees.The algorithm works by rounding an optimal fractional solution to a linear programming relaxation. Our techniques use properties of optimal solutions to the linear program, randomized rounding, as well as a generalization of the decomposition techniques of Shmoys, Tardos, and Aardal [Proceedings of the 29th ACM Symposium on Theory of Computing, El Paso, TX, 1997, pp. 265--274].\nTitle:\nImproved Approximation Algorithms for the Uncapacitated Facility Location Problem\n\nAbstract:\nThe location-routing problem arises in the context of providing integrated support for logistics in a number of transportation settings, where given a set of requests and potential depot locations, one must simultaneously decide where to locate depots as well as how to route tours so that all requests are connected to an open depot. This problem can be formulated either with specific costs incurred for choosing to open each depot, or with an upper bound k on the number of open depots, which we call the k-location-routing problem. We develop a primal-dual schema and use Lagrangian relaxation to provide a 2-approximation algorithm for the k-location-routing problem; no constant performance guarantee was known previously for this problem. This strengthens previous work of Goemans & Williamson who gave a 2-approximation algorithm for the variant in which there are opening costs, but no limit on the number of depots. We give a new primal-dual algorithm and a strengthened analysis that proves a so-called Lagrangianpreserving performance guarantee. In contrast to the results of Jain & Vazirani for the uncapacitated facility location and k-median problems, our results have the surprising property that our performance guarantee for the k-location-routing problem matches the guarantee for the version in which there are depot opening costs; furthermore, this relies on a simple structural property of the algorithm that allows us to identify the critical Lagrangian value for the opening cost with a single execution of the primal-dual algorithm, rather than invoking a bisection search.\nTitle:\nPrimal-dual schema and lagrangian relaxation for the k-location-routing problem\n\nAbstract:\nWe consider the following single-machine scheduling problem, which is often denoted 1||\u03a3fj: we are given n jobs to be scheduled on a single machine, where each job j has an integral processing time pj, and there is a nondecreasing, nonnegative cost function fj (Cj) that specifies the cost of finishing j at time Cj; the objective is to minimize n \u03a3j=1n fj (Cj). Bansal & Pruhs recently gave the first constant approximation algorithm and we improve on their 16-approximation algorithm, by giving a primal-dual pseudo-polynomial-time algorithm that finds a solution of cost at most twice the optimal cost, and then show how this can be extended to yield, for any \u03b5 0, a (2 + \u03b5)-approximation algorithm for this problem. Furthermore, we generalize this result to allow the machine's speed to vary over time arbitrarily, for which no previous constant-factor approximation algorithm was known.\nTitle:\nA primal-dual approximation algorithm for min-sum single-machine scheduling problems\n\nAbstract:\nThis work gives new insight into two well-known approximation algorithms for the uncapacitated facility location problem: the primal-dual algorithm of Jain & Vazirani, and an algorithm of Mettu & Plaxton. Our main result answers positively a question posed by Jain & Vazirani of whether their algorithm can be modified to attain a desired \"continuity\" property. This yields an upper bound of 3 on the integrality gap of the natural LP relaxation of the k-median problem, but our approach does not yield a polynomial time algorithm with this guarantee. We also give a new simple proof of the performance guarantee of the Mettu-Plaxton algorithm using LP duality, which suggests a minor modification of the algorithm that makes it Lagrangian-multiplier preserving.\nTitle:\nLagrangian Relaxation for the k-Median Problem: New Insights and Continuity Properties\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.656, mean value for all the writers is 8.648, which makes it 0.007 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe consider a generalization of the uncapacitated facility location problem which we call Facility Location with Service Installation Costs. We are given a set of facilities, F,a set of demands or clients D, and a set of services S. Each facility i has a facility opening cost fi, and we have a service installation cost of fli for every facility-service pair (i, l). Each client j in D requests a specific service g(j) \u2208 S and the cost of assigning a client j to facility i is given by cij. We want to open a set of facilities, install services at the open facilities, and assign each client j to an open facility at which service g(j) is installed, so as to minimize the sum of the facility opening costs, the service installation costs and the client assignment costs.Our main result is a primal-dual 6-approximation algorithm under the assumption that there is an ordering on the facilities such that if i comes before i' in this ordering then for every service type l, fli \u2264 fl i. This includes (as special cases) the settings where the service installation cost fli depends only on the service type l, or depends only on the location i. With arbitrary service installation costs, the problem becomes as hard as the set-cover problem. Our algorithm extends the algorithm of Jain & Vazirani [9] in a novel way. If the service installation cost depends only on the service type and not on the location, we give an LP rounding algorithm that attains an improved approximation ratio of 2.391. The algorithm combines both clustered randomized rounding [6] and the filtering based technique of [10, 14]. We also consider the k-median version of the problem where there is an additional requirement that at most k facilities may be opened. We use our primal-dual algorithm to give a constant-factor approximation for this problem when the service installation cost depends only on the service type.\nTitle:", "model_inf_time": 1.77}, {"id": "41391", "output": "A Customizable Simulation Environment for Testing Real-Time Specifications", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper describes MTSim, an extensible, customizable simulation platform for the Modechart toolset (MT). MTSim provides support for \u9a74plugging in\u9a74 user-defined viewers useful in simulating system behavior in different ways, including application-specific ways. MTSim also supports full user participation in the generation of simulations by allowing users to inject events into the execution trace. Moreover, MTSim provides monitoring and assertion checking of execution traces and the invocation of user-specified handlers upon assertion violation. This paper also introduces an MTSim component called WebSim, a suite of simulation tools for MT, and an application-specific component of MTSim which displays the cockpit of an F-18 aircraft and which responds to user inputs to model a bomb release function.\nTitle:\nA Flexible, Extensible Simulation Environment for Testing Real-Time Specifications\n\nAbstract:\nThe Salamander distribution system is a wide-area network datadissemination substrate that has been used daily for over a year byseveral groupware and webcasting Internet applications.Specifically, Salamander is designed to support push-basedapplications using attribute-based routing. This support provides avariety of delivery semantics ranging from collaborative groupcommunication, used by the Upper Atmospheric Research Collaboratory(UARC) project, to basic data delivery, used by the InternetPerformance Measurement and Analysis (IPMA) project. The substrateis designed to accommodate the large variation in Internetconnectivity and client resources through the use ofapplication-specific plug-in modules. This paper illustrates thearchitecture and design of the Salamander system driven by theneeds of its set of current applications. The main architecturalfeatures described include&colon; the attribute-based datarouting mechanism, persistent data queries, negotiatedpush-technology, support for Application-level Quality of Servicepolicies, and a lightweight temporal database.\nTitle:\nAttribute-based data dissemination for Internet applications\n\nAbstract:\nAs software control of time-critical functions in embedded systems becomes more common, a means for the precise specification of their behavior and formal methods for analyzing system requirements become increasingly important. Modechart is a graphical specification language introduced to meet this need. The main focus of this paper is on methods and supporting tools for representing and reasoning about properties of time-critical systems specified in Modechart. The paper describes a verification methodology which takes advantage of the structuring inherent in a Modechart specification to determine whether a system specification satisfies the required properties. The paper also describes the implementation of a mechanical verifier, based on the proposed approach, which has been recently integrated as part of the Modechart Toolset prototype development environment from the Naval Research Lab [7].\nTitle:\nA Methodology And Support Tools For Analysis Of Real-Time Specifications\n\nAbstract:\nWe propose a lightweight fault tolerant multicast and membership service for real time process groups which may exchange periodic and aperiodic messages. The service supports bounded time message transport, atomicity, and order for multicasts within a group of communicating processes in the presence of processor crashes and communication failures. It guarantees agreement on membership among the communicating processors, and ensures that membership changes (e.g., resulting from processor joins or departures) are atomic and ordered with respect to multicast messages. We provide the flexibility of an event triggered approach with the fast message delivery time of time triggered protocols, such as TTP (H. Kopetz and G. Grunstidl, 1994), where messages are delivered to the application immediately upon reception. This is achieved without compromising agreement order and atomicity properties. In addition to the design and details of the algorithm, we describe our implementation of the protocol using the x-Kernel protocol architecture running on RT Mach 3.0.\nTitle:\nRTCAST: lightweight multicast for real-time process groups\n\nAbstract:\nAs software control of time-critical functions in embedded systemsbecomes more common, a means for the precise specification of their behavior becomes increasingly important. Modechart is a graphical specification language introduced to meet this need. This paper presents a method for verifying properties of systems specified in Modechart. The proposed approach makes use of a computation graph which takes advantage of the structuring inherent in a Modechart specification. Two classes of properties are presented for which decision procedures are developed.\nTitle:\nA Method for Verifying Properties of Modechart Specifications\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.207, mean value for all the writers is 8.648, which makes it 0.376 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe testtng and valadatzon of formal speczficatzons of hzgh-assurance real-tzme systems repiizres an erten- szble szniulatzon enrwonment wzth support for users to generate events, customzze dzsplays and monz- tor cwnt traces Thzs paper descrzbes MTSzm an customzzable szmulation platform for the Modechart Toolset (MT) MTSzm prozlzdes support for \"pluggzng zn\" user-defined vzeu'ers as well as user partactpataon in the generatzon of szmiilatzoiis by allowtng users to znject events znto fhe erecutton trace Moreover, MT- Szm provzdes rnonitorzng and assertion checkzng of ea- ecutzon traces and the zntiocatzon of user-speczfied han- dlers upon assertzon vzolatzon Thzs paper also zntro- duces a MTSzm component called WebSzm. a suzte of simulation iools for MT. nnd nn applzcntzon-speczfic component of MTSzm, whzch dzsplays fhe cockpzt of an F-I8 azrcraft and models zts bomb release functzon\nTitle:", "model_inf_time": 1.39}, {"id": "41392", "output": "A Bump-and-Refit Approach to Incremental Routing for ECO Applications in FPGAs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIncremental physical CAD is encountered frequently in the so-calledengineering change order (ECO) process in which design changes aremade typically late in the design process in order to correctlogical and/or technological problems in the circuit. Incrementalrouting is a significant part of an incremental physical designmethodology. Typically after an ECO process, a small portion of thecircuit netlist is changed, and in order to capitalize on theenormous resources and time already spent on routing the circuit itis desirable to reroute only the ECO-affected portion of thecircuit, while minimizing any routing changes in the much largerunaffected part. Incremental rerouting also needs to be fast and toeffectively use available routing resources. In this article, wedevelop a complete incremental routing methodology for FPGAs usinga novel approach called bump and refit (B&R). The basic B&Ridea (which was originally proposed in Dutt et al. [1999] in themuch simpler context of extending some nets by a segment for thepurpose of fault tolerance) in our algorithms is to rearrange someportions of some existing nets on other tracks within their currentchannels in order to find valid routings for the new/modified netswithout requiring any extra routing resources and with littleeffect on the electrical properties of existing nets. Here wesignificantly extend the B&R concept to global and detailedincremental routing for FPGAs with complex switchboxes (SBox's)such as those in Lucent's ORCA and Xilinx's Virtex series. Weintroduce new concepts such as a B&R cost in global routing andthe optimal subnet set to relocate for each bumped net (determinedusing an efficient dynamic programming formulation). We developedoptimal and near-optimal algorithms (called Subsec_B&R andSubnet_B&R, respectively) to find incremental routing solutionsusing the B&R paradigm in complex FPGAs (e.g.,Lucent's ORCA FPGA) withi-to-j SBox's, as well as anoptimal version Fullnet_B&R for the VPR architecture from theUniversity of Toronto using the simpleri-to-i SBox's. Wecompared our algorithms (simply called B&R when no distinctionneeds to be made between our versions) to two recent incrementalrouting techniques, Standard (Std) and Rip-up&Reroute(R&R), and to Lucent's A_PAR routing tool and the University ofToronto's VPR router used in complete rerouting modes. Experimentalresults for the ORCA show that B&R is 10 to 20 times fasterthan complete rerouting using A_PAR, and that B&R is alsonearly 27&percnt; faster and yields new nets with nearly10&percnt; smaller lengths compared to previous incrementalrouters. Furthermore, B&R routers do not change either thelengths or topologies of existing nets, a significant advantage inECO applications, in contrast to R&R which increases the lengthof ripped-up nets by an average of 8.75 to 13.6&percnt;.Experimental results for the VPR architecture are dominated by thesignificantly larger (in many cases, orders of magnitude more)number of nets left unrouted by Std and R&R compared toB&R, which highlights the much greater efficacy ofB&R-based incremental routing. However, B&R issignificantly slower than the other two incremental routers,although on an absolute scale it is quite fast for two of fourcases we simulated; in one case, it is about 25 times faster thanVPR used in the full rerouting mode. The relative slowness ofB&R for the VPR architecture arises from the fact that we usedi-to-i SBox's whichforces each net to be routed on the same track, thus causingsignificantly more bumpings and searches for rearranged solutionscompared to i-to-jSBox's where a net can be routed on differentinterconnected tracks to minimize the amount of bumpings (as we didfor the ORCA). Since modern FPGAs generally have the latter type ofSBox's, B&R would be fast as well as very effective onthem.\nTitle:\nA search-based bump-and-refit approach to incremental routing for ECO applications in FPGAs\n\nAbstract:\nIn the engineering change order (ECO) process, engineers make changes to VLSI circuits after their layouts are completed in order to correct electrical problems or design errors. As far as routing is concerned, in order to capitalize on the enormous resources and time already spent on routing the circuit, and to meet time-to-market requirements, it is desirable to re-route only the ECO-affected portion of the circuit, while minimizing any routing changes in the larger unaffected part of the circuit in order to preserve its electrical properties. In this paper, we develop a novel algorithm to find incremental routing solutions using a gridless framework for VLSI circuits that require variable width and variable spacing on interconnects. The basic idea in our algorithm is to route the new or ECO-modified nets by minimally re-arranging, if necessary, some portions of some existing nets using a novel DFS controlled process that does not allow the perturbed existing nets' lengths and topologies to change beyond pre-set limits. With these constraints, it explores a number of low-cost ways of re-routing the portions of these nets within the available routing resources (2 metal layers only). Experimental results show that within the above constraints our incremental router succeeds in routing more than 98% of ECO-generated nets, and also that its failure rate is 5 to 12 and 2.4 to 9 times less than that of previous incremental routing techniques standard (Std) and Rip-up&Reroute (R&R), respectively. It is also able to route most of the wide nets using a reasonable number of vias and with near-minimal net lengths.\nTitle:\nA depth-first-search controlled gridless incremental routing algorithm for VLSI circuits\n\nAbstract:\nIn current very deep submicron (VDSM) circuits, incremental routing is crucial to incorporating engineering change orders (ECOs) late in the design cycle. In this paper, we address the important incremental routing objective of satisfying timing constraints in high-speed designs while minimizing wirelength, vias and routing layers. We develop an effective timing-driven (TD) incremental routing algorithm TIDE for ASIC circuits that addresses the dual goals of time-efficiency, and slack satisfaction coupled with effective optimizations. There are three main novelties in our approach: (i) a technique for locally determining slack satisfaction of the entire routing tree when either a new pin is added to the tree or an interconnect in it is re-routed - this technique is used in both the global and detailed routing phases; (ii) an interval-intersection and tree-truncation algorithm, used in global routing, for quickly determining a near-minimum-length slack-satisfying interconnection of a pin to a partial routing tree; (iii) a depth-first-search process, used in detailed routing, that allows new nets to bump and re-route existing nets in a controlled manner in order to obtain better optimized designs. Experimental results show that within the constraint of routing all nets in only two metal layers, TIDE succeeds in routing more than 94% of ECO-generated nets, and also that its failure rate is 7 and 6.7 times less than that of the TD versions of previous incremental routers Standard (Std) and Ripup & Reroute (R & R), respectively. It is also able to route nets with very little (3.4%) slack violations, while the other two methods have appreciable slack violations (16-19%). TIDE is about 2 times slower than the simple TD-Std method, but more than 3 times faster than TD-R&R\nTitle:\nEfficient Timing-Driven Incremental Routing for VLSI Circuits Using DFS and Localized Slack-Satisfaction Computations\n\nAbstract:\nWe have developed a hop-based complete detailed router ROAD-HOP that uses the Bump & Refit (B&R) approach to route a FPGA circuit in a near-optimal manner. This approach is based on generating a minimum-spanning tree (MST) from the complete pin-to-pin graph of each net with each edge cost based on a combination of its contribution to the net length, channel congestion and potential average \"bumping\" cost in the channels in which the edge lies. Using the MST, a hop-based routing of each net is performed that attempts to minimize the combination of net length, number of hops and total number of tracks needed in the FPGA. Given each net's global route, a FPGA detailed router can minimize net delays by minimizing the number of hops or equivalently the number of track switchings in complex switchboxes of current FPGAs---hop-based routing can model routing using complex switchboxes. By minimizing the number of hops and total net length, ROAD-HOP minimizes net delay. Note that ROAD-HOP can only be compared to another detailed router and we compare it to the best previous detailed router SEGA for the VPR architecture. We use the output of the VPR global router as input to both ROAD-HOP and SEGA. Our new algorithm achieves significantly better results than SEGA with respect to the number of tracks needed and the circuit speed. Across a number of benchmark circuits, our algorithm needs about 8% fewer tracks than SEGA. Furthermore, the average net delay of the routing generated by our algorithm is 34% less than that of SEGA, and is 52% less for the longest net.\nTitle:\nAn effective hop-based detailed router for FPGAs for optimizing track usage and circuit performance\n\nAbstract:\nIt is well known that the solution quality of the detailed routingphase is heavily influenced by the order in which nets arerouted.To alleviate this situation a number of routing strategies havebeendeveloped that rip up and reroute (R&R) previously-routed netsthat \"block\" the current net. In the R&R approach, there is nota significant amount of control over the solution quality (e.g.,length, delay) for the ripped-up nets. In this paper we propose adetailed router ROAD (bump & Refit based OptimAl Detailedrouter) that explores the solution space using an approach calledbump-and-refit (B&R) in which the global routes of prior-routednets are not changed but their track assignments are systematicallyaltered in order to make space for the current net being routed.B&R thus does not have the above drawback of R&R. We startwith an initial depth-first search method for this purpose that isoptimal in finding a detailed routing solution with the minimumnumber of tracks irrespective of the net routing order. We thendevelop various optimality-preserving speedup methods includingsearch space pruning based on clique detection and learning aboutand remembering unsuccessful search spaces, and second-level orlookahead transition costs. The combination of these methodsresults in an average speedup of 604 for small to medium VPRcircuits and an extrapolated speedup of more than 5763 for largercircuits. Furthermore, comparison of ROAD run times to that ofVPR's estimated detailed routing phase show that we are almost twotimes faster than VPR. This is noteworthy because an optimaldetailed router is able to obtain solutions in reasonable timeswhich are also faster than those of a non-optimal (thougheffective) router.\nTitle:\nROAD: An Order-Impervious Optimal Detailed Router for FPGAs\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.102, mean value for all the writers is 8.648, which makes it 1.241 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIncremental physical CAD is encountered frequently in the so-called engineering change order (ECO) process in which design changes are made typically late in the design process in order to correct logical and/or technological problems in the circuit. As far as routing is concerned, in order to capitalize on the enormous resources and time already spent on routing the circuit, and to meet time-to-market requirements, it is desirable to re-route only the ECO-affected portion of the circuit, while minimizing any routing changes in the much larger unaffected part of the circuit. Incremental re-routing also needs to be fast and to effectively use available routing resources. In this paper, we develop a complete incremental routing methodology for FPGAs using a novel approach called bump and refit (B&R); B&R was initially proposed in [4] in the much simpler context of extending some nets by a segment (for the purpose of fault tolerance) for FPGAs with simple i-to-i switchboxes. Here we significantly extend this concept to global and detailed incremental routing for FPGAs with complex switchboxes such as those in Lucent's ORCA and Xilinx's Virtex series. We also introduce new concepts such as B&R cost estimation during global routing, and determination of the optimal subnet set to bump for each bumped net, which we obtain using an efficient dynamic programming formulation. The basic B&R idea in our algorithms is to re-arrange some portions of some existing nets on other tracks within their current channels to find valid routings for the incrementally changed circuit without requiring any extra routing resources (i.e., completely unused tracks), and with little effect on the electrical properties of existing nets.We have developed optimal and near-optimal algorithms (called Subsec_B&R and Subnet_B&R, respectively) to find incremental routing solutions using the B&R paradigm in complex FPGAs. We implemented these algorithms for Lucent's ORCA-2C FPGA, and compared our algorithms to two recent incremental routing techniques, Standard and Rip-up&Reroute, and to Lucent's A_PAR routing tool. Experimental results show that our incremental routers perform very well for ECO applications. Firstly, B&R is 10 to 20 times faster than complete re-routing using A_PAR. Further, the B&R incremental routers are nearly 27% faster and the new nets have nearly 10% smaller lengths than in previous incremental techniques. Also, the B&R routers do not change either the lengths or topologies of existing nets, a significant advantage in ECO applications, in contrast to Rip-up&Reroute which increases the length of ripped up nets by an average of 8.75% to 13.6%.\nTitle:", "model_inf_time": 3.01}, {"id": "41393", "output": "WCP-OFDM with DFT Precoding for Time-Frequency Selective Channels", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn order to reduce the Peak-to-Average Power Ratio (PAPR) of multicarrier signals, a DFT precoding technique can be particularly useful. In this paper we propose to combine such a DFT precoding with the OFDM/OQAM modulation to provide a new frequency access scheme, that we call in short DFT-OQAMA. This DFT-OQAMA technique is compared to the Single-Carrier Frequency Division Multiplex Access (SC-FDMA) recently proposed for the UpLink (UL) of the 3GPP LTE system.\nTitle:\nDFT-OQAMA: An Alternative Multiple Access for Future Mobile Networks\n\nAbstract:\nThe oversampled OFDM modulation is a possible alternative to conventional OFDM for the transmission of signals over multipath fading channels. Indeed with oversampled OFDM, appropriate pulse-shaping can be introduced to fight against time and frequency dispersion. In this paper, we propose a theoretical and experimental analysis of the peak-to-average power ratio (PAPR) of oversampled OFDM. Our analysis illustrates the impact of the oversampling ratio and of the pulse shape on the PAPR distribution\nTitle:\nAnalysis of the Peak-To-Average Power Ratio of the Oversampled Ofdm\n\nAbstract:\nWe propose an alternative to the well-known multicarrier code-division multiple access (MC-CDMA) technique for downlink transmission by replacing the conventional cyclic-prefix orthogonal frequency division multiplexing (OFDM) modulation by an advanced filterbank-based multicarrier system (OFDM/OQAM). Indeed, on one hand, MC-CDMA has already proved its ability to fight against frequency-selective channels thanks to the use of the OFDM modulation and its high flexibility in multiple access thanks to the CDMA component. On the other hand, OFDM/OQAM modulation confers a theoretically optimal spectral efficiency as it operates without guard interval. However, its orthogonality is limited to the real field. In this paper, we propose an orthogonally multiplex quadrature amplitude modulation (OQAM-) CDMA combination that permits a perfect reconstruction of the complex symbols transmitted over a distortion-free channel. The validity and efficiency of our theoretical scheme are illustrated by means of a comparison, using realistic channel models, with conventional MC-CDMA and also with an OQAM-CDMA combination conveying real symbols.\nTitle:\nCDMA transmission with complex OFDM/OQAM\n\nAbstract:\nMulti-carrier Modulation (MCM) and especially Orthogonal Frequency Division Multiplexing (OFDM) are currently used in many radio transmission standards. Among these modulations, OFDMOQAM is an interesting alternative to OFDM. However, a simple one-tap Zero Forcing (ZF) equalization of OFDM/OQAM modulation may cause a performance floor problem, while the transmission meets a requirement of high order constellations. Therefore, more complex equalization needs to be considered, such as Equalization with Interference Cancellation (EIC) technique, which has been presented to counteract the performance floor problem. In this paper, we propose a system design based on OFDM/OQAM modulation with Cyclic Prefix (CP) to also counteract the performance floor problem and keep simple equalization at receiver. The simulation results illustrate the efficiency of this system.\nTitle:\nA New Transceiver System For The Ofdm/Oqam Modulation With Cyclic Prefix\n\nAbstract:\nWe propose an alternative to the well-known MC-CDMA technique for downlink transmission by replacing the conventional cyclic-prefix OFDM modulation by an advanced filterbank-based multicarrier system (OFDM/OQAM). Indeed, in one hand, MC-CDMA has already proved its ability to fight against frequency selective channels thanks to the use of the OFDM modulation and its high flexibility in multiple access thanks to the CDMA component. In the other hand, OFDM/OQAM modulation confers a theoretically optimal spectral efficiency as it operates without, guard interval. In this paper, we study OFDM/OQAM with CDMA combination and we compare it to the conventional MC-CDMA scheme in terms of Bit Error Rate (BER).\nTitle:\nOFDM/OQAM for Spread-Spectrum Transmission\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.98, mean value for all the writers is 8.648, which makes it 1.136 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we present a weighted cyclic prefix orthogonal frequency-division multiplexing (WCP-OFDM) transceiver as a generalization of traditional cyclic prefix (CP)-OFDM. In time-variant channels, this multicarrier transmission scheme may mitigate inter-channel interference (ICI) thanks to the use of non-rectangular pulse shapes. A precoding step may be required in order to reduce the peak-to-average power ratio (PAPR) at the transmitter output. For instance, a discrete Fourier transform (DFT) precoder leads to a single carrier transmission scheme with frequency domain equalization. We analyze the consequences of such a precoding, in terms of performances, in the context of a time-frequency selective channel.\nTitle:", "model_inf_time": 1.81}, {"id": "41394", "output": "Temporal Transcoding from H.264/AVC to SVC with GOP Size Analysis", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nVideo contents needs to be compressed in order to reduce the resources required for storage and network transmission. It is desirable that the encoded bitstreams are adaptable to the varying characteristics of consumer electronics devices and heterogeneous networks. Scalable Video Coding provides temporal, spatial and quality scalability by organizing the encoded bitstream into layers. Since the majority of the produced video content is currently encoded using H.264/AVC, it will be necessary to implement techniques for converting from single-layer H.264/AVC to scalable bitstreams. In this paper, a technique for transcoding from H.264/AVC-to-SVC with temporal scalability in Baseline Profile is discussed. Applying the presented approach, a reduction of 64% of the coding complexity is achieved while maintaining the coding efficiency.\nTitle:\nMotion-based temporal transcoding from H.264/AVC-to-SVC in baseline profile\n\nAbstract:\nThe scalable extension (SVC) of H.264/AVC uses a notion of layers within the encoded bitstream for providing temporal, spatial and quality scalability, separately or combined This scalability allows adaptation depending on the scenarios with different devices and heterogeneous networks The SVC design requires scalability to be provided at the encoder side by exploiting inter-layer dependencies during encoding This implies that existing H.264/AVC content cannot benefit from the scalability tools in SVC due to the lack of intrinsic scalability provided in the bitstream at encoding time Since a lot of technical and financial effort is currently being spent on the migration from MPEG-2 equipment to H.264/AVC, it is unlikely that a new migration to SVC will occur in the short term Due to broadcasters and content distributors want to have scalable bitstreams at their disposal, efficient techniques for migration of single-layer content to a scalable format are desirable In this paper, an approach for temporal scalability transcoding from H.264/AVC to SVC is discussed This approach is applied to the upper layers of SVC, where coding complexity is higher, and it is capable to reduce this coding complexity around to 55.75% while maintaining the coding efficiency.\nTitle:\nAn approach for an AVC to SVC transcoder with temporal scalability\n\nAbstract:\nNowadays, networks and terminals with diverse characteristics of bandwidth and capabilities coexist. To ensure a good quality of experience, this diverse environment demands adaptability of the video stream. In general, video contents are compressed to save storage capacity and to reduce the bandwidth required for its transmission. Therefore, if these compressed video streams were compressed using scalable video coding schemes, they would be able to adapt to those heterogeneous networks and a wide range of terminals. Since the majority of the multimedia contents are compressed using H.264/AVC, they cannot benefit from that scalability. This paper proposes a low-complexity algorithm to convert an H.264/AVC bitstream without scalability to scalable bitstreams with temporal scalability in baseline and main profiles by accelerating the mode decision task of the scalable video coding encoding stage using machine learning tools. The results show that when our technique is applied, the complexity is reduced by 87% while maintaining coding efficiency.\nTitle:\nLow-complexity transcoding algorithm from H.264/AVC to SVC using data mining.\n\nAbstract:\nMobile digital television is one of the new services introduced recently by telecommunications operators. Due to the possibilities of personalization and interaction provided, together with the increasing demand for this type of portable services, it will undoubtedly be a successful technology in the near future. Multimedia content is generally encoded by reducing the storage capacity necessary and bandwidth consumption in order to be transmitted. In order to adapt to the different characteristics of the networks and the varying capabilities of the devices, scalable video coding schemes have been proposed that provide temporal, spatial, and quality scalability, or a combination of these. Most of the existing video content is compressed using H.264/AVC, which is a single-layer codec, so these contents cannot benefit from the scalability tools due to the lack of intrinsic scalability provided in the bitstream at encoding time. This paper proposes a technique to convert from the single-layer H.264/AVC bitstream to a scalable bitstream with temporal scalability. Applying this approach, a reduction of 60% in coding complexity is achieved while maintaining the coding efficiency.\nTitle:\nVideo transcoding for mobile digital television\n\nAbstract:\nMobile Digital TV environments demand flexible video compression like Scalable Video Coding (SVC) because of varying bandwidths and devices. Since existing infrastructures highly rely on H.264/AVC video compression, network providers could adapt the current H.264/AVC encoded video to SVC. This adaptation needs to be done efficiently to reduce processing power and operational cost. This paper proposes two techniques to convert an H.264/AVC bitstream in Main Profile (B-pictures based) without scalability to a scalable bitstream with temporal scalability as part of a framework for low-complexity video adaptation for mobile digital TV. Our approaches are based on accelerating the interprediction, focusing on reducing the coding complexity of mode decision and motion estimation tasks of the encoder stage by using information available after the H.264/AVC decoding stage. The results show that when our techniques are applied, the complexity is reduced by 98% while maintaining coding efficiency.\nTitle:\nTemporal video transcoding for Digital TV broadcasting\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.212, mean value for all the writers is 8.648, which makes it 0.481 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nScalable video coding is a recent extension of the advanced video coding H.264/AVC standard developed jointly by ISO/IEC and ITU-T, which allows adapting the bitstream easily by dropping parts of it named layers. This adaptation makes it possible for a single bitstream to meet the requirements for reliable delivery of video to diverse clients over heterogeneous networks using temporal, spatial or quality scalability, combined or separately. Since the scalable video coding design requires scalability to be provided at the encoder side, existing content cannot benefit from it. Efficient techniques for converting contents without scalability to a scalable format are desirable. In this paper, an approach for temporal scalability transcoding from H.264/AVC to scalable video coding in baseline and main profile is presented and the impact of the GOP size is analyzed. Independently of the GOP size chosen, time savings of around 63 % for baseline profile and 60 % for main profile are achieved while maintaining the coding efficiency.\nTitle:", "model_inf_time": 2.05}, {"id": "41395", "output": "Low Complexity Macroblock Partition Mode Decision for MPEG-2 to H.264 Transcoding", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we introduce and evaluate a low complexity macroblock partition mode decision algorithm for inter-frame prediction in MPEG-2 to H.264 transcoder. The proposed tools are used to compute an optimal MB coding mode decision with significantly reduced computational complexity. Specifically, we achieve the computational savings by using the following MB information coming from MPEG-2: the MB coding modes, the coded block pattern (CBPC) in MPEG-2, and the mean and variance of the 16 4times4 sub blocks of the MPEG-2 residual MBs. We use data mining algorithms to develop a decision tree for H.264 coding mode decisions. The decision trees are built using RD optimized mode decisions and result in highly efficient mode decisions. The proposed transcoder is 35% faster than the RD optimized H.264 reference transcoder without a significant PSNR degradation. The proposed transcoder performs over 3 dB better than the SAE cost based H.264 transcoding\nTitle:\nRD-Optimization for MPEG-2 to H.264 Transcoding\n\nAbstract:\nThis paper presents a novel macroblock mode decision algorithm for inter-frame prediction based on machine learning techniques to be used as part of a very low complexity MPEG-2 to H.264 video transcoder. Since coding mode decisions take up the most resources in video transcoding, a fast macro block (MB) mode estimation would lead to reduced complexity. The proposed approach is based on the hypothesis that MB coding mode decisions in H.264 video have a correlation with the distribution of the motion compensated residual in MPEG-2 video. We use machine learning tools to exploit the correlation and derive decision trees to classify the incoming MPEG-2 MBs into one of the 11 coding modes in H.264. The proposed approach reduces the H.264 MB mode computation process into a decision tree lookup with very low complexity. The proposed transcoder is compared with a reference transcoder comprised of a MPEG-2 decoder and an H.264 encoder. Our results show that the proposed transcoder reduces the H.264 encoding time by over 95% with negligible loss in quality and bitrate.\nTitle:\nVery low complexity MPEG-2 to H.264 transcoding using machine learning\n\nAbstract:\nThe H.264 video compression standard provides tools for coding improvements of at least 2 dB, in terms of PSNR, and at least 50% in bit rate savings as compared with MPEG-2 video compression standard. It is expected that the H.264/MPEG-4 AVC will take over the digital video market, replacing the use of MPEG-2 in most digital video applications. The complete migration to the new video-coding algorithm will take several years given the wide scale use of MPEG-2 in the market place today. This creates an important need for transcoding technologies for converting the large volume of existent video material from the MPEG-2 into the H.264 format and vice versa. However, given the significant differences between the MPEG-2 and the H.264 encoding algorithms, the transcoding process of such systems is much more complex to other heterogeneous video transcoding processes. In this paper, we introduce and evaluate two versions of a fast intra-frame mode decision algorithm to be used as part of a high-efficient MPEG-2 to H.264 transcoder. In this work, we utilize an architecture of pixel domain video transcoding but we use the DC coefficient of the MPEG-2 DCT 8x8 blocks. Our evaluation results show that the proposed algorithm considerably reduces the complexity involved in the intra-frame prediction.\nTitle:\nFIMDA: a fast intra-frame mode decision algorithm for MPEG-2/H.264 transcoding\n\nAbstract:\nRecent developments have given birth to H.264/AVC: a video coding standard offering better bandwidth to video quality ratios than MPEG-2. It is expected that the H.264/AVC will take over the digital video market, replacing the use of MPEG-2 in most digital video applications. The complete migration to the new video-coding algorithm will take several years given the wide scale use of MPEG-2 in the market place today. This creates an important need for MPEG-2/H264 transcoding technologies. However, given the significant differences between both encoding algorithms, the transcoding process of such systems is much more complex to other heterogeneous video transcoding processes. In this work, we start by analyzing the methods defined in the H.264 video coding standard for the intra prediction: a central element of every H.264 encoder. We then introduce and evaluate six fast intra mode decision algorithms which should enable the development of MPEG-2 to H.264 transcoders. Having evaluated all the proposed methods, we have come out with a high-efficient method, namely DC-ABS pixel. Our results show that our algorithm considerable reduces the complexity involved in the intra prediction with respect the mode decision algorithms used in H.264 JM reference software, while exhibiting a slight degradation on the RD function.. Finally, we analyze a comparative study with two of the most prominent fast intra prediction methods presented in the literature. The results show that the proposed DC-ABS pixel method achieves the best results for video transcoding applications.\nTitle:\nSimple intra prediction algorithms for heterogeneous MPEG-2/H.264 video transcoders\n\nAbstract:\nMPEG-2 is the most widely digital video-encoding standard in use nowadays. It is being widely used in the development and deployment of digital TV services. DVD and video-on-demand services. However, recent developments have given birth to the H.264/AVC, offering better bandwidth to video quality ratios than MPEG2. It is expected that the H.264/AVC will take over the digital video market, replacing the use of MPEG-2 in most digital video applications. The complete migration to the new video-coding algorithm will take several years given the wide scale use of MPEG-2 in the market place today. This creates an important need for transcoding technologies for converting the large volume of existent video material from the MPEG-2 into the H.264 format and vice versa. However, given the significant differences between the MPEG-2 and the H.264 encoding algorithms, the transcoding process of such systems is much more complex to other heterogeneous video transcoding processes. In this paper, we introduce and evaluate a novel intra-frame prediction algorithm to be used as part of a high-efficient MPEG-2 to H.264 transcoder. Our evaluation results show that the proposed algorithm considerable reduces the complexity involved in the intra-frame prediction: a key operation in the transcoding process.\nTitle:\nComputational Complexity Reduction Of Intra-Frame Prediction In Mpeg-2/H.264 Video Transcoders\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.73, mean value for all the writers is 8.648, which makes it 0.923 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe H.264 standard achieves much higher coding efficiency than the MPEG-2 standard, due to its improved inter and intra prediction modes which come with a cost of higher computation complexity. Transcoding MPEG-2 video to H.264 is important to enable gradual migration to H.264. However, given the significant differences between the MPEG-2 and the H.264 coding algorithms, transcoding is much more complex and new approaches to transcoding are necessary. In this paper, we introduce and evaluate a low complexity macroblock partition mode decision algorithm, to be used as part of a high-efficient inter-frame prediction in MPEG-2 to H.264 transcoder. The proposed tools are used to compute an optimal MB coding mode decision with significantly reduced computational complexity. Specifically, we achieve the computational savings by using the following MB information coming from MPEG-2: the MB coding modes, the coded block pattern (CBPC) in MPEG-2, and the mean and variance of the 16 4\u9a74\u8133\u9a744 sub blocks of the MPEG-2 residual MBs. We use data mining algorithms to develop a decision tree for H.264 coding mode decisions. The decision trees are built using RD optimized mode decisions and result in highly efficient mode decisions, with significantly reduced computational complexity. The proposed transcoder is 35% faster than the RD optimized H.264 reference transcoder without a significant PSNR degradation (0.05 dB on average). The proposed transcoder performs over 0.4 dB better on average than the SAE cost based H.264 transcoding.\nTitle:", "model_inf_time": 2.39}, {"id": "41396", "output": "Late Fusion of Classifiers for Photo Annotation in CLEF 2011", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nPhotographic images annotation is a complex problem. Indeed, the visual character- istics of objects of a class vary with the considered instance and the shooting conditions. In this paper we proposed a visual characterization of object parts, called \"Visual Phrase\", robust to these variations. A Visual Phrase is a set of regions of interest built according to pre-difined criteria; a topological criterium was studied in this paper. An automatic annotation method is proposed based on our definition and characterization of Visual Phrases. An experiment on VOC2009 corpus is presented, and we show that the fusion of our method with a standard bag of visual words approach on full images provides better results than those obtained via the standard approach. MOTS-CLES : Phrase Visuelle, Region du0027interet, Sac de mot visuel, Annotation du0027image\nTitle:\nPhrases Visuelles pour l'annotation automatique d'images\n\nAbstract:\nThis paper describes the different experiments that have been conducted by the MRIM group at the LIG in Grenoble for the ImageCLEF 2009 campaign. The group partic- ipated in the following tasks: Image Retrieval and Image Annotation. For the Image Retrieval task, we submitted runs with both text and image features, and a diversifica- tion process was applied. For the Image Annotation task, we used several features and classifiers in a way to generate keyword descriptions. For these two tasks, the results obtained are above the average of the participants.\nTitle:\nMRIM-LIG at ImageCLEF 2009: Photo Retrieval and Photo Annotation Tasks.\n\nAbstract:\nIn this paper, a language model adapted to graph-based representation of image content is proposed and assessed. The full indexing and retrieval processes are evaluated on two different image corpora. We show that using the spatial relationships with graph model has a positive impact on the results of standard Language Model (LM) and outperforms the baseline built upon the current state-of-the-art Support Vector Machine (SVM) classification method.\nTitle:\nSpatial relationships in visual graph modeling for image categorization\n\nAbstract:\nWe present an application (called SmartAlbum) for photo indexing and retrieval that unifies two different image indexing approaches. The system uses two modalities to extract information about a digital photograph; i.e. content-based and speech annotation for image description. The result is a powerful image retrieval tool that has capabilities beyond what current single-mode retrieval systems can offer. We show on a corpus of 1200 images the interest of our approach.\nTitle:\nSmartAlbum-towards unification of approaches for image retrieval\n\nAbstract:\nIn this paper, we present a system for the image indexing and retrieval using speech annotations based on a pre-defined structured syntax. In addition to the introduction of N- best lists for index generation, a query expansion technique is explored to enhance the query terms and to improve retrieval effectiveness. By adding the most probable substitutions for the query terms, more relevant images are distinguished from the data collection. This approach is particularly helpful to deal with those less frequently used words, including out- of-vocabulary (OOV) words, which are very common for names of people and places. Experiments on a collection of 1,200 photos show that the retrieval effectiveness is increased considerably for segment of individual domain on People, Location and Event. With this method, the average value of precision versus recall over a combination of segments has improved significantly, from 50% to 72.4%.\nTitle:\nAn Improved Method for Image Retrieval Using Speech Annotation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.852, mean value for all the writers is 8.648, which makes it 0.174 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe describe in this paper the different approaches tested for the Photo Annotation task for CLEF 2011. We experimented state of the art techniques, by proposing late fusions of several classifiers trained on several features extracted from the images. The classifiers are SVMs and the late fusion is a simple addition of classification probabilities coming from the SVMs. The results obtained place our runs in the middle of the pack, with our best visual-based MAP at 0.337 We also integrated of Flickr human annotations, leading to a large increase of the MAP with a value of 0.377.\nTitle:", "model_inf_time": 1.75}, {"id": "41397", "output": "Online Traffic Engineering with TeXCP", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDynamic load balancing is a popular recent technique that protects ISP networks from sudden congestion caused by load spikes or link failures. Dynamic load balancing protocols, however, require schemes for splitting traffic across multiple paths at a fine granularity. Current splitting schemes present a tussle between slicing granularity and packet reordering. Splitting traffic at the granularity of packets quickly and accurately assigns the desired traffic share to each path, but can reorder packets within a TCP flow, confusing TCP congestion control. Splitting traffic at the granularity of a flow avoids packet reordering but may overshoot the desired shares by up to 60% in dynamic environments, resulting in low end-to-end network goodput Contrary to popular belief, we show that one can systematically split a single flow across multiple paths without causing packet reordering. We propose FLARE, a new traffic splitting algorithm that operates on bursts of packets, carefully chosen to avoid reordering. Using a combination of analysis and trace-driven simulations, we show that FLARE attains accuracy and responsiveness comparable to packet switching without reordering packets. FLARE is simple and can be implemented with a few KB of router state\nTitle:\nDynamic load balancing without packet reordering\n\nAbstract:\nTheory and experiments show that as the per-flow product of bandwidth and latency increases, TCP becomes inefficient and prone to instability, regardless of the queuing scheme. This failing becomes increasingly important as the Internet evolves to incorporate very high-bandwidth optical links and more large-delay satellite links.To address this problem, we develop a novel approach to Internet congestion control that outperforms TCP in conventional environments, and remains efficient, fair, scalable, and stable as the bandwidth-delay product increases. This new eXplicit Control Protocol, XCP, generalizes the Explicit Congestion Notification proposal (ECN). In addition, XCP introduces the new concept of decoupling utilization control from fairness control. This allows a more flexible and analytically tractable protocol design and opens new avenues for service differentiation.Using a control theory framework, we model XCP and demonstrate it is stable and efficient regardless of the link capacity, the round trip delay, and the number of sources. Extensive packet-level simulations show that XCP outperforms TCP in both conventional and high bandwidth-delay environments. Further, XCP achieves fair bandwidth allocation, high utilization, small standing queue size, and near-zero packet drops, with both steady and highly varying traffic. Additionally, the new protocol does not maintain any per-flow state in routers and requires few CPU cycles per packet, which makes it implementable in high-speed routers.\nTitle:\nCongestion control for high bandwidth-delay product networks\n\nAbstract:\nThis paper proposes COPE, a new architecture for wireless mesh networks. In addition to forwarding packets, routers mix (i.e., code) packets from different sources to increase the information content of each transmission. We show that intelligently mixing packets increases network throughput. Our design is rooted in the theory of network coding. Prior work on network coding is mainly theoretical and focuses on multicast traffic. This paper aims to bridge theory with practice; it addresses the common case of unicast traffic, dynamic and potentially bursty flows, and practical issues facing the integration of network coding in the current network stack. We evaluate our design on a 20-node wireless network, and discuss the results of the first testbed deployment of wireless network coding. The results show that using COPE at the forwarding layer, without modifying routing and higher layers, increases network throughput. The gains vary from a few percent to several folds depending on the traffic pattern, congestion level, and transport protocol.\nTitle:\nXORs in the air: practical wireless network coding\n\nAbstract:\nThis paper proposes GIA, a scalable architecture for global IP-anycast. Existing designs for providing IP-anycast must either globally distribute routes to individual anycast groups, or confine each anycast group to a pre-configured topological region. The first approach does not scale because of excessive growth in the routing tables, whereas the second one severely limits the utility of the service. Our design scales by dividing inter-domain anycast routing into two components. The first component builds inexpensive default anycast routes that consume no bandwidth or storage space. The second component, controlled by the edge domains, generates enhanced anycast routes that are customized according to the beneficiary domain's interests. We evaluate the performance of our design using simulation, and prove its practicality by implementing it in the Multi-threaded Routing Toolkit.\nTitle:\nA framework for scalable global IP-anycast (GIA)\n\nAbstract:\nAvailable bandwidth estimation is useful for route selection in overlay networks, QoS verification, and traffic engineering. Recent years have seen a surge in interest in available bandwidth estimation. A few tools have been proposed and evaluated in simulation and over a limited number of Internet paths, but there is still great uncertainty in the performance of these tools over the Internet at large.This paper introduces Spruce, a simple, light-weight tool for measuring available bandwidth, and compares it with two existing tools, IGI and Pathload, over 400 different Internet paths. The comparison focuses on accuracy, failure patterns, probe overhead, and implementation issues. The paper verifies the measured available bandwidth by comparing it to Multi-Router Traffic Grapher (MRTG) data and by measuring how each tool responds to induced changes in available bandwidth.The measurements show that Spruce is more accurate than Pathload and IGI. Pathload tends to overestimate the available bandwidth whereas IGI becomes insensitive when the bottleneck utilization is large.\nTitle:\nA measurement study of available bandwidth estimation tools\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.459, mean value for all the writers is 8.648, which makes it 1.014 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCurrent intra-domain Traffic Engineering (TE) relies on offline methods, which use long term average traffic demands. It cannot react to realtime traffic changes caused by BGP reroutes, diurnal traffic variations, attacks, or flash crowds. Further, current TE deals with network failures by pre-computing alternative routings for a limited set of failures. It may fail to prevent congestion when unanticipated or combination failures occur, even though the network has enough capacity to handle the failure.This paper presents TeXCP, an online distributed TE protocol that balances load in realtime, responding to actual traffic demands and failures. TeXCP uses multiple paths to deliver demands from an ingress to an egress router, adaptively moving traffic from over-utilized to under-utilized paths. These adaptations are carefully designed such that, though done independently by each edge router based on local information, they balance load in the whole network without oscillations. We model TeXCP, prove the stability of the model, and show that it is easy to implement. Our extensive simulations show that, for the same traffic demands, a network using TeXCP supports the same utilization and failure resilience as a network that uses traditional offline TE, but with half or third the capacity.\nTitle:", "model_inf_time": 1.52}, {"id": "41398", "output": "A Min-Actor CDS Construction Algorithm for Wireless Sensor and Actor Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWireless sensor and actor network (WSAN) has received growing interests with the rapid development of Internet of things. Multiple function actors are added into the traditional wireless sensor networks to sense the environment, conduct tasks and give feedback. In this paper, an efficient distributed actor deployment strategy is proposed to address the connectivity maintenance and coverage optimization issues in WSAN with the consideration of unknown sensors' locations. In this proposed strategy, an artificial potential force based algorithm is utilized to detect the unknown sensors, and a fully distributed link deletion algorithm is introduced to delete redundant communication links thus expanding the search area. Then, an actor location optimization algorithm is developed based on the Voronoi diagram method to get a better coverage result. The global connectivity of the actors is guaranteed during the entire process. The performances of proposed strategy are validated analytically and experimentally.\nTitle:\nA distributed coverage optimization and connectivity maintenance strategy based on unknown sensors in WSAN\n\nAbstract:\nThis paper focuses on the theoretical modeling of sensor cloud, which is one of the first attempts in this direction. We endeavor to theoretically characterize virtualization, which is a fundamental mechanism for operations within the sensor-cloud architecture. Existing related research works on sensor cloud have primarily focused on the ideology and the challenges that wireless sensor network (WS...\nTitle:\nOn Theoretical Modeling of Sensor Cloud: A Paradigm Shift From Wireless Sensor Network.\n\nAbstract:\nWireless Sensor Networks (WSNs) are popular nowadays for applications ranging from environmental monitoring to object tracking and surveillance. But the sensors which are deployed are energy constrained. The nodes in WSNs deplete of their very limited energy over time. In this work, we consider a network of rechargeable sensors deployed in a random sensing environment. Using Markov Decision Processes (MDP), we analyze the sensor nodes having different levels of energies in the batteries, so as to maximize a generalized system performance in terms of recharge delay and the number of sensor nodes recharged. We have formulated a relationship between the residual energy level of a sensor node and the recharge delay. The simulation results establish the effectiveness of our approach in decreasing the number of passive nodes and the recharge delay in WSNs.\nTitle:\nMarkov decision process-based analysis of rechargeable nodes in wireless sensor networks\n\nAbstract:\nThe emergence of wireless ad-hoc networks is considered extremely attractive in terms of new applications' enabler. The integration of reliable sensors in nodes of wireless ad-hoc networks has posed various interesting challenges to the community of researchers and engineers. We focus in this paper on two fundamental issues: supporting high mobility of sensors and guaranteeing continuity of sensing, while providing for the correlation of collected data. We propose an efficient scheme for the management of data, sensor mobility, and sensing security\nTitle:\nMobility and security issues in wireless ad-hoc sensor networks\n\nAbstract:\nIn this paper, we address the problem of intrusion detection in Wireless Sensor Networks (WSNs) using a Learning Automata (LA)-based approach. We are not aware of any LA-based intrusion detection systems (IDSs) solutions for WSNs. Additionally, the S-model approach that we have taken to solve the problem, where in the feedback of the environment to the automaton can not only be completely favourable or completely unfavourable, but also be any continuous value within these extremities, makes it one of the attractive solution approaches in LA. We have rigorously evaluated the performance of our proposed solution by performing a variety of experiments and have found our solution approach to be promising.\nTitle:\nIntrusion Detection in Wireless Sensor Networks: The S-Model Learning Automata Approach\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.707, mean value for all the writers is 8.648, which makes it 1.757 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAs the virtual backbone of the network, connected dominating set (CDS) plays an important role in supporting data communication, reducing the routing overhead, and enhancing the scalability of the network. Traditional CDS construction algorithms are usually applied to the networks with homogeneous nodes, e.g., a pure wireless sensor network. In the past few years, Wireless Sensor and Actor Networks (WSANs) have attracted growing attentions due to its capabilities of sensing and gathering information, as well as performing certain tasks by actor nodes. One of the most critical issues in WSAN is how to construct an appropriate CDS with the consideration of heterogeneous nodes. Specifically, how to construct the CDS that contains minimum number of actors, so that the movement of actors poses a minimal threat to the integrity of the CDS. In this paper, we take the nature of the actor nodes into consideration and propose a CDS construction algorithm named Min-Actor algorithm (MIA), which can reduce nodes' movement constraints in WSAN. Simulation results confirm that MIA can effectively reduce the number of the actors in CDS, thus reducing the impact of actors' movement on the network.\nTitle:", "model_inf_time": 1.59}, {"id": "41399", "output": "Haptic and Visual Spatial Layout Learning: Modality Integration and Switching Costs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nParticipants learned through feedback to haptically classify the identity of upright versus inverted versus scrambled faces depicted in simple 2D raised-line displays. We investigated whether identity classification would make use of a configural face representation, as is evidenced for vision and 3D haptic facial displays. Upright and scrambled faces produced equivalent accuracy, and both were identified more accurately than inverted faces. The mean magnitude of the haptic inversion effect for 2D facial identity was a sizable 26 percent, indicating that the upright orientation was \u201cprivileged\u201d in the haptic representations of facial identity in these 2D displays, as with other facial modalities. However, given the effect of scrambling, we conclude that configural processing was not employed; rather, only local information about the features was used, the features being treated as oriented objects within a body-centered frame of reference. The results indicate a fundamental difference between haptic identification of 2D facial depictions and 3D faces, paralleling a corresponding difference in recognition of nonface objects.\nTitle:\nHaptic Classification of Facial Identity in 2D Displays: Configural versus Feature-Based Processing\n\nAbstract:\nThe compliance of a material can be conveyed through mechanical interactions in a virtual environment and perceived through both visual and haptic cues. We investigated this basic aspect of perception. In two experiments, subjects performed compliance discriminations, and the mean perceptual estimate (PSE) and the perceptual standard deviation (proportional to JND) were derived from psychophysical functions. Experiment 1 supported a model in which each modality acted independently to produce a compliance estimate, and the two estimates were then integrated to produce an overall value. Experiment 2 tested three mathematical models of the integration process. The data ruled out exclusive reliance on the more reliable modality and stochastic selection of one modality. Instead, the results supported an integration process that constitutes a weighted summation of two random variables, which are defined by the single modality estimates. The model subsumes optimal fusion but provided valid predictions also if the weights were not optimal. Weights were optimal (i.e., minimized variance) when vision and haptic inputs were congruent, but not when they were incongruent.\nTitle:\nCombination and Integration in the Perception of Visual-Haptic Compliance Information\n\nAbstract:\nThe current study assessed the relative effectiveness with which unimodal tactile, unimodal touch-produced auditory, and bimodal tactile + auditory cues contribute to the performance of an absolute texture identification task via remote touch. The study contributes to our fundamental understanding of the unimodal perception and intersensory integration of multimodal surface texture cues generated during surface exploration with rigid probes. The results also have significant implications for the design of unimodal and multisensory displays for use with teleoperation and virtual environment systems, as it addresses which modality(ies) may be used to most effectively present sensory information about remotely explored surface textures.\nTitle:\nRelative Performance Using Haptic and/or Touch-Produced Auditory Cues in a Remote Absolute Texture Identification Task\n\nAbstract:\nWe investigated people's ability to report the shape and scale of a spatial layout after sparse contact, without vision. We propose that the initial representation of sparsely contacted layout is kinesthetic. From this can be computed a configural representation that supports reports of shape and scale, but at the cost of increased error. In four experiments, participants' fingers were guided to a two-point layout, after which they returned to the points or reported distance and/or angle, subject to a change in location and sometimes a rotation as well. Errors in reproducing inter-point distance, i.e., the scale of the layout, were smallest for the task of returning to the touched points and nearly twice as great when distance was reported at a new location. Errors in reproducing inter-point angle, i.e., the shape of the layout, were smallest for the task of returning to the touched points and nearly twice as great when angle was reported subject to rotation. The data highlight limitations on reporting the shape and scale of a haptically rendered layout after sparse contact.\nTitle:\nHow Well Can We Encode Spatial Layout from Sparse Kinesthetic Contact?\n\nAbstract:\nGrowing evidence suggests that the functional specialization of the two cortical visual pathways may not be as distinct as originally proposed. Here, we explore possible contributions of the dorsal \"where/how\" visual stream to shape perception and, conversely, contributions of the ventral \"what\" visual stream to location perception in human adults. Participants performed a shape detection task and a location detection task while undergoing fMRI. For shape detection, comparable BOLD activation in the ventral and dorsal visual streams was observed, and the magnitude of this activation was correlated with behavioral performance. For location detection, cortical activation was significantly stronger in the dorsal than ventral visual pathway and did not correlate with the behavioral outcome. This asymmetry in cortical profile across tasks is particularly noteworthy given that the visual input was identical and that the tasks were matched for difficulty in performance. We confirmed the asymmetry in a subsequent psychophysical experiment in which participants detected changes in either object location or shape, while ignoring the other, task-irrelevant dimension. Detection of a location change was slowed by an irrelevant shape change matched for difficulty, but the reverse did not hold. We conclude that both ventral and dorsal visual streams contribute to shape perception, but that location processing appears to be essentially a function of the dorsal visual pathway.\nTitle:\nVentral and dorsal visual stream contributions to the perception of object shape and object location.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.14, mean value for all the writers is 8.648, which makes it 1.273 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nParticipants learned circular layouts of six objects presented haptically or visually, then indicated the direction from a start target to an end target of the same or different modality (intramodal versus intermodal). When objects from the two modalities were learned separately, superior performance for intramodal trials indicated a cost of switching between modalities. When a bimodal layout intermixing modalities was learned, intra- and intermodal trials did not differ reliably. These findings indicate that a spatial image, independent of input modality, can be formed when inputs are spatially and temporally congruent, but not when modalities are temporally segregated in learning.\nTitle:", "model_inf_time": 1.82}, {"id": "41400", "output": "Conceptual Priming and the FN400: Re-establishing the Dissociation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nRecent interest has been drawn to the separate components of recognition memory, as studied by event-related potentials (ERPs). In ERPs, recollection is usually accompanied by a late, parietal positive deflection. An earlier, frontal component has been suggested to be a counterpart, accompanying recognition by familiarity. However, this component, the FN400, has alternatively been suggested to reflect a form of implicit memory, conceptual priming. The present study examined the ERP components of recognition memory using an episodic memory task with a stimulus material consisting of names, half of which were famous. Along a different dimension, the names varied in how rare or common they were. These dimensions, frequency and fame, exerted powerful effects on memory accuracy, and dissociated the two recognition processes, such that frequency gave rise to familiarity and fame fostered recollection, when the receiver operating characteristics data were analyzed with Yonelinas' dual-process signal detection model. The ERPs corresponded fully to the behavioral data because frequency affected the frontal component exclusively, and fame affected the parietal component exclusively. Moreover, a separate behavioral experiment showed that conceptual priming was sensitive to fame, but not to frequency. Our data therefore indicate that the FN400 varies jointly with familiarity, but independently of conceptual priming.\nTitle:\nFamiliarity or conceptual priming: event-related potentials in name recognition.\n\nAbstract:\nInterference and link dynamics constitute great concerns for stability and performance of protocols in WSNs. In this paper we evaluate the impact of channel hopping and adaptive routing on delay and reliability focusing on delay critical applications.\nTitle:\nMulti-channel communication vs. adaptive routing for reliable communication in WSNs\n\nAbstract:\nRetrieval orientation describes the modulation in the processing of retrieval cues by the nature of the targeted material in memory. Retrieval orientation is usually investigated by analyzing the cortical responses to new (unstudied) material when different memory contents are targeted. This approach avoids confounding effects of retrieval success. We investigated the neural correlates of retrieval orientation in reality monitoring with event-related potentials (ERPs) and assessed the impact of retrieval accuracy on obtained ERP measures. Thirty-two subjects studied visually presented object names that were followed either by a picture of that object (perceived condition) or by the instruction to mentally generate such a picture (imagine condition). Subsequently, subjects had to identify object names of one study condition and reject object names of the second study condition together with newly presented object names. The data analysis showed that object names were more accurately identified when they had been presented in the perceived condition. Two topographically distinct ERP effects of retrieval orientation were revealed: From 600 to 1100ms after stimulus representation, ERPs were more positive at frontal electrode sites when object names from the imagine condition were targeted. The analysis of response-locked ERP data revealed an additional effect at posterior electrode sites, with more negative ERPs shortly after response onset when items from the imagine condition were targeted. The ERP effect at frontal electrode sites, but not at posterior electrode sites was modulated by relative memory accuracy, with stronger effects in subjects who had lower memory accuracy for items of the imagine condition. The findings are suggestive for a contribution of frontal brain areas to retrieval orientation processes in reality monitoring and indicate that neural correlates of retrieval orientation can be modulated by retrieval effort, with stronger activation of these correlates with increasing task demands.\nTitle:\nElectrophysiological correlates of retrieval orientation in reality monitoring.\n\nAbstract:\nIn this work, we study a cooperative network with multiple full-duplex buffer-aided relays. A hybrid cooperative relaying policy is proposed that employs power adaptation and consists of two alternative schemes: (i) full-duplex transmission through the relay which requires the least total power expenditure and loop interference is mitigated through power adaptation; (ii) buffer-aided max - link selection with power adaptation, when full-duplexity is not feasible. Aiming to reduce the overhead of channel state information (CSI) acquisition and processing, we propose a suboptimal distributed method for relay selection, for which the network performance is not degraded significantly. We show that power adaptation offers reduced overhead of CSI acquisition. Numerical results and comparisons with other state-of-the-art relaying schemes are provided and performance evaluation in terms of throughput, power minimization and switching rate, show the benefits of the proposed hybrid scheme.\nTitle:\nHybrid cooperation through full-duplex opportunistic relaying and max-link relay selection with transmit power adaptation\n\nAbstract:\nThis paper discusses tuning of PID controllers for varying time-delay systems. We analyze the properties of the AMIGO tuning rules of Astrom and Hagglund applied to varying time-delay systems and propose improved tuning rules which increase the robustness to delay variations at the expense of a small degradation in nominal performance. We suggest a tuning scheme that uses the simple AMIGO tuning on an extended plant, and define the design concepts for extending the plant. This approach allows treating the maximum time-delay as a design parameter for the tuning rules. The proposed tuning rules are compared via simulations.\nTitle:\nSimple Pid Tuning Rules For Varying Time-Delay Systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.919, mean value for all the writers is 8.648, which makes it 1.084 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nLucas, Voss, and Paller sympathize with our intentions but disagree with our findings. They argue that a relation between frequency and conceptual priming may have been obscured by methodological details in our second experiment, therefore failing to complete a bridge between conceptual priming and FN400 with name frequency as the mediator. However, renewed inspections of our experiment and a new additional experiment, designed to meet the objections, fail to find any role for name frequency in conceptual priming and therefore re-establish the dissociation of priming and the FN400. On closer inspection, our differing views seem to derive from different interpretations of the term \u201cconcept.\u201d\nTitle:", "model_inf_time": 1.9}, {"id": "41401", "output": "Robust Face Detection and Tracking for Real-Life Telecommunication Applications", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose a novel framework for face alignment based on the Active Appearance Model (AAM) in surveillance systems with Pan-Tilt-Zoom (PTZ) cameras. The AAM converges poorly in face images which are affected by illumination factors, cluttered backgrounds and status of the camera. To search for robust face model parameters, we propose a robust AAM fitting method based on segmenting faces and combining Person-specific and Generic models to achieve accurate face alignment. We segment faces using histogram back-projection and a skin color histogram, which is updated using a skin mask extracted by the AAM. For robust face recognition, we combined Generic and Person-specific models with a slight reduction in processing time. The extracted AAM parameters are as accurate as those when using the Person-specific model and can be used as features for face recognition. Empirical experiments show that our proposed method extracts very accurate face parameters and is not sensitive to initial shapes.\nTitle:\nFace Alignment Using Segmentation and a Combined AAM in a PTZ Camera\n\nAbstract:\nA vision-based real-time human detection and tracking capability is one of the key components of surveillance systems, human computer interfaces and monitoring systems. In this paper, we propose a method which uses color and disparity information obtained with a stereo camera. In order to achieve optimal performance with respect to detection or tracking of objects, it is better to consider multiple features together. We have developed a tracking method in which color and disparity information can be combined in a histogram. We used skin color and disparity distribution information to distinguish between different people. For human tracking, we propose a color histogram that is weighted by the disparity distribution of the target. The proposed method is simple and robust for moving camera environments and overcomes the drawbacks of conventional color histogram-based tracking methods. Experimental results show the robustness of the proposed method in environments with changing backgrounds and the tracking capabilities of targets which have similar color distributions as backgrounds or other targets. The proposed method can be used in real-time mobile robot applications.\nTitle:\nDisparity weighted histogram-based object tracking for mobile robot systems\n\nAbstract:\nDue to the increases in processing power and storage capacity of mobile devices over the years, an incorporation of realtime face recognition to mobile devices is no longer unattainable. However, the possibility of the realtime learning of a large number of samples within mobile devices must be established. In this paper, we attempt to establish this possibility by presenting a realtime training algorithm in mobile devices for face recognition related applications. This is differentiated from those traditional algorithms which focused on realtime classification. In order to solve the challenging realtime issue in mobile devices, we extract local face features using some local random bases and then a sequential neural network is trained incrementally with these features. We demonstrate the effectiveness of the proposed algorithm and the feasibility of its application in mobile devices through empirical experiments. Our results show that the proposed algorithm significantly outperforms several popular face recognition methods with a dramatic reduction in computational speed. Moreover, only the proposed method shows the ability to train additional samples incrementally in realtime without memory failure and accuracy degradation using a recent mobile phone model.\nTitle:\nRealtime training on mobile devices for face recognition applications\n\nAbstract:\nSince facial images are affected by various factors, the representation capacity for face database is limited by the prototypes collected for training. Therefore, to extend the capacity covering variations of facial images, we should adopt a complex classifier. It is desirable to use output coding method by considering the number of classes changes. We propose new output coding methods and then compare them with representative conventional output coding methods to investigate the properties of decomposition schemes through the experiment on the ORL face dataset. Finally, we give discussions on some factors that should be considered in the designing of decomposition scheme, to provide some foundation for designing new output coding methods in face recognition.\nTitle:\nEmpirical remarks on output coding methods for face recognition\n\nAbstract:\nMost research on face recognition has focused on representation of face appearances rather than the classifiers. For robust classification performance, we need to adopt elaborate classifiers. Output coding is suitable for this purpose because it can allow online learning. In this paper, we propose an N-division output coding method. In the experiments we demonstrate such properties as problem complexity, margin of separation, machine relevance and the recognition performance among different output coding methods.\nTitle:\nN-division output coding method applied to face recognition\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.57, mean value for all the writers is 8.648, which makes it 0.787 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we propose a new face detection and tracking algorithm for real-life telecommunication applications, such as video conferencing, cellular phone and PDA. We combine template-based face detection and tracking method with color information to track a face regardless of various lighting conditions and complex backgrounds as well as the race. Based on our experiments, we generate robust face templates from wavelet-transformed lowpass and two highpass subimages at the second level low-resolution. However, since template matching is generally sensitive to the change of illumination conditions, we propose a new type of preprocessing method. Tracking method is applied to reduce the computation time and predict precise face candidate region even though the movement is not uniform. Facial components are also detected using k-means clustering and their geometrical properties. Finally, from the relative distance of two eyes, we verify the real face and estimate the size of facial ellipse. To validate face detection and tracking performance of our algorithm, we test our method using six different video categories of QCIF size which are recorded in dynamic environments.\nTitle:", "model_inf_time": 1.65}, {"id": "41402", "output": "QoS Negotiation for Efficient Failure Recovery in Multi-resolution Video Servers", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nVideo servers are highly vulnerable to disk failures because they employ large-scale disk arrays to support many clients and multimedia contents. To cope with such a structural weakness, they reserve a significant portion of their resource contingent on disk operability, leading to under-utilization in normal operating mode. The paper addresses the improvement of resource utilization in fault-tolerant multi-resolution video servers. For this purpose, we propose a QoS degradation method and an admission control algorithm to maximize the system performance measured as revenue while guaranteeing every admitted client the minimum quality level in the event of disk failure. Our scheme exploits the multi-resolution property of video streams, and achieves graceful degradation at near-optimal level when the disk fails. Simulation results show that, in the best case, our proposed technique almost doubles the number of clients admitted for video service by greatly enhancing the resource utilization rates.\nTitle:\nA QoS degradation policy for revenue maximization in fault-tolerant multi-resolution video servers\n\nAbstract:\nMulti-resolution video compression techniques are used for the efficient support of heterogeneous clients with different quality of service (QoS) parameters. However, data rates for high- and low-resolution video streams can vary widely; so the number of admitted clients may be limited due to the unbalanced use of server resources (i.e., disk bandwidth and buffer). Thus, the server may not accommodate new users although there is sufficient buffer or disk bandwidth. To remedy this problem, we propose a new replication scheme called Splitting Striping units by Replication (SSR) for multi-resolution video servers. To increase the number of admitted clients, we define two striping unit sizes of data which are stored on the primary and backup copies in different ways. In addition, we present a new admission control algorithm which adaptively decides whether to read data from the primary or the backup copy in order to make the best use of disk bandwidth and buffer of the video server. The effectiveness of the proposed scheme is evaluated through simulations.\nTitle:\nReplica Striping for Multi-resolution Video Servers\n\nAbstract:\nIn this paper we propose storage allocation schemes to minimize the buffer requirements for fault-tolerant video-on-demand (VOD) servers with disk arrays. To guarantee the high reliability of video service, the servers usually adopt parity-encoding techniques in which data blocks and their associated parity block form a parity group. For real-time video service, all the blocks in a parity group are prefetched to cope with disk failure. Buffer overhead incurred by a prefetch can be reduced by decreasing the parity group size, which demands more storage space for parity blocks. Our proposed scheme called round-level parity grouping (RPG) aims at restoring VBR video streams efficiently. Based on RPG we have developed a heuristic algorithm working towards effective buffer management. The experimental results show that our proposed schemes produce near-optimal buffer requirements.\nTitle:\nMinimization of buffer requirements using variable-size parity groups for fault-tolerant video servers\n\nAbstract:\nIncreasing the number of concurrent streams while guaranteeing jitter-free operation is a primary issue for video servers. Disks storing popular videos tend to become overloaded, preventing the server accommodating more clients due to the unbalanced use of bandwidth. We propose an adaptive data retrieval scheme for load sharing in clustered video servers. We analyze how the data retrieval period affects the utilization of disk bandwidth and buffer space, and then develop a robust period management policy to satisfy the real-time requirements of video streams. We go on to propose a new data retrieval scheme in which the period can be dynamically adjusted so as to increase the disk bandwidth capacity of heavily loaded clusters and increase the number of clients admitted. Simulations demonstrate that our scheme is able to cope effectively with dynamically changing workloads and enables the server to admit many more clients.\nTitle:\nAdaptive data retrieval for load sharing in clustered video servers\n\nAbstract:\nWe propose a new energy-aware data retrieval scheme (EDR) for mirrored video servers. We start by analytically determining the retrieval period that balances bandwidth and buffer size. We then propose a data retrieval scheme in which the period can be dynamically changed to reflect server utilization, with the aim of giving disks more chance to enter low-power mode. Simulation results show that it saves up to 36% energy compared with conventional video server operation.\nTitle:\nAn adaptive data retrieval scheme for reducing energy consumption in mirrored video servers\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.275, mean value for all the writers is 8.648, which makes it 1.388 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we present a Quality of Service (QoS) negotiation scheme for efficient failure recovery in multi-resolution video servers with disk arrays. This scheme exploits multi-resolution property of video streams by negotiating service resolutions in order to provide graceful QoS degradation when a disk fails. Using the proposed scheme, not only can we increase the number of admitted clients greatly when all disks are operational but also utilize server resources efficiently. Furthermore, it can provide each client with acceptable QoS even in the presence of disk failure while maximizing server-perceived rewards. The effectiveness of the proposed algorithm is evaluated through simulation-based experiments.\nTitle:", "model_inf_time": 1.69}, {"id": "41403", "output": "Measuring Domain Engineering Impact on Software Change Effort", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe describe a methodology for precise quantitative measurement of technology impact on software change effort. The methodology employs measures of small software changes to determine the effect of technology. We illustrate this approach in a detailed case study on the impact of using two particular technologies \u2014 a version-sensitive source code editor and a domain-engineered application environmen...\nTitle:\nMeasuring technology effects on software change cost\n\nAbstract:\nIn this paper we describe a methodology and algorithm for historical analysis of the effort necessary for developers to make changes to software. The algorithm identifies factors which have historically increased the difficulty of changes. This methodology has implications for research into cost drivers. As an example of a research finding, we find that a system under study was ``decaying'' in that changes grew more difficult to implement at a rate of 20\\% per year. We also quantify the difference in costs between changes that fix faults and additions of new functionality: fixes require 80\\% more effort after accounting for size. Since our methodology adds no overhead to the development process, we also envision it being used as a project management tool: for example, developers can identify code modules which have grown more difficult to change than previously, and can match changes to developers with appropriate expertise. The methodology uses data from a change management system, supported by monthly time sheet data if available. The method's performance does not degrade much when the quality of the time sheet data is limited. We validate our results using a survey of the developers under study: the change efforts resulting from the algorithm match the developers' opinions. Our methodology includes a technique based on the jackknife to determine factors that contribute significantly to change effort.\nTitle:\nInferring Change Effort from Configuration Management Databases\n\nAbstract:\nLegacy systems are difficult and expensive to maintain due to size, complexity, and age of their code base. Business needs require continuously adding new features and maintaining older releases. This and the ever present worry about feature breakage are often the reason why the sweeping changes for reversing design degradation are considered too costly, risky and difficult to implement. We study a refactoring carried out on a part of a large legacy business communication product where protocol logic in the registration domain was restructured. We pose a number of hypotheses about the strategies and effects of the refactoring effort on aspects of changeability and measure the outcomes. The results of this case study show a significant decrease in customer reported defects and in effort needed to make changes.\nTitle:\nRefactoring for Changeability: A Way to Go?\n\nAbstract:\nWe investigate relationships among software quality measures commonly used to assess the value of a technology, and several aspects of customer perceived quality measured by Interval Quality (IQ): a novel measure of the probability that a customer will observe a failure within a certain interval after software release. We integrate information from development and customer support systems to compare defect density measures and IQ for six releases of a major telecommunications system. We find a surprising negative relationship between the traditional defect density and IQ. The four years of use in several large telecommunication products demonstrates how a software organization can control customer perceived quality not just during development and verification, but also during deployment by changing the release rate strategy and by increasing the resources to correct field problems rapidly. Such adaptive behavior can compensate for the variations in defect density between major and minor releases.\nTitle:\nInterval quality: relating customer-perceived quality to process quality\n\nAbstract:\nA key problem in software engineering is changing the code. We present a sequence of visualizations and visual metaphors designed to help engineers understand and manage the software change process. The principal metaphors are matrix views, cityscapes, bar and pie charts, data sheets, and networks. Linked by selection mechanisms, multiple views are combined to form perspectives that both enable discovery of high-level structure in software change data and allow effective access to details of those data. Use of the views and perspectives is illustrated in two important contexts: understanding software change by exploration of software change data and management of software development. Our approach complements existing visualizations of software structure and software execution.\nTitle:\nVisualizing Software Changes\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.031, mean value for all the writers is 8.648, which makes it 0.327 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDomain Engineering (DE) is an increasingly popular process for efficiently producing software. DE uses detailed knowledge of a particular application domain to define rigorously a family of software products within that domain.We describe methodology for precise quantitative measurement of DE impact on software change effort. The methodology employs measures of small software changes to determine the effect of DE.We illustrate this approach in a detailed case study of DE in a telecommunications product. In the particular case the change effort was dramatically reduced. The methodology can precisely measure cost savings in change effort and is simple and inexpensive since it relies on information automatically collected by version control systems.\nTitle:", "model_inf_time": 1.3}, {"id": "41404", "output": "Invariance and Attractivity of Memristor-Based Cellular Neural Networks with Time-Varying Delays", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents theoretical results on the convergence and attractivity of memristor-based cellular neural networks (MCNNs) with time delays. Based on a realistic memristor model, an MCNN is modeled using a differential inclusion. The essential boundedness of its global solutions is proven. The state of MCNNs is further proven to be convergent to a critical-point set located in saturated region of the activation function, when the initial state locates in a saturated region. It is shown that the state convergence time period is finite and can be quantitatively estimated using given parameters. Furthermore, the positive invariance and attractivity of state in non-saturated regions are also proven. The simulation results of several numerical examples are provided to substantiate the results.\nTitle:\nConvergence and attractivity of memristor-based cellular neural networks with time delays.\n\nAbstract:\nIn this paper, the complete stability of cellular neural networks with time-varying delays is analyzed using the induction method and the contraction mapping principle. Delay-dependent and delay-independent conditions are obtained for locally stable equilibrium points to be located anywhere, which differ from the existing results on complete stability where the existence of equilibrium points in t...\nTitle:\nComplete stability of cellular neural networks with time-varying delays.\n\nAbstract:\nIn this paper, we present a complex-valued projection neural network for solving constrained convex optimization problems of real functions with complex variables, as an extension of real-valued projection neural networks. Theoretically, by developing results on complex-valued optimization techniques, we prove that the complex-valued projection neural network is globally stable and convergent to t...\nTitle:\nA Complex-Valued Projection Neural Network for Constrained Optimization of Real Functions in Complex Variables.\n\nAbstract:\nThis paper presents theoretical results on the global exponential synchronization of multiple memristive neural networks with time delays. A novel coupling scheme is introduced, in a general topological structure described by a directed or undirected graph, with a linear diffusive term and discontinuous sign term. Several criteria are derived based on the Lyapunov stability theory to ascertain the...\nTitle:\nGlobal Exponential Synchronization of Multiple Memristive Neural Networks With Time Delay via Nonlinear Coupling.\n\nAbstract:\nThis paper addresses the global exponential dissipativity of memristor-based recurrent neural networks with time-varying delays. By constructing proper Lyapunov functionals and using M-matrix theory and LaSalle invariant principle, the sets of global exponentially dissipativity are characterized parametrically. It is proven herein that there are 2(2n(2)-n) equilibria for an n-neuron memristor-based neural network and they are located in the derived globally attractive sets. It is also shown that memristor-based recurrent neural networks with time-varying delays are stabilizable at the origin of the state space by using a linear state feedback control law with appropriate gains. Finally, two numerical examples are discussed in detail to illustrate the characteristics of the results.\nTitle:\nGlobal exponential dissipativity and stabilization of memristor-based recurrent neural networks with time-varying delays.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.692, mean value for all the writers is 8.648, which makes it 2.597 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents new theoretical results on the invariance and attractivity of memristor-based cellular neural networks (MCNNs) with time-varying delays. First, sufficient conditions to assure the boundedness and global attractivity of the networks are derived. Using state-space decomposition and some analytic techniques, it is shown that the number of equilibria located in the saturation regio...\nTitle:", "model_inf_time": 1.8}, {"id": "41405", "output": "A Complete Algorithm for Synthesizing Planar Modular Fixtures", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nCommercially available modular fixturing systems typically include a lattice of holes with precise spacing and an assortment of precision locating and clamping modules that can be rigidly attached to the lattice. Currently, machinists manually design a suitable arrangement of these modules to hold a given part. This requires expertise and can delay production. Futhermore, a machinist may in many cases settle upon an arrangement that is not optimal for a given machining operation.In this paper we present an implemented algorithm that accepts a polygonal description of the part silhouette, and efficiently constructs the set of all feasible fixture designs that kinematically constrain the part in the plane. Each fixture is comprised of three locators rigidly attached to the lattice and one sliding clamp, and constrains the part without relying on friction.The algorithm is based on an efficient enumeration of admissible designs that exploits part geometry and a graphical force analysis. The algorithm run time is linear in the number of designs found, which is bounded by a polynomial in the number of part edges and the part's maximal diameter in lattice units. Our review of the literature suggests that this is the first fixturing algorithm that is complete in the sense that it is guaranteed to find all admissible fixture designs for an arbitrary polygonal part silhouette and to identify the optimal fixture relative to an arbitrary quality metric. The algorithm does not consider out-of-plane forces or motions; however, we view this planar result as an essential component of a larger algorithm that solves the 3-D fixture design problem by treating the planar and out-of-plane constraint problems separately. This approach is analogous to the widely used 3-2-1 fixture design heuristic, and appears to be applicable to a broad class of man-made parts.\nTitle:\nA Complete Algorithm For Designing Planar Fixtures Using Modular Components\n\nAbstract:\nModular fixtures are gaining wide use for flexible manufacturing and job shop machining. A modular fixture is an arrangement of fixture elements (fixels) that will locate and securely hold a given part. Typically, a human combines intuition with trial-and-error to design fixtures. In some cases designers are unable to design a fixture with given fixels and must resort to custom tooling. It is possible that human designers have overlooked a solution. It is also possible that no solution exists. In this paper the authors explore the existential question: given a fixture model and a part, does a fixture exist that will hold this part in form closure? If so, the authors say that the part is fixturable. The authors consider two classes of fixtures, one using 3 locators and a clamp, the other using 4 clamps. The authors provide one negative result-a class of cross-sections that is not fixturable-and two positive results-two classes of cross-sections that are guaranteed to be fixturable. These results give insight into the application range for different models of modular fixtures\nTitle:\nOn the existence of modular fixtures\n\nAbstract:\nA fixture is a device that locates and holds parts during machining or assembly. A modular fixture employs reusable components on a regular lattice. Given a part, machinists combine intuition with trial-and-error to design an appropriate fixture. When a machinist is unable to find a design, it may be the case that (1) a feasible design was overlooked or (2) no feasible design exists. Complete algorithms for modular fixturing, such as (Brost and Goldberg 1994), insure that no fixture design is overlooked. But the question remains: are there parts for which no modular fixture exists? For the class of modular fixtures using 3 locators and a clamp, we show that there exists a class of polygonal parts that cannot be fixtured. We believe that this is the first negative result in the area of fixturing. We also show two positive results, namely that a modular fixture always exists when we broaden the class of fixtures to include T-slot and narrow the class of parts. We show that one class of fixtures strictly dominates the other. These results raise a number of open problems concerning the existence of solutions for other classes of fixtures and parts and suggest a hierarchy of fixturing models.\nTitle:\nOn the existence of solutions in modular fixturing\n\nAbstract:\nTo fixture sheet metal parts for welding, we propose \"unilateral fixtures\" consisting of modular fixturing elements that lie almost completely on one side of the part. These are based on cylindrical jaws with conical grooves which provide the equivalent of 4 point contacts.We propose a two-phase procedure for designing unilateral fixtures. The first phase is a geometric algorithm that assumes the part is rigid and computes vg-grips (vertex-groove grips). The vg-grip algorithm uses a fast sufficient test for immobility to generate a list of vg-grips and find bounds on jaw cone angles for each. The second phase is a fast heuristic procedure that uses FEM to arrange secondary contacts to reduce part deformation.For a part described by n concave \"virtual vertices\", a list of vg-grips and minimum half cone angles for each vg-grip can be generated in O(n(2)) time. We also propose a quality metric based on the sensitivity of the part's orientation to an infinitesimal relaxation of the jaws that can be evaluated in constant time for a given fixture. For an FEM model with in nodes, the second phase takes O(m(3)r) time to arrange r secondary contacts for each vg-grip.\nTitle:\n\"Unilateral\" Fixturing Of Sheet Metal Parts Using Modular Jaws With Plane-Cone Contacts\n\nAbstract:\nA fixture is a device for locating and holding parts. Since the initial position and orientation of a part may be uncertain, the act of loading the part into the fixture must compensate for this uncertainty. Machinists often refer to the 3-2-1 rule: place the part onto 3-point contact with a horizontal support plane, slide the part along this plane into 2-point contact with the fixture, then translate along this edge until a 1-point contact uniquely locates the part. Finally, apply clamps (Mani and Wilson 1988; Chou, Chandru, and Barash 1989). This rule of thumb implicitly assumes both sensing and compliance: applied forces change as contacts are detected.In this paper; we geometrically formalize robotic fixture loading as a sensor-based compliant assembly problem and give a complete planning algorithm. We consider the class of modular fixtures that use three locators and one clamp (Brost and Goldberg 1996), and discuss a class of robot commands that cause the part to slide and rotate in the support plane. Sensing is achieved with binary contact sensors on each locator; compliance is achieved with a passive spring-loaded mechanism at the robot end effector. We extend the theory of sensor-based compliant motion planning (Lozano-Perez, Mason, and Taylor 1984; Erdmann 1986) to generalized-polygonal C-spaces, and give a complete planning algorithm: it is guaranteed to find a loading plan when one exists and to return a negative report otherwise. We report on experiments using the resulting plans. Finally we use this formalization to prove a sufficient condition for the 3-2-1 rule.\nTitle:\nA Complete Algorithm For Fixture Loading\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.159, mean value for all the writers is 8.648, which makes it 0.436 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCommercially-available modular fixturing systems typically include a square lattice of tapped and bushed holes with precise spacing and an assortment of precision locating and clamping elements that can be rigidly attached to the lattice using dowel pins or expanding mandrels. Currently, human expertise is required to synthesize a suitable arrangement of these elements to hold a given part. Besides being time consuming, if the set of alternatives is not systematically explored, the designer may fail to find an acceptable fixture or may settle upon a suboptimal fixture. The authors consider a class of modular fixtures that prevent a part from translating or rotating in the plane using four point contacts on the part's boundary. These fixtures are based on three round locators, each centered on a lattice point, and one translating clamp. The authors present an algorithm that accepts a polygonal part shape as input and synthesizes the set of all fixture designs that achieve form closure for the given part. The algorithm also allows the user to specify geometric access constraints on fixtures. If the part has n edges and its maximal diameter is d lattice units, the asymptotic running time of the algorithm is O(n5d5). The authors have implemented the algorithm and present example fixtures that it has synthesized. This implementation includes a metric to rank fixtures based on their ability to resist applied forces. The authors believe this is the first fixture synthesis algorithm that is complete in the sense that it is guaranteed to find an admissible fixture if one exists. Furthermore, the algorithm is guaranteed to find the optimal fixture, relative to any well-defined quality metric\nTitle:", "model_inf_time": 1.87}, {"id": "41406", "output": "Unsupervised Deslanting Techniques for Cursive Word Recognition", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents a cursive character recognizer, a crucial module in any Cursive Script Recognition system based on a segmentation and recognition approach.\nTitle:\nCombining neural gas and learning vector quantization for cursive character recognition\n\nAbstract:\nThis paper presents a survey on off-line Cursive Word Recognition. The approaches to the problem are described in detail. Each step of the process leading from raw data to the final result is analyzed. This survey is divided into two parts, the first one dealing with the general aspects of Cursive Word Recognition, the second one focusing on the applications presented in the literature.\nTitle:\nA survey on off-line Cursive Word Recognition\n\nAbstract:\n This paper presents a cursive character recognizer embedded in an off-line cursive script recognition system. The recognizer is composed of two modules: the first one is a feature extractor, the second one an LVQ. The selected feature set was compared to Zernike polynomials using the same classifier. Experiments are reported on a database of about 49000 isolated characters. \nTitle:\nCursive character recognition by learning vector quantization\n\nAbstract:\nThis paper presents a cursive character recognizer embedded in an off-line cursive script recognition system. The recognizer is composed of two modules: the first one is a feature extractor, the second one an LVQ. Experiments are reported on a database of about 58000 isolated characters.\nTitle:\nA neural cursive character recognizer\n\nAbstract:\nThis work presents an Offline Cursive Word Recognition System dealing with single writer samples. The system is based on a continuous density Hiddden Markov Model trained using either the raw data, or data transformed using Principal Component Analysis or Independent Component Analysis. Both techniques significantly improved the recognition rate of the system.Preprocessing, normalization and feature extraction are described as well as the training technique adopted. Several experiments were performed using a publicly available database. The accuracy obtained is the highest presented in the literature over the same data.\nTitle:\nOffline Cursive Word Recognition using Continuous Density Hidden Markov Models Trained with PCA or ICA Features\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.379, mean value for all the writers is 8.648, which makes it 1.477 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents new techniques for slant and slope removal in cursive handwritten words. Both methods require neither heuristics nor parameter tuning. This avoids the heavy experimental effort required to find the optimal configuration of a parameter set. A comparison between the new deslanting technique and the method proposed by Bozinovic and Srihari was made by measuring the performance of both methods within a word recognition system tested on different databases. The proposed technique is shown to improve the recognition rate by 10.8% relative to traditional normalization methods. Moreover, a long exploration of the parameter space is avoided.\nTitle:", "model_inf_time": 1.34}, {"id": "41407", "output": "Incremental Iconic Query Resolution Using Graph Subgraph Isomorphism", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAn important topic in pattern recognition is retrieval of candidate patterns from a database according to a given sample input pattern. Using graphs, the database retrieval problem is turned into a graph matching problem. In this paper we propose a method based on decision trees to filter a database of graphs according to a given input graph. The present paper extends previous work concerned with graph and subgraph isomorphism to the case of error-tolerant graph matching.\nTitle:\nDecision trees for error-tolerant graph database filtering\n\nAbstract:\nIn this paper, we propose a new algorithm for error-correcting subgraph isomorphism detection from a set of model graphs to an unknown input graph. The algorithm is based on a compact representation of the model graphs. This representation is derived from the set of model graphs in an off-line preprocessing step. The main advantage of the proposed representation is that common subgraphs of different model graphs are represented only once. Therefore, at run time, given an unknown input graph, the computational effort of matching the common subgraphs for each model graph onto the input graph is done only once. Consequently, the new algorithm is only sublinearly dependent on the number of model graphs. Furthermore, the new algorithm can be combined with a future cost estimation method that greatly improves its run-time performance.\nTitle:\nA new algorithm for error-tolerant subgraph isomorphism detection\n\nAbstract:\nGraphs are a powerful and universal data structure useful in various subfields of science and engineering. In this paper, we propose a new algorithm for subgraph isomorphism detection from a set of a priori known model graphs to an input graph that is given online. The new approach is based on a compact representation of the model graphs that is computed offline. Subgraphs that appear multiple times within the same or within different model graphs are represented only once, thus reducing the computational effort to detect them in an input graph. In the extreme case where all model graphs are highly similar, the run-time of the new algorithm becomes independent of the number of model graphs. Both a theoretical complexity analysis and practical experiments characterizing the performance of the new approach will be given.\nTitle:\nEfficient Subgraph Isomorphism Detection: A Decomposition Approach\n\nAbstract:\nA key concept in case-based reasoning is similarity. In this paper, we first propose a similarity measure for structured representations that is based on graph edit operations. Then we show how this similarity measure can be computed by means of state space search. Subsequently, subgraph isomorphism is considered as a special case of graph similarity and a new efficient algorithm for its detection is proposed. The new algorithm is particularly suitable if there is a large number of library cases being tested against an input graph. Finally, we present experimental results showing the computational efficiency of the proposed approach.\nTitle:\nSimilarity Measures for Structured Representations\n\nAbstract:\nIn this paper, a new method for graph and subgraph isomorphism detection based on a decision tree representation is proposed. The decision tree is generated off-line from a priori known model graphs. At run time the decision tree is used to detect all graph and subgraph isomorphisms from an input graph to any of the model graphs in time that is only polynomial in the size of the graphs and independent of the number of model graphs. However, the decision tree is of exponential size. In order to reduce the size of the decision tree, we propose two pruning techniques. Experimental results confirming the efficiency of the method will be given.\nTitle:\nSubgraph Isomorphism Detection in Polynominal Time on Preprocessed Model Graphs\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.488, mean value for all the writers is 8.648, which makes it 0.717 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents an algorithm for resolution of a sequence of incrementally changing iconic queries, against a known database of model graphs. The algorithm is based on a representation using graphs and subgraph isomorphism detection.\nTitle:", "model_inf_time": 1.36}, {"id": "41408", "output": "GOAFR: Asymptotically Optimal and Average-Case Efficient Geometric Routing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we present AFR, a new geometric mobile ad-hoc routing algorithm. The algorithm is completely distributed; nodes need to communicate only with direct neighbors in their transmission range. We show that if a best route has cost c, AFR finds a route and terminates with cost &Ogr;(c2) in the worst case. AFR is the first algorithm with cost bounded by a function of the optimal route. We also give a tight lower bound by showing that any geometric routing algorithm has worst-case cost $Ogr;(c2). Thus AFR is asymptotically optimal. We give a non-geometric algorithm that also matches the lower bound, but needs some memory at each node. This establishes an intriguing trade-off between geometry and memory.\nTitle:\nAsymptotically optimal geometric mobile ad-hoc routing\n\nAbstract:\nIn many applications of wireless ad hoc and sensor networks, position-awareness is of great importance. Often, as in the case of geometric routing, it is sufficient to have virtual coordinates, rather than real coordinates. In this paper, we address the problem of obtaining virtual coordinates based on connectivity information. In particular, we propose the first approximation algorithm for this problem and discuss implementational aspects.\nTitle:\nVirtual coordinates for ad hoc and sensor networks\n\nAbstract:\nIn this paper we propose a new routing paradigm, called pseudo-geometric routing. In pseudo-geometric routing, each node u of a network of computing elements is assigned a pseudo coordinate composed of the graph (hop) distances from u to a set of designated nodes (the anchors) in the network. On theses pseudo coordinates we employ greedy geometric routing. Almost as a side effect, pseudo-geometric routing is not restricted to planar unit disk graph networks anymore, but succeeds on general networks.\nTitle:\nGeometric routing without geometry\n\nAbstract:\nIn this paper we study the problem of scheduling wireless links in the geometric SINR model, which explicitly uses the fact that nodes are distributed in the Euclidean plane. We present the first NP-completeness proofs in such a model. In particular, we prove two problems to be NP-complete: Scheduling and One-Shot Scheduling. The first problem consists in finding a minimum-length schedule for a given set of links. The second problem receives a weighted set of links as input and consists in finding a maximum-weight subset of links to be scheduled simultaneously in one shot. In addition to the complexity proofs, we devise an approximation algorithm for each problem.\nTitle:\nComplexity in geometric SINR\n\nAbstract:\nIn hot-potato (deflection) routing, nodes in the network have no buffers for packets in transit, which causes conflicting packets to be deflected away from their destinations. We study one-to-many batch routing problems on arbitrary tree topologies. We present two hot-potato routing algorithms, one deterministic and one randomized, whose routing times are asymptotically near-optimal (within poly-logarithmic factors from optimal). Both algorithms are distributed and greedy; so, routing decisions are made locally, and packets are advanced towards their destinations whenever possible.\nTitle:\nNear-Optimal Hot-Potato Routing on Trees\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.437, mean value for all the writers is 8.648, which makes it 1.033 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper we present GOAFR, a new geometric ad-hoc routing algorithm combining greedy and face routing. We evaluate this algorithm by both rigorous analysis and comprehensive simulation. GOAFR is the first ad-hoc algorithm to be both asymptotically optimal and average-case efficient. For our simulations we identify a network density range critical for any routing algorithm. We study a dozen of routing algorithms and show that GOAFR outperforms other prominent algorithms, such as GPSR or AFR.\nTitle:", "model_inf_time": 1.53}, {"id": "41409", "output": "Stochastic Framework for Articulated 3D Human Motion Tracking", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present a hybrid framework for articulated 3-D human motion tracking from multiple synchronized cameras with potential uses in surveillance systems. Although the recovery of 3-D motion provides richer information for event understanding, existing methods based on either deterministic search or stochastic sampling lack robustness or efficiency. We therefore propose a hybrid sample-and-refine framework that combines both stochastic sampling and deterministic optimization to achieve a good compromise between efficiency and robustness. Similar motion patterns are used to learn a compact low-dimensional representation of the motion statistics. Sampling in a low-dimensional space is implemented during tracking, which reduces the number of particles drastically. We also incorporate a local optimization method based on simulated physical force/moment into our framework, which further improves the optimality of the tracking. Experimental results on several real human motion sequences show the accuracy and robustness of our method, which also has a higher sampling efficiency than most particle filtering-based methods.\nTitle:\nA Hybrid Framework for 3-D Human Motion Tracking\n\nAbstract:\nIn this paper, we present a 3D registration algorithm based on simulated physical force/moment for articulated human motion tracking. Provided with sparsely reconstructed 3D human surface points from multiple synchronized cameras, the tracking problem is equivalent to fitting the 3D model to the scene points. The simulated physical force/ moment generated by the displacement between the model and the scene points is used to align the model with the scene points in an Iterative Closest Points (ICP) [1] approach. We further introduce a hierarchical scheme for model state updating, which automatically incorporates human kinematic constraints. Experimental results on both synthetic and real data from several unconstrained motion sequences demonstrate the efficiency and robustness of our proposed method.\nTitle:\nArticulated object registration using simulated physical force/moment for 3D human motion tracking\n\nAbstract:\nOcclusion is a difficult problem for visual tracking and we use multiple wide baseline cameras to deal with occlusion. We propose a data fusion approach for visual tracking using multiple cameras with overlapping fields of view. First, we present a spatial and temporal recursive Bayesian filter to fuse information from multiple cameras. An adaptive particle filter is formulated to realize the spatial and temporal recursive Bayesian filter. Our algorithm is able to recover the target's position even under complete occlusion in a camera.\nTitle:\nAdaptive Particle Filter for Data Fusion of Multiple Cameras\n\nAbstract:\nThis paper presents a new method for segmentation of medical images by extracting organ contours, using minimal path deformable models incorporated with statistical shape priors. In our approach, boundaries of structures are considered as minimal paths, i.e., paths associated with the minimal energy, on weighted graphs. Starting from the theory of minimal path deformable models, an intelligent \"worm\" algorithm is proposed for segmentation, which is used to evaluate the paths and finally find the minimal path. Prior shape knowledge is incorporated into the segmentation process to achieve more robust segmentation. The shape priors are implicitly represented and the estimated shapes of the structures can be conveniently obtained. The worm evolves under the joint influence of the image features, its internal energy, and the shape priors. The contour of the structure is then extracted as the worm trail. The proposed segmentation framework overcomes the short-comings of existing deformable models and has been successfully applied to segmenting various medical images.\nTitle:\nMedical image segmentation using minimal path deformable models with implicit shape priors.\n\nAbstract:\nWe apply a multi-target recursive Bayes filter, the probability hypothesis density (PHD) filter, to a visual tracking problem: tracking a variable number of human groups in video. First, we use background subtraction to detect human groups which appear as foreground blobs. The PHD filter is implemented using sequential Monte Carlo methods; and the centroids of the foreground blobs are used as the measurements to update the PHD filter. Our experimental results show that when human groups appear, merge, split, and disappear in the field of view of a camera, our method can track them correctly\nTitle:\nTracking a Variable Number of Human Groups in Video Using Probability Hypothesis Density\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.716, mean value for all the writers is 8.648, which makes it 0.911 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, we present a stochastic framework for articulated 3D human motion tracking. Tracking full body human motion is a challenging task, because the tracking performance normally suffers from several issues such as self-occlusion; foreground segmentation noise and high computational cost. In our work, we use explicit 3D reconstructions of the human body based on a visual hull algorithm as our system input, which effectively eliminates self-occlusion. To improve tracking efficiency as well as robustness, we use a Kalman particle filter framework based on an interacting multiple model (IMM). The posterior density is approximated by a set of weighted particles, which include both sample means and covariances. Therefore, tracking is equivalent to searching the maximum a posteriori (MAP) of the probability distribution. During Kalman filtering, several dynamical models of human motion (e.g., zero order, first order) are assumed which interact with each other for more robust tracking results. Our measurement step is performed by a local optimization method using simulated physical force/moment for 3D registration. The likelihood function is designed to be the fitting score between the reconstructed human body and our 3D human model, which is composed of a set of cylinders. This proposed tracking framework is tested on a real motion sequence. Our experimental results show that the proposed method improves the sampling efficiency compared with most particle filter based methods and achieves high tracking accuracy.\nTitle:", "model_inf_time": 1.59}, {"id": "41410", "output": "CI-Graph: A Conditionally Independent Submapping Approach for Efficient SLAM", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSubmapping and graphical methods have been shown to be valuable approaches to simultaneous localization and mapping (SLAM), providing significant advantages over the classical extended Kalman filter (EKF) solution: they are faster and, when using local coordinates, produce more consistent estimates. The main contribution of this paper is CI-Graph SLAM, a novel algorithm that is able to efficiently map large environments by building a graph of submaps and a spanning tree of this graph with the following properties: (1) any pair of neighboring submaps in the spanning tree are conditionally independent and (2) the current submap is always up to date, containing the marginal probabilities of the submap variables given all previous measurements. Thanks to these properties, an old submap can be updated at any time by performing a single propagation from the current map to the old submap along the spanning tree. This operation is required only when a map is revisited, with a cost linear with the number of maps in the loop. At the end of the experiment the method performs a single propagation through the whole tree, recovering exactly the same marginals for all the map variables as the EKF\u2013SLAM algorithm does, without ever needing to compute the whole covariance matrix. To evaluate CI-Graph performance in extremely loopy environments, the method was tested using a synthetic Manhattan world. The behavior of the algorithm in large real environments is shown using the public data sets from the RAWSEEDS project in which a robot equipped with a trinocular camera traversed indoor and outdoor environments with several loops and revisited areas. Loops are robustly closed using a novel technique that detects candidate loop closures using a visual vocabulary tree and filters them using temporal and geometric constraints. Our experiments show that when using frontal cameras, the technique outperforms FAB-MAP. The epipolar geometry of the loop-closing images is used to find feature matches that are imposed on the CI-Graph to correct the submap estimations along the loop. \u00a9 2010 Wiley Periodicals, Inc.\nTitle:\nCI-Graph simultaneous localization and mapping for three-dimensional reconstruction of large and complex environments using a multicamera system\n\nAbstract:\nSimultaneous localization and mapping (SLAM) algorithms based on local maps have been demonstrated to be well suited for mapping large environments as they reduce the computational cost and improve the consistency of the final estimation. The main contribution of this paper is a novel submapping technique that does not require independence between maps. The technique is based on the intrinsic structure of the SLAM problem that allows the building of submaps that can share information, remaining conditionally independent. The resulting algorithm obtains local maps in constant time during the exploration of new terrain and recovers the global map in linear time after simple loop closures without introducing any approximations besides the inherent extended Kalman filter linearizations. The memory requirements are also linear with the size of the map. As the algorithm works in a covariance form, well-known data-association techniques can be used in the usual manner. We present experimental results using a handheld monocular camera, building a map along a closed-loop trajectory of 140 m in a public square, with people and other clutter. Our results show that the combination of conditional independence, which enables the system to share the camera and feature states between submaps, and local coordinates, which reduce the effects of linearization errors, allow us to obtain precise maps of large areas with pure monocular SLAM in real time.\nTitle:\nLarge-Scale SLAM Building Conditionally Independent Local Maps: Application to Monocular Vision\n\nAbstract:\nLocal maps algorithms have demonstrated to be well suited for mapping large environments as can reduce the computational cost and improve the consistency of the final estimation. In this paper we present a new technique that allows the use of local mapping algorithms in the context of EKF SLAM but without the constrain of probabilistic independence between local maps. By means of this procedure, salient features of the environment or vehicle state components as velocity or global attitude, can be shared between local maps without affecting the posterior joining process or introducing any undesirable approximations in the final global map estimate. The overload cost introduced by the technique is minimum since building up local maps does not require any additional operations apart from the usual EKF steps. As the algorithm works with covariance matrices, well-known data association techniques can be used in the usual manner. To test the technique, experimental results using a monocular camera in an outdoor environment are provided. The initialization of the features is based on the inverse depth algorithm.\nTitle:\nScalable Slam Building Conditionally Independent Local Maps\n\nAbstract:\nIn this paper we describe an algorithm to compute cycle constraints that can be used in many graph-based SLAM algorithms; we exemplify it in hierarchical SLAM. Our algorithm incrementally computes the minimum cycle basis of constraints from which any other cycle can be derived. Cycles in this basis are local and of minimum length, so that the associated cycle constraints have less linearization problems. This also permits to construct regional maps, that is, it makes possible efficient and accurate intermediate mapping levels between local maps and the whole global map. We have extended our algorithm to the multi-robot case. We have tested our methodology using the Victoria Park data set with satisfactory results.\nTitle:\nFinding good cycle constraints for large scale multi-robot SLAM\n\nAbstract:\nIn this paper we show that all processes associated to the move-sense-update cycle of EKF SLAM can be carried out in time linear in the number of map features. We describe Divide and Conquer SLAM, an EKF SLAM algorithm where the computational complexity per step is reduced from O(n2) to O(n) (the total cost of SLAM is reduced from O(n3) to O(n2)). In addition, the resulting vehicle and map estimates have better consistency properties than standard EKF SLAM in the sense that the computed state covariance more adequately represents the real error in the estimation. Both simulated experiments and the Victoria Park Dataset are used to provide evidence of the advantages of this algorithm. Index Terms\u2014SLAM, Computational Complexity, Consistency, Linear Time.\nTitle:\nData Association in O(n) for Divide and Conquer SLAM\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.067, mean value for all the writers is 8.648, which makes it 1.211 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWhen solving the Simultaneous Localization and Mapping (SLAM) problem, submapping and graphical methods have shown to be valuable approaches that provide significant advantages over the standard EKF solution: they are faster and can produce more consistent estimates when using local coordinates. In this paper we present CI-Graph, a submapping method for SLAM that uses a graph structure to efficiently solve complex trajectories reducing the computational cost. Unlike other submapping SLAM approaches, we are able to transmit and share information through maps in the graph in a consistent manner by using conditionally independent submaps. In addition, the current submap always summarizes, without further computations, all information available making CI-Graph be an intrinsically \u201cup to date\u201d algorithm. Moreover, the technique is also efficient in memory requirements since it does not need to recover the full covariance matrix. To evaluate CI-Graph performance, the method has been tested using a synthetic Manhattan world and Victoria Park data set.\nTitle:", "model_inf_time": 1.99}, {"id": "41411", "output": "A Context-Sensitive Tourist Guide for Lancaster", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn mobile computing, context-awareness indicates the ability of a system to obtain and use information on aspects of the system environment. To implement context-awareness, mobile system components have to be augmented with the ability to capture aspects ...\nTitle:\nUsing history to improve mobile application adaptation\n\nAbstract:\nWe present the design, implementation, and evaluation of D-GRAID, a gracefully-degrading and quickly-recovering RAID storage array. D-GRAID ensures that most files within the file system remain available even when an unexpectedly high number of faults ...\nTitle:\nIntegrating Portable and Distributed Storage\n\nAbstract:\nDuring the first half of the 1990s, IBM developed a set of operating system products called Workplace OS that was based on the Mach 3.0 microkernel and Taligent's object-oriented TalOS. These products were intended to be scalable, portable and capable ...\nTitle:\nThe Role of Trace Modulation in Building Mobile Computing Systems\n\nAbstract:\nThe success of cloud computing can lead to large, centralized collections of virtual machine (VM) images. The ability to interactively search these VM images at a high semantic level emerges as an important capability. This paper examines the opportunities and challenges in creating such a search capability, and presents early evidence of its feasibility.\nTitle:\nThe Case for Content Search of VM Clouds\n\nAbstract:\nThis paper shows how today's complex computing landscape can be understood in simple terms through a 4-tier model. Each tier represents a distinct and stable set of design constraints that dominate attention at that tier. There are typically many alternative implementations of hardware and software at each tier, but all of them are subject to the same set of design constraints. We discuss how this simple and compact framework has explanatory power and predictive value in reasoning about system design.\n\n\nTitle:\nThe Computing Landscape of the 21st Century\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.364, mean value for all the writers is 8.648, which makes it 1.095 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper describes work carried out as part of the GUIDE project at Lancaster University. The overall aim of the project is to develop a context-sensitive tourist guide for visitors to the city of Lancaster. Visitors are equipped with portable GUIDE ...\nTitle:", "model_inf_time": 1.06}, {"id": "41412", "output": "Evaluating Greenbelt Effectiveness for Delaying Urban Sprawl: A Multi-Model Approach", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe use a GIS-based agent-based model (ABM), named dynamic ecological exurban development (DEED), with spatial data in hypothetical scenarios to evaluate the individual and interacting effects of lot-size zoning and municipal land-acquisition strategies on possible forest-cover outcomes in Scio Township, a municipality in Southeastern Michigan. Agent types, characteristics, behavioural methods, and landscape perceptions (i.e. landscape aesthetics) are empirically informed using survey data, spatial analyses, and a USDA methodology for mapping landscape aesthetic quality. Results from our scenario experiments computationally verified literature that show large lot-size zoning policies lead to greater sprawl, and large lot-size zoning policies can lead to increased forest cover, although we found this effect to be small relative to municipal land acquisition. The return on land acquisition for forest conservation was strongly affected by the location strategy used to select parcels for conservation. Furthermore, the location strategy for forest conservation land acquisition was more effective at increasing aggregate forest levels than the independent zoning policies, the quantity of area acquired for forest conservation, and any combination of the two. The results using an integrated GIS and ABM framework for evaluating land-use development policies on forest cover provide additional insight into how these types of policies may act out over time and what aspects of the policies were more influential towards the goal of maximising forest cover.\nTitle:\nEvaluating The Effects Of Land-Use Development Policies On Ex-Urban Forest Cover: An Integrated Agent-Based Gis Approach\n\nAbstract:\nAlthough survival analysis is known to outperform logistic regression, theoretically and according to evidence from other disciplines, little is known about how true this is in situations where the goal is detecting spatial predictors of land change. Furthermore, with the increasing availability of longitudinal land-change data, evidence is needed on the relative performance of these two different methods in situations with differing levels of data abundance. To fill this gap, we generated a pseudo land-change data set using an agent-based model of residential development in a virtual landscape. This agent-based model simulated the decisions of homebuyers in choosing residential locations based on the values of several spatial variables. Pseudo land-change maps, generated by the agent-based model with different weights on these spatial variables, were exposed to statistical analysis under the logistic and survival approaches. We evaluated how well the two approaches could reveal the spatial variables that were used in the agent-based model and compared the performance of the two methods when land-change data were collected under different sampling frequencies. Our results suggest that survival analysis outperforms logistic regression in detecting the variables that were included in agent decisions, largely because it takes into account time-dependent variables. Also, this research suggests that various properties of land-change processes like amount of developed area and access of agents to information affect the relative performance of these statistical approaches aimed at uncovering land-change predictor variables.\nTitle:\nComparative performance of logistic regression and survival analysis for detecting spatial predictors of land-use change\n\nAbstract:\nThe use of object-orientation for both spatial data and spatial process models facilitates their integration, which can allow exploration and explanation of spatial-temporal phenomena. In order to better understand how tight coupling might proceed and to evaluate the possible functional and efficiency gains from such a tight coupling, we identify four key relationships affecting how geographic data (fields and objects) and agent-based process models can interact: identity, causal, temporal and topological. We discuss approaches to implementing tight integration, focusing on a middleware approach that links existing GIS and ABM development platforms, and illustrate the need and approaches with example agent-based models.\nTitle:\nSpatial process and data models: Toward integration of agent-based models and GIS\n\nAbstract:\nIn this paper, we identify two distinct notions of accuracy of land-use models and highlight a tension between them. A model can have predictive accuracy: its predicted land-use pattern can be highly correlated with the actual land-use pattern. A model can also have process accuracy: the process by which locations or land-use patterns rare,determined can be consistent with real world processes. To balance these two potentially conflicting motivations, we introduce the concept of the invariant region, i.e., the area where land-use type is almost certain, and thus path independent; and the variant region, i.e., the area where land use depends;on a particular series of events, and is thus path dependent. We demonstrate our methods using an agent-based land-use model and using multi-temporal land-use data collected for Washtenaw County, Michigan, USA. The results indicate that, using the methods we describe, researchers can improve their ability to communicate how well their model performs, the situations or instances in which it,does not perform well, and the cases in which it is relatively unlikely to predict well because of either path dependence or stochastic uncertainty.\nTitle:\nPath Dependence And The Validation Of Agent-Based Spatial Models Of Land Use\n\nAbstract:\nThis paper presents the conceptual design and application of a new land-change modelling framework that represents geographical, sociological, economic, and ecological aspects of a land system. The framework provides an overarching design that can be extended into specific model implementations to evaluate how policy, land-management preferences, and land-market dynamics affect (and are affected by) land-use and land-cover change patterns and subsequent carbon storage and flux. To demonstrate the framework, we implement a simple integration of a new agent-based model of exurban residential development and land-management decisions with the ecosystem process model BIOME-BGC. Using a stylized scenario, we evaluate the influence of different exurban residential-land-management strategies on carbon storage at the parcel level over a 48-year period from 1958 to 2005, simulating stocks of carbon in soil, litter, vegetation, and net primary productivity. Results show 1) residential parcels with management practices that only provided additions in the form of fertilizer and irrigation to turfgrass stored slightly more carbon than parcels that did not include management practices, 2) conducting no land-management strategy stored more carbon than implementing a strategy that included removals in the form of removing coarse woody debris from dense tree cover and litter from turfgrass, and 3) the removal practices modelled had a larger impact on total parcel carbon storage than our modelled additions. The degree of variation within the evaluated land-management practices was approximately 42,104 kg C storage on a 1.62 ha plot after 48 years, demonstrating the substantial effect that residential land-management practices can have on carbon storage.\nTitle:\nEffects of land markets and land management on ecosystem function: A framework for modelling exurban land-change\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.582, mean value for all the writers is 8.648, which makes it 1.65 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe present several models of residential development at the rural\u2013urban fringe to evaluate the effectiveness of a greenbelt located beside a developed area, for delaying development outside the greenbelt. First, we develop a mathematical model, under two assumptions about the distributions of service centers, that represents the trade-off between greenbelt placement and width, their effects on the rate of development beyond the greenbelt, and how these interact with spatial patterns of aesthetic quality and the locations of services. Next, we present three agent-based models (ABMs) that include agents with the potential for heterogeneous preferences and a landscape with the potential for heterogeneous attributes. Results from experiments run with a one-dimensional ABM agree with the starkest of the results from the mathematical model, strengthening the support for both models. Further, we present two different two-dimensional ABMs and conduct a series of experiments to supplement our mathematical analysis. These include examining the effects of heterogeneous agent preferences, multiple landscape patterns, incomplete or imperfect information available to agents, and a positive aesthetic quality impact of the greenbelt on neighboring locations. These results suggest how width and location of the greenbelt could help determine the effectiveness of greenbelts for slowing sprawl, but that these relationships are sensitive to the patterns of landscape aesthetic quality and assumptions about service center locations.\nTitle:", "model_inf_time": 2.17}, {"id": "41413", "output": "Ambient Assisted Living: Technologies and Services for an Aging Population", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAmbient assisted living (AAL) can be defined as \u201cthe use of information and communication technologies (ICT) in a person's daily living and working environment to enable them to stay active longer, remain socially connected and live independently into old age\u201d (www.aal-europe.eu). Research in the AAL community covers a wide range of topics, but one of the largest is human activity recognition and behavior understanding, with the objectives of detecting and recognizing actions, activity, and situations within an environment. AAL systems go beyond observing to interact with users. A frequently employed form of human-machine interaction is via prompts-for example, prompting the user to perform the next step in a sequence of actions. Another type of interaction is haptic responses, which have been used in systems for visually impaired people. Two important elements of an appropriate response are an understanding of the user's context and anticipatory capability. Context awareness underpins much of AAL research-human activities are generally context dependent. In the broader sense, context awareness uses sensor observations to abstract information about the current situation. Endowing a system with predictive capability lets it anticipate and thereby produce a timely and useful response.\nTitle:\nAmbient Assisted Living [Guest editors' introduction]\n\nAbstract:\nIn this paper, we describe a model-based behavior analysis system for assisted living. The goal is monitoring the well-being of a single occupant in a home. Behavior is defined as any pattern in a sequence of observations. In analyzing behavior in a smart home, we aim to detect gradual changes in behavior, and atypical (anomalous) behavior. The anomalous behavior may be the result of equipment failure or the result of significant variations in the behavior of the occupant. In the context of a smart home, both situations require human intervention although the response will differ. For the purpose of observing behavior, the smart home is equipped with embedded sensors that unobtrusively record various environmental parameters. Models of behavior are generated from the sensor data. These models are employed to detect trends and infer atypical behavior.\nTitle:\nBehavior Analysis for Assisted Living\n\nAbstract:\nThe main focus of this special issue is to bring together solutions from research and engineering for the automatic understanding of a complex scene, via a multimodal array of sensors with automation of adaptation as the theme connecting various research areas. Four papers comprise this special issue.\nTitle:\nGuest Editorial Introducing Automation and Engineering for Ambient Intelligence\n\nAbstract:\nThe objective is to detect activities taking place in a home and to create a model of behavior for the occupant. A behavior is a pattern in the sequence of activities. An array of sensors captures the status of appliances. Models for the occupant's activities are built from the captured data using supervised and unsupervised learning techniques. The models of behavior are built using the hidden Markov model (HMM) technique. Predictive models can be used in a number of ways: to enhance user experience, to maximize resource usage efficiency, for safety and security. This work focuses on supporting independent living and enhancing quality of life of older persons. The ultimate goal is for the system to distinguish between normal and anomalous behavior. In this paper, we present the results of comparing supervised and unsupervised classification techniques applied to the problem of modeling activity for the purpose of modeling behavior in a home.\nTitle:\nMonitoring Behavior With An Array Of Sensors\n\nAbstract:\nWe propose an adaptive tracking system for assisted living that integrates user information about emergency events. Information fusion between user data and visual data is performed in order to estimate and assess the situation at hand. The system is able to dynamically switch between different segmentation and tracking algorithms improving its performance, as shown by the proposed examples.\nTitle:\nAn Adaptive Tracker for Assisted Living\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.817, mean value for all the writers is 8.648, which makes it 0.144 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe papers in this special section focus on the technologies and services that support ambient assisted living (ASL). It is now generally accepted that technology has a role to play in caring for this aging population. The concept of ambient-assisted living (AAL) is a term coined to describe the use of information and communication technologies in people\u2019s daily living and working environment to enable them to stay active longer, remain socially connected, and live independently into old age.\nTitle:", "model_inf_time": 1.46}, {"id": "41414", "output": "Teaching Machines to Author Chinese Handwriting.", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWriter identification is an important topic for pattern recognition and artificial intelligence. Traditional methods rely heavily on sophisticated hand-crafted features to represent the characteristics of different writers. In this paper, we propose an end-to-end framework for online text-independent writer identification by using a recurrent neural network (RNN). Specifically, the handwriting dat...\nTitle:\nEnd-to-End Online Writer Identification With Recurrent Neural Network.\n\nAbstract:\nThis paper proposes a new radical-based approach for online handwritten Chinese character recognition. The approach is novel in three respects: statistical classification of radicals, over-segmentation Of characters into candidate radicals, and lexicon-driven recognition of characters. Currently, we have applied the approach to Chinese characters of left-right structure and are extending to other structures. Preliminary results on a sample set of 4,284 characters consisting of 1,118 radicals demonstrate the superiority of the proposed approach.\nTitle:\nA New Radical-Based Approach To Online Handwritten Chinese Character Recognition\n\nAbstract:\nOnline handwriting recognition is gaining renewed interest owing to the increase of pen computing applications and new pen input devices. The recognition of Chinese characters is different from western handwriting recognition and poses a special challenge. To provide an overview of the technical status and inspire future research, this paper reviews the advances in online Chinese character recognition (OLCCR), with emphasis on the research works from the 1990s. Compared to the research in the 1980s, the research efforts in the 1990s aimed to further relax the constraints of handwriting, namely, the adherence to standard stroke orders and stroke numbers and the restriction of recognition to isolated characters only. The target of recognition has shifted from regular script to fluent script in order to better meet the requirements of practical applications. The research works are reviewed in terms of pattern representation, character classification, learning/adaptation, and contextual processing. We compare important results and discuss possible directions of future research.\nTitle:\nOnline recognition of Chinese characters: the state-of-the-art.\n\nAbstract:\nThe intelligent analysis of video data is currently in wide demand because a video is a major source of sensory data in our lives. Text is a prominent and direct source of information in video, while the recent surveys of text detection and recognition in imagery focus mainly on text extraction from scene images. Here, this paper presents a comprehensive survey of text detection, tracking, and rec...\nTitle:\nText Detection, Tracking and Recognition in Video: A Comprehensive Survey.\n\nAbstract:\nThis paper describes a system for handwritten Chinese text recognition integrating language model. On a text line image, the system generates character segmentation and word segmentation candidates, and the candidate paths are evaluated by character recognition scores and language model. The optimal path, giving segmentation and recognition result, is found using a pruned dynamic programming search method. We evaluate various language models, including the character-based n-gram, word-based n-gram, and hybrid n-gram models. Experimental results on the HIT-HW database show that the language models improve the recognition performance remarkably.\nTitle:\nIntegrating Language Model in Handwritten Chinese Text Recognition\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.725, mean value for all the writers is 8.648, which makes it 0.919 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nRecent deep learning based approaches have achieved great success on handwriting recognition. Chinese characters are among the most widely adopted writing systems in the world. Previous research has mainly focused on recognizing handwritten Chinese characters. However, recognition is only one aspect for understanding a language, another challenging and interesting task is to teach a machine to aut...\nTitle:", "model_inf_time": 1.16}, {"id": "41415", "output": "Supporting Children's Collaborative Learning: Insights from Research Projects", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe developed a new system called CarettaKids that supports face-to-face collaborative learning by children. CarettaKids uses a sensing board based on the Radio Frequency Identification (RFID) technology to support collaboration in a shared space, and the Personal Digital Assistant (PDA) device to support activity in personal spaces. We also introduced this system into an actual classroom environment to evaluate its performance in support for children's collaborative learning, by analyzing the childrens' interaction. As a result, we have confirmed that CarettaKids's feature of transition between two spaces, makes it possible for children to reflect on problems detected in the shared space so as to find solutions in their respective personal space, and to engage in an active exchange of opinions in the shared space, based on ideas generated from personal-space learning.\nTitle:\nCarettaKids: a system for supporting children's face-to-face collaborative learning by integrating personal and shared spaces.\n\nAbstract:\nIn this paper, a system called GENTORO that supports children's storytelling activities is proposed. By using GENTORO, children can make a robot play their story in the real world augmented with mobile projected graphical images. Preliminary user studies have been conducted in collaboration with elementary school children and university students, in order to investigate the acceptance level of GENTORO and clarify its design requirements, respectively. The development of GENTORO is in progress. The future plans of the GENTORO project are discussed.\nTitle:\nA storytelling support system using robots and handheld projectors\n\nAbstract:\nIn this paper we describe our project to design a system that can be used as a teaching aid to support group learning in elementary school education. The system enhances the learning outcome for pupils who have studied environmental problems using a textbook, by allowing them to construct a town in a physical space and to assess the construction through computer simulations. The system was designed in collaboration with teachers and their pupils in elementary schools. Lessons learned from the collaborative design processes are described.\nTitle:\nDesign of an interactive system for group learning support\n\nAbstract:\nIn this paper, a rhythm instruction tool for school children using vibration devices is discussed. The proposed system called T-RHYTHM is for supporting individual children in playing musical instruments or singing, in solo or ensemble situations. T-RHYTHM provides each child with rhythm patterns of musical pieces through tactile senses, and supports her so that she can recognize her own rhythm without being confused by other children's performances or singing voices. The rhythm of the music given to individual children is determined based on a performance by an accompanist and transmitted to their own vibration devices via a wireless communication. We have evaluated T-RHYTHM in a music class in a Japanese elementary school and gained initial feedback from children and their teacher.\nTitle:\nSupporting children's rhythm learning using vibration devices\n\nAbstract:\nWe present the SketchMap system, which integrates outdoor and classroom activities to support children's collaborative learning. Individual children create maps near their school in an outdoor environment using a SketchMap client. The maps are uploaded to the SketchMap web server for sharing among the children, who have created maps of different areas. Children can edit or add new information to the maps in their classroom or in their home. The goal of the SketchMap project is to investigate whether integrating outdoor and classroom activities, and sharing children's experiences through the maps can actually promote their collaborative learning and nurture learning communities including teachers and parents. The SketchMap system has been used in \"Safety Map\" and \"Nature Exploration\" classes in a Japanese elementary school. Evaluation of the SketchMap system is in progress, and issues found through the educational practices are described.\nTitle:\nNurturing Learners' Communities By Creating And Sharing Maps\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.818, mean value for all the writers is 8.648, which makes it 1.851 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this talk, I will present key ideas and underlying philosophies of our research projects. I will also describe about several systems for supporting children's collaborative learning, such as CarettaKids, T-RHYTHM, and so on. Lessons learned through the design and e, aluation of these systems, and some thoughts for DIGITEL researches will be discussed.\nTitle:", "model_inf_time": 1.43}, {"id": "41416", "output": "Comparative Study of Scheduling Algorithms for Dynamic SMP Clusters with Data Transfers on the Fly", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe paper concerns program scheduling methods oriented towards \"System on Chip\" (SoC) -based modular parallel architectures with efficient features of inter-processor communication. In the assumed architecture, a global communication network connects many SoC modules in which SMP clusters are dynamically created at program run-time to provide transfers of shared data on the fly for many processors at a time. Programs are represented as extended macro data flow graphs, which adequately describe communication between processor data caches and shared memory modules. The proposed scheduling algorithm is composed of two phases. The first phase distributes program graph nodes among SoC modules, assuming full inter-processor connection networks. It is implemented as a genetic algorithm with internally embedded ETF heuristics. The second phase of the algorithm schedules computation and communication inside SoC modules, so as to optimally use dynamic processor switching between clusters and data read on the fly mechanisms. Scheduling results of sample program graphs evaluated using simulation methods illustrate the efficiency of the proposed algorithm.\nTitle:\nTask Scheduling for SoC-Based Dynamic SMP Clusters with Communication on the Fly\n\nAbstract:\nThe paper concerns task scheduling in dynamic SMP clusters based on the notion of moldable computational tasks. Such tasks have been used as atomic elements in program scheduling algorithms with warranty of schedule length. For program execution, a special shared memory system architecture is used. It is based on dynamic processor clusters, organized around shared memory modules by switching of processors between memory module busses. Fast shared data transfers between processors inside such clusters can be performed through data reads on the fly. The dynamic SMP clusters are implemented inside system on chip (SoC) modules additionally connected by a central global network. A task scheduling algorithm is presented for program macro dataflow graphs for execution in the assumed architecture. The algorithm first identifies a set moldable tasks in a given program graph. Next, this set is scheduled using a 2-phase algorithm including allotment of resources to moldable tasks and final list scheduling, with a warranty of schedule length. The complete algorithm has been implemented as a program package and examined using simulated execution of scheduled program graphs.\nTitle:\nProgram Graph Structuring for Execution in Dynamic SMP Clusters Using Moldable Tasks\n\nAbstract:\nThe paper presents a method for the optimized control of program execution in modular systems based on Chip Multi Processor (CMP) modules interconnected by a special global inter-connection network. The applied CMP modules are based on communication on the fly, which is a novel efficient group communication paradigm implemented inside the interconnection network. Communication on the fly is based on a synergy of dynamic processor switching between memory modules and data read on the fly mechanism, which enables to many processors simultaneous reads of data, when present on shared memory buses. The paper presents a two-stage scheduling algorithm for programs expressed in a graph notation. The first stage schedules program tasks inside the CMP modules using an algorithm based on the notion of moldable tasks. In the result, a scheduled program moldable task graph is produced. The moldable task graph is next structurized for optimized communication execution in the global network working according to the look-ahead link connection setting paradigm. Results of simulation experiments evaluate the efficiency and other properties of the proposed architectural solution.\nTitle:\nProgram Execution Control in a Multi CMP Module System with a Look-Ahead Configured Global Network\n\nAbstract:\nThe paper presents an algorithm for scheduling parallel programs for execution in a parallel architecture based on dynamic SMP processor clusters with data transfers on the fly. The algorithm is based on the concept of moldable computational tasks. First, an initial program graph is decomposed into sub\u2013graphs, which are then treated as moldable tasks. So identified moldable tasks are then scheduled using an algorithm with warranted schedule length.\nTitle:\nScheduling moldable tasks for dynamic SMP clusters in soc technology\n\nAbstract:\nThe paper concerns task graph scheduling in parallel programs using the concept of moldable computational tasks for a parallel architecture based on dynamic SMP processor clusters with data transmissions on the fly. The presented algorithm for scheduling parallel program graphs decomposes an initial program graph to sub-graphs, which fulfill the definition of a moldable task. So identified moldable tasks are then scheduled using an algorithm with warranted schedule length.\nTitle:\nMoldable Task Scheduling in Dynamic SMP Clusters with Communication on the Fly\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.677, mean value for all the writers is 8.648, which makes it 1.731 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe paper presents comparison of the two scheduling algorithms developed for program structurization for execution in dynamic SMP clusters implemented in Systems on Chip (SoC) technology. SoC modules are built of a set of processors, memory modules and a multibus interconnection network. A set of such SoCs is interconnected by a global communication network. Inter-processor communication inside SoC modules uses a novel technique of data transfers on the fly. The algorithms present two different scheduling approaches. The first uses ETF-based genetically supported list scheduling heuristics to map nodes of a program to processors. The second is a clustering-based algorithm using Moldable Tasks (MT) to structure the graph. Both algorithms structure computations and local data transfers to introduce processor switching and data transfers on the fly. The algorithms were tested using a set of automatically generated parameterized program graphs. The results were compared to results obtained using a classic ETF-based list scheduling without data transmissions on the fly.\nTitle:", "model_inf_time": 1.82}, {"id": "41417", "output": "Preamble Pattern Design and Receiver Techniques for IEEE P802.15.4k LECIM Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we present preamble design for millimeter-wave single carrier wireless personal area networks. Several factors are considered for a successful preamble design, such as sequence complexity, operation range, associated delay, robustness to frequency offset. Complementary Golay sequences are selected, which combine flexibility and performance. Simulation results indicate at optimum threshold level, it is possible to reduce both false alarm probability and miss detection probability less than 10(-3) in non-line-of-sight channels.\nTitle:\nPreamble Design For Millimeter-Wave Single Carrier Wpans\n\nAbstract:\nThis study focuses on system throughput by taking into account the channel interference in IEEE 802.15.3c WPAN, which is based on the hybrid multiple access of CSMA/CA and TDMA, namely CSMA/CA-TDMA. To study the system throughput, we construct a novel analytical model by taking into consideration the channel interference caused by the hidden networks in CSMA/CA-TDMA. The obtained results show that the system throughput achieved by TDMA is highly affected by frame transmission in CSMA/CA. Furthermore, we show that channel interference, which causes a degradation in the system throughput, is a very significant problem in the IEEE 802.15.3c WPAN.\nTitle:\nNumerical Throughput Analysis On Channel Interference In Ieee 802.15.3c Wpan Based On Hybrid Multiple Access Of Csma/Ca-Tdma\n\nAbstract:\nThis paper studies communication channel assignment in IEEE 802.15.3 WPANs. We clarify the channel interference problems as well as introduce an interesting channel interference caused by a hidden PNC in IEEE 802.15.3 WPANs. In this paper, we propose a predictive channel assignment method (PCAM) to efficiently avoid the channel interference among piconets and to minimize throughput degradation by channel interference. The basic idea of the proposed method is to synchronize the communication channel time among the piconets residing in the channel interference area, to predict the future channel interference time and to make a schedule of the communication channel time to minimize throughput degradation. The obtained results show that the proposed PCAM highly improves the system throughput by avoiding channel interference compared to the existing method of IEEE 802.15.3 WPANs.\nTitle:\nA Predictive Channel Assignment Method (PCAM) for Interference Avoidance in IEEE 802.15.3 WPANs.\n\nAbstract:\nThis paper presents a low power coherent \u03c0/4-shift QPSK demodulator LSIC for personal communication systems in Japan. The developed LSIC, implemented on a CMOS 0.8 \u03bcm standard cell with 2-V operation, achieves better frame error rate performance and lower power consumption than conventional demodulators. It uniquely employs (1) a reverse-modulation carrier recovery scheme with a -\u03c0/4 signal-phase rotator and a band-width-varied carrier filter, (2) a clock recovery scheme using initial clock phase estimation, (3) a fully digital orthogonal detection scheme suitable for low power consumption, and (4) a burst detection scheme using phase information in preamble. Performance evaluation confirms that the demodulator LSIC reduces the irreducible frame error rate by 40%, and achieves an Eb/No improvement of 3 dB at the frame error rate of 10% compared with differential detection under the Rayleigh fading channel common in personal communication\nTitle:\nA low power demodulator LSIC for personal communications: high performance coherent detection demodulator\n\nAbstract:\nWe propose a multi-layer space-time block coded orthogonal frequency division multiplexing (Multi-Layer STBC OFDM) scheme. It combines the spatial diversity and spatial multiplexing in order to exploit the benefits of both techniques. The transmit antennas are divided into several layers and each layer is encoded by STBC. The antennas transmit OFDM signals in order to deal with frequency-selective fading channels. In addition to Exhaustive Detection Scheme which applies the same detection at each subcarrier independently, we exploit the subcarrier correlation to develop two subcarrier-grouping based detection schemes. The subcarriers bound into the same group share the same detection parameters, e.g., layer detection order or nulling matrices, which are obtained at the center subcarrier of that group. The proposed schemes can significantly reduce the complexity with very small performance loss. The simulation results show that compared with Exhaustive Detection Scheme, the propose detection scheme is capable of reducing 51.79% complexity with only 0.5dB performance loss for 4 X 4 system, and reducing 69.96% complexity with only 1.0dB performance loss for 6 X 6 system.\nTitle:\nSubcarrier-Grouping Based Detection Schemes For Multi-Layer Stbc Coded Mimo-Ofdm\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.82, mean value for all the writers is 8.648, which makes it 2.706 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this work, we evaluate preamble patterns of IEEE P802.15.4k Low Energy Critical Infrastructure Monitoring Networks (LECIM) DSSS PHY. Due to energy constraints of the P802.15.4k systems, preambles should have robust performance to realize high throughput at low Eb/No situations. Our results reveal that the detection performances of the proposed 2-octet and 4-octet preambles are not adequate. For the case of 4-octet preamble, the main drawback is poor pattern design. To improve the performance of the 4-octet preamble, we propose a novel pattern. The new 4-octet preamble pattern achieves the required detection rate of 10\u22123 at the target Eb/No of 4 dB. For the case of the 2-octet preamble, its length is the limiting factor. We focus on the receiver side techniques. We propose using aperture and flywheel techniques to improve detection. Using these techniques, target detection rate is achieved and higher throughputs at low Eb/No values are enabled.\nTitle:", "model_inf_time": 2.23}, {"id": "41418", "output": "Interrupting Business Process Activities: A Data-Driven Approach to Context Preservation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nRecently, a variety of workflow patterns was suggested for capturing different aspects in process-aware information systems (PAISs) including control and data flow, resources, process change, and exception handling. All these patterns are highly relevant for implementing PAISs and for designing process modeling languages. However, current patterns provide only a partial answer to the question which business functions a designer might want to reuse when modeling processes. This paper presents a revised version of a collection of activity patterns to deal with this challenge. Each of them is related to a recurrent business function as it can be frequently found in process models (e.g., task execution request, notification, approval). We describe the identified activity patterns and their variants in detail. The main purpose of our paper is to discuss results from empirical studies, in which we analyzed more than 200 process models in order to evidence the practical relevance of the patterns. This includes a detailed analysis of the context in which activity patterns occur as well the frequency of this occurrence. These empirical findings can be used for the design of more intelligent, pattern-based process modeling tools.\nTitle:\nActivity patterns in process-aware information systems: basic concepts and empirical evidence\n\nAbstract:\nProcess-aware information systems will be not accepted by users if rigidity or idleness due to failures comes with them. When implementing business processes based on process management technology one fundamental goal is to ensure robustness of the resulting process-aware information system. Meeting this goal becomes extremely complicated if high flexibility demands need to be fulfilled. This paper shows how the AristaFlow BPM Suite assists process participants in coping with errors and exceptional situations in a flexible and robust way. In particular, we focus on novel error handling procedures and capabilities using the flexibility provided by ad-hoc changes not shown in other context so far.\nTitle:\nRobust and Flexible Error Handling in the AristaFlow BPM Suite.\n\nAbstract:\nBusiness processes are often characterized by high variability and dynamics, which cannot be always captured in contemporary process management systems (PMS). Adaptive PMS have emerged in recent years, but do not completely solve this problem. In particular, users are not adequately supported in dealing with real\u2013world exceptions. Exception handling usually requires manual interactions and necessary process adaptations have to be defined at the control flow level. Altogether, only experienced users are able to cope with these tasks. As an alternative, changes on process data (elements) can be more easily accomplished, and a more data\u2013driven view on (adaptive) PMS can help to bridge the gap between real\u2013world processes and computerized ones. In this paper we present an approach for data\u2013driven process control allowing for the automated expansion and adaptation of task nets during runtime. By integrating and exploiting context information this approach further enables automated exception handling at a high level and in a user\u2013friendly way. Altogether, the presented work provides an added value to current adaptive PMS.\nTitle:\nData\u2013driven process control and exception handling in process management systems\n\nAbstract:\nUsually, for a particular business process a multitude of variants exist. Each of them constitutes an adjustment of a reference process model to specific requirements building the process context. While some progress has been achieved regarding the configuration of process variants, there exists only little work on how to accomplish this in a sound and efficient manner, especially when considering the large number of process variants that exist in practice as well as the many syntactical and semantical constraints they have to obey. In this paper we discuss advanced concepts for the context- and constraint-based configuration of process variants, and show how they can be utilized to ensure soundness of the configured process variants. Enhancing process-aware information systems with the capability to easily configure sound process models, belonging to the same process family and fitting to the given application context, will enable a new quality in engineering process-aware information systems.\nTitle:\nGuaranteeing Soundness of Configurable Process Variants in Provop\n\nAbstract:\nCompanies increasingly adopt process management technology which offers promising perspectives for realizing flexible information systems. However, there still exist numerous process scenarios not adequately covered by contemporary information systems. One major reason for this deficiency is the insufficient understanding of the inherent relationships existing between business processes on the one side and business data on the other. Consequently, these two perspectives are not well integrated in many existing process management systems. This paper emphasizes the need for both object- and process-awareness in future information systems, and illustrates it along several examples. Especially, the relation between these two fundamental perspectives will be discussed, and the role of business objects and data as drivers for both process modeling and process enactment be emphasized. In general, any business process support should consider object behavior as well as object interactions, and therefore be based on two levels of granularity. In addition, data-driven process execution and integrated user access to processes and data are needed. Besides giving insights into these fundamental characteristics, an advanced framework supporting them in an integrated manner will be presented and its application to real-world process scenarios be shown. Overall, a holistic and generic framework integrating processes, data, and users will contribute to overcome many of the limitations of existing process management technology.\nTitle:\nProcess and Data: Two Sides of the Same Coin?\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.403, mean value for all the writers is 8.648, which makes it 0.644 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe capability to safely interrupt business process activities is an important requirement for advanced process-aware information systems. Indeed, exceptions stemming from the application environment often appear while one or more application-related process activities are running. Safely interrupting an activity consists of preserving its context, i.e., saying the data associated with this activity. This is important since possible solutions for an exceptional situation are often based on the current data context of the interrupted activity. In this paper, a data classification scheme based on data relevance and on data update frequency is proposed and discussed with respect to two different real-world applications. Taking into account this classification. a correctness criterion for interrupting running activities while preserving their context is proposed and analyzed.\nTitle:", "model_inf_time": 1.77}, {"id": "41419", "output": "Concurrency Fault Detection in Process Model Executions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nEnsuring anomaly-free process model executions is crucial in order to prevent fraud and security breaches. Existing anomaly detection approaches focus on the control flow, point anomalies, and struggle with false positives in the case of unexpected events. By contrast, this paper proposes an anomaly detection approach that incorporates perspectives that go beyond the control flow, such as, time and resources (i.e., to detect contextual anomalies). In addition, it is capable of dealing with unexpected process model execution events: not every unexpected event is immediately detected as anomalous, but based on a certain likelihood of occurrence, hence reducing the number of false positives. Finally, multiple events are analyzed in a combined manner in order to detect collective anomalies. The performance and applicability of the overall approach are evaluated by means of a prototypical implementation along and based on real life process execution logs from multiple domains.\nTitle:\nMulti-perspective Anomaly Detection in Business Process Execution Events.\n\nAbstract:\nThe analysis of process instance similarity offers valuable input for certain application fields including the evaluation of instance clusters, the identification of compliance abuses, and process optimization. In this paper, we discuss the topic of instance similarity in general: We show that similarity might be determined from different process perspectives such as control flow, time, and instance attributes. Each of these perspectives impose individual requirements on the similarity calculation concerning data and structure. Four metrics for process instance similarity are proposed covering different perspectives. The applicability and feasibility of the proposed metrics are evaluated based on a prototypical implementation and real-world process logs from the BPI challenges.\nTitle:\nProcess Instance Similarity: Potentials, Metrics, Applications.\n\nAbstract:\nProcesses control critical IT systems and business cases in dynamic environments. Hence, ensuring secure model executions is crucial to prevent misuse and attacks. In general, anomaly detection approaches can be employed to tackle this challenge. Existing ones analyze each process instance individually. Doing so does not consider attacks that combine multiple instances, e.g., by splitting fraudulent fund transactions into multiple instances with smaller \u201cunsuspicious\u201d amounts. The proposed approach aims at detecting such attacks. For this, anomalies between the temporal behavior of a set of historic instances (ex post) and the temporal behavior of running instances are identified. Here, temporal behavior refers to the temporal order between the instances and their events. The proposed approach is implemented and evaluated based on real life process logs from different domains and artificial anomalies.\nTitle:\nMulti Instance Anomaly Detection in Business Process Executions.\n\nAbstract:\nExisting business process anomaly detection approaches typically fall short in supporting experts when analyzing identified anomalies. Hereby, false positives and insufficient anomaly countermeasures might impact an organization in a severely negative way. This work tackles this limitation by basing anomaly detection on association rule mining. It will be shown that doing so enables to explain anomalies, support process change and flexible executions, and to facilitate the estimation of anomaly severity. As a consequence, the risk of choosing an inappropriate countermeasure is likely reduced which, for example, helps to avoid the termination of benign process executions due to mistaken anomalies and false positives. The feasibility of the proposed approach is shown based on a publicly available prototypical implementation as well as by analyzing real life logs with injected artificial anomalies.\nTitle:\nAssociation Rules for Anomaly Detection and Root Cause Analysis in Process Executions\n\nAbstract:\nFor enterprises it has become crucial to check compliance of their business processes with certain rules such as medical guidelines or financial regulations. When automating compliance checks on process models, existing approaches have mainly addressed process-specific compliance rules so far, i.e., rules that correspond to a particular process model. However, in practice, we will rather find process-independent compliance rules that are nevertheless to be checked over process models. Thus, in this paper, we present an approach that enables the instantiation and verification of process-independent compliance rules over process models using domain models. For this, we provide an intuitive visualization of compliance rules and compliance rule instances at user level and show how rules and instances can be formalized and verified at system level. The overall approach is validated by a pattern-based comparison to existing approaches and by means of a prototypical implementation.\nTitle:\nDesign and verification of instantiable compliance rule graphs in process-aware information systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.707, mean value for all the writers is 8.648, which makes it 0.904 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIt is crucial to ensure correct process model executions. However, existing process testing approaches struggle with the verification of concurrent resource access patters that can lead to concurrency faults, such as, deadlocks or data corruption during runtime. Thus, we provide a concurrency verification approach that exploits recorded executions to verify the most frequently occurring concurrent resource access patterns with low test execution time. A prototypical implementation along with real life and artificial process execution logs is utilized for an evaluation.\nTitle:", "model_inf_time": 1.36}, {"id": "41420", "output": "Accelerated Actor-Critic Learning via Alternative Learning Critic (ALC)", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe objective of this paper is to accelerate the process of policy improvement in reinforcement learning. The proposed Dyna-style system combines two learning schemes, one of which utilizes a temporal difference method for direct learning; the other uses relative values for indirect learning in planning between two successive direct learning cycles. Instead of establishing a complicated world model, the approach introduces a simple predictor of average rewards to actor-critic architecture in the simulation (planning) mode. The relative value of a state, defined as the accumulated differences between immediate reward and average reward, is used to steer the improvement process in the right direction. The proposed learning scheme is applied to control a pendulum system for tracking a desired trajectory to demonstrate its adaptability and robustness. Through reinforcement signals from the environment, the system takes the appropriate action to drive an unknown dynamic to track desired outputs in few learning cycles. Comparisons are made between the proposed model-free method, a connectionist adaptive heuristic critic, and an advanced method of Dyna-Q learning in the experiments of labyrinth exploration. The proposed method outperforms its counterparts in terms of elapsed time and convergence rate.\nTitle:\nPolicy improvement by a model-free Dyna architecture.\n\nAbstract:\nIn the paper a method called the reinforcement nonlinear control system (RNCS) is proposed for intelligent control of nonlinear systems. The RNCS integrates two artificial neural networks (ANNs) into a learning system. One of the ANNs plays a role of identification predictor and the other as a linearization controller. The RNCS is trained to linearize a plant by a desired linear dynamics imposed. The plant-network system is treated as a linear system and the wealth of linear control theories can be applied after linearizing the control affine plant\nTitle:\nReinforcement nonlinear control system\n\nAbstract:\nA neuro-fuzzy system which is embedded in the conventional control theory is proposed to tackle physical learning control problems in this paper. The control scheme is composed of two elements. The first element, the fuzzy sliding mode controller (FSMC), is used to drive the state variables to a specific snitching hyperplane or a desired trajectory. The second one is developed based on the concept of the self-organizing fuzzy cerebellar model articulation controller (FCMAC) and adaptive heuristic critic (AHC). Both compose a forward compensator to reduce the chattering effect or cancel the influence of system uncertainties, A geometrical explanation on how the FCMAC algorithm works is provided and some refined procedures of the AHC are presented as well. Simulations on smooth motion of three-link robot is given to illustrate the performance and applicability of the proposed control scheme.\nTitle:\nSmooth trajectory tracking of three-link robot: a self-organizing CMAC approach.\n\nAbstract:\nA modified fuzzy cerebellar model articulation controller (FCMAC) with reinforcement learning capability is introduced in this article. This model utilizes the likelihood scheme to predict the evaluation of successive actions. Based on an approximating evaluation model, the proper output (action) is always selected. The structure of the proposed FCMAC consists of three parts: a fuzzy quantizer, which is used to represent the associative mapping function from the receptive field to the actual memory; an action evaluation module, which models and produces the expected evaluation signal and an action selection unit that generates an action with the expectation of better performance using a probability distribution function that estimates an optimal action selection policy. To demonstrate its excellent performance, the proposed self-improving model is implemented as a neural network controller for the swing control of a pendulum system. The results from both the simulation and experiment demonstrates better performance and applicability of the proposed learning model.\nTitle:\nA Self-Improving Fuzzy Cerebellar Model Articulation Controller with Stochastic Action Generation\n\nAbstract:\nDyna-Q, a well-known model-based reinforcement learning RL method, interplays offline simulations and action executions to update Q functions. It creates a world model that predicts the feature values in the next state and the reward function of the domain directly from the data and uses the model to train Q functions to accelerate policy learning. In general, tabular methods are always used in Dyna-Q to establish the model, but a tabular model needs many more samples of experience to approximate the environment concisely. In this article, an adaptive model learning method based on tree structures is presented to enhance sampling efficiency in modeling the world model. The proposed method is to produce simulated experiences for indirect learning. Thus, the proposed agent has additional experience for updating the policy. The agent works backwards from collections of state transition and associated rewards, utilizing coarse coding to learn their definitions for the region of state space that tracks back to the precedent states. The proposed method estimates the reward and transition probabilities between states from past experience. Because the resultant tree is always concise and small, the agent can use value iteration to quickly estimate the Q-values of each action in the induced states and determine a policy. The effectiveness and generality of our method is further demonstrated in two numerical simulations. Two simulations, a mountain car and a mobile robot in a maze, are used to verify the proposed methods. The simulation result demonstrates that the training rate of our method can improve obviously.\nTitle:\nADAPTIVE MODEL LEARNING BASED ON DYNA-Q LEARNING\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.938, mean value for all the writers is 8.648, which makes it 0.247 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAn approach to accelerating the learning process of the actor-critic learning algorithm for reinforcement learning is presented. The algorithm was derived from principles based on the prediction of average rewards and temporal difference (TD) learning with averaged and discounted rewards. The derived algorithm was applied to neural networks, demonstrating their effective operation in nonlinear control problems. The motivation of the proposed algorithm was to elaborate how a learning scheme, implemented by artificial neural networks (ANNs), can speed up learning processes based on an arrangement akin to the Dyna-Q learning, where a simulative model of the controlled plant is established for virtual learning between two control cycles. Instead of modeling the complicated plant, the approach just introduced a simple predictor of rewards for virtual learning in simulation mode. Two TD learning methods based discounted and averaged rewards respectively, are used alternatively in the control and simulation mode to facilitate the derived algorithm. The proposed Alternative Learning Critic (ALC) algorithm consists of two sub-systems: one is Evaluation Predictor (EP), which performs an approximation of a long-term evaluation function, and the other is an immediate action selector, which is composed of two ANNs: Action Controller (AC) and Reinforcement Predictor (RP). The proposed learning scheme is then applied to control a pendulum system for tracking a desired trajectory to demonstrate its applausive performance and robustness. Through reinforcement signals from the environment, the system takes an appropriate action to a plant with unknown dynamics so the actual output of the plant can track the desired one concisely within a short learning cycles. Further, ALC is used as the compensator of a PI controller, which is actually only working well on a linear system, to control that pendulum system. The results show the affined system, trained ALC and the PI controller can man- - ipulate together on a nonlinear system with unknown dynamics.\nTitle:", "model_inf_time": 1.88}, {"id": "41421", "output": "Secure Pattern Matching and Oblivious Automata Evaluation Protocols", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents efficient protocols for securely computing the following two problems: (1) The fundamental problem of pattern matching. This problem is defined in the two-party setting, where party holds a pattern and party holds a text. The goal of is to learn where the pattern appears in the text, without revealing it to or learning anything else about 's text. This problem has been widely studied for decades due to its broad applicability. We present several protocols for several notions of security. We further generalize one of our solutions to solve additional pattern matching-related problems of interest. (2) Our construction from above, in the malicious case, is based on a novel protocol for secure oblivious automata evaluation which is of independent interest. In this problem, party holds an automaton and party holds an input string, and they need to decide whether the automaton accepts the input, without learning anything else. Our protocol obtains full security in the face of malicious adversaries.\nTitle:\nAutomata Evaluation and Text Search Protocols with Simulation Based Security.\n\nAbstract:\nThis paper presents an efficient protocol for securely computing the fundamental problem of pattern matching. This problem is defined in the two-party setting, where party P1 holds a pattern and party P2 holds a text. The goal of P1 is to learn where the pattern appears in the text, without revealing it to P2 or learning anything else about P2\u2019s text. Our protocol is the first to address this problem with full security in the face of malicious adversaries. The construction is based on a novel protocol for secure oblivious automata evaluation which is of independent interest. In this problem party P1 holds an automaton and party P2 holds an input string, and they need to decide if the automaton accepts the input, without learning anything else.\nTitle:\nText search protocols with simulation based security\n\nAbstract:\nIndependence is a fundamental property needed to achieve security in fault-tolerant distributed computing. In practice, distributed communication networks are neither fully synchronous or fully asynchronous, but rather loosely synchronized. By this, we mean that in a communication protocol, messages at a given round may depend on messages from other players at the same round.These possible dependencies among messages create problems if we need $n$ players to announce independently chosen values. This task is called simultaneous broadcast. In this paper, we present the first constant round protocol for simultaneous broadcast in a reasonable computation model (which includes a common shared random string among the players). The protocol is provably secure under general cryptographic assumptions. In the process, we develop a new and stronger formal definition for this problem. Previously known protocols for this task required either $O(\\log n)$ or expected constant rounds to complete (depending on the computation model considered).\nTitle:\nA Protocol to Achieve Independence in Constant Rounds\n\nAbstract:\nWe present new protocols for the verification of space bounded polytime computations against a rational adversary. For such computations requiring sublinear space our protocol requires only a verifier running in sublinear-time. We extend our main result in several directions: (i) we present protocols for randomized complexity classes, using a new composition theorem for rational proofs which is of independent interest; (ii) we present lower bounds (i.e. conditional impossibility results) for Rational Proofs for various complexity classes.\nTitle:\nEfficient Rational Proofs for Space Bounded Computations.\n\nAbstract:\nOnion routing is a privacy-enabling protocol that allows users to establish anonymous channels over a public network. In such a protocol, parties send their messages through $$n$$ anonymizing servers (called a circuit ) using several layers of encryption. Several proposals for onion routing have been published in recent years, and TOR, a real-life implementation, provides an onion routing service to thousands of users over the Internet. This paper puts forward a new onion routing protocol which outperforms TOR by achieving forward secrecy in a fully non-interactive fashion, without requiring any communication from the router and/or the users and the service provider to update time-related keys. We compare this to TOR which requires $$O(n^2)$$ rounds of interaction to establish a circuit of size $$n$$ . In terms of the computational effort required to the parties, our protocol is comparable to TOR, but the network latency associated with TOR's high round complexity ends up dominating the running time. Compared to other recently proposed alternative to TOR, such as the PB-OR (PETS 2007) and CL-OR (CCS 2009) protocols, our scheme still has the advantage of being non-interactive (both PB-OR and CL-OR require some interaction to update time-sensitive information), and achieves similar computational performances. We performed implementation and simulation tests that confirm our theoretical analysis. Additionally, while comparing our scheme to PB-OR, we discovered a flaw in the security of that scheme which we repair in this paper. Our solution is based on the application of forward secure encryption. We design a forward secure encryption scheme (of independent interest) to be used as the main encryption scheme in our onion routing protocol.\nTitle:\nFully non-interactive onion routing with forward secrecy\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.5, mean value for all the writers is 8.648, which makes it 0.979 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents efficient protocols for securely computing the following two problems: (1) The fundamental problem of pattern matching. This problem is defined in the two-party setting, where party $$P_1$$P1 holds a pattern and party $$P_2$$P2 holds a text. The goal of $$P_1$$P1 is to learn where the pattern appears in the text, without revealing it to $$P_2$$P2 or learning anything else about $$P_2$$P2's text. This problem has been widely studied for decades due to its broad applicability. We present several protocols for several notions of security. We further generalize one of our solutions to solve additional pattern matching-related problems of interest. (2) Our construction from above, in the malicious case, is based on a novel protocol for secure oblivious automata evaluation which is of independent interest. In this problem, party $$P_1$$P1 holds an automaton and party $$P_2$$P2 holds an input string, and they need to decide whether the automaton accepts the input, without learning anything else. Our protocol obtains full security in the face of malicious adversaries.\nTitle:", "model_inf_time": 1.59}, {"id": "41422", "output": "Structured Event-Driven Decentralized Markov Decision Processes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nFormal treatment of collaborative multi-agent systems has been lagging behind the rapid progress in sequential decision making by individual agents. Recent work in the area of decentralized Markov Decision Processes (MDPs) has contributed to closing this gap, but the computational complexity of these models remains a serious obstacle. To overcome this complexity barrier, we identify a specific class of decentralized MDPs in which the agents' transitions are independent. The class consists of independent collaborating agents that are tied together through a structured global reward function that depends on all of their histories of states and actions. We present a novel algorithm for solving this class of problems and examine its properties, both as an optimal algorithm and as an anytime algorithm. To the best of our knowledge, this is the first algorithm to optimally solve a non-trivial subclass of decentralized MDPs. It lays the foundation for further work in this area on both exact and approximate algorithms.\nTitle:\nSolving transition independent decentralized Markov decision processes\n\nAbstract:\nThere has been substantial progress with formal models for sequential decision making by individual agents using the Markov decision process (MDP). However, similar treatment of multi-agent systems is lacking. A recent complexity result, showing that solving decentralized MDPs is NEXP-hard, provides a partial explanation. To overcome this complexity barrier, we identify a general class of transition-independent decentralized MDPs that is widely applicable. The class consists of independent collaborating agents that are tied together through a global reward function that depends upon both of their histories. We present a novel algorithm for solving this class of problems and examine its properties. The result is the first effective technique to solve optimally a class of decentralized MDPs. This lays the foundation for further work in this area on both exact and approximate solutions.\nTitle:\nTransition-independent decentralized markov decision processes\n\nAbstract:\n  To deal with the prohibitive complexity of calculating policies in Decentralized MDPs, researchers have proposed models that exploit structured agent interactions. Settings where most agent actions are independent except for few actions that affect the transitions and/or rewards of other agents can be modeled using Event-Driven Interactions with Complex Rewards (EDI-CR). Finding the optimal joint policy can be formulated as an optimization problem. However, existing formulations are too verbose and/or lack optimality guarantees. We propose a compact Mixed Integer Linear Program formulation of EDI-CR instances. The key insight is that most action sequences of a group of agents have the same effect on a given agent. This allows us to treat these sequences similarly and use fewer variables. Experiments show that our formulation is more compact and leads to faster solution times and better solutions than existing formulations. \nTitle:\nCompact Mathematical Programs For DEC-MDPs With Structured Agent Interactions\n\nAbstract:\nIn this paper we divide multi-agent policies into two categories: centralized ones and decentralized ones. They reflect different views of multi-agent systems and different decision-theoretic underpinnings. While the centralized policies specify the decision of the agents according to the global system state, the decentralized policies, which correspond to the decisions of situated agents, must assume only a partial knowledge of the system in each agent and must deal with communication explicitly. In this paper we relate these two types of policies by introducing a formal and systematic methodology for transforming centralized policies into a variety of decentralized policies. We introduce a set of transformation strategies, and provide a representation for discussing decentralized communication decisions. Through our experiments, we show that our methodology enables us to derive a class of interesting policies that have a range of expected utilities and amount of communication, and allows us to gain important insights into decentralized coordination strategies from a decision-theoretic perspective.\nTitle:\nMulti-agent policies: from centralized ones to decentralized ones\n\nAbstract:\nTechniques were developed in previous work for managing communication in a controlled satisficing manner in two layer distributed Bayesian Networks. DEC-MDPs were used to sequence the information transferred in order to guarantee the required confidence level. In this paper, we introduce multiple abstraction layers into the Distributed Bayesian Network as a way of carrying more useful information in transmitted data to further reduce the number of messages that need to be sent. An algorithm is developed to automatically generate appropriate abstraction data. Techniques are introduced to effectively incorporate this abstraction data set into the DEC-MDP framework. We show that the appropriate addition of abstraction data actions simplifies the DEC-MDP while reducing the expected communication cost. This work provides us with a formal view of the use of abstraction in agent cooperation and begins to give us an understanding of when the less abstract data needs to be transmitted.\nTitle:\nCommunication management using abstraction in distributed Bayesian networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.284, mean value for all the writers is 8.648, which makes it 0.311 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDecentralized MDPs provide a powerful formal framework for planning in multi-agent systems, but the complexity of the model limits its usefulness. We study in this paper a class of DEC-MDPs that restricts the interactions between the agents to a structured, event-driven dependency. These dependencies can model locking a shared resource or temporal enabling constraints, both of which arise frequently in practice. The complexity of this class of problems is shown to be no harder than exponential in the number of states and doubly exponential in the number of dependencies. Since the number of dependencies is much smaller than the number of states for many problems, this is significantly better than the doubly exponential (in the state space) complexity of DEC-MDPs. We also demonstrate how an algorithm we previously developed can be used to solve problems in this class both optimally and approximately. Experimental work indicates that this solution technique is significantly faster than a naive policy search approach.\nTitle:", "model_inf_time": 1.47}, {"id": "41423", "output": "A Novel Local Surface Descriptor for 3D Object Recognition and Surface Matching", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents a highly distinctive local surface feature called the TriSI feature for recognizing 3D objects in the presence of clutter and occlusion. For a feature point, we first construct a unique and repeatable Local Reference Frame (LRF) using the implicit geometrical information of neighboring triangular faces. We then generate three signatures from the three orthogonal coordinate axes of the LRF. These signatures are concatenated and then compressed into a TriSI feature. Finally, we propose an effective 3D object recognition algorithm based on hierarchical feature matching. We tested our TriSI feature on two popular datasets. Rigorous experimental results show that the TriSI feature was highly descriptive and outperformed existing algorithms under all levels of Gaussian noise, Laplacian noise, shot noise, varying mesh resolutions, occlusion, and clutter. Moreover, we tested our TriSI-based 3D object recognition algorithm on four standard datasets. The experimental results show that our algorithm achieved the best overall recognition results on these datasets.\nTitle:\nA novel local surface feature for 3D object recognition under clutter and occlusion.\n\nAbstract:\nLocal surface description is a critical stage for feature matching and recognition tasks. This paper presents a rotation invariant local surface descriptor, called 3D-Div. The proposed descriptor is based on the concept of 3D vector field's divergence, extensively used in electromagnetic theory. To generate a 3D-Div descriptor of a 3D surface, a local surface patch is parameterized around a randomly selected 3D point at a fixed scale. A unique Local Reference Frame (LRF) is then constructed at that 3D point using all the neighboring points forming the patch. A normalized 3D vector field is then computed at each point in the patch and referenced with LRF vectors. The 3D-Div descriptor is finally generated as the divergence of the reoriented 3D vector field. We tested our proposed descriptor on the challenging low resolution Washington RGB-D (Kinect) object dataset, for the task of automatic 3D object recognition. Reported experimental results show that 3D-Div based recognition achieves 93% accuracy as compared to 85% for existing state-of-the-art depth kernel descriptors [2].\nTitle:\nA Novel Local Surface Description for Automatic 3D Object Recognition in Low Resolution Cluttered Scenes\n\nAbstract:\nA number of 3D local feature descriptors have been proposed in literature. It is however, unclear which descriptors are more appropriate for a particular application. This paper compares nine popular local descriptors in the context of 3D shape retrieval, 3D object recognition, and 3D modeling. We first evaluate these descriptors on six popular datasets in terms of descriptiveness. We then test their robustness with respect to support radius, Gaussian noise, shot noise, varying mesh resolution, image boundary, and keypoint localization errors. Our extensive tests show that Tri-Spin-Images (TriSI) has the best overall performance across all datasets. Unique Shape Context (USC), Rotational Projection Statistics (RoPS), 3D Shape Context (3DSC), and Signature of Histograms of OrienTations (SHOT) also achieved overall acceptable results.\nTitle:\nPerformance Evaluation Of 3d Local Feature Descriptors\n\nAbstract:\nRecognizing 3D objects in the presence of noise, varying mesh resolution, occlusion and clutter is a very challenging task. This paper presents a novel method named Rotational Projection Statistics (RoPS). It has three major modules: local reference frame (LRF) definition, RoPS feature description and 3D object recognition. We propose a novel technique to define the LRF by calculating the scatter matrix of all points lying on the local surface. RoPS feature descriptors are obtained by rotationally projecting the neighboring points of a feature point onto 2D planes and calculating a set of statistics (including low-order central moments and entropy) of the distribution of these projected points. Using the proposed LRF and RoPS descriptor, we present a hierarchical 3D object recognition algorithm. The performance of the proposed LRF, RoPS descriptor and object recognition algorithm was rigorously tested on a number of popular and publicly available datasets. Our proposed techniques exhibited superior performance compared to existing techniques. We also showed that our method is robust with respect to noise and varyingmesh resolution. Our RoPS based algorithm achieved recognition rates of 100, 98.9, 95.4 and 96.0% respectively when tested on the Bologna, UWA, Queen's and Ca' Foscari Venezia Datasets. \u00a9 Springer Science+Business Media New York 2013.\nTitle:\nRotational Projection Statistics for 3D Local Surface Description and Object Recognition.\n\nAbstract:\nWe present a novel local surface description technique for automatic three dimensional (3D) object recognition. In the proposed approach, highly repeatable keypoints are first detected by computing the divergence of the vector field at each point of the surface. Being a differential invariant of curves and surfaces, the divergence captures significant information about the surface variations at each point. The detected keypoints are pruned to only retain the keypoints which are associated with high divergence values. A keypoint saliency measure is proposed to rank these keypoints and select the best ones. A novel integral invariant local surface descriptor, called 3D-Vor, is built around each keypoint by exploiting the vorticity of the vector field at each point of the local surface. The proposed descriptor combines the strengths of signature-based methods and integral invariants to provide robust local surface description. The performance of the proposed fully automatic 3D object recognition technique was rigorously tested on three publicly available datasets. Our proposed technique is shown to exhibit superior performance compared to state-of-the-art techniques. Our keypoint detector and descriptor based algorithm achieves recognition rates of 100%, 99.35% and 96.2% respectively, when tested on the Bologna, UWA and Ca' Foscari Venezia datasets. HighlightsA novel keypoint detection technique is proposed.Highly repeatable keypoints are detected by exploiting 3D vector field's divergence.A local surface descriptor (3D-Vor) is also introduced forsurface representation.The proposed 3D-Vor exploits the vector field's vorticity.A novel 3D object recognition algorithm is also proposed.Proposed technique is tested on 3 popular 3D object recognition datasets.Proposed technique achieves superior recognition rates on these 3D object datasets.\nTitle:\nA Novel Feature Representation for Automatic 3D Object Recognition in Cluttered Scenes\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.779, mean value for all the writers is 8.648, which makes it 0.965 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nLocal surface description is a critical stage for surface matching. This paper presents a highly distinctive local surface descriptor, namely TriSI. From a keypoint, we first construct a unique and repeatable local reference frame (LRF) using all the points lying on the local surface. We then generate three spin images from the three coordinate axes of the LRF. These spin images are concatenated and further compressed into a TriSI descriptor using the principal component analysis technique. We tested our TriSI descriptor on the Bologna Dataset and compared it to several existing methods. Experimental results show that TriSI outperformed existing methods under all levels of noise and varying mesh resolutions. The TriSI was further tested to demonstrate its effectiveness in 3D modeling. Experimental results show that it can accurately perform pairwise and multiview range image registration. We finally used the TriSI descriptor for 3D object recognition. The results on the UWA Dataset show that TriSI outperformed the state-of-the-art methods including spin image, tensor and exponential map. The TriSI based method achieved a high recognition rate of 98.4%.\nTitle:", "model_inf_time": 1.97}, {"id": "41424", "output": "Adaptive Spatio-Spectral Support for Spectral Reflectance Recovery", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nExisting hyperspectral imaging systems produce low spatial resolution images due to hardware constraints. We propose a sparse representation based approach for hyperspectral image super-resolution. The proposed approach first extracts distinct reflectance spectra of the scene from the available hyperspectral image. Then, the signal sparsity, non-negativity and the spatial structure in the scene are exploited to explain a high-spatial but low-spectral resolution image of the same scene in terms of the extracted spectra. This is done by learning a sparse code with an algorithm G-SOMP+. Finally, the learned sparse code is used with the extracted scene spectra to estimate the super-resolution hyperspectral image. Comparison of the proposed approach with the state-of-the-art methods on both ground-based and remotely-sensed public hyperspectral image databases shows that the presented method achieves the lowest error rate on all test images in the three datasets.\nTitle:\nSparse Spatio-Spectral Representation For Hyperspectral Image Super-Resolution\n\nAbstract:\nWe propose to recover spectral details from RGB images of known spectral quantization by modeling natural spectra under Gaussian Processes and combining them with the RGB images. Our technique exploits Process Kernels to model the relative smoothness of reflectance spectra, and encourages non-negativity in the resulting signals for better estimation of the reflectance values. The Gaussian Processes are inferred in sets using clusters of spatio-spectrally correlated hyperspectral training patches. Each set is transformed to match the spectral quantization of the test RGB image. We extract overlapping patches from the RGB image and match them to the hyperspectral training patches by spectrally transforming the latter. The RGB patches are encoded over the transformed Gaussian Processes related to those hyperspectral patches and the resulting image is constructed by combining the codes with the original processes. Our approach infers the desired Gaussian Processes under a fully Bayesian model inspired by Beta-Bernoulli Process, for which we also present the inference procedure. A thorough evaluation using three hyperspectral datasets demonstrates the effective extraction of spectral details from RGB images by the proposed technique.\nTitle:\nHyperspectral recovery from RGB images using Gaussian Processes.\n\nAbstract:\nHyperspectral imaging offers new opportunities for inter-person facial discrimination. However, compact and discriminative feature extraction from high dimensional hyperspectral image cubes is a challenging task. We propose a spatio-spectral feature extraction method based on the 3D Discrete Cosine Transform (3D-DCT). The 3D-DCT optimally compacts information in the low frequency coefficients. Therefore, we represent each hyperspectral facial cube by a small number of low frequency DCT coefficients and formulate Partial Least Square (PLS) regression for accurate classification. The proposed algorithm is evaluated on three standard hyperspectral face databases. Experimental results show that the proposed algorithm outperforms five current state of the art hyperspectral face recognition algorithms by a significant margin.\nTitle:\nHyperspectral Face Recognition Using 3d-Dct And Partial Least Squares\n\nAbstract:\nHyperspectral imaging offers new opportunities for face recognition via improved discrimination along the spectral dimension. However, it poses new challenges, including low signal-to-noise ratio, interband misalignment, and high data dimensionality. Due to these challenges, the literature on hyperspectral face recognition is not only sparse but is limited to ad hoc dimensionality reduction techniques and lacks comprehensive evaluation. We propose a hyperspectral face recognition algorithm using a spatiospectral covariance for band fusion and partial least square regression for classification. Moreover, we extend 13 existing face recognition techniques, for the first time, to perform hyperspectral face recognition.We formulate hyperspectral face recognition as an image-set classification problem and evaluate the performance of seven state-of-the-art image-set classification techniques. We also test six state-of-the-art grayscale and RGB (color) face recognition algorithms after applying fusion techniques on hyperspectral images. Comparison with the 13 extended and five existing hyperspectral face recognition techniques on three standard data sets show that the proposed algorithm outperforms all by a significant margin. Finally, we perform band selection experiments to find the most discriminative bands in the visible and near infrared response spectrum.\nTitle:\nHyperspectral face recognition with spatiospectral information fusion and PLS regression.\n\nAbstract:\nHyperspectral cameras acquire precise spectral information, however, their resolution is very low due to hardware constraints. We propose an image fusion based hyperspectral super resolution approach that employes a Bayesian representation model. The proposed model accounts for spectral smoothness and spatial consistency of the representation by using Gaussian Processes and a spatial kernel in a hierarchical formulation of the Beta Process. The model is employed by our approach to first infer Gaussian Processes for the spectra present in the hyperspectral image. Then, it is used to estimate the activity level of the inferred processes in a sparse representation of a high resolution image of the same scene. Finally, we use the model to compute multiple sparse codes of the high resolution image, that are merged with the samples of the Gaussian Processes for an accurate estimate of the high resolution hyperspectral image. We perform experiments with remotely sensed and ground-based hyperspectral images to establish the effectiveness of our approach.\nTitle:\nHierarchical Beta Process With Gaussian Process Prior For Hyperspectral Image Super Resolution\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.152, mean value for all the writers is 8.648, which makes it 0.43 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAccurate knowledge of spectral reflectance is crucial for hyperspectral image analysis. We propose a novel spectral reflectance recovery method by adaptive spatio-spectral support. The proposed technique is evaluated in both simulated and real illumination scenarios. A multi-illuminant hyper-spectral scene database has been collected and made publicly available. Experiments show that the adaptive illuminant estimation reduces the mean angular error of recovered spectra by 13%.\nTitle:", "model_inf_time": 1.66}, {"id": "41425", "output": "Automatic Generation of Trigger Events from State Conditions for Silicon Debug", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe decisions on when to acquire debug data during post-silicon validation are determined by trigger events that are programmed into on-chip trigger units. In this paper, we investigate how to design trigger units that are both resource-efficient and runtime programmable. To achieve these two goals, we introduce new architectural features, as well as an algorithm for automatically mapping trigger events onto trigger units.\nTitle:\nResource-Efficient Programmable Trigger Units for Post-Silicon Validation\n\nAbstract:\nOn-chip trigger units are employed for detecting events of interest during post-silicon validation and debugging. Their implementation constrains the trigger conditions that can be programmed at runtime. It is often the case that some trigger events of interest, which were not accounted for during design time, cannot be detected due to the constraints imposed by the hardware implementation of the trigger units. To address this issue, we present architectural features that can be included into the trigger units and discuss the algorithmic approach for automatically mapping trigger conditions onto the trigger units.\nTitle:\nMapping Trigger Conditions onto Trigger Units during Post-silicon Validation and Debugging\n\nAbstract:\nTo locate and correct design errors that escape pre-silicon verification, silicon debug has become a necessary step in the implementation flow of digital integrated circuits. Embedded logic analysis, which employs on-chip storage units to acquire data in real time from the internal signals of the circuit-under-debug, has emerged as a powerful technique for improving observability during in-system debug. However, as the amount of data that can be acquired is limited by the on-chip storage capacity, the decision on which signals to sample is essential when it is not known a priori where the bugs will occur. In this paper, we present accelerated algorithms for restoring circuit state elements from the traces collected during a debug session, by exploiting bitwise parallelism. We also introduce new metrics that guide the automated selection of trace signals, which can enhance the real-time observability during in-system debug.\nTitle:\nAlgorithms for state restoration and trace-signal selection for data acquisition in silicon debug\n\nAbstract:\nDesign errors (or bugs) inadvertently escape the pre-silicon verification process. Before committing to a re-spin, it is expected that the escaped bugs have been identified during post-silicon validation. This is however hindered by the presence of blocking bugs in one erroneous module that inhibit the search for bugs in other parts of the chip that process data received from the erroneous module. In this paper we discuss how to design a novel embedded debug module that can bypass blocking bugs and aid the designer in validating the first silicon.\nTitle:\nOn Bypassing Blocking Bugs during Post-Silicon Validation\n\nAbstract:\nEmbedded logic analysis has emerged as a powerful technique for identifying functional bugs during post-silicon validation, as it enables at-speed acquisition of data from the circuit nodes in real-time. Nonetheless, the amount of data that is observed is limited by the capacity of the on-chip trace buffers. This paper introduces an automated method for improving the utilization of the on-chip storage, by identifying a small set of trace signals from which a large number of states can be restored using a compute-efficient algorithm. This enlarged set of data can then be used to aid the search of functional bugs in the fabricated circuit.\nTitle:\nAutomated trace signals identification and state restoration for improving observability in post-silicon validation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.795, mean value for all the writers is 8.648, which makes it 0.979 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWhen searching for functional bugs in silicon, debug data is acquired after a trigger event occurs. A trigger event can be configured at run-time using a set of control registers that uniquely identify the event that initiates data acquisition. Nonetheless the values loaded in these programmable registers interact only with a set of pre-defined trigger signals that are selected at design-time. If the state conditions required for triggering cannot be expressed directly in terms of the pre-defined trigger signals, the common practice is that the designer manually searches for an equivalent trigger event that can be programmed on-chip. In this paper we investigate if trigger events can be automatically generated from a set of state conditions.\nTitle:", "model_inf_time": 1.43}, {"id": "41426", "output": "A heuristic approach to optimal realizations via the tight span", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA realization of a metric d on a finite set X is a weighted graph ( G , w ) whose vertex set contains X such that the shortest-path distance between elements of X considered as vertices in G is equal to d . Such a realization ( G , w ) is called optimal if the sum of its edge weights is minimal over all such realizations. Optimal realizations always exist, although it is NP-hard to compute them in general, and they have applications in areas such as phylogenetics, electrical networks and internet tomography. A. Dress (1984) showed that the optimal realizations of a metric d are closely related to a certain polytopal complex that can be canonically associated to d called its tight-span. Moreover, he conjectured that the (weighted) graph consisting of the zero- and one-dimensional faces of the tight-span of d must always contain an optimal realization as a homeomorphic subgraph. In this paper, we prove that this conjecture does indeed hold for a certain class of metrics, namely the class of totally-decomposable metrics whose tight-span has dimension two. As a corollary, it follows that the minimum Manhattan network problem is a special case of finding optimal realizations of two-dimensional totally-decomposable metrics.\nTitle:\nOptimal realizations of two-dimensional, totally-decomposable metrics\n\nAbstract:\nGiven a metric d on a finite set X, a realization of d is a weighted graph $G=(V,E,w\\colon \\ E \\to {\\Bbb R}_{0})$ with $X \\subseteq V$ such that for all $x,y \\in X$ the length of any shortest path in G between x and y equals d(x,y). In this paper we consider two special kinds of realizations, optimal realizations and hereditarily optimal realizations, and their relationship with the so-called tight span. In particular, we present an infinite family of metrics {dk}k\u22651, and\u2014using a new characterization for when the so-called underlying graph of a metric is an optimal realization that we also present\u2014we prove that dk has (as a function of k) exponentially many optimal realizations with distinct degree sequences. We then show that this family of metrics provides counter-examples to a conjecture made by Dress in 1984 concerning the relationship between optimal realizations and the tight span, and a negative reply to a question posed by Althofer in 1988 on the relationship between optimal and hereditarily optimal realizations.\nTitle:\nConcerning the Relationship between Realizations and Tight Spans of Finite Metrics\n\nAbstract:\nThe tight-span of a finite metric space is a polytopal complex that has appeared in several areas of mathematics. In this paper we determine the polytopal structure of the tight-span of a totally split-decomposable (finite) metric. These metrics are a generalization of tree-metrics and have importance within phylogenetics. In previous work, we showed that the cells of the tight-span of such a metric are zonotopes that are polytope isomorphic to either hypercubes or rhombic dodecahedra. Here, we extend these results and show that the tight-span of a totally split-decomposable metric can be broken up into a canonical collection of polytopal complexes whose polytopal structures can be directly determined from the metric. This allows us to also completely determine the polytopal structure of the tight-span of a totally split-decomposable metric. We anticipate that our improved understanding of this structure may lead to improved techniques for phylogenetic inference.\nTitle:\nThe polytopal structure of the tight-span of a totally split-decomposable metric.\n\nAbstract:\nGiven a finite set X and a proper metric D:X\u00d7X\u2192R\u22650 defined on X, we show that every block realization of D can be \u201cembedded\u201d canonically into the tight span T(D) of D and characterize the subsets of T(X) that can be obtained in that way as the \u201ccanonical image\u201d of the vertex set of a block realization.\nTitle:\nBlock realizations of finite metrics and the tight-span construction I: The embedding theorem\n\nAbstract:\nIn this note, we consider algorithms for computing virtual cut points in finite metric spaces and explain how these points can be used to study compatible decompositions of metrics generalizing the well-known decomposition of a tree metric into a sum of pairwise compatible split metrics.\nTitle:\nAn algorithm for computing virtual cut points in finite metric spaces\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.491, mean value for all the writers is 8.648, which makes it 0.134 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAn important problem that commonly arises in areas such as internet traffic-flow analysis, phylogenetics and electrical circuit design, is to find a representation of any given metric D on a finite set by an edge-weighted graph, such that the total edge length of the graph is minimum over all such graphs. Such a graph is called an optimal realization and finding such realizations is known to be NP-hard. Recently Varone presented a heuristic greedy algorithm for computing optimal realizations. Here we present an alternative heuristic that exploits the relationship between realizations of the metric D and its so-called tight span TD. The tight span TD is a canonical polytopal complex that can be associated to D, and our approach explores parts of TD for realizations in a way that is similar to the classical simplex algorithm. We also provide computational results illustrating the performance of our approach for different types of metrics, including l1-distances and two-decomposable metrics for which it is provably possible to find optimal realizations in their tight spans.\nTitle:", "model_inf_time": 1.6}, {"id": "41427", "output": "Query Expansion Using Web Information for ImageCLEF2007 Photo Task", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nImageCLEF photo task of this year is a little different from those of previous years. The caption field in image annotations and the narrative field in the text queries are removed, and the visual queries (example images) are also removed from the image collection too. In the new definition, the information that can be employed for queries and images is less than before, so that it becomes harder to match query words and annotations directly. To deal with this issue, we explore the web to expand queries and documents. Many images and text information can be found in the web, but we should face the noise embedded. The experiment shows the query expansion improves performance about 16.11%. The document expansion brings too much noise and the performance decrease 28.24% after expansion. The media mapping method that we proposed in previous years is used for query expansion too. The results of formal runs show this method is still very useful in the new task definitions.\nTitle:\nExperiment for Using Web Information to do Query and Document Expansion.\n\nAbstract:\nTwo kinds of intermedia are explored in ImageCLEFphoto2006. The approach of using a word-image ontology maps images to fundamental concepts in an ontology and measure the similarity between two images by using the kind-of relationship of the ontology. The approach of using an annotated image corpus maps images to texts describing concepts in the images, and the similarity of two images is measured by text counterparts using BM25. The official runs show that visual query and intermedia are useful. Comparing the runs using textual query only with the runs merging textual query and visual query, the latter improved 71%-119% of the performance of the former. Even in the situation which example images were removed from the image collection beforehand, the performance was still improved about 21%-43%.\nTitle:\nApproaches of using a word-image ontology and an annotated image corpus as intermedia for cross-language image retrieval\n\nAbstract:\nCross-language image retrieval facilitates the use of text query in one language and image query in one medium to access image collection with text description in another language/medium. The images with annotations are considered as a trans-media parallel corpus. In a media-mapping approach, we transform a query in one medium into a query in another medium by referencing to the aligned trans-media corpus. From the counterpart of results of an initial retrieval, we generate a new query in different medium. In the experiments, we adopted St. Andrews University Library's photographic collection used in ImageCLEF, and explored different models of language translation and media transformation. When both text query and image query are given together, the best MAP of a cross-lingual cross-media model 1L2M (one language translation plus two media transformations) achieve 87.15% and 72.39% of those of mono-lingual image retrieval in the 2004 and the 2005 test sets, respectively. That demonstrates our media transformation is quite useful, and it can compensate for the errors introduced in language translation.\nTitle:\nLanguage translation and media transformation in cross-language image retrieval\n\nAbstract:\nThis paper regards images with captions as a cross-media parallel corpus, and presents a corpus-based relevance feedback approach to combine the results of visual and textual runs. Experimental results show that this approach performs well. Comparing with the mean average precision (MAP) of the initial visual retrieval, the MAP is increased from 8.29% to 34.25% after relevance feedback from cross-media parallel corpus. The MAP of cross-lingual image retrieval is increased from 23.99% to 39.77% if combining the results of textual run and visual run with relevance feedback. Besides, the monolingual experiments also show the consistent effects of this approach. The MAP of monolingual retrieval is improved from 39.52% to 50.53% when merging the results of the text and image queries.\nTitle:\nA corpus-based relevance feedback approach to cross-language image retrieval\n\nAbstract:\nThis paper considers the strategies of query expansion, relevance feedback and result fusion to increase both relevance and diversity in photo retrieval. In the text-based retrieval only experiments, the run with query expansion has better MAP and P20 than that without query expansion, and only has 0.85% decrease in CR20. Although relevance feedback run increases both MAP and P20, its CR20 decreases 10.18% compared with non-feedback run. It shows that relevance feedback brings in relevant but similar images, thus diversity may be decreased. The run with both query expansion and relevance feedback is the best in the four text-based runs. In the content-based retrieval only experiments, the run without feedback outperforms the run with feedback. The latter has 10.84%, 9.13%, and 20.46% performance decrease in MAP, P20, and CR20. In the fusion experiment, integrating text-based and content-based retrieval not only reports more relevant images, but also more diverse ones.\nTitle:\nIncreasing Relevance and Diversity in Photo Retrieval by Result Fusion.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.988, mean value for all the writers is 8.648, which makes it 0.29 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nImageCLEF2007 photo task is different from those of the previous years in two aspects. The caption field in the image annotations and the narrative field in the text queries are removed, and the example images in the visual queries are also removed from the image collection. In the new definition, the information that can be employed is less than before. Thus matching query words and annotations directly is not feasible. This paper explores the web to expand queries and documents. The experiments show that query expansion improves the performance 16.11%, however, document expansion brings in too much noise and the performance decreases 28.24%. The media mapping method based on an image-text parallel corpus is regarded as query expansion. The results of the formal runs show this method performs the best. Compared with the performance of the models without expansion, the MAP improves about 86.69%~143.12%. Integration of the external and the internal resources gains no benefits in the further experiments.\nTitle:", "model_inf_time": 1.93}, {"id": "41428", "output": "Big Data: Challenges and Opportunities", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe design of a distributed data base is a complex and difficult task requiring careful consideration of data organization, data distribution, user interface, updating/retrieval schemes, program/data placement, security policies and reliability issues. In this paper, we have discussed a design methodology which can be used to design a distributed data base. This design methodology is a systematic way to guide the designer in making decisions during the requirement process phase, the design process phase and the implementation phase. We conclude this paper by examining the architectural issues in the design of distributed data bases.\nTitle:\nArchitectural issues in distributed data base systems\n\nAbstract:\nThe authors provide an overview of the current research and development directions in knowledge and data engineering. They classify research problems and approaches in this area and discuss future trends. Research on knowledge and data engineering is examined with respect to programmability and representation, design tradeoffs, algorithms and control, and emerging technologies. Future challenges are considered with respect to software and hardware architecture and system design. The paper serves as an introduction to this first issue of a new quarter.\nTitle:\nKnowledge and data engineering\n\nAbstract:\nAt the 2001 IEEE International Conference on Data Mining in San Jose, California,\non November 29 to December 2, 2001, there was a panel discussion on how data\nmining research meets practical development. One of the motivations for organizing the\npanel discussion was to provide useful advice for industrial people to explore their directions\nin data mining development. Based on the panel discussion, this paper presents\nthe views and arguments from the panel members, the Conference Chair and the Program\nCommittee Co-Chairs. These people as a group have both academic and industrial\nexperiences in different data mining related areas such as databases, machine learning,\nand neural networks. We will answer questions such as (1) how far data mining is from\npractical development, (2) how data mining research differs from practical development,\nand (3) what are the most promising areas in data mining for practical development.\nTitle:\nData Mining: How Research Meets PracticalDevelopment?\n\nAbstract:\nAs a flourishing field, e-learning at large (including distance learning, Web-based learning, and digital game-based learning) has attracted increasing attention from both industry and academic sectors. To facilitate the development of effective e-learning systems, scalable technologies that support an arbitrary number of users while providing them with a good learning environment are needed. In this introduction, we provide an overview of e-learning system development with respect to a layered reference architecture, including the Internet infrastructure layer, the conceptual/modeling layer, and the application layer. In addition, the three articles included in this special issue cover the issues of managing the learning objects in an open and scalable architecture, incorporation of learners' pedagogical features in Web-based learning environments, and support of digital game-based learning. All these issues, though not exhaustive, are important to ensure successful development of e-learning systems on an Internet platform.\nTitle:\nGuest Editors' Introduction: Emerging Internet Technologies for E-Learning\n\nAbstract:\nIn this chapter, we outline a vision of Web Intelligence (WI) research from the viewpoint of Brain Informatics (BI), a new interdisciplinary field that systematically studies the mechanisms of human information processing from both the macro and micro viewpoints by combining experimental cognitive neuroscience with advanced information technology. BI studies human brain from the viewpoint of informatics (i.e., human brain is an information processing system) and uses informatics (i.e., WI centric information technology) to support brain science study. Advances in instrumentation, e.g., based on fMRI and information technologies offer more opportunities for research in both Web intelligence and brain sciences. Further understanding of human intelligence through brain sciences fosters innovative Web intelligence research and development. WI portal techniques provide a powerful new platform for brain sciences. The synergy between WI and BI advances our ways of analyzing and understanding of data, knowledge, intelligence, and wisdom, as well as their interrelationships, organizations, and creation processes. Web intelligence is becoming a central field that revolutionizes information technologies and artificial intelligence to achieve human-level Web intelligence.\nTitle:\nWeb intelligence meets brain informatics\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.166, mean value for all the writers is 8.648, which makes it 0.442 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.\nTitle:", "model_inf_time": 1.29}, {"id": "41429", "output": "Qualitative Model-Based Testing of Simulink Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents a novel model-based testing approach developed in the MOGENTES project. The aim is to test embedded systems controlling a continuous environment, i.e., hybrid systems. We present our two key abstractions against which we systematically test for conformance. (1) Classical action systems are used to model the discrete controller behavior. (2) Qualitative differential equations are used to model the evolutions of the environment. The latter is based on a technique from the domain of Artificial Intelligence called qualitative reasoning. Mutation testing on these models is used to generate effective test cases. A test case generator has been developed that searches for all test cases that would kill a mutant. The mutant models represent our fault models. The generated test cases are then executed on the implementation in order to systematically exclude the possibility that a mutant has been implemented.\nTitle:\nModel-based mutation testing of hybrid systems\n\nAbstract:\nNowadays test engineers use various strategies for the design of test cases. Among others, test cases are designed on basis of structural coverage criteria or test cases are related to specific fault models. In this paper we evaluate these two techniques for test purpose design. We present a heuristic algorithm for the extraction of test cases from TGV's output, i.e., the test process. We discuss the problem of overlapping test purposes and illustrate improvements in terms of test execution time and in terms of number of test cases when minimizing this overlap. Furthermore, we present different strategies for the generation of fault-based test purposes. For our evaluation we apply the presented techniques to a Session Initiation Protocol (SIP) Registrar specification. All extracted test cases are executed against a commercial and an open source implementation of such a SIP Registrar.\nTitle:\nTest purpose generation in an industrial application\n\nAbstract:\nBlack-box components conceal parts of software execution paths, which makes systematic testing, e. g., via symbolic execution, difficult. In this paper, we use automata learning to facilitate symbolic execution in the presence of black-box components. We substitute black-boxes in a software system with learned automata that model them, enabling us to symbolically execute program paths that run through black-boxes. We show that applying the approach on real-world software systems incorporating black-boxes increases code coverage when compared to standard techniques.\nTitle:\nAutomata Learning for Symbolic Execution\n\nAbstract:\nEurope's industry in embedded system design is currently aiming for a better integration of tools that support their development, validation and verification processes. The idea is to combine model-driven development with model-based testing and model-based analysis. The interoperability of tools shall be achieved with the help of meta-models that facilitate the mapping between different modelling notations. However, the syntactic and semantic integration of tools is a complex and costly task. A common problem is that different tools support different subsets of a language. Furthermore, semantic differences are a major obstacle to sound integration efforts. In this paper we advocate an alternative, more pragmatic approach. We propose the exchange of test cases generated from the models instead of exchanging the models themselves. The advantage is that test cases have a much simpler syntax and semantics, and hence, the mapping between different tools is easier to implement and to maintain. With a formal testing approach with adequate testing criteria a set of test cases can be viewed as partial models that can be formally analysed. We demonstrate an integration of our test case generator Ulysses with the CADP toolbox by means of test case exchange. We generate test cases in Ulysses and verify properties in CADP. We also generate test cases in CADP and perform a mutation analysis in Ulysses.\nTitle:\nIntegrating Model-Based Testing and Analysis Tools via Test Case Exchange\n\nAbstract:\nModel-based testing is a popular black-box testing technology that enables automation of test generation and execution, while achieving a given coverage. The application of this technology to large and complex systems is still a challenging problem, due to the state-space explosion resulting from the size of specification models. In this paper, we evaluate a test-case generation approach that tackles this complexity along two axes. Firstly, our approach relies on a synchronous specification language for test models, thus avoiding the problem of interleaving actions. Secondly, our specification language enables incremental test-case generation by providing support for compositional modeling, in which each requirement or view of the system is expressed as a separate partial model. The individual requirement models are then naturally combined by conjunction, which is incrementally computed during the generation of tests. We apply our test-case generation technique to two large industrial case studies: (1) an electronic control unit (ECU) of an agricultural device; and (2) a railway interlocking system. We demonstrate the scalability of our approach by creating a series of test models with increasing complexity and report on the experimental results.\nTitle:\nScalable Incremental Test-case Generation from Large Behavior Models\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.782, mean value for all the writers is 8.648, which makes it 0.739 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nEmbedded systems are of growing importance in industry. For example, in a today's vehicle a huge number of embedded and communicating systems can be found. Exhaustive testing of such systems is a requirement, because changes after delivery and use are expensive and sometimes even impossible. In this paper we propose the use of qualitative models, which are an abstraction of quantitative physical models, for test case generation and test execution. In particular, we show how Simulink models from which control programs are automatically extracted can be tested with respect to qualitative models. Since Simulink models are heavily used in industry, the approach is of practical interest.\nTitle:", "model_inf_time": 1.48}, {"id": "41430", "output": "VDM Combinatorial Testing with Trace Reduction", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMost formal method notations are text based, while tools used in industry often use graphical notations, such as UML. This paper demonstrates how the power of both approaches can be combined by providing the automatic translation of VDM++ models to and from UML. The translation is implemented as a plugin for the popular Eclipse development environment by the open-source Overture initiative. Both UML class diagrams and sequence diagrams can be translated, the latter enabling the novel ability to link with the combinatorial test facility of Overture.\nTitle:\nConnecting UML and VDM++ with Open Tool Support\n\nAbstract:\nWe present a tool-supported approach to the validation of system-level timing properties in formal models of distributed real-time embedded systems. Our aim is to provide system architects with rapid feedback on the timing characteristics of alternative designs in the often volatile early stages of the development cycle. The approach extends the Vienna development method (VDM++), a formal object-oriented modeling language with facilities for describing real-time applications deployed over a distributed infrastructure. A new facility is proposed for stating and checking validation conjectures (assertions concerning real-time properties) against traces derived from the execution of scenarios on VDM++ models. We define validation conjectures and outline their semantics. We describe the checking of conjectures against execution traces as a formally-defined extension of the existing VDM++ tool set, and show tools to visualise traces and validation conjecture violations. The approach and tool support are illustrated with a case study based on an in-car radio navigation system.\nTitle:\nValidation Support for Distributed Real-Time Embedded Systems in VDM++\n\nAbstract:\nThe complexity of real-time embedded systems is increasing, for example due to the use of distributed architectures. An extension to the Vienna Development Method (VDM) is proposed to address the problem of deployment of software on distributed hardware. The limitations of the current notation are discussed and new language elements are introduced to overcome these deficiencies. The impact of these changes is illustrated by a case study. A constructive operational semantics is defined in VDM++ and validated using VDMTools. The associated abstract formal semantics, which is not specific to VDM, is presented in this paper. The proposed language extensions significantly reduce the modeling effort when describing distributed real-time systems in VDM++ and the revised semantics provides a basis for improved tool support.\nTitle:\nModeling and validating distributed embedded real-time systems with VDM++\n\nAbstract:\nThe lightweight use of formal methods is an effective approach to using formal specifications in various phases of software development. This paper proposes tool support specialised for the earlier stages of development that involves incremental and exploratory production of a formal specification. The VDMPad tool is described, as well as its features supporting incremental and exploratory development.\n\n\nTitle:\nVDMPad: a lightweight IDE for exploratory VDM-SL specification\n\nAbstract:\nWhen a system specified using the Vienna Development Method (VDM) is realised using code-generation, no guarantees are currently made about the correctness of the generated code. In this paper, we improve code-generation of VDM models by taking contract-based elements such as invariants and pre- and postconditions into account during the code-generation process. The contract-based elements of the Vienna Development Method Specification Language (VDM-SL) are translated into corresponding constructs in the Java Modelling Language (JML) and used to validate the generated code against the properties of the VDM model. VDM-SL and JML are both Design-by-Contract (DbC) languages, with the difference that VDM-SL supports abstract modelling and system specification, while JML is used for detailed specification of Java classes and interfaces. We describe the semantic differences between the contract-based elements of VDM-SL and JML and formulate the translation as a set of rules. We further demonstrate how dynamic JML assertion checks can be used to ensure the consistency of VDM\u2019s subtypes when a model is code-generated. The translator is fully automated and produces JML-annotated Java programs that can be checked for correctness using JML tools.\nTitle:\nAutomated translation of VDM to JML-annotated Java.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.388, mean value for all the writers is 8.648, which makes it 0.222 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nCombinatorial testing in VDM involves the automatic generation and execution of a large collection of test cases derived from templates provided in the form of trace definitions added to a VDM specification. The main value of this is the rapid detection of run-time errors caused by forgotten preconditions as well as broken invariants and post-conditions. Trace definitions are defined as regular expressions describing possible sequences of operation calls, and are conceptually similar to UML sequence diagrams. In this paper we present a tool enabling test automation based on VDM traces, and explain how it is possible to reduce large collections of test cases in different ways. Its use is illustrated with a small case study.\nTitle:", "model_inf_time": 1.4}, {"id": "41431", "output": "A Parallel Algorithm for Timing-Driven Global Routing of Standard Cells", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we propose three different parallel algorithms based on a state-of-the-art global router called TimberWolfSC. The parallel algorithms have been implemented by using the Message Passing Interface (MPI), and have been evaluated on a wide range of parallel platforms such as the Sun SparcCenter~1000 and the Intel Paragon. Our experimental results show good speedups and qualities from two of these parallel algorithms. We have been able to reduce runtimes of some circuits from half an hour to 5 minutes, obtained speedups of about 4.0 to 5.0 on 8 processors, with less than 2-3% degradation of quality of the solutions.\nTitle:\nParallel global routing algorithms for standard cells\n\nAbstract:\nSimulated annealing based standard cell placement for VLSI designs has long been acknowledged as a compute-intensive process. All previous work in parallel simulated annealing based placement has minimized area, but with deep submicron design, minimizing wirelength delay is also needed. The algorithm discussed in this paper is the first parallel algorithm for timing driven placement. We have used a very accurate Elmore delay model which is more complete intensive and hence the need for parallel placement is more apparent. Parallel placement is also needed for very large circuits that may not fit in the memory of a single processor. Therefore, our algorithm is circuit partitioned and can handle arbitrary large circuits on distributed memory multiprocessors. The algorithm, called mpi PLACE, has been tested on several large benchmarks on a variety of parallel architectures.\nTitle:\nA parallel circuit-partitioned algorithm for timing driven cell placement\n\nAbstract:\nIn this paper, we present some novel algorithms for scheduling hierarchical signal flow graphs in the domain of high-level synthesis. With complex chips that need to be designed in the future, it is expected that the runtimes of these scheduling algorithms will be quite large. The key contributions of this paper are as follows: First, we develop a novel extension of the sequential force-directed scheduling algorithm which naturally handles loops and conditionals by coming up with a scheme of scheduling hierarchical signal flow graphs. Second, we develop three new parallel algorithms for the scheduling problem. Our parallel algorithms are portable across a wide range of parallel platforms. We report results on a set of high-level synthesis benchmarks on 8-processor SGI Origin and a 64 processor IBM SP-2. While some parallel algorithms for VLSI CAD reported by earlier researchers have reported a loss of qualities of results, our parallel algorithms produce exactly the same results as the sequential algorithms on which they are based.\nTitle:\nParallel Algorithms for Force Directed Scheduling of Flattened and Hierarchical Signal Flow Graphs\n\nAbstract:\nCombinational logic synthesis is a very important but computationally expensive phase of VLSI system design. Parallel processing offers an attractive solution to reduce this design cycle rime. In this paper we describe ProperMIS, a portable parallel algorithm for logic synthesis based on the MIS multi-level logic synthesis system. As part of this work, we have developed novel parallel algorithms for the different logic transformations of the MIS system. Our algorithm uses art asynchronous message-driven computing model with no synchronizing barriers separating phases of parallel computation. The algorithm is portable across a wide variety of parallel architectures, and is built around a well-defined sequential algorithm interface, so that we can benefit from future expansion of the sequential algorithm. We present results on several MCNC and ISCAS benchmark circuits for a variety of shared memory and distributed processing architectures. Our implementation produces speedups of an average of 4 on 8 processors.\nTitle:\nParallel algorithms for logic synthesis using the MIS approach\n\nAbstract:\nIn deep sub-micron fabrication technology, clock skew is one of the dominant factors which determine system performance. Previous works in zero skew clock tree routing assume that the wires have uniform size, and previous wire-sizing algorithms for general signal nets do not produce the exact zero skew. In this paper, we first propose an algorithm to get the exact zero skew wire-sizing by using an iterative method to make the wire size improvement. Our experiments on benchmark clock trees show that the algorithm reduces the source sink delay more than 3 times that of the clock trees with uniform wire sizes and keeps the clock skew zero. Motivated by the computation intensive nature of the zero skew clock tree construction and wire-sizing, we propose a parallel algorithm using a cluster-based clock tree construction algorithm and our zero skew wire-sizing algorithm. Without sacrificing the quality of the solution, on the average we obtain speedups of 7.8 from the parallel clustering based clock tree construction algorithm on an 8 processor SUN SPARC Server 1000E shared memory multi-processor.\nTitle:\nA parallel algorithm for zero skew clock tree routing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.051, mean value for all the writers is 8.648, which makes it 0.344 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe timing-driven global routing problem is an extremely important and time consuming phase of any automated layout system. In this paper, by integrating high performance interconnection tree construction, wire-sizing, and switch-able segment channel optimization together, we propose an adaptive timing-driven global routing algorithm which minimizes the timing delay as well as circuit area. Our experiments on MONO benchmarks show that our timing-driven global routing algorithm reduces the maximum path delays significantly from the global router TimberWolfSC. Based on this adaptive timing-driven global routing algorithm, a parallel algorithm on timing-driven global routing for standard cells is given. This algorithm has been implemented on an 8 processor IBM J-40 shared memory multi-processor by using the Message Passing Interface (MPI). Our experimental results show good speedup and circuit delay results for this parallel algorithm using MONO benchmark circuits.\nTitle:", "model_inf_time": 1.66}, {"id": "41432", "output": "Automatic Generation of Graphical Data Encoding Languages", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper presents a graph-grammar based approach to multimedia document design. By extending an existing powerful graph grammar formalism with spatial relations, we are able to define the spatial and syntactic constraints graphically. An efficient syntax analyzer developed for the graph grammar formalism can validate and automatically perform a presentation on a user provided document structure.\nTitle:\nToward a Graphical Approach to Multimedia Document Design\n\nAbstract:\nThis paper presents a global layout approach used in a general-purpose visual language generation system system. Our approach is grammar-based graph drawing, in which layout rules are embedded in the productions of reserved graph grammars. Thus, the grammar formalism serves both the visual language grammar and the layout grammar. An example visual language is demonstrated.\nTitle:\nGrammar-Based Layout for a Visual Programming Language Generation System\n\nAbstract:\nRapid prototyping of domain-specific software requires asystematic software development methodology and user-friendlytools. Being both executable and easy to use, visuallanguages and their automatic generation mechanisms arehighly suitable for software prototyping. This paper presentsa software prototyping methodology based on thevisual language generation technology, that allows visualprototyping languages to be specified and generated usingan expressive graph grammar formalism. Executable prototypesand their verification and code generation are madepossible by syntax-directed computations. The paper demonstratesthis methodology through a prototyping examplebuilt on our current implementation.\nTitle:\nRapid Software Prototyping Using Visual Language Techniques\n\nAbstract:\nAs a commonly acceptable standard for guiding Web markup documents, XML allows the Internet users to create multimedia documents of their preferred structures and share with other people. The creation of various multimedia document structures, typically as trees, implies that some kinds of conversion mechanisms are needed for people using different structures to understand each other. This paper presents a visual approach to the representation and validation of multimedia document structures specified in XML and transformation of one structure to another. The underlying theory of our approach is a context-sensitive graph grammar formalism. The paper demonstrates the conciseness and expressiveness of the graph grammar formalism. An example XML structure is provided and its graph grammar representation, validation and transformation to a multimedia representation are presented.\nTitle:\nGraphical Transformation of Multimedia XML Documents\n\nAbstract:\nBased on the Reserved Graph Grammar (RGG), this paper presents a unified framework to manage model-based information on the Web in a hierarchical structure. The framework allows models, schemas, and data instances to be represented explicitly and uniformly. The uniform representation of the framework also enables simple user-defined graph transformation rules for different Web data models to translate schemas and data instances between different formats. In addition, the framework implements a set of prototype tools for users to identify meta-primitives at the meta-model level, to define a model or schema by specifying a set of graph grammar rules and to draw the structure of data instances. These features promote a wide scope of Web-related applications, such as information exchange between different organizations, and integration of data coming from heterogeneous information sources.\nTitle:\nManagement of Web Data Models Based on Graph Transformation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.577, mean value for all the writers is 8.648, which makes it 0.061 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nAiming at providing user-friendly means for exchange of digital artifacts, this paper presents a language-generation mechanism that allows graphical data-encoding languages and schemas to be specified and automatically generated. The generated language environments can automatically verify the syntactical structure of any constructed digital artifacts and, when translation specifications are provided, automatically translate a source artifact expressed in one encoding language or schema to its equivalent in another language or schema.\nTitle:", "model_inf_time": 1.1}, {"id": "41433", "output": "BinProlog: From Binary Logic to First-Class Logic Engines", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nBinProlog is an efficient, compact and portable Prolog system, based on a source-level transformation to continuation passing binary clauses, a completely side-effect free compiler written in Prolog and a simplified WAM, optimized for execution of binary logic programs. We give a short description of the compiler and the engine, some performance data, and we point out some of the reasons why BinProlog compares so well with systems based on the full WAM in terms of absolute performance.\nTitle:\nBinProlog: a Continuation Passing Style Prolog Engine\n\nAbstract:\nIn this paper we present the novel term representation of the BinWAM (a simplified WAM engine for executing binary logic programs) and evaluate its impact in BinProlog, a C-emulated system based on the BinWAM and on the mapping of logic programs to binary Prolog introduced in [13]. Terms in the BinWAM are compressed with a new technique called last argument overlapping which takes advantage of an unconventional untagged pointer representation, called tag-on-data. A Cheney-style copy-term algorithm using these term representations is described for BinProlog's fast copy once implementation of findall. While BinProlog's performance is competitive with the best commercial Prolog systems, its implementation is significantly simpler. Our analysis shows that this term representation and a limited amount of instruction folding on top of a reduced basic instruction set make the BinWAM a realistic alternative to its more complex forerunner.\nTitle:\nA Novel Term Compression Scheme and Data Representation in the BinWAM\n\nAbstract:\nStarting from a simple ecological metaphor, we introduce a new memory management scheme (heaplifting) implemented in BinProlog, a continuation passing style variant of WAM. We discuss copying garbage collection mechanisms based on heaplifting and an OR-parallel execution model. We point out some surprising similarities with related work on functional languages and the difficulties that arise in the context of nondeterministic execution. Finally, we describe the full implementation of two builtins: a recursive copy_term and a very fast heap-lifting based findall and we evaluate their impact on the performances of BinProlog.\nTitle:\nEcological Memory Management in a Continuation Passing Prolog Engine\n\nAbstract:\nIn the context of direct and reflection based extension mechanisms for the Jinni 2000 Java based Prolog system, we discuss the design and the implementation of a reflection based Prolog to Java interface. While the presence of dynamic type information on both the Prolog and the Java sides allows us to automate data conversion between method parameters, the presence of subtyping and method overloading makes finding the most specific method corresponding to a Prolog call pattern fairly difficult. We describe a run-time algorithm which closely mimics Java's own compile-time method dispatching mechanism and provides accurate handling of overloaded methods beyond the reflection package's limitations. As an application of our interfacing technique, a complete GUI library is built in Prolog using only 10 lines of application specific Java code.\nTitle:\nA Most Specific Method Finding Algorithm for Reflection Based Dynamic Prolog-to-Java Interfaces\n\nAbstract:\nWe overview the design and implementation of Jinni (Java INference engine and Networked Interactor), a lightweight, multi-threaded, pure logic programming language, intended to be used as a flexible scripting tool for gluing together knowledge processing components and Java objects in networked client/server applications, as well as through applets over the Web. Mobile threads, implemented by capturing first order continuations in a compact data structure sent over the network, allow Jinni components to interoperate with remote high performance BinProlog servers for CPU-intensive knowledge processing and with other Jinni components over the Internet. These features make Jinni a perfect development platform for intelligent mobile agent systems.\nTitle:\nTowards Inference and Computation Mobility: The Jinni Experiment\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.795, mean value for all the writers is 8.648, which makes it 0.125 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe describe the BinProlog system's compilation technology, runtime system and its extensions supporting first-class Logic Engines while providing a short history of its development, details of some of its newer re-implementations as well as an overview of the most important architectural choices involved in their design. With focus on its differences with conventional Warren Abstract Machine (WAM) implementations, we explain key details of BinProlog's compilation technique, which replaces the WAM with a simplified continuation passing runtime system (the \"BinWAM\"), based on a mapping of full Prolog to binary logic programs. This is followed by a description of a term compression technique using a \"tag-on-data\" representation. Later derivatives, the Java-based Jinni Prolog compiler and the recently developed Lean Prolog system refine the BinProlog architecture with first-class Logic Engines, made generic through the use of an Interactor interface. An overview of their applications with focus on the ability to express at source level a wide variety of Prolog built-ins and extensions covers these newer developments.\nTitle:", "model_inf_time": 1.69}, {"id": "41434", "output": "Case-Based Retrieval for Hemodialysis: A Quality Assessment Tool", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present a case-based retrieval system called RHENE (Retrieval of HEmodialysis in NEphrological disorders) working in the domain of patients affected by nephropatologies and treated with hemodialysis. Defining a dialysis session as a case, retrieval of past similar cases has to operate both on static and on dynamic (time-dependent) features, since most of the monitoring variables of a dialysis session are time series. In RHENE, retrieval relies upon a multi-step procedure. In particular, a preliminary grouping/classification step, based on static features, reduces the retrieval search space. Intra-class retrieval then takes place by considering time-dependent features, and is articulated as follows: (1) \"locally\" similar cases (considering one feature at a time) are extracted and the intersection of the retrieved sets is computed; (2) \"global\" similarity is computed - as a weighted average of local distances - and the best cases are listed. The main goal of the paper is to present an approach for efficiently implementing step (2), by taking into account specific information regarding the final application. We concentrate on a classical dimensionality reduction technique for time series allowing for efficient indexing, namely Discrete Fourier Transform (DFT). Thanks to specific index structures (i.e. k-d trees) range queries (on local feature similarity) can be efficiently performed on our case base; as mentioned above, results of such local queries are then suitably combined, allowing the physician to examine the most similar stored dialysis sessions with respect to the current one and to assess the quality of the overall hemodialysis service.\nTitle:\nRHENE: A Case Retrieval System for Hemodialysis Cases with Dynamically Monitored Parameters\n\nAbstract:\nInterpreting time series of measurements and exploring a repository of cases with time series data looking for similarities, are non-trivial, but very important tasks. Classical methodological solutions proposed to deal with (some of) these goals, typically based on mathematical techniques, are characterized by strong limitations, such as unclear or incorrect retrieval results and reduced interactivity and flexibility. In this paper, we describe a novel case base exploration and retrieval architecture, which supports time series summarization and interpretation by means of Temporal Abstractions, and in which multi-level abstraction mechanisms and proper indexing techniques are provided, in order to grant expressiveness in issuing queries, as well as efficiency and flexibility in answering queries themselves. Relying on a set of concrete examples, taken from the haemodialysis domain, we illustrate the system facilities, and we demonstrate the advantages of relying on this methodology, with respect to more classical mathematical ones.\nTitle:\nIntelligent data interpretation and case base exploration through temporal abstractions\n\nAbstract:\nThe problem of retrieving time series similar to a specified query pattern has been recently addressed within the Case Based Reasoning (CBR) literature. Providing a flexible and efficient way of dealing with such an issue would be of paramount importance in medical domains, where many patient parameters are often collected in the form of time series. In this paper, we describe a novel framework for retrieving cases with time series features, relying on Temporal Abstractions. With respect to more classical (mathematical) approaches, our framework provides significant advantages. In particular, multi-level abstraction mechanisms and proper indexing techniques allow for flexible query issuing, and for efficient and interactive query answering. The framework is currently being applied to the hemodialysis domain. In this field, experimental results have shown the superiority of our approach with respect to the use of a classical mathematical technique in flexibility, user friendliness, and also quality of results. Tests in other application domains, as well as further enhancements, are foreseen in our future work.\nTitle:\nFlexible and efficient retrieval of haemodialysis time series\n\nAbstract:\nTime series retrieval is a critical issue in all domains in which the observed phenomenon dynamics have to be dealt with. In this paper, we propose a novel, domain independent time series retrieval framework, based on Temporal Abstractions (TA). Our framework allows for multi-level abstractions , according to two dimensions , namely a taxonomy of (trend or state) symbols, and a variety of time granularities. Moreover, we allow for flexible querying , where queries can be expressed at any level of detail in both dimensions, also in an interactive fashion, and ground cases as well as generalized ones can be retrieved. We also take advantage of multi-dimensional orthogonal index structures , which can be refined progressively and on demand . The framework in practice is illustrated by means of a case study in hemodialysis.\nTitle:\nMulti-level Abstractions and Multi-dimensional Retrieval of Cases with Time Series Features\n\nAbstract:\nTime-varying information embedded in cases has often been neglected and its role oversimplified in case-based reasoning systems. In several real-world problems, and in particular in medical applications, a case should capture the evolution of the observed phenomenon over time. To this end, we propose to represent temporal information at two levels: (1) at the case level, when some features are collected in the form of time series, because they describe parameters varying within a period of time (which corresponds to the case duration), and we aim at analyzing the system behavior within the case duration interval itself; (2) at the history level, when we are interested in reconstructing the evolution of the system by retrieving temporally related cases. In this paper, we describe a framework for case representation and retrieval that is able to take into account the temporal dimension, and is meant to be used in any time dependent domain, which is particularly well suited for medical applications. To support case retrieval, we provide an analysis of similarity-based time series retrieval techniques; to support history retrieval, we introduce possible ways to summarize the case content, together with the corresponding strategies for identifying similar instances in the knowledge base. A concrete application of our framework is represented by RHENE, a system for intelligent retrieval in the hemodialysis domain.\nTitle:\nAccounting For The Temporal Dimension In Case-Based Retrieval: A Framework For Medical Applications\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.747, mean value for all the writers is 8.648, which makes it 0.938 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the present paper, we describe an application of case-based retrieval to the domain of end stage renal failure patients, treated with hemodialysis.Defining a dialysis session as a case, retrieval of past similar cases has to operate both on static and on dynamic features, since most of the monitoring variables of a dialysis session are time series. Retrieval is then articulated as a two-step procedure: (1) classification, based on static features and (2) intra-class retrieval, in which dynamic features are considered. As regards step (2), we concentrate on a classical dimensionality reduction technique for time series allowing for efficient indexing, namely discrete Fourier transform (DFT). Thanks to specific index structures (i.e. k -d trees), range queries (on local feature similarity) can be efficiently performed on our case base, allowing the physician to examine the most similar stored dialysis sessions with respect to the current one.The retrieval tool has been positively tested on real patients' data, coming from the nephrology and dialysis unit of the Vigevano hospital, in Italy.The overall system can be seen as a means for supporting quality assessment of the hemodialysis service, providing a useful input from the knowledge management perspective.\nTitle:", "model_inf_time": 1.94}, {"id": "41435", "output": "A Multi-Modal Reasoning System for IDDM Therapy Planning", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present a knowledge management and decision support methodology for insulin dependent diabetes mellitus (IDDM) patients care. Such methodology exploits the integration of case based reasoning (CBR) and rule based reasoning (RBR), with the aim of helping physicians during therapy planning, by overcoming the intrinsic limitations shown by the independent application of the two reasoning paradigms. RBR provides suggestions on the basis of a situation detection mechanism that relies on formalized prior knowledge; CBR is used to specialize and dynamically adapt the rules on the basis of the patient's characteristics and of the accumulated experience. When the case library is not representative of the overall population, only RBR is applied to define a therapy for the input situation, which can then be retained, enriching the case library competence. The paper reports the first evaluation results, obtained both on simulated examples and on real patients. This work was developed within the EU funded telematic management of insulin dependent diabetes mellitus (T-IDDM) project, and is fully integrated in its web-based architecture.\nTitle:\nA multi-modal reasoning methodology for managing IDDM patients\n\nAbstract:\nThe integration of rule-based and case-based reasoning is particularly useful in medical applications, where both general rules and specific patient cases are usually available. In the present paper we aim at presenting a decision support tool for Insulin Dependent Diabetes Mellitus management relying on such a kind of integration. This multimodal reasoning system aims at providing physicians with a suitable solution to the problem of therapy planning by exploiting, in the most exible way, the strengths of the two selected methods. In particular, the integration is pursued without considering one of the modality as the most prominent reasoning method, but exploiting complementarity in all possible ways. In fact, while rules provide suggestions on the basis of a situation detection mechanism that relies on structured prior knowledge, CBR may be used to specialize and dynamically adapt the rules on the basis of the patient's characteristics and of the accumulated experience. On the other hand, if a particular patient class is not sufficiently covered by cases, the use of rules may be exploited to try to learn suitable situations, in order to improve the competence of the case-based component. Such a work will be integrated in the EU funded project T-IDDM architecture, and has been preliminary tested on a set of cases generated by a diabetic patient simulator.\nTitle:\nIntegrating Rule-Based and Case-Based Decision Making in Diabetic Patient Management\n\nAbstract:\nWe present a successful application of Artificial Intelligence (AI) methodologies in the context of a telemedicine service for diabetic patients management, developed within the EU-funded T-IDDM project. The system architecture is distributed, and composed by a Patient Unit and by a Medical Unit, connected through a telecommunication link. Several AI methods have been exploited to implement the T-IDDM functionality. The data base relies on an explicit representation of the domain ontology. Temporal Abstractions and other Intelligent Data Analysis techniques are used to analyse the patient's monitoring data; the Case Based Reasoning (CBR) methodology is applied to perform the Knowledge Management task. Finally, CBR is integrated with Rule Based Reasoning to provide physicians with a multi-modal reasoning decision support tool. The T-IDDM service is being tested through a small on field trial in Pavia; the first results, though preliminary, seem to substantiate the hypothesis that the use of an AI-based telemedicine system could present an advantage in the management of type 1 diabetic patients, leading to a more tight control of the patients' metabolic situation, in a cost-effective way.\nTitle:\nArtificial Intelligence Techniques For Diabetes Management: The T-Iddm Project\n\nAbstract:\nIn the context of Insulin Dependent Diabetes Mellitus care, we developed a decision support system that relies on a tight integration of Case Based Reasoning and Rule Based Reasoning methodologies. In this paper, we aim at presenting the evaluation strategy we have defined to test the system accuracy, safety and reliability, and the first results obtained both on simulated and on real patients data. Reliability was positively judged by a group of expert diabetologists; an increase in the performances of the system is foreseen as new knowledge will be acquired, through its usage in clinical practice.\nTitle:\nEvaluating a Multi-modal Reasoning Sytem in Diabetes Care\n\nAbstract:\nIn this paper we propose a case-based decision support tool, designed to help physicians in Ist type diabetes therapy revision through the intelligent retrieval of data related to past situations (or 'cases') similar to the current one. A case is defined as a set of variable values (or features) collected during a visit. We defined taxonomy of prototypical patients' conditions, or classes, to which each case should belong. For each input case, the system allows the physician to find similar past cases, both from the same patient and from different ones. We have implemented a two-steps procedure; (1) it finds the classes to which the input case could belong; (2) it lists the most similar cases from these classes, through a nearest neighbor technique, and provides some statistics useful for decision taking. The performance of the system has been tested on a data-base of 147 real cases, collected at the Policlinico S. Matteo Hospital of Pavia. The tool is fully integrated in the web-based architecture of the EU funded Telematic management of Insulin Dependent Diabetes Mellitus (T-IDDM) project. (C) 2000 Elsevier Science Ireland Ltd. All rights reserved.\nTitle:\nDiabetic Patients Management Exploiting Case-Based Reasoning Techniques\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.281, mean value for all the writers is 8.648, which makes it 1.393 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe present a decision support tool for Insulin Dependent Diabetes Mellitus management, that relies on the integration of two different methodologies: Rule-Based Reasoning (RBR) and Case-Based Reasoning (CBR). This multi-modal reasoning system aims at providing physicians with a suitable solution to the problem of therapy planning by exploiting the strengths of the two selected methods. RBR provides suggestions on the basis of a situation detection mechanism that relies on structured prior knowledge; CBR is used to specialize and dynamically adapt the rules on the basis of the patient's characteristics and of the accumulated experience. Such work will be integrated in the EU funded project T-IDDM architecture, and has been preliminary tested on a set of cases generated by a diabetic patient simulator.\nTitle:", "model_inf_time": 1.65}, {"id": "41436", "output": "A Dual Hesitant Fuzzy VIKOR Approach for Multi-Criteria Group Decision Making", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe purpose of this paper is to investigate the methods of constructing distance measures for hesitant fuzzy linguistic term sets (HFLTSs) and their applications in multi-criteria decision making (MCDM). We first present some methods to construct distance measures for HFLTSs. Then, we discuss the properties of different distance measures in detail, which are very helpful in comparison of these distance measures for HFLTSs. After that, based upon the distance measures for HFLTSs, a decision-making method is proposed to solve the MCDM problems, in which the assessment values of alternatives over criteria are HFLTSs and the weights of criteria are completely unknown or expressed by HFLTSs. Finally, an example concerning the evaluation of movie quality is provided to demonstrate the practicability of the proposed method, and necessary comparative analyses are conducted to illustrate the effectiveness and advantages of the developed distance measures and the proposed decision-making method.\nTitle:\nSome Approaches To Constructing Distance Measures For Hesitant Fuzzy Linguistic Term Sets With Applications In Decision-Making\n\nAbstract:\nIn this paper, we develop an interactive approach to probabilistic hesitant fuzzy multi-attribute group decision making (P-HFMAGDM) with incomplete weight information, in which the assessments provided by the decision makers for alternatives over attributes are expressed by probabilistic hesitant fuzzy elements (P-HFEs) and the weight information on attributes is partly known. Firstly, we propose the axiomatic definition of distance measures for P-HFEs, and then develop several kinds of distance measures for P-HFEs. Afterwards, we put forward the probabilistic hesitant fuzzy positive ideal solution and the probabilistic hesitant fuzzy negative ideal solution, respectively. By using the distance measures for P-HFEs, we define the closeness coefficients of alternatives, based on which we further establish a multi-objective optimization model to handle the P-HFMAGDM problems with incomplete weight information. Additionally, to facilitate the decision makers to provide new preference information or modify the previous preference information, an interactive approach is developed to deal with the P-HFMAGDM problems with incomplete weight information. Finally, a practical example involving the evaluation of the VR project declaration is provided to illustrate our approach.\nTitle:\nAn interactive approach to probabilistic hesitant fuzzy multi-attribute group decision making with incomplete weight information.\n\nAbstract:\nHesitant fuzzy set (HFS) permits the membership degrees of an element to a set represented by several possible values. Thus, it provides a suitable means to express uncertain information of different group members within the process of multi-criteria decision making (MCDM). In this paper, we suggest a new approach, named HF-ELECTRE II approach that combines the idea of HFSs with the ELECTRE II method, to efficiently handle different opinions of group members that are frequently encountered when handling the MCDM problems. We formulate the approach by defining the concepts of hesitant fuzzy concordance and discordance sets and by constructing the strong and weak outranking relations, which are employed to decide the ranking for a set of alternatives. Numerical examples are presented to exhibit the applications of the proposed method. Furthermore, a comparison of the alternatives\u2019 rankings derived from the HF-ELECTRE II method with those derived from the aggregation operators and the fuzzy group ELECTRE approach is made. After that, a decision supporting system based on the HF-ELECTRE II method is constructed to aid decision making, and the prominent characteristics of the HF-ELECTRE II method and future research challenges are also discussed.\nTitle:\nHesitant fuzzy ELECTRE II approach: A new way to handle multi-criteria decision making problems.\n\nAbstract:\nAbstract   Pythagorean fuzzy sets (PFSs) as a new generalization of fuzzy sets (FSs) can handle uncertain information more flexibly in the process of decision making. In our real life, we also may encounter a hesitant fuzzy environment. In view of the effective tool of hesitant fuzzy sets (HFSs) for expressing the hesitant situation, we introduce HFSs into PFSs and extend the existing research work of PFSs. Concretely speaking, this paper considers that the membership degree and the non-membership degree of PFSs are expressed as hesitant fuzzy elements. First, we propose a new concept of hesitant Pythagorean fuzzy sets (HPFSs) by combining PFSs with HFSs. It provides a new semantic interpretation for our evaluation. Meanwhile, the properties and the operators of HPFSs are studied in detail. For the sake of application, we focus on investigating the normalization method and the distance measures of HPFSs in advance. Then, we explore the application of HPFSs to multi-criteria decision making (MCDM) by employing the technique for order preference by similarity to ideal solution (TOPSIS) method. A new extension of TOPSIS method is further designed in the context of MCDM with HPFSs. Finally, an example of the energy project selection is presented to elaborate on the performance of our approach.\nTitle:\nThe new extension of TOPSIS method for multiple criteria decision making with hesitant Pythagorean fuzzy sets.\n\nAbstract:\nIn the process of group decision making (GDM), preference relations (e.g. fuzzy preference relation and multiplicative preference relation) are very popular tools to express decision makers' preferences, especially when a set of alternatives (or criteria) are compared. However, most of the existing preference relations don't consider the hesitancy information, which allows the decision makers to provide all the possible values when comparing two alternatives (or criteria), and is a common situation in daily life. In this paper, we first define the concept of hesitant fuzzy preference relation and study its properties, based on which we give an approach to GDM. Motivated by the multiplicative preference relation and the hesitant fuzzy set, we introduce the hesitant multiplicative set and develop a series of hesitant multiplicative aggregation operators. Hesitant multiplicative preference relation is also defined to provide decision makers a very useful tool to express their hesitant preferences over alternatives, and then is applied to GDM. Additionally, an example is given to illustrate our results.\nTitle:\nManaging Hesitant Information In Gdm Problems Under Fuzzy And Multiplicative Preference Relations\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.839, mean value for all the writers is 8.648, which makes it 1.869 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the process of group decision making, dual hesitant fuzzy sets (DHFSs) are a very flexible tool for decision makers (DMs) to express their preferences for alternatives. Based on an extended VIKOR method, in this paper, we propose an approach for multi-criteria group decision making (MCGDM) with dual hesitant fuzzy information. To distinguish dual hesitant fuzzy elements (DHFEs) more efficiently, we propose a score function and a comparison method of DHFEs, and based on which we develop a new distance measure for DHFEs. After that, we use the fuzzy measures to characterize the interactive relationships among criteria, and then put forward a dual hesitant fuzzy VIKOR method for solving the MCGDM problems. Finally, we utilize a real example concerning the selection of a cooperative partner to illustrate the availability of the proposed method.\nTitle:", "model_inf_time": 2.05}, {"id": "41437", "output": "Minimum-Cost Traffic Shaping for QoS-Based Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nFor voice/video applications that are based on Peer-to-Peer (P2P) models, ensuring the end-to-end Quality of Service (QoS) is crucial, especially if users are paying fees. In this paper we propose an End-to-end Performance Inference Technique (EPIT) that uses a prediction-based approach to map the ingress traffic levels of the P2P network to the end-to-end QoS in the network. Furthermore, by coupling Simulated Annealing (SA) with EPIT, we describe a traffic engineering solution in such a way that the QoS constraints are met while traffic flows into the network are maximized.\nTitle:\nAn End-to-End Performance Inference Technique for Peer-to-Peer Networks\n\nAbstract:\nIn this paper, we propose the use of network decomposition under an optimal resource allocation framework. We develop a methodology where recursive formulas can be utilized for calculating the desired end-to-end performance bounds (i.e., backlog bound violation probability) of flows traversing tandem, acyclic queueing networks. We use those performance metrics in an optimization framework that allocates resources to network services with specific quality-of-service requirements. Finally, we evaluate our framework and compare its performance against a system utilizing deterministic bounds obtained from network calculus.\nTitle:\nNetwork Decomposition in Practice: An Application to Optimal Resource Allocation.\n\nAbstract:\nWe consider the network design and optimization of high-speed multiservice networks. Meeting different service requirements is facilitated by dividing the network into virtual bands. A band corresponds to a service type. To achieve high transport efficiency, bands should be reconfigured frequently to track the varying traffic. Our work aims at developing a self-sizing system which can allocate network capacity automatically and adaptively using on-line traffic data. We study the optimization problem for band partitioning for high-speed multiservice networks. A two-step optimization approach is presented. An optimization model is developed to partition bandwidth among bands to minimize total system cost under capacity constraints while the QoS at call level and cell level are guaranteed. An online traffic measurement system allows the network to automatically detect the amount of bandwidth necessary to satisfy the QoS requirements at cell level. To this end our system exploits the notion of effective bandwidth. To meet the requirement of frequently band partitioning, a fast algorithm based on simulated annealing is presented to solve the model. Simulation results are reported to demonstrate the effectiveness of the optimization approach.\nTitle:\nSelf-Sizing And Optimization Of High-Speed Multiservice Networks\n\nAbstract:\nIn this paper we propose a traffic predictor based on multiresolution decomposition for the adaptive bandwidth control in locally controlled self-sizing networks. A selfsizing network can provide quantitative packet-level QoS to aggregate traffic by allocating link/switch capacity automatically and adaptively using online traffic data. In a locally controlled network such as Internet, resource allocation decisions are made at the node level. We show that wavelet based adaptive bandwidth control method performs better than other popular methods like Gaussian predictor for such applications. We have compared the performance of different ortho-normal wavelets and found that Haar wavelet is best suited for traffic prediction. We have studied the effect of other wavelet parameters such as size of the window and number of filter coefficients. We also propose a novel adaptive wavelet predictor which can adapt very well to the changes of incoming bursty traffic.\nTitle:\nA Framework for Adaptive Wavelet Prediction in Self-Sizing Networks\n\nAbstract:\nIn this paper, we model users of next generation services by means of utility differentiated classes. Noncooperative game theory is employed to explain user behavior with respect to the network price. Rate control algorithms for attaining such a noncooperative Nash equilibrium are then presented. We extend our previous model of a single link fed by Poisson traffic to encompass a generic network and non-Poisson traffic. These results can be applied to a wide variety of future networks ranging from LSPs in MPLS networks to wavelength paths in WDM networks.\nTitle:\nPricing mediated bandwidth allocation for the next generation Internet.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.113, mean value for all the writers is 8.648, which makes it 1.25 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose a minimum-cost method for traffic shaping in the context of quality of service (QoS)-based networks. Given the user's desired QoS and the network's resource availability, our method determines the least cost parameters for a shaper while guaranteeing access to the network and satisfying the QoS requirements.\nTitle:", "model_inf_time": 1.41}, {"id": "41438", "output": "A TIMIT Baseline for LVCSR Research", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nExemplar-based techniques, such as k-nearest neighbors (kNNs) and Sparse Representations (SRs), can be used to model a test sample from a few training points in a dictionary set. In past work, we have shown that using a SR approach for phonetic classification allows for a higher accuracy than other classification techniques. These phones are the basic units of speech to be recognized. Motivated by this result, we create a new dictionary which is a function of the phonetic labels of the original dictionary. The SR method now selects relevant samples from this new dictionary to create a new feature representation of the test sample, where the new feature is better linked to the actual units to be recognized. We will refer to these new features as Spif. We present results using these new Spif features in a Hidden Markov Model (HMM) framework for speech recognition. We find that the Spif features allow for a 2.9% relative reduction in Phonetic Error Rate (PER) on the TIMIT phonetic recognition task. Furthermore, we find that the Spif features allow for a 4.8% relative improvement in Word Error Rate (WER) on a large vocabulary 50 hour Broadcast News task.\nTitle:\nExemplar-based Sparse Representation phone identification features\n\nAbstract:\nWhile Deep Neural Networks (DNNs) have achieved tremendous success for large vocabulary continuous speech recognition (LVCSR) tasks, training these networks is slow. Even to date, the most common approach to train DNNs is via stochastic gradient descent, serially on one machine. Serial training, coupled with the large number of training parameters (i.e., 10\u201350 million) and speech data set sizes (i.e., 20\u2013100 million training points) makes DNN training very slow for LVCSR tasks. In this work, we explore a variety of different optimization techniques to improve DNN training speed. This includes parallelization of the gradient computation during cross-entropy and sequence training, as well as reducing the number of parameters in the network using a low-rank matrix factorization. Applying the proposed optimization techniques, we show that DNN training can be sped up by a factor of 3 on a 50-hour English Broadcast News (BN) task with no loss in accuracy. Furthermore, using the proposed techniques, we are able to train DNNs on a 300-hr Switchboard (SWB) task and a 400-hr English BN task, showing improvements between 9\u201330% relative over a state-of-the art GMM/HMM system while the number of parameters of the DNN is smaller than the GMM/HMM system.\nTitle:\nOptimization Techniques to Improve Training Speed of Deep Neural Networks for Large Speech Tasks\n\nAbstract:\nWhile recurrent neural network language models based on Long Short Term Memory (LSTM) have shown good gains in many automatic speech recognition tasks, Convolutional Neural Network (CNN) language models are relatively new and have not been studied in-depth. In this paper we present an empirical comparison of LSTM and CNN language models on English broadcast news and various conversational telephone speech transcription tasks. We also present a new type of CNN language model that leverages dilated causal convolution to efficiently exploit long range history. We propose a novel criterion for training language models that combines word and class prediction in a multi-task leaming framework. We apply this criterion to train word and character based LSTM language models and CNN language models and show that it improves performance. Our results also show that CNN and LSTM language models are complementary and can be combined to obtain further gains.\nTitle:\nEmpirical Exploration Of Novel Architectures And Objectives For Language Models\n\nAbstract:\nDespite their theoretical appeal and grounding in tractable convex optimization techniques, kernel methods are often not the first choice for large-scale speech applications due to their significant memory requirements and computational expense. In recent years, randomized approximate feature maps have emerged as an elegant mechanism to scale-up kernel methods. Still, in practice, a large number of random features is required to obtain acceptable accuracy in predictive tasks. In this paper, we develop two algorithmic schemes to address this computational bottleneck in the context of kernel ridge regression. The first scheme is a specialized distributed block coordinate descent procedure that avoids the explicit materialization of the feature space data matrix, while the second scheme gains efficiency by combining multiple weak random feature models in an ensemble learning framework. We demonstrate that these schemes enable kernel methods to match the performance of state of the art Deep Neural Networks on TIMIT for speech recognition and classification tasks. In particular, we obtain the best classification error rates reported on TIMIT using kernel methods.\nTitle:\nKernel methods match Deep Neural Networks on TIMIT\n\nAbstract:\nConvolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, we hypothesize that CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary continuous speech recognition (LVCSR) tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is an appropriate number of hidden units, what is the best pooling strategy. Second, investigate how to incorporate speaker-adapted features, which cannot directly be modeled by CNNs as they do not obey locality in frequency, into the CNN framework. Third, given the importance of sequence training for speech tasks, we introduce a strategy to use ReLU+dropout during Hessian-free sequence training of CNNs. Experiments on 3 LVCSR tasks indicate that a CNN with the proposed speaker-adapted and ReLU+dropout ideas allow for a 12%\u201314% relative improvement in WER over a strong DNN system, achieving state-of-the art results in these 3 tasks.\nTitle:\nDeep Convolutional Neural Networks for Large-scale Speech Tasks.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.99, mean value for all the writers is 8.648, which makes it 0.292 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWhile research in large vocabulary continuous speech recognition (LVCSR) has sparked the development of many state of the art research ideas, research in this domain suffers from two main drawbacks. First, because of the large number of parameters and poorly labeled transcriptions, gaining insight into further improvements based on error analysis is very difficult. Second, LVCSR systems often take a significantly longer time to train and test new research ideas compared to small vocabulary tasks. A small vocabulary task like TIMIT provides a phonetically rich and hand-labeled corpus and offers a good test bed to study algorithmic improvements. However, oftentimes research ideas explored for small vocabulary tasks do not always provide gains on LVCSR systems. In this paper, we address these issues by taking the standard \"recipe\" used in typical LVCSR systems and applying it to the TIMIT phonetic recognition corpus, which provides a standard benchmark to compare methods. We find that at the speaker-independent (SI) level, our results offer comparable performance to other SI HMM systems. By taking advantage of speaker adaptation and discriminative training techniques commonly used in LVCSR systems, we achieve an error rate of 20%, the best results reported on the TIMIT task to date, moving us closer to the human reported phonetic recognition error rate of 15%. We propose the use of this system as the baseline for future research and believe that it will serve as a good framework to explore ideas that will carry over to LVCSR systems.\nTitle:", "model_inf_time": 1.62}, {"id": "41439", "output": "Ant-Miner with Continuous Attribute Handling: A Discretization Approach", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe paper proposes an algorithm for data mining called Ant-Miner (ant-colony-based data miner). The goal of Ant-Miner is to extract classification rules from data. The algorithm is inspired by both research on the behavior of real ant colonies and some data mining concepts as well as principles. We compare the performance of Ant-Miner with CN2, a well-known data mining algorithm for classification, in six public domain data sets. The results provide evidence that: 1) Ant-Miner is competitive with CN2 with respect to predictive accuracy, and 2) the rule lists discovered by Ant-Miner are considerably simpler (smaller) than those discovered by CN2\nTitle:\nData mining with an ant colony optimization algorithm\n\nAbstract:\nThe Ant-Miner algorithm, first proposed by Parpinelli and colleagues, applies an ant colony optimization heuristic to the classification task of data mining to discover an ordered list of classification rules. In this paper we present a new version of the Ant-Miner algorithm, which we call Unordered Rule Set Ant-Miner, that produces an unordered set of classification rules. The proposed version was evaluated against the original Ant-Miner algorithm in six public-domain datasets and was found to produce comparable results in terms of predictive accuracy. However, the proposed version has the advantage of discovering more modular rules, i.e., rules that can be interpreted independently from other rules - unlike the rules in an ordered list, where the interpretation of a rule requires knowledge of the previous rules in the list. Hence, the proposed version facilitates the interpretation of discovered knowledge, an important point in data mining.\nTitle:\nA new version of the ant-miner algorithm discovering unordered rule sets\n\nAbstract:\nThis work proposes a new rule pruning procedure for Ant-Miner, an Ant Colony algorithm that discovers classification rules in the context of data mining. The performance of Ant-Miner with the new pruning procedure is evaluated and compared with the performance of the original Ant-Miner across several datasets. The results show that the new pruning procedure has a mixed effect on the performance of Ant-Miner. On one hand, overall it tends to decrease the classification accuracy more often than it improves it. On the other hand, the new pruning procedure in general leads to the discovery of classification rules that are considerably shorter, and so simpler (more easily interpretable by the users) than the rules discovered by the original Ant-Miner.\nTitle:\nA new classification-rule pruning procedure for an ant colony algorithm\n\nAbstract:\nThe cAnt-Miner algorithm is an Ant Colony Optimization (ACO) based technique for classification rule discovery in problem domains which include continuous attributes. In this paper, we propose several extensions to cAnt-Miner. The main extension is based on the use of multiple pheromone types, one for each class value to be predicted. In the proposed @mcAnt-Miner algorithm, an ant first selects a class value to be the consequent of a rule and the terms in the antecedent are selected based on the pheromone levels of the selected class value; pheromone update occurs on the corresponding pheromone type of the class value. The pre-selection of a class value also allows the use of more precise measures for the heuristic function and the dynamic discretization of continuous attributes, and further allows for the use of a rule quality measure that directly takes into account the confidence of the rule. Experimental results on 20 benchmark datasets show that our proposed extension improves classification accuracy to a statistically significant extent compared to cAnt-Miner, and has classification accuracy similar to the well-known Ripper and PART rule induction algorithms.\nTitle:\nUtilizing multiple pheromones in an ant-based algorithm for continuous-attribute classification rule discovery.\n\nAbstract:\nThe vast majority of Ant Colony Optimization (ACO) algorithms for inducing classification rules use an ACO-based procedure to create a rule in an one-at-a-time fashion. An improved search strategy has been proposed in the cAnt-MinerPB algorithm, where an ACO-based procedure is used to create a complete list of rules (ordered rules) - i.e., the ACO search is guided by the quality of a list of rules, instead of an individual rule. In this paper we propose an extension of the cAnt-MinerPB algorithm to discover a set of rules (unordered rules). The main motivation for discovering a set of rules is to improve the interpretation of individual rules and evaluate the impact on the predictive accuracy of the algorithm. We also propose a new measure to evaluate the interpretability of the discovered rules to mitigate the fact that the commonly-used model size measure ignores how the rules are used to make a class prediction. Comparisons with state-of-the-art rule induction algorithms and the cAnt-MinerPB producing ordered rules are also presented.\nTitle:\nImproving the interpretability of classification rules discovered by an ant colony algorithm\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.407, mean value for all the writers is 8.648, which makes it 1.501 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents an extension to Ant-Miner, named cAnt-Miner (Ant-Miner coping with continuous attributes), which incorporates an entropy-based discretization method in order to cope with continuous attributes during the rule construction process. By having the ability to create discrete intervals for continuous attributes \"on-the-fly\", cAnt-Miner does not requires a discretization method in a preprocessing step, as Ant-Miner requires. cAnt-Miner has been compared against Ant-Miner in eight public domain datasets with respect to predictive accuracy and simplicity of the discovered rules. Empirical results show that creating discrete intervals during the rule construction process facilitates the discovery of more accurate and significantly simpler classification rules.\nTitle:", "model_inf_time": 1.65}, {"id": "41440", "output": "A Hybrid Summarization Technique for Farsi Documents Based on Term Co-occurrence and Linguistic Features", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSpelling errors in digital documents are often caused by operational and cognitive mistakes, or by the lack of full knowledge about the language of the written documents. Computer-assisted solutions can help to detect and suggest replacements. In this paper, we present a new string distance metric for the Persian language to rank respelling suggestions of a misspelled Persian word by considering the effects of keyboard layout on typographical spelling errors as well as the homomorphic and homophonic aspects of words for orthographical misspellings. We also consider the misspellings caused by disregarded diacritics. Since the proposed string distance metric is custom-designed for the Persian language, we present the spelling aspects of the Persian language such as homomorphs, homophones, and diacritics. We then present our statistical analysis of a set of large Persian corpora to identify the causes and the types of Persian spelling errors. We show that the proposed string distance metric has a higher mean average precision and a higher mean reciprocal rank in ranking respelling candidates of Persian misspellings in comparison with other metrics such as the Hamming, Levenshtein, Damerau-Levenshtein, Wagner-Fischer, and Jaro-Winkler metrics.\nTitle:\nA novel string distance metric for ranking Persian respelling suggestions.\n\nAbstract:\nClustering is an important technique for improving the performance and availability of computer systems. The use of cluster systems is also continuously growing because they present excellent features like scalability, high availability and high performance computing. Availability is mainly administered by failure detection and recovery mechanism, including proactive failure mechanisms that try to prevent occurrences of faults. Given the criticality and importance of availability for high performance computing, this paper uniquely surveyes noticeable existing mechanisms for prevention of faults in high availability and high performance computing cluster systems, and presents a comparative overview.\nTitle:\nFailure Prediction Mechanisms in Cluster Systems\n\nAbstract:\nAlthough virtualization technology is recently applied to next-generation distributed high-performance computing systems, theoretical aspects of scheduling jobs on these virtualized environments are not sufficiently studied, especially in online and non-clairvoyant cases. Virtualization of computing resources results in interference and virtualization overheads that negatively impact the load balancing objectives on commonly used cluster of multi-core physical machines. We present a technique for non-clairvoyant online scheduling of globally synchronized jobs, each of which spawns tasks to execute compute-intensive works. Our technique considers both load balancing of physical cores and per job synchronization cost minimization. We show that in the presence of arbitrary virtualization overheads, interference effects and synchronization cost, the problem can be reduced to an online unrelated parallel machine scheduling, which is solved using routing of virtual circuits. We present a new opportunity cost model to reduce the problem to the routing of virtual circuits and prove the effectiveness of our scheduling technique using mathematical analysis and simulative experiments.\nTitle:\nNon-clairvoyant online scheduling of synchronized jobs on virtual clusters.\n\nAbstract:\nThe tremendous rise in microprocessor technology has offered high speed processors and has increased the processor-memory speed gap dramatically. On the other hand, real-time embedded systems often have a hard deadline to complete their instructions. Consequently, the design of cache memory hierarchy is a critical issue in embedded systems. This paper describes a simulation-based performance evaluation of typical cache design issues in embedded systems such as using split caches for data and instruction versus unified cache for data and instruction, cache size and associativity and replacement policy. The evaluation is done using SimpleScalar simulation tools based on its Alpha version. We select some benchmarks for this study based on some previous researches about the clustering of SPEC CPU2000 benchmark suite. The contribution of this work is identifying important parameters for cache design in general-purpose embedded systems. Our results show that the Pseudo LRU techniques for cache replacement, such as MRU can approximate LRU with much lower complexity for a wide variety of cache sizes and degree of associativities.\nTitle:\nPerformance Evaluation of Cache Memory Organizations in Embedded Systems\n\nAbstract:\nThis paper presents a high performance technique for virtualization-unaware scheduling of compute-intensive synchronized (i.e., tightly-coupled) jobs in virtualized high performance computing systems. Online tightly-coupled jobs are assigned/reassigned to clustered virtual machines based on synchronization costs. Virtual machines are in turn assigned/reassigned to clustered physical machines based on CPU load. Our analytical study shows that it is possible to minimize the performance and scalability degradation of high performance computing applications such as ExaScale and PetaScale systems and applications that are recommended to use virtualization technology to achieve higher degree of performability, namely higher utilization, energy efficiency, portability, flexibility and configurability. Presented HPvC as a realizable model for applying virtualization technology on HPC systems.Presented a novel two level scheduling technique for tightly-coupled jobs scheduling.Proved the effectiveness and scalability of the proposed scheduling technique.\nTitle:\nScheduling of online compute-intensive synchronized jobs on high performance virtual clusters.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.628, mean value for all the writers is 8.648, which makes it 1.689 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe importance of text summarization grows rapidly as the amount of information increases exponentially. This paper presents a new hybrid summarization technique that combines statistical properties of documents with Farsi linguistic features. The originality of the technique lies on the use of term co-occurrence property of the text. It could detect the number of subjects. The proposed technique summarizes the document in proportion to the subject treated in a document. It considers the conceptual property of the text algorithm and based on word synonymy prevents similar sentences to be included in the summary. It also preserves the cohesion of the summarized text. Our results show better performance in comparison with FarsiSum, well known Farsi Summarizer, which is based only on the heuristic property of the text and do not consider the Farsi challenges.\nTitle:", "model_inf_time": 2.02}, {"id": "41441", "output": "Towards Autonomic Self-Optimization of Distributed MARF with ASSL", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we discuss our work towards self-healing property specification of an autonomic behavior in the Distributed Modular Audio Recognition Framework (DMARF) by using the Autonomic System Specification Language (ASSL). ASSL aids in enhancing DMARF with an autonomic middleware that enables it to perform in autonomous systems that theoretically require less-to-none human intervention. Here, we add an autonomic middleware layer to DMARF by specifying the core four stages of the DMARF's pattern-recognition pipeline as autonomic elements managed by a distinct autonomic manager. We devise the algorithms corresponding to this specification.\nTitle:\nTowards Autonomic Specification of Distributed MARF with ASSL: Self-healing\n\nAbstract:\nThis paper presents practical results of our endeavor towards formal specification and code generation of the Autonomic Distributed Modular Audio Recognition Framework (AD-MARF) system. We used the Autonomic System Specification Language (ASSL) to design and specify a self-protecting mechanism that must be incorporated by DMARF. Our overall goal is to have an autonomic computing layer covering DMARF by specifying autonomic properties at each of the pattern recognition stages of the same. Here we present results that complement our related work on the self-healing and self-optimizatin autonomic properties.\nTitle:\nAutonomic specification of self-protection for distributed MARF with ASSL\n\nAbstract:\nWe discuss our research towards developing special properties that introduce autonomic behavior in distributed pattern-recognition systems. In our approach we use ASSL (Autonomic System Specification Language) to formally develop such properties for DMARF (Distributed Modular Audio Recognition Framework). These properties enhance DMARF with an autonomic middleware that manages the four stages of the framework's pattern-recognition pipeline. DMARF is a biologically inspired system employing pattern recognition, signal processing, and natural language processing helping us process audio, textual, or imagery data needed by a variety of scientific applications, e.g., biometric applications. In that context, the notion go autonomic DMARF (ADMARF) can be employed by autonomous and robotic systems that theoretically require less-to-none human intervention other than data collection for pattern analysis and observing the results. In this article, we explain the ASSL specification models for the autonomic properties of DMARF.\nTitle:\nDeveloping autonomic properties for distributed pattern-recognition systems with ASSL: A Distributed MARF case study\n\nAbstract:\nIn this work we present the software architecture design and implementation of a Distributed Modular Audio Recognition Framework (DMARF), and its applications, such as Speaker Identification, that can run distributively over the Web Services architecture using XML-RPC. We describe some of the challenges occurred during the design and implementation, the advantages and disadvantages of such an implementation, and its possible future directions.\nTitle:\nOn Design and Implementation of the Distributed Modular Audio Recognition Framework: Requirements and Specification Design Document\n\nAbstract:\n\n In this work we present the software architecture design and implementation of a Distributed Modular Audio Recognition Framework\n (DMARF), and its applications, such as Speaker Identification, that can run distributively over the Web Services architecture\n using XML-RPC. We describe some of the challenges occurred during the design and implementation, the advantages and disadvantages\n of such an implementation, and its possible future directions.\n \n \nTitle:\nDistributed Modular Audio Recognition Framework (DMARF) and its Applications Over Web Services\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.686, mean value for all the writers is 8.648, which makes it 2.592 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this work, we venture out to develop self-optimization features in the Distributed Modular Audio Recognition Framework (DMARF). Here, we use the Autonomic System Specification Language (ASSL) to specify a self-optimization policy and generate the code for the same. This completes the first iteration of the autonomic specification layer for DMARF and enables re-engineered autonomic DMARF system, which also includes self-healing and self-protection, both developed earlier.\nTitle:", "model_inf_time": 1.54}, {"id": "41442", "output": "Exclusive Generalization: Efficiency and Utility Trade-offs", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nTo release micro-data tables containing sensitive data, generalization algorithms are usually required for satisfying given privacy properties, such as k -anonymity and l -diversity. It is well accepted that k -anonymity and l -diversity are proposed for different purposes, and the latter is a stronger property than the former. However, this paper uncovers an interesting relationship between these two properties when the generalization algorithms are publicly known. That is, preserving l -diversity in micro-data generalization can be done by preserving a new property, namely, l -cover, which is to satisfy l -anonymity in a special way. The practical impact of this discovery is that it may potentially lead to better heuristic generalization algorithms in terms of efficiency and data utility, that remain safe even when publicized.\nTitle:\nL-Cover: Preserving Diversity by Anonymity\n\nAbstract:\nIn releasing data with sensitive information, a data owner usually has seemingly conflicting goals, including privacy preservation, utility optimization, and algorithm efficiency. In this paper, we observe that a high computational complexity is usually incurred when an algorithm conflates the processes of privacy preservation and utility optimization. We then propose a novel privacy streamliner approach to decouple those two processes for improving algorithm efficiency. More specifically, we first identify a set of potential privacy-preserving solutions satisfying that an adversary's knowledge about this set itself will not help him/her to violate the privacy property; we can then optimize utility within this set without worrying about privacy breaches since such an optimization is now simulatable by adversaries. To make our approach more concrete, we study it in the context of micro-data release with publicly known generalization algorithms. The analysis and experiments both confirm our algorithms to be more efficient than existing solutions.\nTitle:\nPrivacy streamliner: a two-stage approach to improving algorithm efficiency\n\nAbstract:\nIn disclosing micro-data with sensitive attributes, the goal is usually two fold. First, the data utility of disclosed data should be maximized for analysis purposes. Second, the private information contained in such data must be limited to an acceptable level. Recent studies show that adversarial inferences using knowledge about a disclosure algorithm can usually render the algorithm unsafe. In this paper, we show that an existing unsafe algorithm can be transformed into a large family of distinct safe algorithms, namely, k-jump algorithms. We prove that the data utility of different k-jump algorithms is generally incomparable. Therefore, a secret choice can be made among all k-jump algorithms to eliminate adversarial inferences while improving the data utility of disclosed micro-data.\nTitle:\nk-jump strategy for preserving privacy in micro-data disclosure\n\nAbstract:\nUncertainty and indistinguishability are two independent aspects of privacy. Uncertainty refers to the property that the attacker cannot tell which private value, among a group of values, an individual actually has, and indistinguishability refers to the property that the attacker cannot see the difference among a group of individuals. While uncertainty has been well studied and applied to many scenarios, to date, the only effort in providing indistinguishability has been the well-known notion of k-anonymity. However, k-anonymity only applies to anonymized tables. This paper defines indistinguishability for general situations based on the symmetry among the possible private values associated with individuals. The paper then discusses computational complexities of and provides practical algorithms for checking whether a set of database views provides enough indistinguishability.\nTitle:\nIndistinguishability: the other aspect of privacy\n\nAbstract:\nA privacy violation occurs when the association between an individual identity and data considered private by that individual is obtained by an unauthorized party. Uncertainty and indistinguishability are two independent aspects that characterize the degree of this association being revealed. Indistinguishability refers to the property that the attacker cannot see the difference among a group of individuals, while uncertainty refers to the property that the attacker cannot tell which private value, among a group of values, an individual actually has. This paper investigates the notion of indistinguishability as a general form of anonymity, applicable, for example, not only to generalized private tables, but to relational views and to sets of views obtained by multiple queries over a private database table. It is shown how indistinguishability is highly influenced by certain symmetries among individuals, in the released data, with respect to their private values. The paper provides both theoretical results and practical algorithms for checking if a specific set of views over a private table provide sufficient indistinguishability.\nTitle:\nEvaluating privacy threats in released database views by symmetric indistinguishability\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.368, mean value for all the writers is 8.648, which makes it 0.614 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWhen generalization algorithms are known to the public, an adversary can obtain a more precise estimation of the secret table than what can be deduced from the disclosed generalization result. Therefore, whether a generalization algorithm can satisfy a privacy property should be judged based on such an estimation. In this paper, we show that the computation of the estimation is inherently a recursive process that exhibits a high complexity when generalization algorithms take a straightforward inclusive strategy. To facilitate the design of more efficient generalization algorithms, we suggest an alternative exclusive strategy, which adopts a seemingly drastic approach to eliminate the need for recursion. Surprisingly, the data utility of the two strategies are actually not comparable and the exclusive strategy can provide better data utility in certain cases.\nTitle:", "model_inf_time": 1.53}, {"id": "41443", "output": "Combinatorial Approaches to Haplotyping", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n\n This paper deals with the computational problem of inferring complete information on haplotypes from haplotypes with missing\n data. This problem is one of the main issues in haplotyping, as the current DNA sequencing technology often produces haplotypes with missing bases and thus the complete information\n on haplotypes has to be inferred through computational methods. In this paper we propose a new algorithmic approach to the\n problem that assumes both the Coalescent and the Minimum Entropy models and we provide an experimental analysis relating it\n to the previously investigated approaches. In particular, the reconstruction of a perfect phylogeny from haplotypes with missing\n data is addressed.\n \n \nTitle:\nExperimental analysis of a new algorithm for partial haplotype completion.\n\nAbstract:\nThe MINIMUM-RECOMBINANT HAPLOTYPE CONFIGURATION problem (MRHC) has been highly successful in providing a sound combinatorial formulation for the important problem of genotype phasing on pedigrees. Despite several algorithmic advances that have improved the efficiency, its applicability to real data sets has been limited since it does not take into account some important phenomena such as mutations...\nTitle:\nHaplotype Inference on Pedigrees with Recombinations, Errors, and Missing Genotypes via SAT solvers\n\nAbstract:\nIn this paper we deal with the general problem of recombining the information from evolutionary trees representing the relationships between distinct gene families. First we solve a problem from [8] regarding the construction of a minimum reconciled tree by giving an efficient algorithm. Then we show that the exemplar problem, arising from the exemplar analysis of multigene genomes [2], is NP-hard even when the number of copies of a given label is at most two. Finally we introduce two novel formulations for the problem of recombining evolutionary trees, extending the notion of the gene duplication problem studied in [8,11,9,10,6], and we give an exact algorithm (via dynamic programming) for one of the formulations given.\nTitle:\nReconciling gene trees to a species tree\n\nAbstract:\nHaplotype Inference (HI) is a computational challenge of crucial importance in a range of genetic studies. Pedigrees allow to infer haplotypes from genotypes more accurately than population data, since Mendelian inheritance restricts the set of possible solutions. In this work, we define a new HI problem on pedigrees, called MINIMUM-CHANGE HAPLOTYPE CONFIGURATION (MCHC) problem, that allows two types of genetic variation events: recombinations and mutations. Our new formulation extends the MINIMUM-RECOMBINANT HAPLOTYPE CONFIGURATION (MRHC) problem, that has been proposed in the literature to overcome the limitations of classic statistical haplotyping methods. Our contribution is twofold. First, we prove that the MCHC problem is APX-hard under several restrictions. Second, we propose an efficient and accurate heuristic algorithm for MCHC based on an L-reduction to a well-known coding problem. Our heuristic can also be used to solve the original MRHC problem and can take advantage of additional knowledge about the input genotypes. Moreover, the L-reduction proves for the first time that MCHC and MRHC are O(nm/(log nm))-approximable on general pedigrees, where n is the pedigree size and m is the genotype length. Finally, we present an extensive experimental evaluation and comparison of our heuristic algorithm with several other state-of-the-art methods for HI on pedigrees.\nTitle:\nAn efficient algorithm for haplotype inference on pedigrees with recombinations and mutations.\n\nAbstract:\nComputational methods for gene allele prediction have been proposed to substitute dedicated and expensive assays with cheaper in-silico analyses that operate on routinely collected data, such as SNP genotypes. Most of these methods are tailored to the needs and characteristics of human genetic studies where they achieve good prediction accuracy. However, genomic analyses are becoming increasingly important in livestock species too. For livestock species generally the underlying---usually quite large and complex---pedigree is known and available; this information is not fully exploited by current allele prediction methods. In this paper, we propose a new gene allele prediction method based on a simple, but robust, combinatorial formulation for the problem of discovering haplotype-allele associations. The inherent uncertainty of the haplotype inference process is reduced by taking into account the inheritance of gene alleles across the population pedigree while genotypes are phased. The accuracy of the method has been extensively evaluated on a representative real-world livestock dataset under several scenarios and choices of parameters. The median error rate ranged from 0.0537 to 0.0896, with an average of 0.0678; this is 21% better than another state-of-the-art prediction algorithm that does not use the pedigree information. The experimental results support the validity of the proposed approach and, in particular, of the use of pedigree information in gene allele predictions.\nTitle:\nHaplotype-based prediction of gene alleles using pedigrees and SNP genotypes\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.7, mean value for all the writers is 8.648, which makes it 0.898 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe investigation of genetic differences among humans has given evidence that mutations in DNA sequences are responsible for some genetic diseases. The most common mutation is the one that involves only a single nucleotide of the DNA sequence, which is called a single nucleotide polymorphism (SNP). As a consequence, computing a complete map of all SNPs occurring in the human populations is one of the primary goals of recent studies in human genomics. The construction of such a map requires to determine the DNA sequences that from all chromosomes. In diploid organisms like humans, each chromosome consists of two sequences called haplotypes. Distinguishing the information contained in both haplotypes when analyzing chromosome sequences poses several new computational issues which collectively form a new emerging topic of Computational Biology known as Haplotyping.This paper is a comprehensive study of some new combinatorial approaches proposed in this research area and it mainly focuses on the formulations and algorithmic solutions of some basic biological problems. Three statistical approaches are briefly discussed at the end of the paper.\nTitle:", "model_inf_time": 1.36}, {"id": "41444", "output": "The Tripartition Metric for Phylogenetic Networks: Separation and Indistinguishability", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe prove that Nakhleh's metric for reduced phylogenetic networks is also a metric on the classes of tree-child phylogenetic networks, semibinary tree-sibling time consistent phylogenetic networks, and multilabeled phylogenetic trees. We also prove that it separates distinguishable phylogenetic networks. In this way, it becomes the strongest dissimilarity measure for phylogenetic networks available so far. Furthermore, we propose a generalization of that metric that separates arbitrary phylogenetic networks.\nTitle:\nOn Nakhleh's Metric for Reduced Phylogenetic Networks\n\nAbstract:\nPhylogenetic networks are a generalization of phylogenetic trees that allow for the representation of nontreelike evolutionary events, like recombination, hybridization, or lateral gene transfer. While much progress has been made to find practical algorithms for reconstructing a phylogenetic network from a set of sequences, all attempts to endorse a class of phylogenetic networks (strictly extending the class of phylogenetic trees) with a well-founded distance measure have, to the best of our knowledge and with the only exception of the bipartition distance on regular networks, failed so far. In this paper, we present and study a new meaningful class of phylogenetic networks, called tree-child phylogenetic networks, and we provide an injective representation of these networks as multisets of vectors of natural numbers, their path multiplicity vectors. We then use this representation to define a distance on this class that extends the well-known Robinson-Foulds distance for phylogenetic trees and to give an alignment method for pairs of networks in this class. Simple polynomial algorithms for reconstructing a tree-child phylogenetic network from its path multiplicity vectors, for computing the distance between two tree-child phylogenetic networks and for aligning a pair of tree-child phylogenetic networks, are provided. They have been implemented as a Perl package and a Java applet, which can be found at http://bioinfo.uib.es/~recerca/phylonetworks/mudistance/.\nTitle:\nComparison of tree-child phylogenetic networks.\n\nAbstract:\nThe assessment of phylogenetic network reconstruction methods requires the ability to compare phylogenetic networks. This is the second in a series of papers devoted to the analysis and comparison of metrics for tree-child time consistent phylogenetic networks on the same set of taxa. In this paper, we generalize to phylogenetic networks two metrics that have already been introduced in the literature for phylogenetic trees: the nodal distance and the triplets distance. We prove that they are metrics on any class of tree-child time consistent phylogenetic networks on the same set of taxa, as well as some basic properties for them. To prove these results, we introduce a reduction/expansion procedure that can be used not only to establish properties of tree-child time consistent phylogenetic networks by induction, but also to generate all tree-child time consistent phylogenetic networks with a given number of leaves.\nTitle:\nMetrics for Phylogenetic Networks II: Nodal and Triplets Metrics\n\nAbstract:\nThe assessment of phylogenetic network reconstruction methods requires the ability to compare phylogenetic networks. This is the first in a series of papers devoted to the analysis and comparison of metrics for tree-child time consistent phylogenetic networks on the same set of taxa. In this paper, we study three metrics that have already been introduced in the literature: the Robinson-Foulds distance, the tripartitions distance and the $\\mu$-distance. They generalize to networks the classical Robinson-Foulds or partition distance for phylogenetic trees. We analyze the behavior of these metrics by studying their least and largest values and when they achieve them. As a by-product of this study, we obtain tight bounds on the size of a tree-child time consistent phylogenetic network.\nTitle:\nMetrics for phylogenetic networks I: generalizations of the Robinson-Foulds metric.\n\nAbstract:\nHybridization networks are representations of evolutionary histories that allow for the inclusion of reticulate events like recombinations, hybridizations, or lateral gene transfers. The recent growth in the number of hybridization network reconstruction algorithms has led to an increasing interest in the definition of metrics for their comparison that can be used to assess the accuracy or robustness of these methods. In this paper we establish some basic results that make it possible the generalization to tree-child time consistent (TCTC) hybridization networks of some of the oldest known metrics for phylogenetic trees: those based on the comparison of the vectors of path lengths between leaves. More specifically, we associate to each hybridization network a suitably defined vector of 'splitted' path lengths between its leaves, and we prove that if two TCTC hybridization networks have the same such vectors, then they must be isomorphic. Thus, comparing these vectors by means of a metric for real-valued vectors defines a metric for TCTC hybridization networks. We also consider the case of fully resolved hybridization networks, where we prove that simpler, 'non-splitted' vectors can be used.\nTitle:\nPath lengths in tree-child time consistent hybridization networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.824, mean value for all the writers is 8.648, which makes it 0.703 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nPhylogenetic networks are a generalization of phylogenetic trees that allow for the representation of non-treelike evolutionary events, like recombination, hybridization, or lateral gene transfer. In a recent series of papers devoted to the study of reconstructibility of phylogenetic networks, Moret, Nakhleh, Warnow and collaborators introduced the so-called tripartition metric for phylogenetic networks. In this paper we show that, in fact, this tripartition metric does not satisfy the separation axiom of distances (zero distance means isomorphism, or, in a more relaxed version, zero distance means indistinguishability in some specific sense) in any of the subclasses of phylogenetic networks where it is claimed to do so. We also present a subclass of phylogenetic networks whose members can be singled out by means of their sets of tripartitions (or even clusters), and hence where the latter can be used to define a meaningful metric.\nTitle:", "model_inf_time": 1.95}, {"id": "41445", "output": "Identifying Types of Japanese Relative Clause Constructions with Machine Learning", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we present a method that improves Japanese dependency parsing by using large-scale statistical information. It takes into account two kinds of information not considered in previous statistical (machine learning based) parsing methods: information about dependency relations among the case elements of a verb, and information about co-occurrence relations between a verb and its case element. This information can be collected from the results of automatic dependency parsing of large-scale corpora. The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method.\nTitle:\nJapanese dependency parsing using co-occurrence information and a combination of case elements\n\nAbstract:\nWe investigated of the characteristics of in-text causal relations. We designed causal relation tags. With our designed tag set, three annotators annotated 750 Japanese newspaper articles. Then, using the annotated corpus, we investigated the causal relation instances from some viewpoints. Our quantitative study shows that what amount of causal relation instances are present, where these relation instances are present, and which types of linguistic expressions are used for expressing these relation instances in text.\nTitle:\nInvestigating the characteristics of causal relations in Japanese text\n\nAbstract:\nJapanese books are usually classified into ten genres by Nippon Decimal Classification (NDC) based on their subject. However, this classification is sometimes insufficient for corpus studies which describe characteristics of the texts in the book. Here, we propose a method of classifying text samples taken from Japanese books into some registers and text types. Firstly, we discuss useful criteria to describe various characteristics of the texts and propose a two-step approach for stable annotation. We then apply our method to 161 book samples from the prerelease version of the Balanced Corpus of Contemporary Written Japanese (BCCWJ), a balanced Japanese corpus comprising 100 million words developed by National Institute for Japanese Language and Linguistics. Finally, we evaluate our method in terms of stability of annotation using kappa coefficients and correlation coefficients.\nTitle:\nAn Approach Toward Register Classification Of Book Samples In The Balanced Corpus Of Contemporary Written Japanese\n\nAbstract:\nRecently there have been a number of works that model the zero pronoun resolution with the concept called 'center.' However, the usefulness of the previous centering frameworks has not fully evaluated with naturally occurring discourses. Furthermore, the previous centering theory has handled only the phenomena in successive simple sentences and has not adequately addressed the way to handle complex sentences that are prevalent in naturally occurring discourses. In this paper, we present a method to handle complex sentences with the centering theory and describe our framework that identifies the autecedents of zero pronouns in naturally occurring Japanese discourses. We also present the evaluation of our framework with real discourses.\nTitle:\nZero pronoun resolution in Japanese discourse based on centering theory\n\nAbstract:\nThis paper describes a method of domain adaptation, which involves adapting a classifier developed from source to target data. We automatically select the training data set that is suitable for the target data from the whole source data of multiple domains. This is unsupervised domain adaptation for Japanese word sense disambiguation (WSD). Experiments revealed that the accuracies of WSD improved when we automatically selected the training data set using two criteria, the degree of confidence and the leave-one-out (LOO)-bound score, compared with when the classifier was trained with all the data.\nTitle:\nSelecting Training Data for Unsupervised Domain Adaptation in Word Sense Disambiguation.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.299, mean value for all the writers is 8.648, which makes it 0.555 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nJapanese relative clause constructions (RCC's) are defined as being the NP's of structure \u2018S NP', noting the lack of a relative pronoun or any other explicit form of noun-clause demarcation. Japanese relative clause modification should be classified into at least two major semantic types: case-slot gapping and head restrictive. However, these types for relative clause modification cannot apparently be distinguished. In this paper we propose a method of identifying a RCC's type with a machine learning technique. The features used in our approach are not only representing RCC's characteristics, but also automatically obtained from large corpora. The results we obtained from evaluation revealed that our method outperformed the traditional case frame-based method, and the features that we presented were effective in identifying RCC's types.\nTitle:", "model_inf_time": 1.46}, {"id": "41446", "output": "Recovering Object Parts from Cluttered Scenes Using a Deformable Disc Model", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSymmetry is a powerful shape regularity that's been exploited by perceptual grouping researchers in both human and computer vision to recover part structure from an image without a priori knowledge of scene content. Drawing on the concept of a medial axis, defined as the locus of centers of maximal inscribed discs that sweep out a symmetric part, we model part recovery as the search for a sequence of deformable maximal inscribed disc hypotheses generated from a multiscale super pixel segmentation, a framework proposed by LEV09. However, we learn affinities between adjacent super pixels in a space that's invariant to bending and tapering along the symmetry axis, enabling us to capture a wider class of symmetric parts. Moreover, we introduce a global cost that perceptually integrates the hypothesis space by combining a pair wise and a higher-level smoothing term, which we minimize globally using dynamic programming. The new framework is demonstrated on two datasets, and is shown to significantly outperform the baseline LEV09.\nTitle:\nDetecting Curved Symmetric Parts Using a Deformable Disc Model\n\nAbstract:\nThis paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects in 3D by enclosing them with tight oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model[Felz.] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach outperforms the state-of-the-art in both 2D[Felz09] and 3D object detection[Hedau12].\nTitle:\n3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model.\n\nAbstract:\nWith the growing interest in object categorization various methods have emerged that perform well in this challenging task, yet are inherently limited to only a moderate number of object classes. In pursuit of a more general categorization system this paper proposes a way to overcome the computational complexity encompassing the enormous number of different object categories by exploiting the statistical properties of the highly structured visual world. Our approach proposes a hierarchical acquisition of generic parts of object structure, varying from simple to more complex ones, which stem from the favorable statistics of natural images. The parts recovered in the individual layers of the hierarchy can be used in a top-down manner resulting in a robust statistical engine that could be efficiently used within many of the current categorization systems. The proposed approach has been applied to large image datasets yielding important statistical insights into the generic parts of object structure.\nTitle:\nHierarchical Statistical Learning of Generic Parts of Object Structure\n\nAbstract:\nGiven a set of captioned images of cluttered scenes containing various objects in different positions and scales, we learn named contour models of object categories without relying on bounding box annotation. We extend a recent language-vision integration framework that finds spatial configurations of image features that co-occur with words in image captions. By substituting appearance features with local contour features, object categories are recognized by a contour model that grows along the object's boundary. Experiments on ETHZ are presented to show that 1) the extended framework is better able to learn named visual categories whose within-class variation is better captured by a shape model than an appearance model, and 2) typical object recognition methods fail when manually annotated bounding boxes are unavailable.\nTitle:\nLearning Categorical Shape from Captioned Images\n\nAbstract:\nIn recent years, region proposals have replaced sliding windows in support of object recognition, offering more discriminating shape and appearance information through improved localization. One powerful approach for generating region proposals is based on minimizing parametric energy functions with parametric maxflow. In this paper, we introduce Parametric Min-Loss (PML), a novel structured learning framework for parametric energy functions. While PML is generally applicable to different domains, we use it in the context of region proposals to learn to combine a set of mid-level grouping cues to yield a small set of object region proposals with high recall. Our learning framework accounts for multiple diverse outputs, and is complemented by diversification seeds based on image location and color. This approach casts perceptual grouping and cue combination in a novel structured learning framework which yields baseline improvements on VOC 2012 and COCO 2014.\nTitle:\nLearning to Combine Mid-Level Cues for Object Proposal Generation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.782, mean value for all the writers is 8.648, which makes it 0.114 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe role of symmetry in computer vision has waxed and waned in importance during the evolution of the field from its earliest days. At first figuring prominently in support of bottom-up indexing, it fell out of favour as shape gave way to appearance and recognition gave way to detection. With a strong prior in the form of a target object, the role of the weaker priors offered by perceptual grouping was greatly diminished. However, as the field returns to the problem of recognition from a large database, the bottom-up recovery of the parts that make up the objects in a cluttered scene is critical for their recognition. The medial axis community has long exploited the ubiquitous regularity of symmetry as a basis for the decomposition of a closed contour into medial parts. However, today's recognition systems are faced with cluttered scenes and the assumption that a closed contour exists, i.e., that figure-ground segmentation has been solved, rendering much of the medial axis community's work inapplicable. In this article, we review a computational framework, previously reported in [1-3], that bridges the representation power of the medial axis and the need to recover and group an object's parts in a cluttered scene. Our framework is rooted in the idea that a maximally-inscribed disc, the building block of a medial axis, can be modelled as a compact superpixel in the image. We evaluate the method on images of cluttered scenes.\nTitle:", "model_inf_time": 1.87}, {"id": "41447", "output": "Registration of Range Images Based on Parametric Model Projection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present a new method for registration of range images, which is based on the results we obtain from the segmentation process. To obtain the first set of points needed for registration, we use set of points from first range image. The novelty is how we obtain the second set of points. To obtain the second set we project the first set of points onto geometric parametric models obtained in the second range image. Then we compute the transformation between the two sets of points. The results have shown a significant improvement in precision of the registration in comparison with traditional approach.\nTitle:\nRegistration of Range Images Based on Segmented Data\n\nAbstract:\nWe present a reliable and efficient method for extracting simple geometric structures, i.e., straight lines, parabolas, and ellipses, from edge images. The reliability of the recovery procedure which builds the parametric models is ensured by an iterative procedure through simultaneous data classification and parameter estimation. The overall relative insensitivity to noise and minor changes in input data is achieved by considering many competitive solutions and selecting those that produce the simplest description, i.e., the one that accounts for the largest number of data points with the smallest number of parameters while keeping the deviations between data points and models low. The presented method is efficient for two reasons: firstly, it is designed as a search which utilizes intermediate results as a guidance toward the final result, and secondly, it combines model recovery and model selection in a computationally efficient procedure.\nTitle:\nFinding Parametric Curves in an Image\n\nAbstract:\nThe basic limitations of the current appearance-based matching methods using eigenimages are non-robust estimation of coefficients and inability to cope with problems related to occlusions and segmentation. In this paper we present a new approach which successfully solves these problems. The major novelty of our approach lies in the way how the coefficients of the eigenimages are determined. Instead of computing the coefficients by a projection of the data onto the eigenimages, we extract them by a hypothesize-and-test paradigm using subsets of image points. Competing hypotheses are then subject to a selection procedure based on the Minimum Description Length principle. The approach enables us not only to reject outliers and to deal with occlusions but also to simultaneously use multiple classes of eigenimages.\nTitle:\nDealing with occlusions in the eigenspace approach\n\nAbstract:\nIn this paper we present a Minimum Description Length (MDL) framework for fuzzy clustering algorithms. This framework enables us to find an optimal number of cluster centers. We applied our approach to the fuzzy c-means algorithm for which we designed a computationally efficient procedure. We report the results of our approach on a 2D clustering problem and on RGB color image segmentation.\nTitle:\nFuzzy C-Means In An Mdl-Framework\n\nAbstract:\nStandard approaches to recognition using parametric eigenspaces have been designed under the assumption that the training images and the input images to be recognized are of the same type. In this paper we propose a novel approach which demonstrates that having an eigenspace encompassing a set of grey-level images, it is possible to recover the eigenspace coefficients and consequently recognize the input images even in the cases when the input images are 2-tone images of the objects in the training set. We present the recognition results of 2-tone images using the eigenspace built from a set of grey-level images.\nTitle:\nRecognizing 2-tone images in grey-level parametric eigenspaces\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.676, mean value for all the writers is 8.648, which makes it 0.024 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe present a new method for registration of range images, which is based on the results we obtain from the segmentation process. We need two range images segmented into regions, each of them modeled by a parametric model and the approximation of the transformation between the two range images. Then two sets of corresponding points, one from each range image, are chosen and the transformation between them is computed to further refine the initial approximation of the transformation. The novelty is how we obtain the a corresponding points for the original sets of points from the range image. Namely, to obtain them we project set of points from the first range image onto geometric parametric models taht were recovered in the second range image and viceversa. This way we obtain two sets of corresponding points. Then we compute the transformation between the two sets. Few iterations are required to improve the initial approximation of the transformation. The results have shown a significant improvement in precision of the registration in comparison with traditional approches.\nTitle:", "model_inf_time": 1.36}, {"id": "41448", "output": "Asymmetric Lossless Source Coding Using Syndrome Forming and Inverse Syndrome Forming", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe recently proposed turbo-binning scheme is shown to be both efficient and optimal foruniform source Slepian-Wolf coding problem. This paper studies the case when sources arei.i.d but nonuniformly distributed. It is firstly shown that any algebraic binning scheme basedon linear codes is optimal for nonuniform sources only asymptotically. Next two modificationsare proposed to improve the performance of the turbo-binning scheme for nonuniform sources.The first is to carefully design the constituent encoder structures to maximally match the turbocode to the nonuniform source distribution, and the second is to use variable-length syndromesequences to index the bins. Simulations show that the combination of both strategies can lead to an improvement of as much as 0.22 bit/symbol in overall compression rate for highly nonuniform sources.\nTitle:\nSlepian-Wolf Coding for Nonuniform Sources Using Turbo Codes\n\nAbstract:\nSpace time convolutional codes (STCCs) are an effective way to combine transmit diversity with coding. The computational complexity of designing STCCs generally increases exponentially with the constellation size of the transmitted symbols. In this paper, we first present an innovative approach to design STCCs with high spectral efficiencies by utilizing QPSK STCCs as component codes and consequently. Unlike existing techniques, the search space does not grow exponentially with the constellation size. Then, we present two approaches to reduce the computational complexity of our proposed scheme. This scheme is applicable to cases with any number of transmit antennas without any requirement to change the encoder design. Simulation results evaluate the performance of our approach for the case of two transmit antennas and several different number of receive antennas, a spectral efficiency of 4 bits/s/Hz, and slow Rayleigh fading channels.\nTitle:\nLow complexity design of space-time convolutional codes with high spectral efficiencies\n\nAbstract:\nImproved space-time coding for multiple-input and multiple-output orthogonal frequency division multiplexing is studied for wireless systems using QPSK modulation for four transmit and four receive antennas. A 256-state code is shown to perform within 3 dB of outage capacity (and within 2 dB with perfect channel estimation), which is better than any other published result without using iterative decoding.\nTitle:\nImproved Space-Time Coding For Mimo-Ofdm Wireless Communications\n\nAbstract:\nOptimum space-time convolutional codes, that provide maximum diversity and coding gain, are produced for cases with PSK modulation and various numbers of states and antennas. The codes are found using a new approach introduced recently in a companion paper. The new approach provides an efficient method that allows a search for optimum codes for many practical problems, while previous research was only able to find optimum codes for a single case. The new approach also provides a simple method for augmenting the criteria of maximum diversity and coding gain with a new measure which is shown to be extremely useful for evaluating code performance without extensive simulations\nTitle:\nOptimum space-time convolutional codes\n\nAbstract:\nWe consider in this paper the design of a cooperative relay strategy by exploiting the [POUND SYMBOL]nite-alphabet property of the source. Assuming a single source-sink pair with L relay nodes all communicating in orthogonal channels, we derive necessary conditions for optimal relay signaling that minimizes the error probability at the sink node. The derived conditions allow us to construct an iterative algorithm to End the distributed relay signaling that is at least locally optimal. As a byproduct, one can show that the so-called decode-and-forward (DF) relay scheme does not satisfy the necessary condition hence is not optimal in its error probability performance. Indeed, numerical examples show that the proposed scheme provides substantial performance improvement over both DF and the amplify-and-forward approach.\nTitle:\nExploiting The Finite-Alphabet Property For Cooperative Relays\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.383, mean value for all the writers is 8.648, which makes it 1.48 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nA simple but powerful scheme exploiting the binning concept for asymmetric lossless distributed source coding is proposed. The novelty in the proposed scheme is the introduction of a syndrome former (SF) in the source encoder and an inverse syndrome former (ISF) in the source decoder to efficiently exploit an existing linear channel code without the need to modify the code structure or the decoding strategy. For most channel codes, the construction of SF-ISF pairs is a light task. For parallelly and serially concatenated codes and particularly parallel and serial turbo codes where this appears less obvious, an efficient way for constructing linear complexity SF-ISF pairs is demonstrated. It is shown that the proposed SF-ISF approach is simple, provenly optimal, and generally applicable to any linear channel code. Simulation using conventional and asymmetric turbo codes demonstrates a compression rate that is only 0.06 bit/symbol from the theoretical limit, which is among the best results reported so far.\nTitle:", "model_inf_time": 1.59}, {"id": "41449", "output": "Secure and Efficient N-Party Password-Authenticated Group Diffie-Hellman Key Exchange", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMost password-authenticated key exchange schemes in the literature provide an authenticated key exchange between a client and a server based on a pre-shared password. With a rapid change in modern communication environments, it is necessary to construct a secure end-to-end channel between clients, which is a quite different paradigm from the existing ones. In this paper we propose a new framework which provides a password-authenticated key exchange between clients based only on their two different passwords without any pre-shared secret, so called Client-to-Client Password-Authenticated Key Exchange (C2CPAKE). Security notions and types of possible attacks are newly defined according to the new framework. We prove our scheme is secure against all types of attacks considered in the paper. Two secure C2C-PAKE schemes are suggested, one in a cross-realm setting and the other in a single-server setting.\nTitle:\nPassword-Authenticated Key Exchange between Clients with Different Passwords\n\nAbstract:\nMost password-authenticated key agreement schemes described in the literature have focused on authenticated key agreement using a shared password between a client and a server. With rapid changes in the modern communication environment such as ad hoc networks and ubiquitous computing, it is necessary to construct a secure end-to-end channel between clients. This paradigm is a quite different paradigm from the existing ones. In this paper, we study client-to-client password-authenticated key agreement (C2C-PAKA) enabling two clients in different realms to agree on a common session key using different passwords. Byun et al. first presented a C2C-PAKA protocol under the cross-realm setting. However, the scheme was not formally treated, and subsequently found to be flawed. In addition, in this scheme, there is still opportunity for improvements both in the computation and communication aspects. We provide formal treatments for the C2C-PAKA protocol by using Bellare et al.'s security model. We also suggest an efficientC2C-PAKA protocol and prove that the protocol is secure under the decisional Diffie-Hellman assumption in the ideal cipher and random oracle models.\nTitle:\nEC2C-PAKA: An efficient client-to-client password-authenticated key agreement\n\nAbstract:\nPassword-based authentication key exchange (PAKE) protocols in the literature typically assume a password that is shared between a client and a server. PAKE has been applied in various environments, especially in the \"client-server\" applications of remotely accessed systems, such as e-banking. With the rapid developments in modern communication environments, such as ad-hoc networks and ubiquitous computing, it is customary to construct a secure peer-to-peer channel, which is quite a different paradigm from existing paradigms. In such a peer-to-peer channel, it would be much more common for users to not share a password with others. In this paper, we consider password-based authentication key exchange in the three-party setting, where two users do not share a password between themselves but only with one server. The users make a session-key by using their different passwords with the help of the server. We propose an efficient password-based authentication key exchange protocol with different passwords that achieves forward secrecy in the standard model. The protocol requires parties to only memorize human-memorable passwords; all other information that is necessary to run the protocol is made public. The protocol is also light-weighted, i.e., it requires only three rounds and four modular exponentiations per user. In fact, this amount of computation and the number of rounds are comparable to the most efficient password-based authentication key exchange protocol in the random-oracle model. The dispensation of random oracles in the protocol does not require the security of any expensive signature schemes or zero-knowlegde proofs.\nTitle:\nPractical Password-Authenticated Three-Party Key Exchange\n\nAbstract:\nPassword-based authenticated group key exchange (denoted by PGKE) provides n parties holding a common human-memorable password with secure group communication. Most PGKE protocols proposed so far are inefficient since they require O(n) communication rounds where n is the number of group members. In the paper, we propose the first 2-round PGKE protocol with 3-exponentiations required per user and prove its security in the random oracle model and the ideal cipher model under the intractability of the decision Diffie-Hellman problem and computation Diffie-Hellman problem. The proposed protocol also provides forward secrecy.\nTitle:\nEfficient Password-Based Group Key Exchange\n\nAbstract:\nAn authenticated group key exchange (AGKE) scheme allows a group of users in a public network to share a session key which may later be used to achieve desirable cryptographic goals. In the paper, we study AGKE schemes for dynamically changing groups in ad hoc networks, i.e., for environments such that a member of a group may join and/or leave at any given time and a group key is exchanged without the help of any central sever. Difficulties in group key managements under such environments are caused by dynamically changing group and existence of no trustee. In most AGKE schemes proposed so far in the literature, the number of rounds is linear with respect to the number of group members. Such schemes are neither scalable nor practical since the number of group members may be quite large and the efficiency of the schemes is severely degraded with only one member's delay. We propose an efficient provably secure AGKE scheme with constant-round. The propose scheme is still contributory and efficient, where each user executes three modular exponentiations and at most O(n) XOR operations.\nTitle:\nConstant-Round Authenticated Group Key Exchange for Dynamic Groups\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.81, mean value for all the writers is 8.648, which makes it 0.138 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe consider the problem of password-authenticated group Diffie-Hellman key exchange among N parties, N\u20131 clients and a single-server, using different passwords. Most password-authenticated key exchange schemes in the literature have focused on an authenticated key exchange using a shared password between a client and a server. With a rapid change in modern communication environment such as ad-hoc networks and ubiquitous computing, it is necessary to construct a secure end-to-end channel between clients, which is a quite different paradigm from the existing ones. To achieve this end-to-end security, only a few schemes of three-party setting have been presented where two clients exchange a key using their own passwords with the help of a server. However, up until now, no formally treated and round efficient protocols which enable group members to generate a common session key with clients' distinct passwords have been suggested. In this paper we securely and efficiently extend three-party case to N-party case with a formal proof of security. Two provably secure N-party EKE protocols are suggested; N-party EKE-U in the unicast network and N-party EKE-M in the multicast network. The proposed N-party EKE-M is provable secure and provides forward secrecy. Especially, the scheme is of constant-round, hence scalable and practical.\nTitle:", "model_inf_time": 2.12}, {"id": "41450", "output": "Improving the Gallant-Lambert-Vanstone Method for Scalar Multiplication", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe Frobenius endomorphism is known to be useful in efficient implementation of multiplication on certain elliptic curves. In this note a method to minimize the length of the Frobenius expansion of integer multiplier, ellipticc urves defined over small finite fields, is introduced. It is an optimization of previous works by Solinas and M\u7709ller. Finally, experimental results are presented and compared with curves recommended in standards by time-performance of multiplication.\nTitle:\nAn Improved Method of Multiplication on Certain Elliptic Curves\n\nAbstract:\nThe efficient computation of the arithmetic operations in finite fields is closely related to the particular ways in which the field elements are presented. The common field representations are a polynomial basis representation and a normal basis representation. In this paper, we introduce a nonconventional basis and present a new bit-parallel multiplier which is as efficient as the modified Massey-Omura multiplier using the type I optimal normal basis\nTitle:\nA new hardware architecture for operations in GF(2n)\n\nAbstract:\nAs Koblitz curves were generalized to hyperelliptic Koblitz curves for faster point multiplication by G\u7709nter, et al. [10] we extend the recent work of Gallant, et al. [8] to hyperelliptic curves. So the extended method for speeding point multiplication applies to a much larger family of hyperelliptic curves over finite fields that have efficiently-computable endomorphisms. For this special family of curves, a speedup of up to 55 (59) % can be achieved over the best general methods for a 160-bit point multiplication in case of genus g =2 (3).\nTitle:\nSpeeding Up Point Multiplication on Hyperelliptic Curves with Efficiently-Computable Endomorphisms\n\nAbstract:\nIt is well-known that a class of finite fields $GF(2^n)$ using an optimal normal basis is most suitable for a hardware implementation of arithmetic in finite fields. In this paper, we introduce composite fields of some hardware-applicable properties resulting from the normal basis representation and the optimal condition. We also present a hardware architecture of the proposed composite fields including a bit-parallel multiplier.\nTitle:\nEfficient Normal Basis Multipliers in Composite Fields\n\nAbstract:\nIn this paper, we firstly evaluate the resistance of the reduced 5-round version of the block cipher CIKS-1 against linear cryptanalysis (LC). A feature of the CIKS-1 is the use of both Data-Dependent permutations(DDP) and internal key scheduing which consist in datadapendent transformation of the round subkeys. Taking into account the structure of CIKS-1 we investigate linear approximation. That is, we consider 16 linear approximations with p = 3/4 for 16 parallel modulo 22 additions to construct one-round linear approximation and derive one-round linear approximation with the probability of P = 1/2 + 2-17 by Piling-Up lemma. Also we estimate that the P is a valid probability of one-round approximation and achieve that the probability P for oneround approximation is better than 1/2 +2-17 through experiments. Then we construct 3-round linear approximation with P = 1/2 +2-17 using this one-round approximation and can attack the reduced 5-round CIKS-1 with 64-bit block by LC. In conclusion, we present that our attack requires about 236 chosen plaintexts with a probability of success of 78.5 % and 1/5 \u8133 232 \u8133 236 \u9a74 265.7 encryption times to recover last round(5-round) key. In addition, we discuss a few improvements of the cipher CIKS-1.\nTitle:\nA Chosen Plaintext Linear Attack on Block Cipher CIKS-1\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.638, mean value for all the writers is 8.648, which makes it 0.845 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper the Gallant-Lambert-Vanstone method is reexamined for speeding up scalar multiplication. Using the theory of \u788c-Euclidian algorithm, we provide a rigorous method to reduce the theoretical bound for the decomposition of an integer k in the endomorphism ring of an elliptic curve. We then compare the two different methods for decomposition through computational implementations.\nTitle:", "model_inf_time": 1.59}, {"id": "41451", "output": "FAUST: A Dataset for 3D Mesh Registration with Appearance", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThree-dimensional (3D) shape models are powerful because they enable the inference of object shape from incomplete, noisy, or ambiguous 2D or 3D data. For example, realistic parameterized 3D human body models have been used to infer the shape and pose of people from images. To train such models, a corpus of 3D body scans is typically brought into registration by aligning a common 3D human-shaped template to each scan. This is an ill-posed problem that typically involves solving an optimization problem with regularization terms that penalize implausible deformations of the template. When aligning a corpus, however, we can do better than generic regularization. If we have a model of how the template can deform then alignments can be regularized by this model. Constructing a model of deformations, however, requires having a corpus that is already registered. We address this chicken-and-egg problem by approaching modeling and registration together. By minimizing a single objective function, we reliably obtain high quality registration of noisy, incomplete, laser scans, while simultaneously learning a highly realistic articulated body model. The model greatly improves robustness to noise and missing data. Since the model explains a corpus of body scans, it captures how body shape varies across people and poses.\nTitle:\nCoregistration: simultaneous alignment and modeling of articulated 3d shape\n\nAbstract:\nExtracting anthropometric or tailoring measurements from 3D human body scans is important for applications such as virtual try-on, custom clothing, and online sizing. Existing commercial solutions identify anatomical landmarks on high-resolution 3D scans and then compute distances or circumferences on the scan. Landmark detection is sensitive to acquisition noise (e.g. holes) and these methods require subjects to adopt a specific pose. In contrast, we propose a solution we call model-based anthropometry. We fit a deformable 3D body model to scan data in one or more poses; this model-based fitting is robust to scan noise. This brings the scan into registration with a database of registered body scans. Then, we extract features from the registered model (rather than from the scan); these include, limb lengths, circumferences, and statistical features of global shape. Finally, we learn a mapping from these features to measurements using regularized linear regression. We perform an extensive evaluation using the CAESAR dataset and demonstrate that the accuracy of our method outperforms state-of-the-art methods.\nTitle:\nModel-based anthropometry: Predicting measurements from 3D human scans in multiple poses\n\nAbstract:\nThe field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression blendshapes. The pose and expression dependent articulations are learned from 4D face sequences in the D3DFACS dataset along with additional 4D sequences. We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).\n\n\nTitle:\nLearning a model of facial shape and expression from 4D scans.\n\nAbstract:\nWe accurately estimate the 3D geometry and appearance of the human body from a monocular RGB-D sequence of a user moving freely in front of the sensor. Range data in each frame is first brought into alignment with a multi-resolution 3D body model in a coarse-to-fine process. The method then uses geometry and image texture over time to obtain accurate shape, pose, and appearance information despite unconstrained motion, partial views, varying resolution, occlusion, and soft tissue deformation. Our novel body model has variable shape detail, allowing it to capture faces with a high-resolution deformable head model and body shape with lower-resolution. Finally we combine range data from an entire sequence to estimate a high-resolution displacement map that captures fine shape details. We compare our recovered models with high-resolution scans from a professional system and with avatars created by a commercial product. We extract accurate 3D avatars from challenging motion sequences and even capture soft tissue dynamics.\nTitle:\nDetailed Full-Body Reconstructions of Moving People From Monocular RGB-D Sequences\n\nAbstract:\nHumans move their hands and bodies together to communicate and solve tasks. Capturing and replicating such coordinated activity is critical for virtual characters that behave realistically. Surprisingly, most methods treat the 3D modeling and tracking of bodies and hands separately. Here we formulate a model of hands and bodies interacting together and fit it to full-body 4D sequences. When scanning or capturing the full body in 3D, hands are small and often partially occluded, making their shape and pose hard to recover. To cope with low-resolution, occlusion, and noise, we develop a new model called MANO (hand Model with Articulated and Non-rigid defOrmations). MANO is learned from around 1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand poses. The model is realistic, low-dimensional, captures non-rigid shape changes with pose, is compatible with standard graphics packages, and can fit any human hand. MANO provides a compact mapping from hand poses to pose blend shape corrections and a linear manifold of pose synergies. We attach MANO to a standard parameterized 3D body shape model (SMPL), resulting in a fully articulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting complex, natural, activities of subjects captured with a 4D scanner. The fitting is fully automatic and results in full body models that move naturally with detailed hand motions and a realism not seen before in full body performance capture. The models and data are freely available for research purposes at http://mano.is.tue.mpg.de.\n\n\nTitle:\nEmbodied hands: modeling and capturing hands and bodies together.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.963, mean value for all the writers is 8.648, which makes it 0.269 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nNew scanning technologies are increasing the importance of 3D mesh data and the need for algorithms that can reliably align it. Surface registration is important for building full 3D models from partial scans, creating statistical shape models, shape retrieval, and tracking. The problem is particularly challenging for non-rigid and articulated objects like human bodies. While the challenges of real-world data registration are not present in existing synthetic datasets, establishing ground-truth correspondences for real 3D scans is difficult. We address this with a novel mesh registration technique that combines 3D shape and appearance information to produce high-quality alignments. We define a new dataset called FAUST that contains 300 scans of 10 people in a wide range of poses together with an evaluation methodology. To achieve accurate registration, we paint the subjects with high-frequency textures and use an extensive validation process to ensure accurate ground truth. We find that current shape registration methods have trouble with this real-world data. The dataset and evaluation website are available for research purposes at http://faust.is.tue.mpg.de.\nTitle:", "model_inf_time": 2.02}, {"id": "41452", "output": "Learning Dynamic Textured Motion Models", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe introduce a new class of probabilistic latent vari- able model called the Implicit Mixture of Conditional Re- stricted Boltzmann Machines (imCRBM) for use in human pose tracking. Key properties of the imCRBM are as fol- lows: (1) learning is linear in the number of training exem- plars so it can be learned from large datasets; (2) it learns coherent models of multiple activities; (3) it automatically discovers atomic \"movemes\"; and (4) it can infer transi- tions between activities, even when such transitions are not present in the training set. We describe the model and how it is learned and we demonstrate its use in the context of Bayesian filtering for multi-view and monocular pose track- ing. The model handles difficult scenarios including multi- ple activities and transitions among activities. We report state-of-the-art results on the HumanEva dataset.\nTitle:\nDynamical binary latent variable models for 3D human pose tracking\n\nAbstract:\nWe propose a Bayesian framework for representing and recognizing local image motion in terms of two primitive models: translation and motion discontinu- ity. Motion discontinuities are represented using a non-linear generative model that explicitly encodes the orientation of the boundary, the velocities on either side, the motion of the occluding edge over time, and the appearance/disappearance of pixels at the bound- ary. We represent the posterior distribution over the model parameters given the image data using discrete samples. This distribution is propagated over time us- ing the Condensation algorithm. To efficiently repre- sent such a high-dimensional space we initialize sam- ples using the responses of a low-level motion discon- tinuity detector.\nTitle:\nProbabilistic Detection and Tracking of Motion Discontinuities\n\nAbstract:\nA framework for learning parameterized models of optical flow from image sequences is presented. A class of motions is represented by a set of orthogonal basis flow fields that are computed from a training set using principal component analysis. Many complex image motions can be represented by a linear combination of a small number of these basis flows. The learned motion models may be used for optical flow estimation and for model-based recognition. For optical flow estimation we describe a robust, multi-resolution scheme for directly computing the parameters of the learned flow models from image derivatives. As examples we consider learning motion discontinuities, non-rigid motion of human mouths, and articulated human motion.\nTitle:\nLearning Parameterized Models of Image Motion\n\nAbstract:\nWe present a technique for the computation of 2D component velocity from image sequences. Initially, the image sequence is represented by a family of spatiotemporal velocity-tuned linear filters. Component velocity, computed from spatiotemporal responses of identically tuned filters, is expressed in terms of the local first-order behavior of surfaces of constant phase. Justification for this definition is discussed from the perspectives of both 2D image translation and deviations from translation that are typical in perspective projections of 3D scenes. The resulting technique is predominantly linear, efficient, and suitable for parallel processing. Moreover, it is local in space-time, robust with respect to noise, and permits multiple estimates within a single neighborhood. Promising quantiative results are reported from experiments with realistic image sequences, including cases with sizeable perspective deformation.\nTitle:\nComputation of component image velocity from local phase information\n\nAbstract:\nWe describe a probabilistic framework for detecting and tracking motion boundaries. It builds on previous work [4] that used a particle filter to compute a posterior distribution over multiple, local motion models, one of which was specific for motion boundaries. We extend that framework in two ways: 1) with an enhanced likelihood that combines motion and edge support, 2) with a spatiotemporal model that propagates beliefs between adjoining image neighborhoods to encourage boundary continuity and provide better temporal predictions for motion boundaries. Approximate inference is achieved with a combination of tools: Sampled representations allow us to represent multimodal non-Gaussian distributions and to apply nonlinear dynamics. Mixture models are used to simplify the computation of joint prediction distributions.\nTitle:\nProbabilistic Tracking Of Motion Boundaries With Spatiotemporal Predictions\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.079, mean value for all the writers is 8.648, which makes it 1.339 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDynamic textured sequences are characterized by the interactions be- tween many particles or objects in the scene. Based on earlier work the im- ages of the sequence are interpreted as the output of a linear autoregressive process driven by white Gaussian noise. We extend earlier work by increas- ing the amount temporal information included when learning the motion in the scene, allowing the models to capture complex motion patterns which ex- tend over multiple frames, thereby increasing the perceptual accuracy of the synthesized results. To overcome problems of dynamic model stability, we apply Burg's Maximum Entropy Spectral Analysis technique f or parameter estimation, which is found to be reliably stable on smaller samples of training data, even with higher-order dynamics.\nTitle:", "model_inf_time": 1.28}, {"id": "41453", "output": "Symmetry-Specific Invariants", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe paper describes a systematic approach to the analysis of symmetries in planar shapes. The shapes can be observed from arbitrary viewpoints. Hence, the results encompass the case of skewed symmetries. In fact, skewing is assumed to arise from perspective distortions, whereas most of the literature restricts the analysis to affine skewing. The point of departure is the identification of structures that symmetries keep fixed in an image. These define subgroups of the projectivities, which in turn assume simpler invariants than their projective counterparts. These invariants allow both simpler and more selective detection and checking of symmetries.\nTitle:\nGroups, fixed sets, symmetries, and invariants\n\nAbstract:\nIn this paper we compare different ways of representing the photometric changes in image intensities caused by changes in illumination and viewpoint, aiming at a balance between goodness-of-fit and low complexity. We derive invariant features based on generalized color moment invariants - that can deal with geometric and photometric changes of a planar pattern - corresponding to the chosen photometric models. The geometric changes correspond to a perspective skew. We compare the photometric models also in terms of the invariants' discriminative power and classification performance in a pattern recognition system.\nTitle:\nComparing Intensity Transformations and Their Invariants in the Context of Color Pattern Recognition\n\nAbstract:\nSemidifferential invariants, combining coordinates in different points together with their derivatives, are used for the description of planar contours. Their use can be seen as a tradeoff between two extreme strategies currently used in shape recognition: (invariant) feature extraction methods, involving high-order derivatives, and invariant coordinate descriptions, leading to the correspondence problem of reference points. The method for the derivation of such invariants, based on Lie group theory and applicable to a wide spectrum of transformation groups, is described. As an example, invariant curve parameterizations are developed for affine and projective transformations. The usefulness of the approach is illustrated with two examples: (1) recognition of a test set of 12 planar objects viewed under conditions allowing affine approximations, and (2) the detection of symmetry in perspective projections of curves\nTitle:\nRecognition and semi-differential invariants\n\nAbstract:\nAffine invariant regions have proved a powerful feature for object recognition and categorization. These features heavily rely on object textures rather than shapes, however. Typically, their shapes have been fixed to ellipses or parallelograms. The paper proposes a novel affine invariant region type, that is built up from a combination of fitted superellipses. These novel features have the advantage of offering a much wider range of shapes through the addition of a very limited number of shape parameters, with the traditional ellipses and parallelograms as subsets. The paper offers a solution for the robust fitting of superellipses to partial contours, which is a crucial step towards the implementation of the novel features.\nTitle:\nFitting Superellipses to Incomplete Contours\n\nAbstract:\nAffine invariant regions have proved a powerful feature for object recognition and categorization. These features heavily rely on object textures rather than shapes, however. Typically, their shapes have been fixed to ellipses or parallelograms. The paper proposes a novel affine invariant region type, that is built up from a combination of fitted superellipses. These novel features have the advantage of offering a much wider range of shapes through the addition of a very limited number of shape parameters, with the traditional ellipses and parallelograms as subsets. The paper offers a solution for the robust fitting of superellipses to partial contours, which is a crucial step towards the implementation of the novel features.\nTitle:\nA shape based, viewpoint invariant local descriptor\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.889, mean value for all the writers is 8.648, which makes it 0.648 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOver recent years, symmetry research has shifted from the detection of affinely to perspectively skewed mirror symmetry. Also, links between invariance research and symmetry-specific geometric constraints have been established. The paper aims to contribute to both strands. Several sets of symmetry specific invariants are derived, that can be used in different situations, depending on the a priori assumptions made. It is also argued that all the results directly apply to the case of perspectively skewed point symmetry.\nTitle:", "model_inf_time": 1.08}, {"id": "41454", "output": "String Pattern Recognition Using Evolving Spiking Neural Networks with Quantum-Inspired Particle Swarm Optimization", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper proposes a novel method for optimizing features and parameters in the Evolving Spiking Neural Network (ESNN) using Quantum-inspired Particle Swarm Optimization (QiPSO). This study reveals the interesting concept of QiPSO in which information is represented as binary structures. The mechanism simultaneously optimizes the ESNN parameters and relevant features using wrapper approach. A synthetic dataset is used to evaluate the performance of the proposed method. The results show that QiPSO yields promising outcomes in obtaining the best combination of ESNN parameters as well as in identifying the most relevant features.\nTitle:\nIntegrated Feature Selection and Parameter Optimization for Evolving Spiking Neural Networks Using Quantum Inspired Particle Swarm Optimization\n\nAbstract:\nThis paper proposes a new model of an Evolving Spiking Neural Network (ESNN) for spatio-temporal data (STD) classification problems. The proposed ESNN model incorporates an additional layer for capturing both spatial and temporal components of the STD and then transforms them into high dimensional spiking patterns. These patterns are learned and classified in the evolving classification layer of the ESNN. A fast time-to-first-spike learning algorithm is used that enables the new model to be more suitable for learning from the STD streams in an adaptive and incremental manner. The proposed method is evaluated on a benchmark sign language video that is spatio-temporal in nature. The results show that the proposed method is able to capture important spatio-temporal information from the STD stream. This results in significantly higher classification accuracy than the traditional time-delay MLP neural network model. Future directions for the development of ESNN models for STD are discussed.\nTitle:\nAn extended Evolving Spiking Neural Network model for spatio-temporal pattern classification.\n\nAbstract:\nThis paper discusses an alternative solution for curve fitting based on particle swarm optimization (PSO). The implementation of this method is conducted by generating randomly weight and control points of the NURBS curve. The weight and generated control points are used to calculate the NURBS point. The results are compared with the example data points to find the minimum error. The implementation results have shown that the proposed method yield better solution compared to the conventional methods with minimum error generated.\nTitle:\nParticle Swarm Optimization for NURBS Curve Fitting\n\nAbstract:\nMost of the existing IDS use all the features in network packet to evaluate and look for known intrusive patterns. Some of these features are irrelevant and redundant. The drawback to this approach is a lengthy detection process. In real-time environment this may degrade the performance of an IDS. Thus, feature selection is required to address this issue. In this paper, we use wrapper approach where we integrate Rough Set and Particle Swarm to form a 2-tier structure of feature selection process. Experimental results show that feature subset proposed by Rough-DPSO gives better representation of data and they are robust.\nTitle:\nFeature selection using rough-DPSO in anomaly intrusion detection\n\nAbstract:\nThis paper presents a new multiobjective evolutionary algorithm applied to a radial basis function (RBF) network design based on multiobjective particle swarm optimization augmented with local search features. The algorithm is named the memetic multiobjective particle swarm optimization RBF network (MPSON) because it integrates the accuracy and structure of an RBF network. The proposed algorithm is implemented on two-class and multiclass pattern classification problems with one complex real problem. The experimental results indicate that the proposed algorithm is viable, and provides an effective means to design multiobjective RBF networks with good generalization capability and compact network structure. The accuracy and complexity of the network obtained by the proposed algorithm are compared with the memetic non-dominated sorting genetic algorithm based RBF network (MGAN) through statistical tests. This study shows that MPSON generates RBF networks coming with an appropriate balance between accuracy and simplicity, outperforming the other algorithms considered.\nTitle:\nMemetic multiobjective particle swarm optimization-based radial basis function network for classification problems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.369, mean value for all the writers is 8.648, which makes it 1.468 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper proposes a novel method for string pattern recognition using an Evolving Spiking Neural Network (ESNN) with Quantum-inspired Particle Swarm Optimization (QiPSO). This study reveals an interesting concept of QiPSO by representing information as binary structures. The mechanism optimizes the ESNN parameters and relevant features using the wrapper approach simultaneously. The N-gram kernel is used to map Reuters string datasets into high dimensional feature matrix which acts as an input to the proposed method. The results show promising string classification results as well as satisfactory QiPSO performance in obtaining the best combination of ESNN parameters and in identifying the most relevant features.\nTitle:", "model_inf_time": 1.82}, {"id": "41455", "output": "Decoding Human Expression: From Vocal Production to Multimodal Affect.", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe expression and experience of human behavior are complex and multimodal and characterized by individual and contextual heterogeneity and variability. Speech and spoken language communication cues offer an important means for measuring and modeling human behavior. Observational research and practice across a variety of domains from commerce to healthcare rely on speech- and language-based informatics for crucial assessment and diagnostic information and for planning and tracking response to an intervention. In this paper, we describe some of the opportunities as well as emerging methodologies and applications of human behavioral signal processing (BSP) technology and algorithms for quantitatively understanding and modeling typical, atypical, and distressed human behavior with a specific focus on speech- and language-based communicative, affective, and social behavior. We describe the three important BSP components of acquiring behavioral data in an ecologically valid manner across laboratory to real-world settings, extracting and analyzing behavioral cues from measured data, and developing models offering predictive and decision-making support. We highlight both the foundational speech and language processing building blocks as well as the novel processing and modeling opportunities. Using examples drawn from specific real-world applications ranging from literacy assessment and autism diagnostics to psychotherapy for addiction and marital well being, we illustrate behavioral informatics applications of these signal processing techniques that contribute to quantifying higher level, often subjectively described, human behavior in a domain-sensitive fashion.\nTitle:\nBehavioral Signal Processing: Deriving Human Behavioral Informatics From Speech and Language.\n\nAbstract:\nDepression is one of the most common mood disorders. Technology has the potential to assist in screening and treating people with depression by robustly modeling and tracking the complex behavioral cues associated with the disorder (e.g., speech, language, facial expressions, head movement, body language). Similarly, robust affect recognition is another challenge which stands to benefit from modeling such cues. The Audio/Visual Emotion Challenge (AVEC) aims toward understanding the two phenomena and modeling their correlation with observable cues across several modalities. In this paper, we use multimodal signal processing methodologies to address the two problems using data from human-computer interactions. We develop separate systems for predicting depression levels and affective dimensions, experimenting with several methods for combining the multimodal information. The proposed depression prediction system uses a feature selection approach based on audio, visual, and linguistic cues to predict depression scores for each session. Similarly, we use multiple systems trained on audio and visual cues to predict the affective dimensions in continuous-time. Our affect recognition system accounts for context during the frame-wise inference and performs a linear fusion of outcomes from the audio-visual systems. For both problems, our proposed systems outperform the video-feature based baseline systems. As part of this work, we analyze the role played by each modality in predicting the target variable and provide analytical insights.\nTitle:\nMultimodal Prediction of Affective Dimensions and Depression in Human-Computer Interactions\n\nAbstract:\nEmotion expression is a complex process involving dependencies based on time, speaker, context, mood, personality, and culture. Emotion classification algorithms designed for real-world application must be able to interpret the emotional content of an utterance or dialog given the modulations resulting from these and other dependencies. Algorithmic development often rests on the assumption that the input emotions are uniformly recognized by a pool of evaluators. However, this style of consistent prototypical emotion expression often does not exist outside of a laboratory environment. This paper presents methods for interpreting the emotional content of non-prototypical utterances. These methods include modeling across multiple time-scales and modeling interaction dynamics between interlocutors. This paper recommends classifying emotions based on emotional profiles, or soft-labels, of emotion expression rather than relying on just raw acoustic features or categorical hard labels. Emotion expression is both interactive and dynamic. Consequently, to accurately recognize emotional content, these aspects must be incorporated during algorithmic design to improve classification performance.\nTitle:\nInterpreting ambiguous emotional expressions\n\nAbstract:\nIncorporating multimodal information and temporal context from speakers during an emotional dialog can contribute to improving performance of automatic emotion recognition systems. Motivated by these issues, we propose a hierarchical framework which models emotional evolution within and between emotional utterances, i.e., at the utterance and dialog level respectively. Our approach can incorporate a variety of generative or discriminative classifiers at each level and provides flexibility and extensibility in terms of multimodal fusion; facial, vocal, head and hand movement cues can be included and fused according to the modality and the emotion classification task. Our results using the multimodal, multi-speaker IEMOCAP database indicate that this framework is well-suited for cases where emotions are expressed multimodally and in context, as in many real-life situations.\nTitle:\nA hierarchical framework for modeling multimodality and emotional evolution in affective dialogs.\n\nAbstract:\nEmotion expression is an essential part of human interaction. Rich emotional information is conveyed through the human face. In this study, we analyze detailed motion-captured facial information of ten speakers of both genders during emotional speech. We derive compact facial representations using methods motivated by Principal Component Analysis and speaker face normalization. Moreover, we model emotional facial movements by conditioning on knowledge of speech-related movements (articulation). We achieve average classification accuracies on the order of 75% for happiness, 50-60% for anger and sadness and 35% for neutrality in speaker independent experiments. We also find that dynamic modeling and the use of viseme information improves recognition accuracy for anger, happiness and sadness, as well as for the overall unweighted performance.\nTitle:\nVisual emotion recognition using compact facial representations and viseme information\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.401, mean value for all the writers is 8.648, which makes it 1.496 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nHuman verbal and nonverbal expressions carry crucial information not only about intent but also emotions, individual identity, and the state of health and wellbeing. From a basic science perspective, understanding how such rich information is encoded in these signals can illuminate underlying production mechanisms including the variability therein, within and across individuals. From a technology perspective, finding ways for automatically processing and decoding this complex information continues to be of interest across a variety of applications.\n\nThe convergence of sensing, communication and computing technologies is allowing access to data, in diverse forms and modalities, in ways that were unimaginable even a few years ago. These include data that afford the multimodal analysis and interpretation of the generation of human expressions.\n\nThe first part of the talk will highlight advances that allow us to perform investigations on the dynamics of vocal production using real-time imaging and audio modeling to offer insights about how we produce speech and song with the vocal instrument. The second part of the talk will focus on the production of vocal expressions in conjunction with other signals from the face and body especially in encoding affect. The talk will draw data from various domains notably in health to illustrate some of the applications.\nTitle:", "model_inf_time": 1.76}, {"id": "41456", "output": "Quantifying Vocal Entrainment in Marital Conflict: A Signal-Derived Approach to Couple Therapy Outcomes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn human-human interactions, entrainment is a naturally occurring phenomenon that happens when interlocutors mutually adapt their behaviors through the course of an interaction. This mutual behavioral dependency has been at the center of psychological studies of human communication for decades. Quantitative descriptors of the degree of entrainment can provide psychologists an objective method to advance studies of human communication including in mental health domains. However, the subtle nature of the entrainment phenomenon makes it challenging for computing such an effect based on just human annotations. In this paper, we propose an unsupervised signal-derived approach within a principal component analysis framework for quantifying one aspect of entrainment in communication, namely, vocal entrainment. The proposed approach to quantify the degree of vocal entrainment involves measuring the similarity of specific vocal characteristics between the interlocutors in a dialog. These quantitative descriptors were analyzed using two psychology-inspired hypothesis tests to not only establish that these signal-derived measures carry meaningful information in interpersonal communication but also offer statistical evidence into aspects of behavioral dependency and associated affective states in marital conflictual interactions. Finally, affect recognition experiments were performed with the proposed vocal entrainment descriptors as features using a large database of real distressed married couples' interactions. An accuracy of 62.56% in differentiating between positive and negative affect was obtained using these entrainment measures with Factorial Hidden Markov Models lending further support that entrainment is an active component underlying affective processes in interactions.\nTitle:\nComputing vocal entrainment: A signal-derived PCA-based quantification scheme with application to affect analysis in married couple interactions\n\nAbstract:\nEntrainment has played a crucial role in analyzing marital couples interactions. In this work, we introduce a novel technique for quantifying vocal entrainment based on Principal Component Analysis (PCA). The entrainment measure, as we define in this work, is the amount of preserved variability of one interlocutor's speaking characteristic when projected onto representing space of the other's speaking characteristics. Our analysis on real couples interactions shows that when a spouse is rated as having positive emotion, he/she has a higher value of vocal entrainment compared when rated as having negative emotion. We further performed various statistical analyses on the strength and the directionality of vocal entrainment under different affective interaction conditions to bring quantitative insights into the entrainment phenomenon. These analyses along with a baseline prediction model demonstrate the validity and utility of the proposed PCA-based vocal entrainment measure.\nTitle:\nAn Analysis Of Pca-Based Vocal Entrainment Measures In Married Couples' Affective Spoken Interactions\n\nAbstract:\nThe expression and experience of human behavior manifestations are complex and are characterized by individual and contextual heterogeneity. Many domains rely on interpreting behavior -- especially those that are distressed and atypical -- through the available signals, both overt e.g., audio-visual data and covert e.g., heart rate. This paper describes the recent developments in behavioral signal processing aimed at understanding dyadic interactions in couple therapy.\nTitle:\nBehavioral signal processing for understanding (distressed) dyadic interactions: some recent developments\n\nAbstract:\nOur work examines the link between head motion entrainment of interacting couples and human expert's judgment on certain overall behavioral characteristics (e.g., Blame patterns). We employ a data-driven model that clusters head motion in an unsupervised manner into elementary types called kinemes. We propose three groups of similarity measures based on Kullback-Leibler divergence to model entrainment. We find that the divergence of the (joint) distribution of kinemes yields consistent and significant correlation with target behavior characteristics. The divergence of the conditional distribution of kinemes is shown to predict the polarity of the behavioral characteristics. We partly explain the strong correlations via associating the conditional distributions with the prominent behavioral implications of their respective associated kinemes. These results show the possibility of inferring human behavioral characteristics through the modeling of dyadic head motion entrainment.\nTitle:\nModeling head motion entrainment for prediction of couples' behavioral characteristics\n\nAbstract:\nWe present a method for characterizing salient behavioral events from audio-visual data of dyadic human interactions. This behavioral signal processing work is aimed at supporting observational analysis of domain experts such as psychologists and clinicians. We extract prosodic and spectral speech features as well as visual motion vector features on overlapping windows from a multimodal corpus. We then apply a technique called multiple instance learning to detect salient audio and visual instances for predicting human expert annotated behavior ratings. We demonstrate the performance gains achieved through multimodal fusion in characterizing complex behavior patterns of interest such as blame and acceptance in recordings of couples' problem solving discussions during marital therapy.\nTitle:\nAn audio-visual approach to learning salient behaviors in couples' problem solving discussions.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.839, mean value for all the writers is 8.648, which makes it 2.722 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nBehavioral entrainment is an important, naturally-occurring dynamic phenomenon in human interactions. In this paper, we carry out two quantitative analyses of the vocal entrainment phenomenon in the context of studying conflictual marital interactions. We investigate the role of vocal entrainment in reflecting different dimensions of couple-specific behaviors, such as withdrawal, that are commonly-used in assessing the effectiveness on the outcome of couple therapy. The results indicate a statistically-significant relation between these behaviors and vocal entrainment, as quantified using our proposed unsupervised signal-derived computational framework. We further demonstrate the potential of the signal-based vocal entrainment framework in characterizing influential factors in distressed couples relationship satisfaction outcomes.\nTitle:", "model_inf_time": 2.06}, {"id": "41457", "output": "Proactive Database Management", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nYears of innovation in file systems have been highly successful in improving their performance and functionality, but at the cost of complicating their interaction with the disk. A variety of techniques exist to ensure consistency and integrity of file ...\nTitle:\nSecure deletion for a versioning file system\n\nAbstract:\nPersonal information management (PIM) is a promising new type of application allowing not only to search a desktop, but to pose complex, structured queries against the data on ones computer. We propose to remove the confines of PIM and make selected ...\nTitle:\nThroughput-optimized, global-scale join processing in scientific federations\n\nAbstract:\nWe present an architecture for network-authenticated disks that implements distributed file systems without file servers or encryption. Our system provides network clients with direct network access to remote storage.\nTitle:\nAuthenticating Network-Attached Storage\n\nAbstract:\nMost applications that access large data objects do so through file systems, but file systems provide an incomplete solution, as they maintain insufficient metadata and do not provide general purpose query engine. Storing large objects in a database addresses these problems, but, for applications that need to update object data, databases are inefficient as they do not provide direct access to data. Additionally, databases often relax the integrity and consistency constraints for large objects, as it the case with objects stored through the Binary Large Object (BLOB) data type. These shortcomings are exacerbated by multiple users or applications that wish to access large objects concurrently. We describe an architecture, based on the Datalink data type, in which large objects in a database are continuously available for read access and can be read and written through a file system interface. Additionally this system does not relax version management, consistency and recoverability guarantees, as with the BLOB data type\nTitle:\nVersion management and recoverability for large object data\n\nAbstract:\nWe describe the design and implementation of a clustering service for a high-performance, shared-disk file system. The service provides failure detection and recovery, reliableend-to-end messaging, and a centralized and recoverable management interface. We implement novel optimizations in the voting protocol that resolves cluster membership. Optimizations allow clusters to form as quickly as possible without introducing livelock or requiring timeout parameters to be tuned carefully. Our treatment includes performance results that quantify the scalability of the system and measure recovery times.\nTitle:\nFastpath Optimizations for Cluster Recovery in Shared-Disk Systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.169, mean value for all the writers is 8.648, which makes it 0.409 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe ability of a database management system (DBMS) to detect problems or problem trends (which have not yet manifested as problems) and either take corrective actions, and/or provide advice or an automated notification to the database administrator (DBA) ...\nTitle:", "model_inf_time": 0.87}, {"id": "41458", "output": "Enumeration and classification of polypentagons", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nA normal perifusene with two internal vertices (n(i):=2) is said to belong to the class PF2. These systems consist of pyrene and pyrene with annealated catafusenes. A complete mathematical solution for the numbers of the PF2 systems, as a function of the number of hexagons (h), is derived using the so-called method of combinatorial summation. A normal peribenzenoid with n(i) = 2, belonging to the class PF2', may be pyrene or pyrene with annealated catabenzenoids. Computerized enumerations of these systems are recalled, and some recent extensions of the data to higher h values are reported. Finally a class PF2* is defined as consisting of normal perihelicenes with n(i)=2. The numbers of the PF2* systems for given h values are reported; they were obtained by appropriate subtractions (PF2* = PF2 - PF2'). Some of the lowest of these numbers are reproduced by an analytical method (without computers), referred to as combinatorial enumeration.\nTitle:\nEnumeration and classification of benzenoid systems. 32. Normal perifusenes with two internal vertices\n\nAbstract:\nIn this paper we establish a simple criterion which enables us to determine whether or not a hexagonal system H has the property that each hexagon s of H is resonant, i.e., the subgraph obtained by deleting from H the vertices of s together with their edges has at least one perfect matching.\nTitle:\nWhen each hexagon of a hexagonal system covers it\n\nAbstract:\nWe give lower and upper bounds for the number of reducible ears as well as upper bounds for the number of perfect matchings in an elementary bipartite graph. An application to chemical graphs is also discussed. In addition, a method to construct all minimal elementary bipartite graphs is described.\nTitle:\nPerfect matchings and ears in elementary bipartite graphs\n\nAbstract:\nIn this paper we give an O( n 2 ) algorithm to determine fixed bonds and normal subhexagonal systems in a hexagonal system. By this algorithm we can decompose a hexagonal system into a number of regions consisting of fixed bonds and a number of normal subhexagonal systems. This decomposition can be used to simplify the procedure of finding Clar's formula, counting the number of Kekul\u00e9 structures and constructing the Z -transformation graph of a hexagonal system with fixed bonds, especially for large hexagonal systems. We also give characterizations of hexagonal systems with fixed single and double bonds, respectively.\nTitle:\nHexagonal systems with fixed bonds\n\nAbstract:\nAn edge of a hexagonal system H is said to be forcing if it belongs to exactly one perfect matching of H . Using the concept of Z-transformation of hexagonal system, we give a characterization for the hexagonal systems with forcing edges and determine all forcing edges is such systems. We also give the generating function of all hexagonal systems with forcing edges.\nTitle:\nHexagonal systems with forcing edges\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.608, mean value for all the writers is 8.648, which makes it 0.034 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nPolypentagons are systems consisting of pentagons exclusively. Some of their topological properties are studied, including the relations between certain invariants. Complete mathematical solutions are reported for the numbers of polypentagons within certain classes: catacondensed systems (without internal vertices) and systems with one internal vertex and with two connected internal vertices. A complete account on proper polypentagons is given. These systems can, by definition, be embedded on a regular dodecahedron. It is found that exactly 39 such systems exist. Their chemical formulas (C(n)H(s)), forms, and symmetries are specified.\nTitle:", "model_inf_time": 1.24}, {"id": "41459", "output": "Atlas-Based Segmentation of Deep Brain Structures Using Spatial Dependency Tree and Non-Rigid Registration", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, a novel method for brain MR image segmentation has been proposed, with deep learning techniques to obtain preliminary labelling and graphical models to produce the final result. A specific architecture, namely multi-scale structured convolutional neural networks (MS-CNN), is designed to capture discriminative features for each sub-cortical structure and to generate a label probability map for the target image. Due to complex background in brain images and the lack of spatial constraints among testing samples, the initial result obtained with MS-CNN is not smooth. To deal with this problem, dynamic random walker with decayed region of interest is then proposed to enforce label consistency. Comprehensive evaluations have been carried out on two publicly available data-sets and experimental results indicate that the proposed method can obtain better segmentation quality efficiently.\nTitle:\nMulti-Scale Structured Cnn With Label Consistency For Brain Mr Image Segmentation\n\nAbstract:\nIn this paper, a novel 3D deep learning network is proposed for brain MR image segmentation with randomized connection, which can decrease the dependency between layers and increase the network capacity. The convolutional LSTM and 3D convolution are employed as network units to capture the long-term and short-term 3D properties respectively. To assemble these two kinds of spatial-temporal information and refine the deep learning outcomes, we further introduce an efficient graph-based node selection and label inference method. Experiments have been carried out on the publicly available database and results demonstrate that the proposed method can obtain the best performance as compared with other state-of-the-art methods.\nTitle:\n3d Randomized Connection Network With Graph-Based Inference\n\nAbstract:\nWe propose a new framework for multi-object segmentation of deep brain structures, which have significant shape variations and relatively small sizes in medical brain images. In the images, the structure boundaries may be blurry or even missing, and the surrounding background is a clutter and full of irrelevant edges. We suggest a template-based framework, which fuses the information of edge features, region statistics and inter-structure constraints to detect and locate all the targeted brain structures such that manual initialization is unnecessary. The multi-object template is organized in the form of a hierarchical Markov dependence tree. It makes the matching of multiple objects efficient. Our approach needs only one example as training data and alleviates the demand of a large training set. The obtained segmentation results on real data are encouraging and the proposed method enjoys several important advantages over existing methods.\nTitle:\nMarkov dependence tree-based segmentation of deep brain structures.\n\nAbstract:\nPerforming segmentation of narrow, elongated structures with low contrast boundaries is a challenging problem. Boundaries of these structures are difficult to be located when noise exists or intensity of objects and background is varying. Using the active contour methods, this paper proposes a new vector field for detection of such structures. In this paper, unlike other work, object boundaries are not defined by intensity gradient but statistics obtained from a set of filters applied on an image. The direction and magnitude of edges are estimated such that the minimal weighted local variance condition is satisfied. This can effectively prevent contour leakage and discontinuity by linking disconnected boundaries with coherent orientation. It is experimentally shown that our method is robust to intensity variation in the image, and very suitable to deal with images with narrow structures and blurry edges, such as blood vessels.\nTitle:\nMinimal weighted local variance as edge detector for active contour models\n\nAbstract:\nThe aim of this work is to develop a new framework for multi-object segmentation of deep brain structures (caudate nucleus, putamen and thalamus) in medical brain images. Deep brain segmentation is difficult and challenging because the structures of interest are of relatively small size and have significant shape variations. The structure boundaries may be blurry or even missing, and the surrounding background is full of irrelevant edges. To tackle these problems, we propose a template-based framework to fuse the information of edge features, region statistics and inter-structure constraints for detecting and locating all target brain structures such that initialization by hand is unnecessary. The multi-object template is organized in the form of a hierarchical Markov dependence tree (MDT), and multiple objects are efficiently matched to a target image by a top-to-down optimization strategy. The final segmentation is obtained through refinement by a B-spline based non-rigid registration between the exemplar image and the target image. Our approach needs only one example as training data. We have validated the proposed method on a publicly available T1-weighted magnetic resonance image database with expert-segmented brain structures. In the experiments, the proposed approach has obtained encouraging results with 0.80 Dice score for the caudate nuclei, 0.81 Dice score for the putamina and 0.84 Dice score for the thalami on average.\nTitle:\nA novel framework for segmentation of deep brain structures based on Markov dependence tree.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.435, mean value for all the writers is 8.648, which makes it 1.525 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSegmentation of deep brain structures is a challenging task for MRI images due to blurry structure boundaries, small object size and irregular shapes. In this paper, we present a new atlas-based segmentation method. It first uses a prior spatial dependency tree to constrain the relative positions between different deep brain structures and determine an optimal sequence for the structure by-structure segmentation. After positioning the structures, the segmentation result is further fine tuned by a non-rigid registration procedure between the atlas image and the target image using the histogram of the gradient magnitudes lying on the structure boundaries. The pro posed method has been applied on a publicly available MRI brain database and can achieve comparatively high segmentation accuracy.\nTitle:", "model_inf_time": 1.94}, {"id": "41460", "output": "Modular ECG Beat Classification", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMethods which combine outputs of multiple pattern classifiers to enhance the overall performance of pattern classification are presented. Specific attention is given to combination rules which are independent of the input feature vectors. Potentials and pitfalls of this so called stack generalization method are discussed, and experimentation using several machine learning databases are reported\nTitle:\nCommittee Pattern Classifiers\n\nAbstract:\nIn this paper, the task of distributed sensor network data aggregation of extreme (say, maximum) value of sensor measurements is investigated. We exploit the local broadcasting topology of a wireless network to facilitate the development of a novel energy efficient aggregation algorithm. The method models the extreme value aggregation process as a search problem, and devises an approach to accelerate convergence. Through both theoretical analysis and extensive simulation, we characterize the performance of these methods both analytically and experimentally. We observe marked performance advantage of these methods.\nTitle:\nA Novel Cross-Layer Data Aggregation Approach For Extreme Values In Wireless Sensor Networks\n\nAbstract:\nIn this paper, we utilize a novel fluidity concept to analyze and develop routing algorithms in Network-On-Chip for congestion avoidance and relief. We develop a new model to capture congestion information which improves the performance of routing algorithms. Comparisons of algorithms using our model consistently outperform the original algorithms themselves.\nTitle:\nFluidity Concept For Noc: A Congestion Avoidance And Relief Routing Scheme\n\nAbstract:\nIn this paper, we show that simulated evolution (SE) can be modeled by an ergodic Markov chain. As such, the global convergence of the SE algorithm is established. Moreover, we propose to use the mean first visit time of an ergodic Markov chain to characterize the convergence time of the SE algorithm such that the fast convergence feature of SE can be assessed theoretically and experimentally\nTitle:\nConvergence analyses of simulated evolution algorithms\n\nAbstract:\nA novel power-line interference (PLI) detection and suppression algorithm is presented to preprocess the electrocardiogram (ECG) signals. A distinct feature of this proposed algorithm is its ability to detect the presence of PLI in the ECG signal before applying the PLI suppression algorithm. No PLI suppression operation will be performed if PLI is not detected. We propose a PLI detector that empl...\nTitle:\nPower-line interference detection and suppression in ECG signal processing.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.533, mean value for all the writers is 8.648, which makes it 0.755 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper. we investigate a modular architecture for ECG beat classification. The feature space is divided into distinct regions and individual classifiers are developed for each region. We compare different combination strategies, and feature space partition strategies. We also describe a novel, batch modular learning method that can be used to incrementally improve the performance of the modular network.\nTitle:", "model_inf_time": 0.87}, {"id": "41461", "output": "Re-splittable Threshold Public Key Encryption from Discrete Logarithm Assumptions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper, we show that both Public Key Encryption (PKE) with non-interactive opening and threshold PKE can be constructed from an arbitrary group signature scheme which is secure in the dynamic group setting. This result implies that group signature (in dynamic groups) is a significantly strong cryptographic primitive, since the above PKEs with additional functionalities are already much stronger primitives than the standard chosen-ciphertext secure PKE, which is itself recognized as a very powerful cryptographic tool. We can interpret our result as meaning that designing secure group signatures is significantly harder than many other ordinary cryptographic primitives.\nTitle:\nGroup signature implies PKE with non-interactive opening and threshold PKE\n\nAbstract:\nThere are many applications in which services are provided only if some values associated with some confidential (encrypted) data are within a specific range. In this paper, we propose the notion of (ciphertext-policy) range encryption (RE) that can be used in many of such applications. RE is a type of public key encryption with additional functionality where an encryptor can freely specify a range to a ciphertext so that it can be decrypted only if the values associated with the key belong to the range. We propose a concrete RE scheme based on the time-specific encryption scheme by Kasamatsu et al. (SCN2012). Our RE scheme is selectively secure under the weak bilinear Diffie-Hellman inversion assumption.\nTitle:\nCiphertext policy multi-dimensional range encryption\n\nAbstract:\nAchieving shorter ciphertext length under weaker assumptions in chosen-ciphertext (CCA) secure public-key encryption (PKE) is one of the most important research topics in cryptography. However, it is also known that it is hard to construct a CCA-secure PKE whose ciphertext overhead is less than two group elements in the underlying prime-order group under non-interactive assumptions. A naive approach for achieving more compactness than the above bound is to use random oracles (ROs), but the full RO has various ideal properties like programmability. In this paper, we pursue how to achieve compact PKE only with a minimum ideal property of ROs. Specifically, only with observability, we can give three CCA-secure PKE schemes whose ciphertext overhead is less than two group elements. Our schemes are provably secure under standard assumptions such as the CDH and DDH assumptions. This study shows that ideal properties other than observability are not necessary to construct compact PKE beyond the bound.\nTitle:\nCompact public key encryption without full random oracles.\n\nAbstract:\nAchieving shorter ciphertext length under weaker assumptions in chosen-ciphertext (CCA) secure public-key encryption (PKE) is one of the most important research topics in cryptography. However, it is also known that it is hard to construct a CCA-secure PKE whose ciphertext overhead is less than two group elements in the underlying prime-order group under non-interactive assumption. A naive approach for achieving more compactness than the above bound is to use random oracles (ROs), but the full RO has various ideal properties like programmability. In this paper, we pursue how to achieve compact PKE only with a minimum ideal property of ROs. Specifically, only with observability, we can give three CCA-secure PKE schemes whose ciphertext overhead is less than two group elements. Our schemes are provably secure under standard assumptions such as the CDH and DDH assumptions. This study shows that ideal properties other than observability are not necessary to construct compact PKE beyond the bound.\nTitle:\nCompact Public Key Encryption With Minimum Ideal Property Of Hash Functions\n\nAbstract:\nRecently Cash, Kiltz, and Shoup [13] showed a variant of the Cramer-Shoup (CS) scheme [14] whose chosen-ciphertext (CCA) security relies on the computational Diffie-Hellman (CDH) assumption. The cost for this high security is that the size of ciphertexts is much longer than the CS scheme (which is based on the decisional Diffie-Hellman assumption). In this paper, we show how to achieve CCA-security under the CDH assumption without increasing the size of ciphertexts. We also show a more efficient scheme under the hashed Diffie-Hellman assumption. Both of our schemes are based on a certain broadcast encryption (BE) scheme while the Cash-Kiltz-Shoup scheme is based on the Twin DH problem. Of independent interest, we also show a generic method of constructing CCA-secure PKE schemes from BE schemes.\nTitle:\nEfficient Chosen Ciphertext Secure Public Key Encryption under the Computational Diffie-Hellman Assumption\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.607, mean value for all the writers is 8.648, which makes it 0.818 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe concept of threshold public key encryption (TPKE) with the special property called key re-splittability (re-splittable TPKE, for short) was introduced by Hanaoka et al. (CT-RSA 2012), and used as one of the building blocks for constructing their proxy re-encryption scheme. In a re-splittable TPKE scheme, a secret key can be split into a set of secret key shares not only once, but also multiple times, and the security of the TPKE scheme is guaranteed as long as the number of corrupted secret key shares under the same splitting is smaller than the threshold. In this paper, we show several new constructions of a re-splittable TPKE scheme by extending the previous (ordinary) TPKE schemes. All of our proposed schemes are based on discrete logarithm (DL)-type assumptions. Therefore, our results suggest that key re-splittability is a very natural property for DL-type TPKE schemes.\nTitle:", "model_inf_time": 1.78}, {"id": "41462", "output": "A filter-driver-layered caching design for flash-memory file systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAs flash memory becomes popular over various platforms, there is a strong demand regarding the performance degradation problem, due to the special characteristics of flash memory. This research proposes the design of a file-system-oriented flash translation layer, in which a filter mechanism is designed to separate the access requests of file-system metadata and file contents for better performance. A recovery scheme is then proposed for maintaining the integrity of a file system. The proposed flash translation layer is implemented as a Linux device driver and evaluated with respect to ext2 and ext3 file systems. Experiments were also done over NTFS by a series of realistic traces. The experimental results show significant performance improvement over ext2, ext3, and NTFS file systems with limited system overheads.\nTitle:\nAn adaptive file-system-oriented FTL mechanism for flash-memory storage systems\n\nAbstract:\nAlthough flash memory has gained very strong momentum in the storage market, the reliability of flash-memory chips has been dropped significantly in the past years. This article presents a reliability enhancement design under the flash management layer (i.e., flash translation layer) to address this concern so as to reduce the design complexity of flash-memory management software/firmware and to improve the maintainability and portability of existing and future products. In particular, a log-based write strategy with a hash-based caching policy is proposed to provide extra ECC redundancy and performance improvement. Strategies for bad block management are also presented. The failure rate of flash-memory storage systems is analyzed with the considerations of bit errors. The proposed design is later evaluated by a series of experiments based on realistic traces. It was shown that the proposed approach could significantly improve the reliability of flash memory with very limited system overheads.\nTitle:\nA reliability enhancement design under the flash translation layer for MLC-based flash-memory storage systems\n\nAbstract:\nAs flash memory became popular over various platforms, there is a strong demand on the performance degradation problem, due to the special characteristics of flash memory. This research proposes the design of a file-system-aware flash translation layer, in which a filter mechanism is designed to separate the access requests of file-system metadata and file contents for better performance. A recovery scheme is then proposed to maintain the integrity of a file system. The proposed flash translation layer is implemented as a Linux device driver and evaluated with respect to ext2 and ext3 file systems. The experimental results show significant performance improvement over ext2 and ext3 file systems with limited system overheads.\nTitle:\nA file-system-aware FTL design for flash-memory storage systems\n\nAbstract:\nIndex structures are widely used in file systems and database applications for efficient data management. This paper exploits the respective characteristics of DRAM and flash memory for tree index designs, for which a native file system is taken as an example target in the research. Different from DRAM caching or buffering of flash-memory access in the past work, a hybrid index design that resides over DRAM and flash memory simultaneously is proposed to improve system performance and space management. Tree nodes migrate between DRAM and flash memory, as needed, in response to user access pattern so as to optimize the performance and to reduce managing overhead. The capability of the proposed design is evaluated by a series of experiments, for which we have very encouraging results.\n\n\nTitle:\nA DRAM-flash index for native flash file systems\n\nAbstract:\nThe reliability of flash-memory chips has dropped dramatically in recent years. In order to solve this problem, a reliable memory technology device (MTD) design is proposed to address this concern at the device driver layer so as to release the design complexity of flash-memory management software/firmware and to improve the maintainability and portability of flash management designs for existing and future products. The proposed design was evaluated through a series of experiments based on realistic traces to show that the proposed approach could significantly improve the reliability of flash memory with limited overheads.\nTitle:\nA reliable MTD design for MLC flash-memory storage systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.229, mean value for all the writers is 8.648, which makes it 1.349 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe growing popularity of flash memory is expected to draw attention to the limitations of file-system performance over flash memory. This work was motivated by the modular designs of operating system components such as bus and device drivers. A filter-driver-layered caching design is proposed to resolve the performance gap among file systems and to improve their performance with the considerations of flash memory characteristics. An efficient hybrid tree structure is presented to organize and manipulate the intervals of cached writes. Algorithms are proposed in the merging, padding, and removing of the data of writes. The effectiveness of the proposed approach is demonstrated with some analysis study of FAT-formatted and NTFS-formatted USB flash disks. The proposed cohesive caching policy was implemented as a filter driver in Windows XP/Vista for performance evaluation. In the experiments, a ten-fold or larger performance improvement was usually achieved when the cache size was only 64KB. Other substantial improvements were also observed in the experiments. For example, the proposed design enabled FAT-formatted and NTFS-formatted flash-memory devices to copy Linux image files 93&percnt; and 14&percnt; faster than conventional flash drives, respectively.\nTitle:", "model_inf_time": 1.76}, {"id": "41463", "output": "Efficient Online Hot Data Identification for Flash Memory Storage Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nHot data identification for flash memory storage systems not only imposes great impacts on flash memory garbage collection but also strongly affects the performance of flash memory access and its lifetime (due to wear-levelling). This research proposes a highly efficient method for on-line hot data identification with limited space requirements. Different from past work, multiple independent hash functions are adopted to reduce the chance of false identification of hot data and to provide predictable and excellent performance for hot data identification. This research not only offers an efficient implementation for the proposed framework, but also presents an analytic study on the chance of false hot data identification. A series of experiments was conducted to verify the performance of the proposed method, and very encouraging results are presented.\nTitle:\nEfficient identification of hot data for flash memory storage systems\n\nAbstract:\nFlash memory has been widely considered as a good alternative for storage system implementations because it offers superior vibration tolerance and power efficiency, compared to hard-disks. Because of its unique characteristics, direct applications of disk management methods over flash memory might result in performance degradation and even the reducing of the lifetime. The management issues become even more challenging, especially when the capacity of flash memory increases significantly in the past few years. In this paper, we summarize our work on several important issues in flash memory management, where system performance and management overheads are considered. The capability of the proposed methodology was evaluated by a series of experiments to provide more insights in system designs\nTitle:\nConfigurability of performance and overheads in flash management\n\nAbstract:\nThis work is motivated by the strong demand for flash-friendly index designs to resolve reliability and performance concerns for data manipulation over flash memory. In comparison to previous work, we propose and explore the impact of hot-data access, sibling-link updates, and different workload types to a tree index structure over flash memory. In particular, a flash-friendly ${B^ + }$ -tree, referred to as an Adaptive Durable ${B^ + }$ -tree, is proposed to not only improve the endurance but also the performance of a tree index structure. The capability of the proposed methodology and index design is evaluated through a series of experiments, in which significant improvement on endurance was achieved in comparison to previous reports on the subject.\nTitle:\nAn Adaptive Endurance-Aware -Tree for Flash Memory Storage Systems\n\nAbstract:\nWith wide applicability of flash memory in various application domains, reliability has become a very critical issue. This research is motivated by the needs to resolve the lifetime problem of flash memory and a strong demand in turning thrown-away flash-memory chips into downgraded products. We proposes a set-based mapping strategy with an effective implementation and low resource requirements, e.g., SRAM. A configurable management design and wear-leveling issue are considered. The behavior of the proposed method is also analyzed with respect to popular implementations in the industry.We show that the endurance of flash memory can be significantly improved by a series of experiments over a realistic trace. Our experiments show that the read performance is even largely improved.\nTitle:\nA set-based mapping strategy for flash-memory reliability enhancement\n\nAbstract:\nFlash memory is among the top choices for storage media in ubiquitous computing. With a strong demand of high-capacity storage devices, the usages of flash memory quickly grow beyond their original designs. The very distinct characteristics of flash memory introduce serious challenges to engineers in resolving the quick degradation of system performance and the huge demand of main-memory space for flash-memory management when high-capacity flash memory is considered. Although some brute-force solutions could be taken, such as the enlarging of management granularity for flash memory, we showed that little advantage is received when system performance is considered. This paper proposes a flexible management scheme for large-scale flash-memory storage systems. The objective is to efficiently manage high-capacity flash-memory storage systems based on the behaviors of realistic access patterns. The proposed scheme could significantly reduce the main-memory usages without noticeable performance degradation.\nTitle:\nAn efficient management scheme for large-scale flash-memory storage systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.041, mean value for all the writers is 8.648, which makes it 0.335 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nHot-data identification for flash-memory storage systems not only imposes great impacts on flash-memory garbage collection but also strongly affects the performance of flashmemory access and its life time (due to wear-levelling). In this research, we propose a highly efficient method for online hot-data identification with limited space requirements. Different from the past work, multiple independent hash functions are adopted to reduce the chance of false identification of hot data and provide predictable and excellent performance for hot-data identification. We not only propose an efficient implementation of the proposed framework but also conduct a series of experiments to verify the performance of the proposed method, in which very encouraging results are presented.\nTitle:", "model_inf_time": 1.42}, {"id": "41464", "output": "Inferring Inter-Domain Congestion with Time Series Latency Probes", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThere is significant interest in the technical and policy communities regarding the extent, scope, and consumer harm of persistent interdomain congestion. We provide empirical grounding for discussions of interdomain congestion by developing a system and method to measure congestion on thousands of interdomain links without direct access to them. We implement a system based on the Time Series Latency Probes (TSLP) technique that identifies links with evidence of recurring congestion suggestive of an under-provisioned link. We deploy our system at 86 vantage points worldwide and show that congestion inferred using our lightweight TSLP method correlates with other metrics of interconnection performance impairment. We use our method to study interdomain links of eight large U.S. broadband access providers from March 2016 to December 2017, and validate our inferences against ground-truth traffic statistics from two of the providers. For the period of time over which we gathered measurements, we did not find evidence of widespread endemic congestion on interdomain links between access ISPs and directly connected transit and content providers, although some such links exhibited recurring congestion patterns. We describe limitations, open challenges, and a path toward the use of this method for large-scale third-party monitoring of the Internet interconnection ecosystem.\n\n\nTitle:\nInferring persistent interdomain congestion.\n\nAbstract:\nWe consider the problem of inferring IPv6 router uninterrupted system availability, or uptime, from a remote vantage point without privileged access. Uptime inference is important to broader efforts to measure and characterize the availability of critical infrastructure, provides insight into network operations, and has subtle security implications. Our approach utilizes active probes to periodically elicit IPv6 fragment identifiers from IPv6 router interfaces, and analyzes the resulting identifier time series for reboots. We demonstrate the approach's potential by characterizing 21,539 distinct IPv6 router interfaces over a five-month period. We find evidence of clustered reboot events, popular maintenance windows, and correlation with globally visible control plane data. Our results, validated by five ASes, provide initial insight into the current state of IPv6 router availability.\nTitle:\nMeasuring And Characterizing Ipv6 Router Availability\n\nAbstract:\nImpediments to resolving IPv6 router aliases have precluded understanding the emerging router-level IPv6 Internet topology. In this work, we design, implement, and validate the first Internet-scale alias resolution technique for IPv6. Our technique, speedtrap, leverages the ability to induce fragmented IPv6 responses from router interfaces in a particular temporal pattern that produces distinguishing per-router fingerprints. Our algorithm surmounts three fundamental challenges to Internet-scale IPv6 alias resolution using fragment identifier values: (1) unlike for IPv4, the identifier counters on IPv6 routers have no natural velocity, (2) the values of these counters are similar across routers, and (3) the packet size required to collect inferences is 46 times larger than required in IPv4. We demonstrate the efficacy of the technique by producing router-level Internet IPv6 topologies using measurements from CAIDA's distributed infrastructure. Our preliminary work represents a step toward understanding the Internet's IPv6 router-level topology, an important objective with respect to IPv6 network resilience, security, policy, and longitudinal evolution.\nTitle:\nSpeedtrap: internet-scale IPv6 alias resolution\n\nAbstract:\nOne challenge in understanding the evolution of Internet infrastructure is the lack of systematic mechanisms for monitoring the extent to which allocated IP addresses are actually used. Address utilization has been monitored via actively scanning the entire IPv4 address space. We evaluate the potential to leverage passive network traffic measurements in addition to or instead of active probing. Passive traffic measurements introduce no network traffic overhead, do not rely on unfiltered responses to probing, and could potentially apply to IPv6 as well. We investigate two challenges in using passive traffic for address utilization inference: the limited visibility of a single observation point; and the presence of spoofed IP addresses in packets that can distort results by implying faked addresses are active. We propose a methodology for removing such spoofed traffic on both darknets and live networks, which yields results comparable to inferences made from active probing. Our preliminary analysis reveals a number of promising findings, including novel insight into the usage of the IPv4 address space that would expand with additional vantage points.\nTitle:\nEstimating internet address space usage through passive measurements\n\nAbstract:\nRecent research on Internet traffic classification algorithms has yield a flurry of proposed approaches for distinguishing types of traffic, but no systematic comparison of the various algorithms. This fragmented approach to traffic classification research leaves the operational community with no basis for consensus on what approach to use when, and how to interpret results. In this work we critically revisit traffic classification by conducting a thorough evaluation of three classification approaches, based on transport layer ports, host behavior, and flow features. A strength of our work is the broad range of data against which we test the three classification approaches: seven traces with payload collected in Japan, Korea, and the US. The diverse geographic locations, link characteristics and application traffic mix in these data allowed us to evaluate the approaches under a wide variety of conditions. We analyze the advantages and limitations of each approach, evaluate methods to overcome the limitations, and extract insights and recommendations for both the study and practical application of traffic classification. We make our software, classifiers, and data available for researchers interested in validating or extending this work.\nTitle:\nInternet traffic classification demystified: myths, caveats, and the best practices\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.17, mean value for all the writers is 8.648, which makes it 1.261 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe introduce and demonstrate the utility of a method to localize and quantify inter-domain congestion in the Internet. Our Time Sequence Latency Probes (TSLP) method depends on two facts: Internet traffic patterns are typically diurnal, and queues increase packet delay through a router during periods of adjacent link congestion. Repeated round trip delay measurements from a single test point to the two edges of a congested link will show sustained increased latency to the far (but not to the near) side of the link, a delay pattern that differs from the typical diurnal pattern of an uncongested link. We describe our technique and its surprising potential, carefully analyze the biggest challenge with the methodology (interdomain router-level topology inference), describe other less severe challenges, and present initial results that are sufficiently promising to motivate further attention to overcoming the challenges.\nTitle:", "model_inf_time": 1.71}, {"id": "41465", "output": "Path Sensitization with Controlled Switching Activity for Small-Delay Fault Detection", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIR-drop effects are increasingly relevant in context of both design and test. We introduce the event-driven simulator MIRID that calculates the impact of IR-drop to the circuit timing. MIRID performs the simulation on two abstraction levels: timing effects in the gate-level net-list, current and voltage waveform propagation in the electrical model of the power-distribution network (PDN). Switching events at the logic gates are forwarded to the electrical model, where induced currents and their impact on the neighboring PDN nodes are computed. From this information, values of voltages at the Vdd and ground terminals of logic gates are determined, which in turn are used to calculate accurate switching delays of the gates. MIRID supports a generic interface to electrical models, allowing for a seamless integration of arbitrary models of PDN and gate timing. We report experiments based on a simple PDN model that was introduced previously and incorporates a pre-characterized library. The simulation accuracy is validated by matching the results from MIRID and SPICE.\nTitle:\nMIRID: Mixed-Mode IR-Drop Induced Delay Simulator\n\nAbstract:\nComprehensive coverage of small-delay faults under massive process variations is achieved when multiple paths through the fault locations are sensitized by the test pair set. Using one test pair per path may lead to impractical test set sizes and test application times due to the large number of near-critical paths in state-of-the-art circuits. We present a novel SAT-based dynamic test-pattern compaction and relaxation method for sensitized paths in sequential and combinational circuits. The method identifies necessary assignments for path sensitization and encodes them as a SAT-instance. An efficient implementation of a bitonic sorting network is used to find test patterns maximizing the number of simultaneously sensitized paths. The compaction is combined with an efficient lifting-based relaxation technique. An innovative implication-based path-conflict analysis is used for a fast identification of conflicting paths. Detailed experimental results demonstrate the applicability and quality of the method for academical and industrial benchmark circuits. Compared to fault dropping the number of patterns is significantly reduced by over 85% on average while at the same time leaving more than 70% of the inputs unspecified.\nTitle:\nEfficient SAT-based dynamic compaction and relaxation for longest sensitizable paths\n\nAbstract:\nWe present a simulator which determines the coverage of small-delay faults, i.e., delay faults with a size below one clock cycle, caused by resistive-open defects. These defects are likely to escape detection by stuck-at or transition fault patterns. For the first time, we couple the calculation of the critical size of a small-delay fault with the computation of the resistance range of the corresponding resistive-open defect for which this size is exceeded. By doing so, we are able to extend probabilistic fault coverage metrics initially developed for static resistive bridging faults to small-delay defects.\nTitle:\nA Simulator of Small-Delay Faults Caused by Resistive-Open Defects\n\nAbstract:\nThe detection of small-delay faults is traditionally performed by sensitizing transitions on a path of sufficient length from an input to an output of the circuit going through the fault site. While this approach allows efficient test generation algorithms, it may result in false positives and false negatives as well, i.e. undetected faults are classified as detected or detectable faults are classified as undetectable. We present an automatic test pattern generation algorithm which considers waveforms and their propagation on each relevant line of the circuit. The model incorporates individual delays for each gate and filtering of small glitches. The algorithm is based on an optimized encoding of the test generation problem by a Boolean satisfiability (SAT) instance and is implemented in the tool WaveSAT. Experimental results for ISCAS-85, ITC-99 and industrial circuits show that no known definition of path sensitization can eliminate false positives and false negatives at the same time, thus resulting in inadequate small-delay fault detection. WaveSAT generates a test if the fault is testable and is also capable of automatically generating a formal redundancy proof for undetectable small-delay faults; to the best of our knowledge this is the first such algorithm that is both scalable and complete.\nTitle:\nSmall-delay-fault ATPG with waveform accuracy\n\nAbstract:\nStochastic circuits (SCs) offer tremendous area-and power-consumption benefits at the expense of computational inaccuracies. Managing accuracy is a central problem in SC design and has no counterpart in conventional circuit synthesis. It raises a basic question: how to build a systematic design flow for stochastic circuits? We present, for the first time, a systematic design approach to control the accuracy of SCs and balance it against other design parameters. We express the (in)accuracy of a circuit processing n-bit stochastic numbers by the numerical deviation of the computed value from the expected result, in conjunction with a confidence level. Using the theory of Monte Carlo simulation, we derive expressions for the stochastic number length required for a desired level of accuracy, or vice versa. We discuss the integration of the theory into a design framework that is applicable to both combinational and sequential SCs. We show that for combinational SCs, accuracy is independent of the circuit's size or complexity, a surprising result. We also show how the analysis can identify subtle errors in both combinational and sequential designs.\nTitle:\nFramework for quantifying and managing accuracy in stochastic circuit design.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.221, mean value for all the writers is 8.648, which makes it 0.364 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDetailed knowledge of a circuit's timing is essential for performance optimization, timing closure, and generation of test patterns to detect small-delay defects. When an input transition is applied to the circuit's inputs, the resulting delay is not only determined by the propagation path, but also influenced by the power-supply noise. We introduce a path-sensitization procedure which precisely controls the switching activity in the circuit region surrounding the path. The procedure can maximize or minimize switching activity, or set it to a user-specified value. We study the accuracy-vs.-efficiency trade-offs for a hierarchy of timing models, from coarse zero-delay assumption to a waveform-accurate approach with sub-cycle resolution. For the first time, we present a MaxSAT formulation which guarantees maximization or minimization of switching activity, stemming from transitions and from glitches, simultaneously with path sensitization. We validate the quality of the generated test patterns using a mixed-mode IR-drop-aware timing simulator.\nTitle:", "model_inf_time": 1.77}, {"id": "41466", "output": "Symbolic Counterexample Generation for Probabilistic Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe propose a new approach to compute counterexamples for violated \u03c9-regular properties of discrete-time Markov chains and Markov decision processes. Whereas most approaches compute a set of system paths as a counterexample, we determine a critical subsystem that already violates the given property. In earlier work we introduced methods to compute such subsystems based on a search for shortest paths. In this paper we use SMT solvers and mixed integer linear programming to determine minimal critical subsystems.\nTitle:\nMinimal critical subsystems for discrete-time markov models\n\nAbstract:\nGeneration of counterexamples is a highly important task in the model checking process. In contrast to, e. g., digital circuits where counterexamples typically consist of a single path leading to a critical state of the system, in the probabilistic setting counterexamples may consist of a large number of paths. In order to be able to handle large systems and to use the capabilities of modern SAT-solvers, bounded model checking (BMC) for discrete-time Markov chains was established. In this paper we introduce the usage of SMT-solving over linear real arithmetic for the BMC procedure. SMT-solving, extending SAT with theories in this context on the one hand leads to a convenient way to express conditions on the probability of certain paths and on the other hand allows to handle Markov reward models. We use the former to find paths with high probability first. This leads to more compact counterexamples. We report on some experiments, which show promising results.\nTitle:\nCounterexample generation for Markov chains using SMT-based bounded model checking\n\nAbstract:\nWe present a uniform signature-based approach to compute the most popular bisimulations. Our approach is implemented symbolically using BDDs, which enables the handling of very large transition systems. Signatures for the bisimulations are built up from a few generic building blocks, which naturally correspond to efficient BDD operations. Thus, the definition of an appropriate signature is the key for a rapid development of algorithms for other types of bisimulation. We provide experimental evidence of the viability of this approach by presenting computational results for many bisimulations on real-world instances. The experiments show cases where our framework can handle state spaces efficiently that are far too large to handle for any tool that requires an explicit state space description.\nTitle:\nSigref: a symbolic bisimulation tool box\n\nAbstract:\nCounterexamples for property violations have a number of important applications like supporting the debugging of erroneous systems and verifying large systems via counterexample-guided abstraction refinement. In this paper, we propose the usage of minimal critical subsystems of discrete-time Markov chains and Markov decision processes as counterexamples for violated \u03c9-regular properties. Minimality can thereby be defined in terms of the number of states or transitions. This problem is known to be NP-complete for Markov decision processes. We show how to compute such subsystems using mixed integer linear programming and evaluate the practical applicability in a number of experiments. They show that our method yields substantially smaller counterexample than using existing techniques.\nTitle:\nMinimal counterexamples for linear-time probabilistic verification.\n\nAbstract:\nSince its introduction in 1999, bounded model checking has gained industrial relevance for detecting errors in digital and hybrid systems. One of the main reasons for this is that it always provides a counterexample when an erroneous execution trace is found. Such a counterexample can guide the designer while debugging the system. In this paper we are investigating how bounded model checking can be applied to generate counterexamples for a different kind of model--namely discrete-time Markov chains. Since in this case counterexamples in general do not consist of a single path to a safety-critical state, but of a potentially large set of paths, novel optimization techniques like loop-detection are applied not only to speed-up the counterexample computation, but also to reduce the size of the counterexamples significantly. We report on some experiments which demonstrate the practical applicability of our method.\nTitle:\nCounterexample Generation for Discrete-Time Markov Chains Using Bounded Model Checking\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.523, mean value for all the writers is 8.648, which makes it 0.107 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper presents several symbolic counterexample generation algorithms for discrete-time Markov chains (DTMCs) violating a PCTL formula. A counterexample is (a symbolic representation of) a sub-DTMC that is incrementally generated. The crux to this incremental approach is the symbolic generation of paths that belong to the counterexample. We consider two approaches. First, we extend bounded model checking and develop a simple heuristic to generate highly probable paths first. We then complement the SAT-based approach by a fully (multi-terminal) BDD-based technique. All symbolic approaches are implemented, and our experimental results show a substantially better scalability than existing explicit techniques. In particular, our BDD-based approach using a method called fragment search allows for counterexample generation for DTMCs with billions of states (up to 1015).\nTitle:", "model_inf_time": 1.46}, {"id": "41467", "output": "A Novel Dewarping Technique for Document Images from Digital Cameras", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper new algorithms with the combination between the Regional-Gradient-Guided Bootstrapping Algorithm and Dynamics Time Warping Technique for imputing incomplete time series data are proposed. The new measurement for curve similarity comparison by using the changing of slope of time series data are used. The main contribution of this paper is to propose new technique for imputing the fluctuate time series data. We compare our new method with Cubic interpolation, Multiple imputation, Windows Varies Similarity Measurement algorithms and Regional-Gradient-Guided Bootstrapping Algorithm. The experimental results showed that our new algorithms are outperform than these method.\nTitle:\nTwo-phase imputation with regional-gradient-guided bootstrapping algorithm and dynamics time warping for incomplete time series data\n\nAbstract:\nClassification of text and image using statistical features (mean and standard deviation of pixel color values) is found to be a simple yet powerful method for text and image segmentation. The features constitute a systematic structure that segregates one from another. We identified this segregation in the form of class clustering by means of Fuzzy C-Mean method, which determined each cluster location using maximum membership defuzzification and neighborhood smoothing techniques. The method can then be applied to classify text, image, and background areas in optical character recognition (OCR) application for elaborated open document systems.\nTitle:\nFuzzy C-Mean: A Statistical Feature Classification Of Text And Image Segmentation Method\n\nAbstract:\nThis paper proposes a new method for extracting the invariant features of an image based on the concept of principal component analysis and a competitive learning algorithm. The proposed algorithm can be applied to binary, gray-level, or colored-texture images with a size greater than 256x256 pixels. In addition to translation, scaling, and rotation invariant extraction, the extraction of a feature invariant to color intensity can be implemented by using this method. In our experiment, the proposed method shows the capability to differentiate images having the same shape but different colored textures. The experimental results report the effectiveness of this technique and its performance as measured by recognition accuracy rate and computational time. These results are also compared with those obtained by classical techniques.\nTitle:\nA new feature extractor invariant to intensity, rotation, and scaling of color images\n\nAbstract:\nThe redundancy in digital image representation can be classified into two categories: local and global. In this paper, we propose a new lossless binary image compression processing scheme which increases local redundancy for more compression efficiency. The algorithm consists of reordering rows and columns of image data for assembling data that has a same value: `0' or `1', merging the data to reduce redundancy, encoding data to bitstream, then the encoded data will be compressed by other image compression algorithms. The experimental results confirm our technique performs better than other existing techniques\nTitle:\nHybrid binary image compression\n\nAbstract:\nThis paper proposed a methodology for finding the potential research collaborators based on structural approach underlying coauthorship network and semantic approach extends from author-topic model. We proposed the valuable features for identifying the closeness between researchers in co-authorship network. We also proved that using the combination between structural approach and semantic approach is work well. Our methodology able to suggest the researchers who appear within the four degrees of separation from the specific researcher who have never collaborated together in the past periods. The experimental results are discussed in the various aspects, for instance, top-n retrieved researchers and researcher's community. The results show that our proposed idea is the applicable method used for collaborator suggestion task.\nTitle:\nFinding potential research collaborators in four degrees of separation\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 11.169, mean value for all the writers is 8.648, which makes it 2.151 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper proposes a novel dewarping technique for document images based on digital cameras, from which images are warped on the curve surface of the book bound volume. Our technique transforms pixels in the curve coordinate system into the planar coordinate system. In the experiments, the test document images composed various font styles and sizes printed in common text books. The validity of our methods compares with the program's curvature correction of Abby FineReader Professional 9.0. Our technique outperforms the other up to 49 percent for word and character precisions. Nevertheless, our technique is independent on document contents, page layout, and font styles. The concept of our approach is based on dewarping the whole document pages by using the function obtained from experiments.\nTitle:", "model_inf_time": 1.52}, {"id": "41468", "output": "Breaking Masked AES with Collision Attacks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAt CHES 2010 two powerful new attacks were presented, namely the Fault Sensitivity Analysis and the Correlation Collision Attack. This paper shows how these ideas can be combined to create even stronger attacks. Two solutions are presented; both extract leakage information by the fault sensitivity analysis method while each one applies a slightly different collision attack to deduce the secret information without the need of any hypothetical leakage model. Having a similar fault injection method, one attack utilizes the non-uniform distribution of faulty ciphertext bytes while the other one exploits the data-dependent timing characteristics of the target combination circuit. The results when attacking several AES ASIC cores of the SASEBO LSI chips in different process technologies are presented. Successfully breaking the cores protected against DPA attacks using either gate-level countermeasures or logic styles indicates the strength of the attacks.\nTitle:\nOn the power of fault sensitivity analysis and collision side-channel attacks in a combined setting\n\nAbstract:\nBy examining the similarity of side-channel leakages, collision attacks evade the indispensable hypothetical leakage models of multi-query based side-channel distinguishers like correlation power analysis and mutual information analysis attacks. Most of the side-channel collision attacks compare two selective observations, what makes them similar to simple power analysis attacks. A multi-query collision attack detecting several collisions at the same time by means of comparing the leakage averages was presented at CHES 2010. To be successful this attack requires the means of the side-channel leakages to be related to the processed intermediate values. It therefore fails in case the mean values and processed data are independent, even though the leakages and the processed values follow a clear relationship. The contribution of this article is to extend the scope of this attack by employing additional statistics to detect the colliding situations. Instead of restricting the analyses to evaluation of means, we propose to employ higher-order statistical moments and probability density functions as the figure of merit to detect collisions. Thus, our new techniques remove the shortcomings of the existing correlation collision attacks using first-order moments. In addition to the theoretical discussion of our approach, practical evidence of its suitability for side-channel evaluation is provided. We provide four case studies, including three FPGA-based masked hardware implementations and a software implementation using boolean masking on a microcontroller, to support our theoretical groundwork.\nTitle:\nStatistical tools flavor side-channel collision attacks\n\nAbstract:\nReducing the entropy of the mask is a technique which has been proposed to mitigate the high performance overhead of masked software implementations of symmetric block ciphers. Rotating S-box Masking (RSM) is an example of such schemes applied to AES with the purpose of maintaining the security at least against univariate first-order side-channel attacks. This article examines the vulnerability of a realization of such technique using the side-channel measurements publicly available through DPA contest V4. Our analyses which focus on exploiting the first-order leakage of the implementation discover a couple of potential attacks which can recover the secret key. Indeed the leakage we exploit is due to a design mistake as well as the characteristics of the implementation platform, none of which has been considered during the design of the countermeasure (implemented in naive C code).\nTitle:\nDetecting Hidden Leakages.\n\nAbstract:\nIn CHES 2010 a correlation-based power analysis collision attack has been introduced which is supposed to exploit any first-order leakage of cryptographic devices. This work examines the effectiveness of the well-known DPA countermea-sures versus the correlation collision attack. The considered countermeasures include masking, shuffling, and noise addition, when applied in hardware. Practical evaluations, which all have been performed using power traces measured from an FPGA board, show an increase in the number of required traces, e.g. from 10,000 to 1,500,000, when combining different counter-measures. This study allows for a fair comparison between the hardware countermeasures and helps identifying an appropriate key lifetime.\nTitle:\nPractical evaluation of DPA countermeasures on reconfigurable hardware\n\nAbstract:\nOur contribution is twofold: first we describe a very compact hardware implementation of AES-128, which requires only 2400 GE. This is to the best of our knowledge the smallest implementation reported so far. Then we apply the threshold countermeasure by Nikova et al. to the AES S-box and yield an implementation of the AES improving the level of resistance against first-order side-channel attacks. Our experimental results on real-world power traces show that although our implementation provides additional security, it is still susceptible to some sophisticated attacks having enough number of measurements.\nTitle:\nPushing the limits: a very compact and a threshold implementation of AES\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.204, mean value for all the writers is 8.648, which makes it 1.328 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nSide-channel based collision attacks are a mostly disregarded alternative to DPA for analyzing unprotected implementations. The advent of strong countermeasures, such as masking, has made further research in collision attacks seemingly in vain. In this work, we show that the principles of collision attacks can be adapted to efficiently break some masked hardware implementation of the AES which still have first-order leakage. The proposed attack breaks an AES implementation based on the corrected version of the masked S-box of Canright and Batina presented at ACNS 2008. The attack requires only six times the number of traces necessary for breaking a comparable unprotected implementation. At the same time, the presented attack has minimal requirements on the abilities and knowledge of an adversary. The attack requires no detailed knowledge about the design, nor does it require a profiling phase.\nTitle:", "model_inf_time": 1.3}, {"id": "41469", "output": "Side-Channel Attacks on Modern Smart Home Access Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWith the increasing reliance on digitally connected systems, the need for secure and dependable systems continues to rise in lockstep with the need for well trained security professionals that can address this rising demand. The University of Lubeck established a new degree program in IT Security in the Fall of 2016. The creation of the degree program results from a high demand for specialists both nationally and internationally, along with the already existing strengths in security and dependability related research areas at the institutes of the university. The educational goals of the new degree program are the training of computer scientists with profound practical skills in building and protecting secure and dependable IT systems as well as all the necessary theoretical background to create systems according to the security by design principle on a large scale.\nTitle:\nIT Security in L\u00fcbeck \u2013 The design of a modern and future-proof security curriculum\n\nAbstract:\nIn this paper ways to efficiently implement public-key schemesbased on Multivariate Qua- dratic polynomials (MQ-schemes for short) are investigated. In particular, they are claimed to resist quantum computer attacks. It is shown that such schemes can have a much better time-area product than elliptic curve cryptosystems. For instance, an optimised FPGA im- plementation of amended TTS is estimated to be over 50 times more efficient with respect to this parameter. Moreover, a general framework for implementing small-field MQ-schemes in hardware is proposed which includes a systolic architecture performing Gaussian elimination over composite binary fields.\nTitle:\nTime-Area Optimized Public-Key Engines: -Cryptosystems as Replacement for Elliptic Curves?\n\nAbstract:\nThis paper presents horizontal and vertical side channel analysis techniques for an implementation of the McEliece cryptosystem. The target of this side-channel attack is a state-of-the-art field-programmable gate array (FPGA) implementation of the efficient quasi-cyclic moderate-density parity-check McEliece decryption operation, as presented at Design, Automation and Test in Europe (DATE) 2014. ...\nTitle:\nHorizontal and Vertical Side Channel Analysis of a McEliece Cryptosystem.\n\nAbstract:\nDigital signatures are one of the most important applications of microprocessor smart cards. The most widely used algorithms for digital signatures, RSA and ECDSA, depend on finite field engines. On 8-bit microprocessors these engines either require costly coprocessors, or the implementations become very large and very slow. Hence the need for better methods is highly visible. One alternative to RSA and ECDSA is the Merkle signature scheme which provides digital signatures using hash functions only, without relying on any number theoretic assumptions. In this paper, we present an implementation of the Merkle signature scheme on an 8-bit smart card microprocessor. Our results show that the Merkle signature scheme provides comparable timings compared to state of the art implementations of RSA and ECDSA, while maintaining a smaller code size.\nTitle:\nFast Hash-Based Signatures on Constrained Devices\n\nAbstract:\nThis paper explores the resistance of MOS Current Mode Logic (MCML) against attacks based on the observation of the power consumption. Circuits implemented in MCML, in fact, have unique characteristics both in terms of power consumption and the dependency of the power profile from the input signal pattern. Therefore, MCML is suitable to protect cryptographic hardware from Differential Power Analysis and similar side-channel attacks. In order to demonstrate the effectiveness of different logic styles against power analysis attacks, two full cores implementing the AES algorithm were realized and implemented with CMOS and MCML technology, and a set of different types of attack was performed using power traces derived from SPICE-level simulations. Although all keys were discovered for CMOS, MCML traces did not presents characteristic that can lead to a successful attack.\nTitle:\nEvaluating Resistance of MCML Technology to Power Analysis Attacks Using a Simulation-Based Methodology\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.408, mean value for all the writers is 8.648, which makes it 0.648 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nZusammenfassung\u00a0\u00a0Durch Automatisierung gelangt immer mehr Komfort in unseren Alltag. Ein System, das in den vergangenen Jahren rasante Verbreitung\n gefunden hat, sind Funkt\u00fcr\u00f6ffner. Garagentor, Haust\u00fcr und Auto entriegeln sich auf Knopfdruck aus mehreren Metern Entfernung.\n Wie sicher aber sind diese Verfahren? Die Autoren entwickelten einen erfolgreichen Seitenkanalangriff auf Funkt\u00fcr\u00f6ffnersysteme.\nTitle:", "model_inf_time": 1.4}, {"id": "41470", "output": "Privacy-Preserving Pre-Payments with Refunds for Lightweight Transit Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nElectronic cash is a suitable solution for payment systems, in which the user's identity should not be revealed during the payment. This is for example the case for public transportation payment systems. One electronic cash scheme, efficient during the spending phase, was proposed by Brands'. This scheme, as all privacy-preserving payment schemes, is based on public-key cryptography. However, payment devices used in those systems need to be cheap and low-power, which restricts their computational performance. These two points conflict with the need for payments to be executed quickly, in order to avoid delays at the entrance points of the system. In this work we demonstrate that using sophisticated implementation techniques, it is possible to realize full-size e-cash schemes even on inexpensive payment tokens. We present a full implementation of Brands' offline cash scheme for the UMass Moo, a computational RFID-token. The spending protocol, which is the time critical part in transportation payment systems can be executed in 13 ms. This meets real-world application requirements. The reloading of the card, which is less time critical, as it is conducted offline, is time consuming. We discuss solutions to this problem.\nTitle:\nPrivacy preserving payments on computational RFID devices with application in intelligent transportation systems\n\nAbstract:\nWe investigated a real-world contactless payment application based on mifare Classic cards. In order to analyze the security of the payment system, we combined previous cryptanalytical results and implemented an improved card-only attack with customized low-cost tools, that is to our knowledge the most efficient practical attack to date. We found several flaws implying severe security vulnerabilities on the system level that allow for devastating attacks including identity theft and recharging the amount of money on the cards. We practically verify and demonstrate the attacks on the commercial system.\nTitle:\nAll you can eat or breaking a real-world contactless payment system\n\nAbstract:\nSince the introduction of RFID technology there have been public debates on security and privacy concerns. In this context the Machine Readable Travel Document (MRTD), also known as e-passport, is of particular public interest. Whereas strong cryptographic mechanisms for authenticity are specified for MRTDs, the mechanisms for access control and confidentiality are still weak. In this paper we revisit the privacy concerns caused by the Basic Access Control mechanism of MRTDs and consider German e-passports as a use case. We present a distributed hardware architecture that can continuously read and record RF based communication at public places with high e-passport density like airports and is capable of performing cryptanalysis nearly in real-time. For cryptanalysis, we propose a variant of the cost-efficient hardware architecture (COPACOBANA) which has been recently realized. Once, MRTD holder identification data are revealed, this information can be inserted into distributed databases enabling global supervision activities. Assuming RF readers and eavesdropping devices are installed in several different airports or used in other similar places, e.g., in trains, one is able to trace any individual similar to tracing packages sent using postal services such as UPS.\nTitle:\nE-passport: the global traceability or how to feel like a UPS package\n\nAbstract:\nDue to their versatile and generic structure, field programmable gate arrays (FPGA) allow dynamic reconfiguration of their logical resources just by loading configuration files. However, this flexibility also opens up the threat of theft of intellectual property (IP) since these configuration files can be easily extracted and cloned. In this context, the ability to bind a configuration to a specific device is an important step to prevent product counterfeiting. In this paper, we present a novel strategy to identify and authenticate FPGAs in applications using intrinsic, device-specific information (also known as physically unclonable functions). Our solution is based on the output of intentionally induced write collisions in synchronous dual-port block RAM (BRAM). We show that the output of such write collisions can be used to create unique device signatures. In addition to applications for chip identification and authentication, we also propose a solution to efficiently create secret keys on-chip. As a last contribution, we outline how to transform our idea into a circuit for true random number generation (TRNG).\nTitle:\nTransforming write collisions in block RAMs into security applications\n\nAbstract:\nThis paper presents a block cipher that is optimized with respect to latency when implemented in hardware. Such ciphers are desirable for many future pervasive applications with real-time security needs. Our cipher, named PRINCE, allows encryption of data within one clock cycle with a very competitive chip area compared to known solutions. The fully unrolled fashion in which such algorithms need to be implemented calls for innovative design choices. The number of rounds must be moderate and rounds must have short delays in hardware. At the same time, the traditional need that a cipher has to be iterative with very similar round functions disappears, an observation that increases the design space for the algorithm. An important further requirement is that realizing decryption and encryption results in minimum additional costs. PRINCE is designed in such a way that the overhead for decryption on top of encryption is negligible. More precisely for our cipher it holds that decryption for one key corresponds to encryption with a related key. This property we refer to as alpha-reflection is of independent interest and we prove its soundness against generic attacks.\nTitle:\nPRINCE - A Low-Latency Block Cipher for Pervasive Computing Applications - Extended Abstract.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.161, mean value for all the writers is 8.648, which makes it 0.438 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe propose a new lightweight cryptographic payment scheme for transit systems, called P4R (Privacy-Preserving Pre-Payments with Refunds), which is suitable for low-cost user devices with limited capabilities. Using P4R, users deposit money to obtain one-show credentials, where each credential allows the user to make an arbitrary ride on the system. The trip fare is determined on-the-fly at the end of the trip. If the deposit for the credential exceeds this fare, the user obtains a refund. Refund values collected over several trips are aggregated in a single token, thereby saving memory and increasing privacy. Our solution builds on Brands\u2019s e-cash scheme to realize the prepayment system and on Boneh-Lynn-Shacham (BLS) signatures to implement the refund capabilities. Compared to a Brands-only solution for transportation payment systems, P4R allows us to minimize the number of coins a user needs to pay for his rides and thus minimizes the number of expensive withdrawal transactions, as well as storage requirements for the fairly large coins. Moreover, P4R enables flexible pricing because it allows for exact payments of arbitrary amounts (within a certain range) using a single fast paying (and refund) transaction. Fortunately, the mechanisms enabling these features require very little computational overhead. Choosing contemporary security parameters, we implemented P4R on a prototyping payment device and show its suitability for future transit payment systems. Estimation results demonstrate that the data required for 20 rides consume less than 10KB of memory, and the payment and refund transactions during a ride take less than half a second. We show that malicious users are not able to cheat the system by receiving a refund that exceeds the overall deposit minus the overall fare and can be identified during double-spending checks. At the same time, the system protects the privacy of honest users in that transactions are anonymous (except for deposits) and trips are unlinkable.\nTitle:", "model_inf_time": 1.88}, {"id": "41471", "output": "A Fully Digital Simplicial CNN Image Processor", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe propose a programmable architecture for a single instruction multiple data image processor that has its foundation on the mathematical framework of a simplicial cellular neural networks. We develop instruction primitives for basic image processing operations and show examples of processing binary and gray scale images. Fabricated in deep submicron CMOS technologies, the complexity of the digital circuits and wiring in each cell is commensurate with pixel level processing.\nTitle:\nA scalable and programmable simplicial CNN digital pixel processor architecture\n\nAbstract:\nThis paper presents the architecture for a SIMD digital Visual Processor Unit (VPU) that is based on the Simplicial CNN (S-CNN) algorithm. The system is designed for three dimensional CMOS integration in the three tier MITLL 3D SOI-CMOS 0.18 mu m technology. The architecture includes input/output sub-systems, in the third tier, arithmetic logic units (ALU) and register files on the third and second tiers and instruction cache memory and a timing state machine on the first tier. The partition of the architecture exploits its physical realization in three dimensional CMOS. Parallel optical data input through an array of photodetectors and analog interface circuits in the third tier facilitate testing and characterization.\nTitle:\nA Simplicial Cnn Visual Processor In 3d Soi-Cmos\n\nAbstract:\nWe present a new approach to the engineering of collective analog computing systems that emphasizes the role of currents as an appropriate signal representation and the need for low-power dissipation and simplicity in the basic functional circuits. The design methodology and implementation style that we describe are inspired by the functional and organizational principles of neuronal circuits in living systems. We have implemented synthetic neurons and synapses in analog CMOS VLSI that are suitable for building associative memories and self-organizing feature maps.\nTitle:\nSynthetic Neural Circuits Using Current-Domain Signal Representations.\n\nAbstract:\nWe present a system level architecture for a scalable, mixed-signal, asynchronous processor, aimed at cortical computations. The design has been implemented in MIT Lincoln Lab's three-tier SOI-CMOS 0.18 mu m digital process. The main circuits are distributed in the two tiers; an asynchronous address-event based read/write middle tier and an odd symmetric spatial filter (8 orientations) on the bottom tier. The top tier includes a photosensitive pixel array (64x64) to facilitate testing and characterization of the system. A highspeed 2-phase asynchronous chip-to-chip communication protocol is built-in to facilitate system scalability.\nTitle:\nA Mixed Analog/Digital Asynchronous Processor For Cortical Computations In 3d Soi-Cmos\n\nAbstract:\nWe present a scheme for implementing highly-connected, reconfigurable networks of integrate-and-fire neurons in VLSI. Neural activity is encoded by spikes, where the address of an active neuron is communicated through an asynchronous request and acknowledgement cycle. We employ probabilistic transmission of spikes to implement continuous-valued synaptic weights, and memory-based look-up tables to implement arbitrary interconnection topologies. The scheme is modular and scalable, and lends itself to the implementation of multi-chip network architectures. Results from a prototype system with 1024 analog VLSI integrate-and-fire neurons, each with up to 128 probabilistic synapses, demonstrate these concepts in an image processing task.\nTitle:\nProbabilistic synaptic weighting in a reconfigurable network of VLSI integrate-and-fire neurons\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.973, mean value for all the writers is 8.648, which makes it 1.13 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nlmagers with on-chip processing capabilities are required for highspeed and high-resolution image processing systems. In this paper, we present an image processor based on a simplicial CNN (S-CNN) cell. As a result of using the S-CNN as the core processor, the architecture is endowed with a solid theoretical framework and the parallel processing capabilities of CNNs. The proposed architecture is fully digital, which is fundamental because it implies that the density of the S-CNN imager will scale directly with technology.\nTitle:", "model_inf_time": 1.25}, {"id": "41472", "output": "Dynamic Value-Added Propositions in Web Services", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWeb services are becoming a major technology for deploying automated interactions between distributed and heterogeneous applications, and for connecting business processes. Service mashups indicate a way to create new Web applications by combining existing Web resources utilizing data and Web APIs. They facilitate the design and development of novel and modern Web applications based on easy-to-accomplish end-user service compositions.\nTitle:\nServices Mashups: The New Generation of Web Applications\n\nAbstract:\nThe growing popularity of service oriented computing based on Web services standards is creating a need for paradigms to represent and design business processes. Significant work has been done in the representation aspects with regards to WSBPEL. However, design and modeling of business processes is still an open issue. In this paper, we present a novel designer for business processes, which allows for intuitive modeling of Web processes, as well as using a template based approach for semi-automatically integrating partners either at design time or at deployment time. This work has been done as part of the METEOR-S project, which concentrates on adding semantics to the entire Web process lifecycle.\nTitle:\nA semantic template based designer for Web processes\n\nAbstract:\nThe emergence of Service Oriented Architectures (SOA) has created a new paradigm of loosely coupled distributed systems. In the METEOR-S project, we have studied the comprehensive role of semantics in all stages of the life cycle of service and process--including annotation, publication, discovery, interoperability/data mediation, and composition. In 2002-2003, we had offered a broad framework of semantics consisting of four types: 1) Data semantics, 2) Functional semantics, 3) Non-Functional semantics and 4) Execution semantics. This talk describes the need for the four types of semantics, its standards-based support through WSDL-S/SAWSDL, and the need for such semantic representation to dynamic and adaptive SOA. We also briefly review the proposal for Adaptive Web Processes introduced earlier in a ICSOC 2005 vision talk.\nTitle:\nRole of semantics in Autonomic and Adaptive Web Services & Processes\n\nAbstract:\nWeb services are in the midst of making the transition from being a promising technology to being widely used in the industry. However, most efforts to use Web services have been manual, thus slowing down the ever changing and dynamic businesses of today. In this paper, we contend that more expressive descriptions of Web services will lead to greater automation and thus provide more agility to businesses. We present the METEOR-S front-end tools for source code annotation and semantic Web service description generation. We also present WSDL-S, a language created for incorporating semantic descriptions in the industry wide accepted WSDL, by extending WSDL 2.0.\nTitle:\nEnhancing web services description and discovery to facilitate composition\n\nAbstract:\nThe Semantic Web Services Initiative Architecture (SWSA) committee has created a set ofarchitectural and protocol abstractions that serve as a foundation for Semantic Web servicetechnologies.This article summarizes the committee's findings, emphasizing its review of requirementsgathered from several different environments.The authors also identify the scope and potential requirements for a Semantic Web services architecture.\nTitle:\nA Semantic Web Services Architecture\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.491, mean value for all the writers is 8.648, which makes it 0.719 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn the past few years, service-oriented architecture (SOA) has transitioned from a partially formed vision into a widely implemented paradigm, with Web services (WS) being the forerunners to implementing SOA-based solutions. But even though the current trend is to use Web services' standards-based nature to establish static connections between various components, businesses are starting to explore dynamic value-added propositions, such as reuse, interoperability, and agility\nTitle:", "model_inf_time": 1.3}, {"id": "41473", "output": "Holes in Unions of Convex Translates", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe consider the problem of finding low-cost spanning trees for sets of $n$ points in the plane, where the cost of a spanning tree is defined as the total number of intersections of tree edges with a given set of $m$ barriers. We obtain the following results: (i) if the barriers are possibly intersecting line segments, then there is always a spanning tree of cost $O(\\min(m^2,m\\sqrt{n}))$; (ii) if the barriers are disjoint line segments, then there is always a spanning tree of cost $O(m)$; (iii) ] if the barriers are disjoint convex objects, then there is always a spanning tree of cost $O(n+m)$. All our bounds are worst-case optimal, up to multiplicative constants.\nTitle:\nSpanning trees crossing few barriers\n\nAbstract:\nWe discuss in this paper a method of finding skyline or non-dominated points\nin a set $P$ of $n_P$ points with respect to a set $S$ of $n_S$ sites. A point\n$p_i \\in P$ is non-dominated if and only if for each $p_j \\in P$, $j \\not= i$,\nthere exists at least one point $s \\in S$ that is closer to $p_i$ than $p_j$.\nWe reduce this problem of determining non-dominated points to the problem of\nfinding sites that have non-empty cells in an additive Voronoi diagram with a\nconvex distance function. The weights of the additive Voronoi diagram are\nderived from the co-ordinates of the points of $P$ and the convex distance\nfunction is derived from $S$. In the 2-dimensional plane, this reduction gives\na $O((n_S + n_P)\\log n_S + n_P \\log n_P)$-time randomized incremental algorithm\nto find the non-dominated points.\nTitle:\nOn Finding Non-dominated Points using Compact Voronoi Diagrams\n\nAbstract:\nGiven a family of k disjoint connected polygonal sites in general position and of total complexity n, we consider the farthest-site Voronoi diagram of these sites, where the distance to a site is the distance to a closest point on it. We show that the complexity of this diagram is O(n), and give an O(nlog^3n) time algorithm to compute it. We also prove a number of structural properties of this diagram. In particular, a Voronoi region may consist of k-1 connected components, but if one component is bounded, then it is equal to the entire region.\nTitle:\nFarthest-polygon Voronoi diagrams\n\nAbstract:\nWe present a near-quadratic time algorithm that computes a point inside a simple polygon P in the plane having approximately the largest visibility polygon inside P, and a near-linear time algorithm for finding the point that will have approximately the largest Voronoi region when added to an n-point set in the plane. We apply the same technique to find the translation that approximately maximizes the area of intersection of two polygonal regions in near-quadratic time, and the rigid motion doing so in near-cubic time.\nTitle:\nFinding a Guard that Sees Most and a Shop that Sells Most\n\nAbstract:\nWe discuss in this paper a method of finding skyline or non-dominated points in a set P of n points with respect to a set S of m sites. A point pi\u2208P is non-dominated if and only if for each pj\u2208P, $j \\not= i$, there exists at least one point s\u2208S that is closer to pi than pj. We reduce this problem of determining non-dominated points to the problem of finding sites that have non-empty cells in an additively weighted Voronoi diagram under convex distance function. The weights of the said Voronoi diagram are derived from the co-ordinates of the points of P and the convex distance function is derived from S. In the 2-dimensional plane, this reduction gives a O((m+n)logm+n logn)-time randomized incremental algorithm to find the non-dominated points.\nTitle:\nComputation of non-dominated points using compact voronoi diagrams\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.036, mean value for all the writers is 8.648, which makes it 1.375 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe show that the union of n translates of a convex body in $$\\\\mathbb {R}^3$$R3 can have $$\\\\varTheta (n^3)$$\u017a(n3) holes in the worst case, where a hole in a set X is a connected component of $$\\\\mathbb {R}^3 \\\\setminus X$$R3\\\\X. This refutes a 20-year-old conjecture. As a consequence, we also obtain improved lower bounds on the complexity of motion planning problems and of Voronoi diagrams with convex distance functions.\nTitle:", "model_inf_time": 1.34}, {"id": "41474", "output": "The Expected Size of the 2D Visibility Complex", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nAbstract We make an experimental assessment of the size of the 2D visibility complex of disjoint unit discs randomly distributed in the plane with density,. We observe that the number of free bitangents is asymptotically linear in the number of discs and we study the dependence of the linear asymptote in terms of the density of the scene. Specically , for a particular range of scene densities , we exhibit an approximation of the number of free bitangents in terms of and the number n of discs, for n larger than some function of,. We also notice how our approximation gained for rather large densities can be used to guess the onset of the linear behavior for small densities.\nTitle:\nAn Experimental Assessment of the 2D Visibility Complex\n\nAbstract:\nWe present a cgal -based univariate algebraic kernel, which provides certified real-root isolation of univariate polynomials with integer coefficients and standard functionalities such as basic arithmetic operations, greatest common divisor (gcd) and square-free factorization, as well as comparison and sign evaluations of real algebraic numbers. We compare our kernel with other comparable kernels, demonstrating the efficiency of our approach. Our experiments are performed on large data sets including polynomials of high degree (up to 2 000) and with very large coefficients (up to 25 000 bits per coefficient). We also address the problem of computing arrangements of x -monotone polynomial curves. We apply our kernel to this problem and demonstrate its efficiency compared to previous solutions available in cgal .\nTitle:\nUnivariate Algebraic Kernel and Application to Arrangements\n\nAbstract:\nIn this paper, we show that, amongst $n$ uniformly distributed unit balls in $\\mathbb{R}^3$, the expected number of maximal nonoccluded line segments tangent to four balls is linear. Using our techniques we show a linear bound on the expected size of the visibility complex, a data structure encoding the visibility information of a scene, providing evidence that the storage requirement for this data structure is not necessarily prohibitive. These results significantly improve the best previously known bounds of $O(n^{8/3})$ [F. Durand, G. Drettakis, and C. Puech, {ACM Transactions on Graphics}, 21 (2002), pp. 176--206]. Our results generalize in various directions. We show that the linear bound on the expected number of maximal nonoccluded line segments that are not too close to the boundary of the scene and tangent to four unit balls extends to balls of various but bounded radii, to polyhedra of bounded aspect ratio, and even to nonfat three-dimensional objects such as polygons of bounded aspect ratio. We also prove that our results extend to other distributions such as the Poisson distribution. Finally, we indicate how our probabilistic analysis provides new insight on the expected size of other global visibility data structures, notably the aspect graph.\nTitle:\nThe Expected Number of 3D Visibility Events Is Linear\n\nAbstract:\nIt is a widely observed phenomenon in computer graphics that the size of the silhouette of a polyhedron is much smaller than the size of the whole polyhedron. This paper provides for the first time theoretical evidence supporting this for a large class of objects, namely for polyhedra that approximate surfaces in some reasonable way; the surfaces may not be convex or differentiable and they may have boundaries. We prove that such polyhedra have silhouettes of expected size O(\u221an) where the average is taken over all points of view and n is the complexity of the polyhedron.\nTitle:\nAn upper bound on the average size of silhouettes\n\nAbstract:\nIn this paper we study various geometric predicates for determining the existence of and categorizing the configurations of lines in 3D that are transversal to lines or segments. We compute the degrees of standard procedures of evaluating these predicates. The degrees of some of these procedures are surprisingly high (up to 168), which may explain why computing line transversals with finite-precision floating-point arithmetic is prone to error. Our results suggest the need to explore alternatives to the standard methods of computing these quantities.\nTitle:\nOn the degree of standard geometric predicates for line transversals in 3D\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.69, mean value for all the writers is 8.648, which makes it 0.036 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe study the expected size of the 2D visibility complex of randomly distributed objects in the plane. We prove that the asymptotic expected number of free bitangents (which correspond to 0-faces of the visibility complex) among unit discs ( or polygons of bounded aspect ration and similar size) is linear and exhibit bounds in terms of the density of the objects. We also make an experimental assessment of the size of the visibility complex for disjoint random unit discs. We provide experimental estimates of the onset of the linear behavior and of the asymptotic slope and y-intercept of the number of free bitangents in terms of the density of discs. Finally, we analyze the quality of our estimates in terms of the density of discs.\nTitle:", "model_inf_time": 1.52}, {"id": "41475", "output": "Interactive Visual Query Processing for Efficient Content-Based Image Retrieval Based on a Hierarchical SOM", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIn this paper we propose a new approach for similarity matching in image retrieval based on approximate query processing in a hierarchical SOM. We first map the high dimensional input vectors to a low dimensional grid by a local membership function which preserves the relationships between the input vectors and their neighboring weight vectors. Then, we use a hierarchical tree to reduce the computation cost for finding the best match unit. Finally we retrieve the k nearest neighbors of the query vector by an approximate query processing approach. The experiments show that the proposed approach works well on both synthetic datasets and image databases.\nTitle:\nApproximate Query Processing For Efficient Content-Based Image Retrieval Based On A Hierarchical Som\n\nAbstract:\nIn this paper, a novel 3D head model retrieval framework is proposed. First, kernel PCA is adopted both to reduce the data dimension and to extract features for model characterization. Second, based on the derived features, a hierarchical indexing structure for 3D model database is constructed using the hierarchical self organizing map (HSOM). Third, an efficient search approach is presented based on the established indexing structure that requires only feature matching between the query model and a small number of SOM nodes. The main advantages of our approach include high retrieval precision due to the discrimination capacity of kernel PCA, and low computation cost due to the hierarchical indexing structure and data dimension reduction. In addition, the topology-preserving property of HSOM also facilitates the exploration of the model database with the possibility of further knowledge discovery.\nTitle:\nHierarchical indexing for 3D head model retrieval based on kernel PCA\n\nAbstract:\nA novel approach for 3D motion capture data retrieval based on the Hierarchical Self Organizing Map (HSOM) is proposed. Given a query motion sequence, our goal is to search for all the similar motions from a database. Specifically, a feature vector based on the distribution of the human motion data is first extracted from each motion sequence in the database. Then, Singular Value Decomposition (SVD) is applied to reduce the dimensionality of the feature vector. To improve the retrieval efficiency, a two-level indexing scheme based on the HSOM is constructed, in which the motion sequences are first partitioned with the reduced feature vectors at the top level and then at the lower level, the original feature vectors are adopted to classify the cluster associated with a parent node at the first level into sub-clusters. Finally, fuzzy search is implemented to traverse the index structure to search for similar motions. Experimental results show that our method can achieve good performance in terms of retrieval accuracy and efficiency.\nTitle:\nSearching Of Motion Database Based On Hierarchical Som\n\nAbstract:\nIn this paper, we propose a novel 3D head model retrieval approach in which the queries are 2D face views instead of less readily available 3D head models. The basic idea is to characterize the corresponding relations between 2D view feature and 3D model feature based on a machine learning approach. Thus the subsequent feature matching can be carried out in 3D feature space. As an effective solution to regression problems, relevance vector machine is used in this paper to establish an association between 2D and 3D features. Experimental results show that our proposed 2D query based method is comparable with the direct 3D query based one.\nTitle:\nRelevance Vector Machine for Content-Based Retrieval of 3D Head Models\n\nAbstract:\nIn this paper, we propose a filter-refinement scheme based on a new approach called Sorted Extended Gaussian Image histogram approach (SEGI) to address the problems of traditional EGI. Specifically, SEGI first constructs a 2D histogram based on the EGI histogram and the shell histogram. Then, SEGI extracts two kinds of descriptors from each 3D model: (i) the descriptor from the sorted histogram bins is used to perform approximate 3D model retrieval in the filter step, and (ii) the descriptor which records the relations between the histogram bins is used to refine the approximate results and obtain the final query results. The experiments show that SEGI outperforms most of state-of-art approaches (e.g., EGI, shell histogram) on the public Princeton Shape Benchmark.\nTitle:\nA Filter-Refinement Scheme for 3D Model Retrieval Based on Sorted Extended Gaussian Image Histogram\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.282, mean value for all the writers is 8.648, which makes it 0.541 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nVisual query processing is one of the new issues for content-based image retrieval. In this paper, we propose (i) a filter-refinement scheme based on a modified form of the self-organizing map and (ii) a new interactive approach for similarity matching in image retrieval based on visual query processing. Specifically, we first propose a new local membership function, which preserves the relationships between the input feature vectors of the images and their neighboring weight vectors, to project the high dimensional input feature vectors to a low dimensional grid. Then, all the input feature vectors are mapped and visualized in the 2D grid. The feature vector of the query image is mapped and visualized in the 2D grid as well. The users not only can visualize the locations of the query image and the image data in the database, but also visualize the locations of the relevant and irrelevant images. Next, the users retrieve the candidates from the 2D grid interactively through visual query processing in the filter phase. Finally, the query results are obtained from the candidates by performing similarity ranking in the original feature space during the refinement phase. In order to accelerate the query process, we use a hierarchical tree to index the weight vectors of the self-organizing map (SOM) units to reduce the computation cost for finding the best matching unit. Our experiments show that (i) the proposed approach works well on both synthetic datasets and image data, (ii) the proposed visual query processing approach is more efficient than conventional approaches and can enhance the overall interactive experience through fast feedback, and (iii) the filter-refinement scheme makes our proposed approach more robust than conventional approaches.\nTitle:", "model_inf_time": 1.89}, {"id": "41476", "output": "Snapping-to-Photos: A Continuum of Virtual Travel Interfaces for 3D Reconstructions", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nVirtually navigating through photos from a 3D image-based reconstruction has recently become very popular in many applications. In this paper, we consider a particular virtual travel maneuver that is important for this type of virtual navigation---orbiting to photos that can see a point-of-interest (POI). The main challenge with this particular type of orbiting is how to give appropriate feedback to the user regarding the existence and information of each photo in 3D while allowing the user to manipulate three degrees-of-freedom (DoF) for orbiting around the POI. We present a hybrid approach that combines features from two baselines---proxy plane and thumbnail approaches. Experimental results indicate that users rated our hybrid approach more favorably for several qualitative questionnaire statements, and that the hybrid approach is preferred over both baselines for outdoor scenes.\n\n\nTitle:\nHybrid orbiting-to-photos in 3D reconstructed visual reality\n\nAbstract:\nAugmented reality annotations and virtual scene navigation add new dimensions to remote collaboration. In this paper, we present a touchscreen interface for creating freehand drawings as world-stabilized annotations and for virtually navigating a scene reconstructed live in 3D, all in the context of live remote collaboration. Two main focuses of this work are (1) automatically inferring depth for 2D drawings in 3D space, for which we evaluate four possible alternatives, and (2) gesture-based virtual navigation designed specifically to incorporate constraints arising from partially modeled remote scenes. We evaluate these elements via qualitative user studies, which in addition provide insights regarding the design of individual visual feedback elements and the need to visualize the direction of drawings.\nTitle:\nIn touch with the remote world: remote collaboration with augmented reality drawings and virtual navigation\n\nAbstract:\nWe present a method for collaborative augmented reality (AR) that enables users from different viewpoints to interpret object references specified via 2D on-screen circling gestures. Based on a user's 2D drawing annotation, the method segments out the userselected object using an incomplete or imperfect scene model and the color image from the drawing viewpoint. Specifically, we propose a novel segmentation algorithm that utilizes both 2D and 3D scene cues, structured into a three-layer graph of pixels, 3D points, and volumes (supervoxels), solved via standard graph cut algorithms. This segmentation enables an appropriate rendering of the user's 2D annotation from other viewpoints in 3D augmented reality. Results demonstrate the superiority of the proposed method over existing methods.\nTitle:\nPPV: Pixel-Point-Volume Segmentation for Object Referencing in Collaborative Augmented Reality\n\nAbstract:\nVision-based user interfaces are a feasible and advantageous modality for wearable computers. To substantiate this claim, we present a robust real-time hand gesture recognition system that is capable of being the sole input provider for a demonstration application. It achieves usability and interactivity even when both the head-worn camera and the object of interest are in motion. We describe a set of general gesture-based interaction styles and explore their characteristics in terms of task suitability and the computer vision algorithms required for their recognition. Preliminary evaluation of our prototype system leads to the conclusion that vision-based interfaces have achieved the maturity necessary to help overcome some limitations of more traditional mobile user interfaces.\nTitle:\nVision-Based Interfaces For Mobility\n\nAbstract:\nIn this paper, we consider the problem of rendering novel views of a live unprepared scene from video input, important to many application scenarios (such as telepresence and remote collaboration). We present an optimization approach to improving incomplete scene reconstructions captured in real time with a single moving monocular camera. We take semi-dense depth maps and convert them into a dense scene model, suitable for rendering plausible novel views of the scene using conventional image-based rendering. Our implementation densifies depth maps at the rate they are generated, and enables us to generate novel views of live scenes with no pre-capture or preprocessing. In evaluations comparing with other approaches, our method performs well even on difficult scenes, and results in higher-quality novel views.\nTitle:\nDensification of Semi-Dense Reconstructions for Novel View Generation of Live Scenes\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.722, mean value for all the writers is 8.648, which makes it 0.79 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nNavigating through a virtual, 3D reconstructed scene has recently become very important in many applications. A popular approach is to virtually travel to the photos used in reconstructing the scene; such an approach may be generally termed a \"snapping-to-photos\" virtual travel interface. While previous work has either used fully constrained interfaces (always at the photos) or minimally constrained interfaces (free-flight navigation), in this paper we introduce new snapping-to-photos interfaces that lie in between these two extremes. Our snapping-to-photos interfaces snap the view to a photo in 3D based on viewpoint similarity and optionally the user's mouse cursor or finger-tap position. Experimental results, with both indoor and outdoor scene reconstructions, found that our snapping-to-photos interfaces are preferred over the baseline fully constrained-to-photos interface, that there exist differences between indoor and outdoor scenes, and that users preferred and were able to reach target photos better with click-to-snap point-of-interest snapping compared to automatic point-of-view snapping.\nTitle:", "model_inf_time": 2.06}, {"id": "41477", "output": "Friction Compensation at Velocity Reversal of CNC X-Y Table", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper aims at precision position control of the X-Y table of a computerized numeric control (CNC) machining center at velocity reversal. The characteristics of presliding fric- tion are analyzed, and a simple and effective method is proposed to compensate the friction on the basis of these characteristics. A large position tracking error occurs at velocity reversal due to the sudden transition of friction between presliding regime and sliding regime. This paper investigates the transition time to reduce the tracking error, and derives a relationship between the transition time and the acceleration at zero velocity. This paper also pro- poses a method of estimating the transition time using this rela- tionship, without having to measure velocity. Experimental obser- vations confirm this correlation holds over a large dynamic range. In addition to friction, there is a large change in a torsional dis- placement at velocity reversal in a two-inertia system with finite stiffness like an X-Y table linked to a motor through a ballscrew. The proposed friction compensation scheme can be easily incorpo- rated with the compensation method for torsional displacement to achieve good tracking performance. The experimental results are described to show the effectiveness of the proposed method.\nTitle:\nPosition control of X-Y table at velocity reversal using presliding friction characteristics\n\nAbstract:\nThis paper presents the virtual height inverted pendulum mode (VHIPM), which is a simple and effective trajectory generation method for the stable walking of biped robots. VHIPM, which is based on the inverted pendulum mode (IPM), can significantly reduce the zero moment point (ZMP) error by adjusting the height in the inverted pendulum. We show the relationship between VHIPM and other popular trajectory generation methods, and compare the ZMP errors in walking when trajectories are generated by various methods including VHIPM. We also investigate the sensitivity of the ZMP error in VHIPM to the step length, walking period and mass distribution of a robot. The simulation results show that VHIPM significantly reduces the ZMP errors compared to other methods under various circumstances.\nTitle:\nAn effective trajectory generation method for bipedal walking\n\nAbstract:\nThis paper proposes a dynamic compensation method for multivariable control systems with saturating actuators to cope with the reset windup phenomenon. A dynamic compensator is explicitly determined based on a performance index of controller states. The proposed method is applicable to any open-loop stable plants with saturating actuators whose controller has been determined by some design technique. A simulation example is included to illustrate the effectiveness of the proposed method\nTitle:\nDynamic compensation method for multivariable control systems with saturating actuators\n\nAbstract:\nThis paper investigates iterative learning control for linear discrete time nonminimum phase systems. First, iterative learning control with advanced output data is considered for maximum phase systems. Next, the results are extended to nonminimum phase systems. The stability of the inverse mapping from the desired output to the input is proven based on the results for maximum phase systems. The input should be updated with the output which is more advanced than the input by the sum of the relative degree of the system and the number of nonminimum phase zeros. An example is given to indicate the importance of proper advances of output in the input update law.\nTitle:\nBrief Iterative learning control for linear discrete time nonminimum phase systems\n\nAbstract:\nWe describe our experiments on a real-time system design, focusing on design alternatives such as scheduling jitter, sensor-to-output latency, intertask communication schemes and the system utilization. The prime objective of these experiments was to evaluate a real-time design produced using the period calibration method (Gerber et al., 1995) and thus identify the limitations of the method. We chose a computerized numerical control (CNC) machine as our target real-time system and built a realistic controller and a plant simulator. Our results were extracted from a controlled series of more than a hundred test controllers obtained by varying four test variables. This study unveils many interesting facts: average sensor-to-output latency is one of the most dominating factors in determining control quality; the effect of scheduling jitter appears only when the average sensor-to-output latency is sufficiently small; and loop processing periods are another dominating factor of performance. Based on these results, we propose a new communication scheme and a new objective function for the period calibration method\nTitle:\nVisual assessment of a real-time system design: a case study on a CNC controller\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.253, mean value for all the writers is 8.648, which makes it 1.369 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThis paper analyzes the characteristics of pre-sliding friction of the X-Y table of a CNC machining center at velocity reversal, and presents a simple and effective method of friction compensation based on these characteristics. During velocity reversal, friction transits from the pre-sliding regime into the sliding regime, and it must not be considered as a sudden discontinuity for accurate control. In order to distinguish its regime at velocity reversal, the relationship between the transition time and the acceleration at zero velocity is investigated using the spring-like friction model. Furthermore, the experimental observations prove this relation. Using the transition time estimated from the relation without any accurate velocity information, the friction at velocity reversal can be well compensated and the experimental results show its effectiveness\nTitle:", "model_inf_time": 1.65}, {"id": "41478", "output": "Supervisory Control of a LEGO\u00ae Car Assembly Line", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nExtended Finite Automata (EFA), i.e., finite automata extended with variables, are a suitable modeling framework for discrete event systems owing to their compactness, resulting from the use of variables. In this paper, we propose a symbolic algorithm that efficiently synthesizes a supervisor for a plant modeled by an EFA and a specification defined by another EFA. The principle of the algorithm is to iteratively strengthen the guards of the plant EFA so that forbidden or blocking states become unreachable in the controlled plant. As a consequence of the algorithm, the controlled behavior is modeled by an EFA having the same structure as the plant EFA, having stronger guards and is shown to be maximally permissive. We illustrate our algorithm via a simple manufacturing example.\nTitle:\nNonblocking and Safe Control of Discrete-Event Systems Modeled as Extended Finite Automata\n\nAbstract:\nThe decentralized supervisory control problem of discrete event systems with local specifications is studied. A necessary and sufficient condition is obtained for the existence of modular supervisors for ensuring that a group of local specifications is achieved by the controlled system. For illustration an example of a simple manufacturing system is given. The paper provides a modular approach for the design of supervisors for large systems\nTitle:\nDecentralized control of discrete event systems with multiple local specifications\n\nAbstract:\nFor Discrete Event Systems (DES) modeled as Finite Automata, supervisory control theory has been extensively studied. Extended Finite Automata (EFA), i.e., finite automata extended with variables, are a suitable modeling framework for discrete event systems owing to their compactness, resulting from the use of variables. In this paper, we propose a symbolic algorithm that efficiently synthesizes a supervisor for a plant modeled by an EFA and a specification defined by another EFA, or equivalently a set of forbidden locations in the plant EFA. The principle of the algorithm is to iteratively strengthen the guards of the plant EFA so that forbidden or blocking states become unreachable in the controlled plant. As a consequence of the algorithm, the controlled behavior is modeled by an EFA having the same structure as the plant EFA, having stronger guards and is shown to be maximally permissive.\nTitle:\nSymbolic approach to nonblocking and safe control of Extended Finite Automata\n\nAbstract:\nWe study the control of a nondeterministic discrete event system (DES) subject to a control specification expressed in the propositional mu-calculus, under complete observation of events. Given a plant automaton model and a mu-calculus specification we provide a set of rules that computes the \"quotient\" of the specification against the plant, which is another mu-calculus formula such that a supervisor exists if and only if the quotiented formula is satisfiable. Thus the control problem is reduced to one of mu-calculus satisfiability. We also present a tableau-based satisfiability solving algorithm that identifies a model for the quotiented formula. The resulting model serves as a supervisor. The complexity of supervisor existence verification as well as model synthesis is single exponential in the size of the plant as well as the size of the specification formula.\nTitle:\nQuotient-Based Control Synthesis For Non-Deterministic Plants With Mu-Calculus Specifications\n\nAbstract:\nFor the control of discrete event systems, the notion of directed control refines that of supervisory control. A directed controller is one that selects at most one controllable event to be enabled at any state (without disabling any uncontrollable event), which is in fact how a discrete event control is implemented. In contrast, a supervisory controller computes a maximal allowable set of controllable events at each state, leaving undecided exactly which controllable event should be enabled. In previous works, we developed a framework for the computation of optimal directed controllers and a polynomial synthesis algorithm for acyclic plants. In this paper, we present a novel synthesis approach for general plants, i.e., plants with or without cycles, thus providing a complete solution to the optimal directed control problem. The complexity of the approach remains polynomial in the size of plant.\nTitle:\nOptimal Nonblocking Directed Control of Discrete Event Systems\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.967, mean value for all the writers is 8.648, which makes it 1.125 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe design of logic controllers for event-driven systems continue to rely largely on intuitive methods rather than on formal techniques. This approach results in a control code that requires extensive verification, is hard to maintain and modify, and may even fail at times. Supervisory control theory (SCT) provides a formal approach to logic control synthesis. In order to demonstrate the usefulness of the supervisory control theory in manufacturing systems, an educational test-bed that simulates an automated car assembly line has been built using LEGO\u00ae blocks. Finite state machines (FSMs) are used for modeling operations of the assembly line, and for the specifications that accomplish the task of successfully completing the assembly repeatedly. Using the technique of SCT, we derive a supervisor that enforces the specifications while offering the maximum flexibility of assembly. Subsequently a controller is extracted from the maximally permissive supervisor for the purpose of implementing the control by selecting, when possible, at most one controllable event from among the ones allowed by the supervisor. Testing to check the correctness of the control code is reduced, since the controller is guaranteed to enforce the specifications.\nTitle:", "model_inf_time": 1.52}, {"id": "41479", "output": "Distance Concentration and Plasticity in Dimensionality Reduction", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDimensionality reduction aims at representing high-dimensional data in low-dimensional spaces, mainly for visualization and exploratory purposes. As an alternative to projections on linear subspaces, nonlinear dimensionality reduction, also known as manifold learning, can provide data representations that preserve structural properties such as pairwise distances or local neighborhoods. Very recently, similarity preservation emerged as a new paradigm for dimensionality reduction, with methods such as stochastic neighbor embedding and its variants. Experimentally, these methods significantly outperform the more classical methods based on distance or transformed distance preservation.\nTitle:\nShift-invariant similarities circumvent distance concentration in stochastic neighbor embedding and variants\n\nAbstract:\nDimensionality reduction aims at representing high-dimensional data in low-dimensional spaces, in order to facilitate their visual interpretation. Many techniques exist, ranging from simple linear projections to more complex nonlinear transformations. The large variety of methods emphasizes the need of quality criteria that allow for fair comparisons between them. This paper extends previous work about rank-based quality criteria and proposes to circumvent their scale dependency. Most dimensionality reduction techniques indeed rely on a scale parameter that distinguish between local and global data properties. Such a scale dependency can be similarly found in usual quality criteria: they assess the embedding quality on a certain scale. Experiments with various dimensionality reduction techniques eventually show the strengths and weaknesses of the proposed scale-independent criteria.\nTitle:\nScale-independent quality criteria for dimensionality reduction\n\nAbstract:\nStochastic neighbour embedding (SNE) and its variants are methods of nonlinear dimensionality reduction that involve soft Gaussian neighbourhoods to measure similarities for all pairs of data. In order to build a suitable embedding, these methods try to reproduce in a low-dimensional space the neighbourhoods that are observed in the high-dimensional data space. Previous works have investigated the immunity of such similarities to norm concentration, as well as enhanced cost functions, like sums of Jensen\u2013Shannon divergences. This paper proposes an additional refinement, namely multi-scale similarities, which are averages of soft Gaussian neighbourhoods with exponentially growing bandwidths. Such multi-scale similarities can replace the regular, single-scale neighbourhoods in SNE-like methods. Their objective is then to maximise the embedding quality on all scales, with the best preservation of both local and global neighbourhoods, and also to exempt the user from having to fix a scale arbitrarily. Experiments with several data sets show that the proposed multi-scale approach captures better the structure of data and improves significantly the quality of dimensionality reduction.\nTitle:\nMulti-scale similarities in stochastic neighbour embedding: Reducing dimensionality while preserving both local and global structure.\n\nAbstract:\nModern data analysis tools have to work on high-dimensional data, whose components are not independently distributed. High-dimensional spaces show surprising, counter-intuitive geometrical properties that have a large influence on the performances of data analysis tools. Among these properties, the concentration of the norm phenomenon results in the fact that Euclidean norms and Gaussian kernels, both commonly used in models, become inappropriate in high-dimensional spaces. This papers presents alternative distance measures and kernels, together with geometrical methods to decrease the dimension of the space. The methodology is applied to a typical time series prediction example.\nTitle:\nThe curse of dimensionality in data mining and time series prediction\n\nAbstract:\nNonlinear dimensionality reduction aims at providing low-dimensional representions of high-dimensional data sets. Many new methods have been recently proposed, but the question of their assessment and comparison remains open. This paper reviews some of the existing quality measures that are based on distance ranking and K-ary neighborhoods. In this context, the comparison of the ranks in the high- and low-dimensional spaces leads to the definition of the co-ranking matrix. Rank errors and concepts such as neighborhood intrusions and extrusions can be associated with different blocks of the co-ranking matrix. The considered quality criteria are then cast within this unifying framework and the blocks they involve are identified. The same framework allows us to propose simpler criteria, which quantify two aspects of the embedding, namely its overall quality and its tendency to favor either intrusions or extrusions. Eventually, a simple experiment illustrates the soundness of the approach.\nTitle:\nQuality assessment of nonlinear dimensionality reduction based on K-ary neighborhoods\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.314, mean value for all the writers is 8.648, which makes it 0.568 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nDimensionality reduction aims at providing faithful low-dimensional representations of high-dimensional data. Its general principle is to attempt to reproduce in a low-dimensional space the salient characteristics of data, such as proximities. A large variety of methods exist in the literature, ranging from principal component analysis to deep neural networks with a bottleneck layer. In this cornucopia, it is rather difficult to find out why a few methods clearly outperform others. This paper identifies two important properties that enable some recent methods like stochastic neighborhood embedding and its variants to produce improved visualizations of high-dimensional data. The first property is a low sensitivity to the phenomenon of distance concentration. The second one is plasticity, that is, the capability to forget about some data characteristics to better reproduce the other ones. In a manifold learning perspective, breaking some proximities typically allow for a better unfolding of data. Theoretical developments as well as experiments support our claim that both properties have a strong impact. In particular, we show that equipping classical methods with the missing properties significantly improves their results.\nTitle:", "model_inf_time": 1.47}, {"id": "41480", "output": "A hybrid meta-heuristic algorithm for fuzzy multi-periodic inventory control with stochastic replenishments, fuzzy purchasing prices, and service level constraints", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMulti-periodic inventory control problems are mainly studied by employing one of two assumptions. First, the continuous review, where depending on the inventory level, orders can happen at any time, and next the periodic review, where orders can only be placed at the beginning of each period. In this paper, we relax these assumptions and assume the times between two replenishments are independent random variables. For the problem at hand, the decision variables (the maximum inventory of several products) are of integer-type and there is a single space-constraint. While demands are treated as fuzzy numbers, a combination of back-order and lost-sales is considered for the shortages. We demonstrate the model of this problem is of an integer-nonlinear-programming type. A hybrid method of fuzzy simulation (FS) and genetic algorithm (GA) is proposed to solve this problem. The performance of the proposed method is then compared with the performance of an existing hybrid FS and simulated annealing (SA) algorithm through three numerical examples containing different numbers of products. Furthermore, the applicability of the proposed methodology along with a sensitivity analysis on its parameters is shown by numerical examples. The comparison results show that, at least for the numerical examples under consideration, the hybrid method of FS and GA shows better performance than the hybrid method of FS and SA.\nTitle:\nA hybrid method of fuzzy simulation and genetic algorithm to optimize constrained inventory control systems with stochastic replenishments and fuzzy demand\n\nAbstract:\nIn this paper, a multiproduct multi-chance constraint stochastic inventory control problem is considered, in which the time-periods between two replenishments are assumed independent and identically distributed random variables. For the problem at hand, the decision variables are of integer-type, the service-level is a chance constraint for each product, and the space limitation is another constraint of the problem. Furthermore, shortages are allowed in the forms of fuzzy random quantities of lost sale that are backordered. The developed mathematical formulation of the problem is shown to be a fuzzy random integer-nonlinear programming model. The aim is to determine the maximum level of inventory for each product such that the total profit under budget and service level constraints is maximized. In order to solve the model, a hybrid method of fuzzy simulation, stochastic simulation, and particle swarm optimization approach (Hybrid FS-SS-PSO) is used. At the end, several numerical illustrations are given to demonstrate the applicability of the proposed methodology and to compare its performances with the ones of another hybrid algorithm as a combination of fuzzy simulation, stochastic simulation, and genetic algorithm (FS-SS-GA). The results of the numerical illustrations show that FS-SS-PSO performs better than FS-SS-GA in terms of both objective functions and CPU time.\nTitle:\nReplenish-up-to multi-chance-constraint inventory control system under fuzzy random lost-sale and backordered quantities\n\nAbstract:\nIn this paper, a multi-product multi-chance constraint joint single-vendor multi-buyers inventory problem is considered in which the demand follows a uniform distribution, the lead-time is assumed to vary linearly with respect to the lot size, and the shortage in combination of backorder and lost-sale is assumed. Furthermore, the orders are placed in multiple of packets, there is a limited space available for the vendor, there are chance constraints on the vendor service rate to supply the products, and there is a limited budget for each buyer to purchase the products. While the elements of the buyers' cost function are holding, shortage, order and transportation costs, the set up and holding costs are assumed for the vendor. The goal is to determine the re-order point and the order quantity of each product for each buyer such that the chain total cost is minimized. We show the model of this problem to be a mixed integer nonlinear programming type and in order to solve it a particle swarm optimization (PSO) approach is used. To justify the results of the proposed PSO algorithm, a genetic algorithm (GA) is applied as well to solve the problem. Then, the quality of the results and the CPU times of reaching the solution are compared through three numerical examples that are given to demonstrate the applicability of the proposed methodology in real world inventory control problems. The comparison results show the PSO approach has better performances than the GA method.\nTitle:\nMultiproduct multiple-buyer single-vendor supply chain problem with stochastic demand, variable lead-time, and multi-chance constraint\n\nAbstract:\nIn this paper, a multi-buyer multi-vendor supply chain problem is considered in which there are several products, each buyer has limited capacity to purchase products, and each vendor has warehouse limitation to store products. In this chain, the demand of each product is stochastic and follows a uniform distribution. The lead-time of receiving products from a vendor to a buyer is assumed to vary linearly with respect to the order quantity of the buyer and the production rate of the vendor. For each product, a fraction of the shortage is backordered and the rest are lost. The ordered product quantities are placed in multiple of pre-defined packets and there are service rate constraints for the buyers. The goal is to determine the reorder points, the safety stocks, and the numbers of shipments and packets in each shipment of the products such that the total cost of the supply chain is minimized. We show that the model of this problem is of an integer nonlinear programming type and in order to solve it a harmony search algorithm is employed. To validate the solution and to compare the performance of the proposed algorithm, a genetic algorithm is utilized as well. A numerical illustration and sensitivity analysis are given at the end to show the applicability of the proposed methodology in real-world supply chain problems.\nTitle:\nMultiple-buyer multiple-vendor multi-product multi-constraint supply chain problem with stochastic demand and variable lead-time: A harmony search algorithm\n\nAbstract:\nThe economic production quantity (EPQ) is one of the most applicable model in production and inventory control environments. Like other classical production and inventory control models, it is derived based upon assumptions that cause limited real-world applications. Continuous and constant-rate delivery of orders, infinite availability of warehouse spaces, and applicability on a single product are some of these assumptions. In order to make the EPQ model more applicable to real-world production and inventory control problems, in this paper, we expand this model by assuming that the orders may be delivered discretely in the form multiple pallets. In addition, we may have more than one product along with warehouse space limitation. Under these conditions, we formulate the problem as a non-linear integer-programming model and propose a genetic algorithm to solve it. At the end, we present a numerical example to demonstrate the application of the proposed methodology.\nTitle:\nA genetic algorithm approach to optimize a multi-products EPQ model with discrete delivery orders and constrained space\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 14.127, mean value for all the writers is 8.648, which makes it 4.675 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWhile the usual assumptions in multi-periodic inventory control problems are that the orders are placed at the beginning of each period (periodic review) or depending on the inventory level they can happen at any time (continuous review), in this article, we relax these assumptions and assume that the periods between two replenishments of the products are independent and identically distributed random variables. Furthermore, assuming that the purchasing price are triangular fuzzy variables, the quantities of the orders are of integer-type and that there are space and service level constraints, total discount are considered to purchase products and a combination of back-order and lost-sales are taken into account for the shortages. We show that the model of this problem is a fuzzy mixed-integer nonlinear programming type and in order to solve it, a hybrid meta-heuristic intelligent algorithm is proposed. At the end, a numerical example is given to demonstrate the applicability of the proposed methodology and to compare its performance with one of the existing algorithms in real world inventory control problems.\nTitle:", "model_inf_time": 2.73}, {"id": "41481", "output": "Secure Delegation of Elliptic-Curve Pairing", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe present a cryptanalysis of a zero-knowledge identification protocol introduced by Naccache et al. at Eurocrypt '95. Our cryptanalysis enables a polynomial-time attacker to pass the identification protocol with probability one, without knowing the private key.\nTitle:\nCryptanalysis of a Zero-Knowledge Identification Protocol of Eurocrypt '95\n\nAbstract:\nAbstract A  prohibitive  barrier  faced  by  elliptic  curve  users  is  the  dif - culty  of  computing  the  curves'  cardinalities Despite  recent  theoretical breakthroughs,  point  counting  still  remains  very  cumbersome  and  inten - sively  time  consuming In  this  paper we  show  that  point  counting  can  be   avoided  at  the  cost  of  a protocol  slow - down This  slow - down  factor  is  quite  important  (typically =  500)  but  proves  that  the  existence  of  secure  elliptic - curve  signatures is   not   necessarily  conditioned  by  point  counting\nTitle:\nECC: Do We Need to Count?\n\nAbstract:\nWe provide a security analysis of the Modular Enhanced Symmetric Role Authentication mERA protocol. We prove that mERA is secure in the Bellare-Rogaway model, the standard model for analyzing the security of cryptographic protocols.\nTitle:\nSecurity Analysis of the Modular Enhanced Symmetric Role Authentication (mERA) Protocol.\n\nAbstract:\nDifferential Power Analysis, first introduced by Kocher et al. in [14], is a powerful technique allowing to recover secret smart card information by monitoring power signals. In [14] a specific DPA attack against smart-cards running the DES algorithm was described. As few as 1000 encryptions were sufficient to recover the secret key. In this paper we generalize DPA attack to elliptic curve (EC) cryptosystems and describe a DPA on EC Diffie-Hellman key exchange and EC EI-Gamal type encryption. Those attacks enable to recover the private key stored inside the smart-card. Moreover, we suggest countermeasures that thwart our attack.\nTitle:\nResistance against Differential Power Analysis for Elliptic Curve Cryptosystems\n\nAbstract:\nWe propose and analyse a new countermeasure against Differential Power Analysis (DPA) for the AES encryption algorithm, based on permutation tables. As opposed to existing AES countermeasures, it does not use random masking. We prove that our new countermeasure is resistant against first-order DPA; we also show that it is quite efficient in practice.\nTitle:\nA New DPA Countermeasure Based on Permutation Tables\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.364, mean value for all the writers is 8.648, which makes it 0.242 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper we describe a simple protocol for secure delegation of the elliptic-curve pairing. A computationally limited device (typically a smart-card) will delegate the computation of the pairing e(A, B) to a more powerful device (for example a PC), in such a way that 1) the powerful device learns nothing about the points A and B, and 2) the limited device is able to detect when the powerful device is cheating.\nTitle:", "model_inf_time": 1.15}, {"id": "41482", "output": "Routing Stability in Autonomous Systems with Autonomous Routing Policies", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nToday's Internet's routing paths are inefficient with respect to both connectivity and the market for interconnection. The former manifests itself via needlessly long paths, de-peering, etc. The latter arises because of a primitive market structure that results in unfulfilled demand and unused capacity. Today's networks make pairwise, myopic interconnection decisions based on business considerations that may not mirror considerations of the edge networks (or end systems) that would benefit from the existence of a particular interconnection. These bilateral contracts are also complex and difficult to enforce. This paper proposes MINT, a market structure and routing protocol suite that facilitates the sale and purchase of end-to-end Internet paths. We present MINT's structure, explain how it improves connectivity and market efficiency, explore the types of connectivity that might be exchanged (vs. today's \"best effort\" connectivity), and argue that MINT's deployment is beneficial to both stub networks and transit providers. We discuss research challenges, including the design both of the protocol that maintains information about connectivity and of the market clearing algorithms. Our preliminary evaluation shows that such a market quickly reaches equilibrium and exhibits price stability.\nTitle:\nMINT: a Market for INternet Transit\n\nAbstract:\nWe consider a network game where the nodes of the network wish to form a graph to route traffic between themselves. We present a model where costs are incurred for routing traffic, as well as for a lack of network connectivity. We focus on directed links and the link stability equilibrium concept, and characterize connected link stable equilibria. The structure of connected link stable networks is analyzed for several special cases.\nTitle:\nA contract-based model for directed network formation\n\nAbstract:\nIn this paper we explore the interaction between content distribution and traffic engineering. Because a traffic engineer may be unaware of the structure of content distribution systems or overlay networks, this management of the network does not fully anticipate how traffic might change as a result of his actions. Content distribution systems that assign servers at the application level can respond very rapidly to changes in the routing of the network. Consequently, the traffic engineer's decisions may almost never be applied to the intended traffic. We use a game-theoretic framework in which infinitesimal users of a network select the source of content, and the traffic engineer decides how the traffic will route through the network. We formulate a game and prove the existence of equilibria. Additionally, we present a setting in which equilibria are socially optimal, essentially unique, and stable. Conditions under which efficiency loss may be bounded are presented, and the results are extended to the cases of general overlay networks and multiple autonomous systems.\nTitle:\nTraffic Engineering vs. Content Distribution: A Game Theoretic Perspective\n\nAbstract:\nIn this paper, we explore the interaction between traffic engineering and the users of a network. Because a traffic engineer may be unaware of the structure of content distribution systems or overlay networks, his management of the network does not fully anticipate how traffic might change as a result of his actions. Content distribution systems that assign servers at the application level can respond very rapidly to changes in the routing of the network. Consequently, the traffic engineer's decisions may not be applied to the intended traffic. We use a game-theoretic framework in which infinitesimal users of a network select the source of content, and the traffic engineer decides how the traffic will route through the network. We formulate a game and prove the existence of equilibria. Additionally, we present a setting in which equilibria are socially optimal, essentially unique, and stable. Conditions under which efficiency loss may be bounded are presented, and the results are extended to the cases of general overlay networks and multiple autonomous systems.\nTitle:\nTraffic engineering with semiautonomous users: a game-theoretic perspective\n\nAbstract:\nWe explore the interaction between content distribution and traffic engineering. Because a traffic engineer may be unaware of the structure of content distribution systems or overlay networks, his management of the network does not fully anticipate how traffic might change as a result of his actions. Content distribution systems that assign servers at the application level can respond very rapidly to changes in the routing of the network. Consequently, the traffic engineer's decisions may almost never be applied to the intended traffic. We use a game-theoretic framework in which infinitesimal users of a network select the source of content, and the traffic engineer decides how the traffic will route through the network. We formulate a game and prove the existence of equilibria. Additionally, we present a setting in which equilibria are socially optimal, essentially unique, and stable. Conditions under which efficiency loss may be bounded are presented, and the results are extended to the cases of general overlay networks and multiple autonomous systems.\nTitle:\nTraffic engineering, content distribution, and continuous potential games.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.967, mean value for all the writers is 8.648, which makes it 0.581 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThousands of competing autonomous systems must cooperate with each other to provide global Internet connectivity. Each autonomous system (AS) encodes various economic, business, and performance decisions in its routing policy. The current interdomain routing system enables each AS to express policy using rankings that determine how each router in the AS chooses among different routes to a destination, and filters that determine which routes are hidden from each neighboring AS. Because the Internet is composed of many independent, competing networks, the interdomain routing system should provide autonomy, allowing network operators to set their rankings independently, and to have no constraints on allowed filters. This paper studies routing protocol stability under these conditions. We first demonstrate that \"next-hop rankings,\" commonly used in practice, may not ensure routing stability. We then prove that, when providers can set rankings and filters autonomously, guaranteeing that the routing system will converge to a stable path assignment imposes strong restrictions on the rankings ASes are allowed to choose. We discuss the implications of these results for the future of interdomain routing.\nTitle:", "model_inf_time": 1.55}, {"id": "41483", "output": "A Completed Dense Scene Flow Framework for 3D Motion Estimation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThe accuracy of scene flow is restricted by several challenges such as occlusion and large displacement motion. When occlusion happens, the positions inside the occluded regions lose their corresponding counterparts in preceding and succeeding frames. Large displacement motion will increase the complexity of motion modeling and computation. Moreover, occlusion and large displacement motion are highly related problems in scene flow estimation, e.g. large displacement motion often leads to considerably occluded regions in the scene. An improved dense scene flow method based on RGB-D data is proposed in this paper. To handle occlusion, we model the occlusion status for each point in our problem formulation, and jointly estimate the scene flow and occluded regions. To deal with large displacement motion, we employ an over-parameterized scene flow representation to model both the rotation and translation components of the scene flow, since large displacement motion cannot be well approximated using translational motion only. Furthermore, we employ a two-stage optimization procedure for this over-parameterized scene flow representation. In the first stage, we propose a new RGB-D PatchMatch method, which is mainly applied in the RGB-D image space to reduce the computational complexity introduced by the large displacement motion. According to the quantitative evaluation based on the Middlebury dataset, our method outperforms other published methods. The improved performance is also comprehensively confirmed on the real data acquired by Kinect sensor.\nTitle:\nHandling Occlusion and Large Displacement through Improved RGB-D Scene Flow Estimation\n\nAbstract:\nWe study the problem of action recognition from depth sequences captured by depth cameras, where noise and occlusion are common problems because they are captured with a single commodity camera. In order to deal with these issues, we extract semi-local features called random occupancy pattern (ROP) features, which employ a novel sampling scheme that effectively explores an extremely large sampling space. We also utilize a sparse coding approach to robustly encode these features. The proposed approach does not require careful parameter tuning. Its training is very fast due to the use of the high-dimensional integral image, and it is robust to the occlusions. Our technique is evaluated on two datasets captured by commodity depth cameras: an action dataset and a hand gesture dataset. Our classification results are superior to those obtained by the state of the art approaches on both datasets.\nTitle:\nRobust 3d action recognition with random occupancy patterns\n\nAbstract:\nHuman action recognition is an important yet challenging task. Human actions usually involve human-object interactions, highly articulated motions, high intra-class variations and complicated temporal structures. The recently developed commodity depth sensors open up new possibilities of dealing with this problem by providing 3D depth data of the scene. This information not only facilitates a rather powerful human motion capturing technique, but also makes it possible to efficiently model human-object interactions and intra-class variations. In this paper, we propose to characterize the human actions with a novel actionlet ensemble model, which represents the interaction of a subset of human joints. The proposed model is robust to noise, invariant to translational and temporal misalignment, and capable of characterizing both the human motion and the human-object interactions. We evaluate the proposed approach on three challenging action recognition datasets captured by Kinect devices, a multiview action recognition dataset captured with Kinect device, and a dataset captured by a motion capture system. The experimental evaluations show that the proposed approach achieves superior performance to the state of the art algorithms.\nTitle:\nLearning Actionlet Ensemble for 3D Human Action Recognition.\n\nAbstract:\nMost commercial eye gaze tracking systems are based on the use of infrared lights. However, such systems may not work outdoor or may have a very limited head box for them to work. This paper proposes a non-infrared based approach to track one's eye gaze with an RGBD camera (in our case, Kinect). The proposed method adopts a personalized 3D face model constructed off-line. To detect the eye gaze, our system tracks the iris center and a set of 2D facial landmarks whose 3D locations are provided by the RGBD camera. A simple onetime calibration procedure is used to obtain the parameters of the personalized eye gaze model. We compare the performance of the proposed method against the 2D approach using only RGB input on the same images, and find that the use of depth information directly from Kinect achieves more accurate tracking. As expected, the results from the proposed method are not as accurate as the ones from infrared-based approaches. However, this method has the potential for practical use with upcoming better and cheaper depth cameras.\nTitle:\nEye gaze tracking using an RGBD camera: a comparison with a RGB solution\n\nAbstract:\nWe present a learning-based 3D facial expression mapping technique that preserves facial expression details and works in real time for even high resolution meshes. Our approach is inspired by a previously developed technique called deformation transfer [1]. The deformation transfer technique preserves facial expression details but its computational overhead makes it not suitable for real time applications. To accelerate computation, we use a piecewise linear function to represent the mapping from the direct motion (the difference between the expression face and the neutral face) to the motion obtained by the deformation transfer method. This piecewise linear function is learned offline from a small set of training data. The online computation is thus reduced to the evaluation of the piecewise linear functions which is significantly faster. As a result, we are able to perform real time expression mapping for even high resolution 3D meshes.\nTitle:\nReal-Time facial expression mapping for high resolution 3d meshes\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.673, mean value for all the writers is 8.648, which makes it 0.021 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nConventional scene flow containing only translational vectors is not able to model 3D motion with rotation properly. Moreover, the accuracy of 3D motion estimation is restricted by several challenges such as large displacement, noise, and missing data (caused by sensing techniques or occlusion). In terms of solution, there are two kinds of approaches: local approaches and global approaches. However, local approaches can not generate smooth motion field, and global approaches is difficult to handle large displacement motion. In this paper, a completed dense scene flow framework is proposed, which models both rotation and translation for general motion estimation. It combines both a local method and a global method considering their complementary characteristics to handle large displacement motion and enforce smoothness respectively. The proposed framework is applied on the RGB-D image space where the computation efficiency is further improved. According to the quantitative evaluation based on Middlebury dataset, our method outperforms other published methods. The improved performance is further confirmed on the real data acquired by Kinect sensor.\nTitle:", "model_inf_time": 1.76}, {"id": "41484", "output": "An Improved Spectral Clustering Method for Large-Scale Hyperspectral Image Classification", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nSpectral graph theoretic methods have been a fundamental and important topic in the field of manifold learning and it has become a vital tool in data clustering. However, spectral clustering approaches are limited by their computational demands. It would be too expensive to provide an optimal approximation for spectral decomposition in dealing with large-scale and high-dimensional data sets. On the other hand, the rapid development of data on the Web has posed many rising challenges to the traditional single-task clustering, while the multi-task clustering provides many new thoughts for real-world applications such as video segmentation. In this paper, we will study a Spectral Clustering based on Iterative Optimization (SCIO), which solves the spectral decomposition problem of large-scale and high-dimensional data sets and it well performs on multi-task clustering. Extensive experiments on various synthetic data sets and real-world data sets demonstrate that the proposed method provides an efficient solution for spectral clustering.\nTitle:\nSpectral clustering based on iterative optimization for large-scale and high-dimensional data.\n\nAbstract:\nThis paper proposes an approach for the combined image authentication and compression of color images by making use of a digital watermarking and data hiding framework. The digital watermark is comprised of two components: a soft-authenticator watermark for authentication and tamper assessment of the given image, and a chrominance watermark employed to improve the efficiency of compression. The multipurpose watermark is designed by exploiting the orthogonality of various domains used for authentication, color decomposition and watermark insertion. The approach is implemented as a DCT-DWT dual domain algorithm. Simulations and comparisons of the proposed approach with state-of-the-art existing work demonstrate the potential of the overall scheme.\nTitle:\nA Steganographic Framework For Dual Authentication And Compression Of High Resolution Imagery\n\nAbstract:\nIn this paper, an effective method based on multi-scale overlapped block LBP is proposed for plant leaf image recognition. Firstly, multi-scale pyramid is employed in order to improve the leaf data utilization. For each scale, each training image is divided into several equal overlapping blocks to extract the LBP histograms. Then, the PCA method is used for LBP feature dimension reduction. Finally, the recognition experiments are performed by using the SVM classifier. We compare the proposed method with Histogram of Oriented Gradients (HOG) method and Inner-Distance Shape Context (IDSC) method on Swedish leaf dataset and our ICL leaf dataset. The experimental results show that the proposed method achieves better performance than IDSC and HOG.\nTitle:\nAn efficient multi-scale overlapped block LBP approach for leaf image recognition\n\nAbstract:\nThis paper proposes an approach for the combined image authentication and compression of color images by making use of a digital watermarking and data hiding framework. The digital watermark is comprised of two components: a soft-authenticator watermark for authentication and tamper assessment of the given image, and a chrominance watermark employed to improve the efficiency of compression. The multipurpose watermark is designed by exploiting the orthogonality of various domains used for authentication, color decomposition and watermark insertion. The approach is implemented as a DCT-DWT dual domain algorithm and is applied for the protection and compression of cultural heritage imagery. Analysis is provided to characterize the behavior of the scheme under ideal conditions. Simulations and comparisons of the proposed approach with state-of-the-art existing work demonstrate the potential of the overall scheme.\nTitle:\nDual domain watermarking for authentication and compression of cultural heritage images.\n\nAbstract:\nWith the development of ultra-high-resolution display devices, the visual perception of fine texture details is becoming increasingly important. Traditional image upsampling methods suffer from either loss of high-frequency texture details or very high time cost. In this paper, we propose an iterative projection reconstruction (IPR) method for fast and efficient image upsampling. The proposed method refines high-frequency texture details with an iterative projection process, and utilizes the pre-computed projection matrix to accelerate the example-based image reconstruction. As a result, the proposed method can reproduce fine texture details with low time cost. Experimental results demonstrate that the proposed method outperforms some state-of-the-art methods. \u00a9 2016 Elsevier B.V.\nTitle:\nIterative projection reconstruction for fast and efficient image upsampling\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.236, mean value for all the writers is 8.648, which makes it 1.355 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nHyperspectral image classification is a challenging and significant domain in the field of remote sensing with numerous applications in agriculture, environmental science, mineralogy, and surveillance. In the past years, a growing number of advanced hyperspectral remote sensing image classification techniques based on manifold learning, sparse representation and deep learning have been proposed and reported a good performance in accuracy and efficiency on state-of-the-art public datasets. However, most existing methods still face challenges in dealing with large-scale hyperspectral image datasets due to their high computational complexity. In this work, we propose an improved spectral clustering method for large-scale hyperspectral image classification without any prior information. The proposed algorithm introduces two efficient approximation techniques based on Nystrom extension and anchor-based graph to construct the affinity matrix. We also propose an effective solution to solve the eigenvalue decomposition problem by multiplicative update optimization. Experiments on both the synthetic datasets and the hyperspectral image datasets were conducted to demonstrate the efficiency and effectiveness of the proposed algorithm.\nTitle:", "model_inf_time": 1.77}, {"id": "41485", "output": "Inferring User Location from Twitter Text and Social Network Structure", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nTwitter has become one of the most popular platforms for sharing user-generated content, which varies from ordinary conversations to information about recent events. Studies have already showed that the content of tweets has a high degree of correlation with what is going on in the real world. A type of event which is commonly talked about in Twitter is traffic. Aiming to help other drivers, many users tweet about current traffic conditions, and there are even user accounts specialized on the subject. With this in mind, this paper proposes a method to identify traffic events and conditions in Twitter, geocode them, and display them on the Web in real time. Preliminary results showed that the method is able to detect neighborhoods and thoroughfares with a precision that varies from 50 to 90%, depending on the number of places mentioned in the tweets.\nTitle:\nTraffic observatory: a system to detect and locate traffic events and conditions using Twitter\n\nAbstract:\nTraditionally, in health surveillance, high risk zones are identified based only on the residence address or the working place of diseased individuals. This provides little information about the places where people are infected, the truly important information for disease control. The recent availability of spatial data generated by geotagged social media posts offers a unique opportunity: by identifying and following diseased individuals, we obtain a collection of sequential geo-located events, each sequence being issued by a social media user. The sequence of map positions implicitly provides an estimation of the users\u2019 social trajectories as they drift on the map. The existing data mining techniques for spatial cluster detection fail to address this new setting as they require a single location to each individual under analysis. In this paper we present two stochastic models with their associated algorithms to mine this new type of data. The Visit Model finds the most likely zones that a diseased person visits, while the Infection Model finds the most likely zones where a person gets infected while visiting. We demonstrate the applicability and effectiveness of our proposed models by applying them to more than 100 million geotagged tweets from Brazil in 2015. In particular, we target the identification of infection hot spots associated with dengue, a mosquito-transmitted disease that affects millions of people in Brazil annually, and billions worldwide. We applied our algorithms to data from 11 large cities in Brazil and found infection hot spots, showing the usefulness of our methods for disease surveillance.\nTitle:\nInfection Hot Spot Mining from Social Media Trajectories.\n\nAbstract:\n  Exploiting the large amount of available data for addressing relevant social problems has been one of the key challenges in data mining. Such efforts have been recently named \"data science for social good\" and attracted the attention of several researchers and institutions. We give a contribution in this objective in this paper considering a difficult public health problem, the timely monitoring of dengue epidemics in small geographical areas. We develop a generative simple yet effective model to connect the fluctuations of disease cases and disease-related Twitter posts. We considered a hidden Markov process driving both, the fluctuations in dengue reported cases and the tweets issued in each region. We add a stable but random source of tweets to represent the posts when no disease cases are recorded. The model is learned through a Markov chain Monte Carlo algorithm that produces the posterior distribution of the relevant parameters. Using data from a significant number of large Brazilian towns, we demonstrate empirically that our model is able to predict well the next weeks of the disease counts using the tweets and disease cases jointly. \nTitle:\nA latent shared-component generative model for real-time disease surveillance using Twitter data\n\nAbstract:\nIn this paper we describe the preliminary results and future directions of a research in progress, which aims at assessing the hashtag effectiveness as a resource for sentiment analysis expressed on Twitter. The results so far support our hypothesis that hashtags may facilitate the detection and automatic tracking of online population sentiment about different events.\nTitle:\nCharacterizing the effectiveness of twitter hashtags to detect and track online population sentiment\n\nAbstract:\nTwitter is a unique social media channel, in the sense that users discuss and talk about the most diverse topics, including their health conditions. In this paper we analyze how Dengue epidemic is reflected on Twitter and to what extent that information can be used for the sake of surveillance. Dengue is a mosquito-borne infectious disease that is a leading cause of illness and death in tropical and subtropical regions, including Brazil. We propose an active surveillance methodology that is based on four dimensions: volume, location, time and public perception. First we explore the public perception dimension by performing sentiment analysis. This analysis enables us to filter out content that is not relevant for the sake of Dengue surveillance. Then, we verify the high correlation between the number of cases reported by official statistics and the number of tweets posted during the same time period (i.e., R2 = 0.9578). A clustering approach was used in order to exploit the spatio-temporal dimension, and the quality of the clusters obtained becomes evident when they are compared to official data (i.e., RandIndex = 0.8914). As an application, we propose a Dengue surveillance system that shows the evolution of the dengue situation reported in tweets, which is implemented in www.observatorio.inweb.org.br/dengue/.\nTitle:\nDengue surveillance based on a computational model of spatio-temporal locality of Twitter.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.497, mean value for all the writers is 8.648, which makes it 0.129 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nOnline social networks are valuable sources of information to monitor real-time events, such as earthquakes and epidemics. For this type of surveillance, users\u2019 location is an essential piece of information, but a substantial number of users choose not to disclose their geographical location. However, characteristics of the users\u05f3 behavior, such as the friends they associate with and the types of messages published may hint on their spatial location. In this paper, we propose a method to infer the spatial location of Twitter users. Unlike the approaches proposed so far, it incorporates two sources of information to learn geographical position: the text posted by users and their friendship network. We propose a probabilistic approach that jointly models the geographical labels and Twitter texts of users organized in the form of a graph representing the friendship network. We use the Markov random field probability model to represent the network, and learning is carried out through a Markov Chain Monte Carlo simulation technique to approximate the posterior probability distribution of the missing geographical labels. We show the accuracy of the algorithm in a large dataset of Twitter users, where the ground truth is the location given by GPS. The method presents promising results, with little sensitivity to parameters and high values of precision.\nTitle:", "model_inf_time": 1.7}, {"id": "41486", "output": "Bayesian Nonparametric Multi-Label Learning via Gamma-Negative Binomial Process.", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nIncorporating the side information of text corpus, i.e., authors, time stamps, and emotional tags, into the traditionaltext mining models has gained significant interests in the area of information retrieval, statistical natural language processing, andmachine learning. One branch of these works is the so-called Author Topic Model (ATM), which incorporates the authors'sinterests as side information into the classical topic model. However, the existing ATM needs to predefine the number of topics, which is difficult and inappropriate in many real-world settings. In this paper, we propose an Infinite Author Topic (IAT) modelto resolve this issue. Instead of assigning a discrete probability on fixed number of topics, we use a stochastic process to determinethe number of topics from the data itself. To be specific, we extend a gamma-negative binomial process to three levels in orderto capture the author-document-keyword hierarchical structure. Furthermore, each document is assigned a mixed gamma processthat accounts for the multi-author's contribution towards this document. An efficient Gibbs sampling inference algorithm witheach conditional distribution being closed-form is developed for the IAT model. Experiments on several real-world datasets showthe capabilities of our IAT model to learn the hidden topics, authors' interests on these topics and the number of topicssimultaneously.\nTitle:\nInfinite Author Topic Model based on Mixed Gamma-Negative Binomial Process.\n\nAbstract:\nGraph mining has been a popular research area because of its numerous application scenarios. Many unstructured and structured data can be represented as graphs, such as, documents, chemical molecular structures, and images. However, an issue in relation to current research on graphs is that they cannot adequately discover the topics hidden in graph-structured data which can be beneficial for both the unsupervised learning and supervised learning of the graphs. Although topic models have proved to be very successful in discovering latent topics, the standard topic models cannot be directly applied to graph-structured data due to the \"bag-of-word\" assumption. In this paper, an innovative graph topic model (GTM) is proposed to address this issue, which uses Bernoulli distributions to model the edges between nodes in a graph. It can, therefore, make the edges in a graph contribute to latent topic discovery and further improve the accuracy of the supervised and unsupervised learning of graphs. The experimental results on two different types of graph datasets show that the proposed GTM outperforms the latent Dirichlet allocation on classification by using the unveiled topics of these two models to represent graphs.\nTitle:\nTopic Model for Graph Mining.\n\nAbstract:\nDue to the semantic gap between low-level visual features and high-level semantic content of images, the methods for image annotation based on low-level visual features, cannot well meet the requirement of knowledge discovery from web images. Therefore, the automatic acquisition for high-level semantic content of image has become a hot research topic. The traditional image annotation methods represent images only by a few keywords, which cannot completely describe and rationally organize the high-level semantics of images, so it will lose a great deal of semantic information. Based on the different levels and different aspects of web images, we propose a new method to express and organize the high-level semantic content of web images. The method expresses the different levels semantic content of one image as a three-level network, composed of background semantic level, complementary semantic level and fine-grained semantic level. The experimental results show that our method is effective and efficient on the image annotation.\nTitle:\nCognition-Based Semantic Annotation for Web Images\n\nAbstract:\nDealing with the large-scale text knowledge on the Web has become increasingly important with the development of the Web, yet it confronts with several challenges, one of which is to find out as much semantics as possible to represent text knowledge. As the text semantic mining process is also the knowledge representation process of text, this paper proposes a text knowledge representation model called text semantic mining model TSMM based on the algebra of human concept learning, which both carries rich semantics and is constructed automatically with a lower complexity. Herein, the algebra of human concept learning is introduced, which enables TSMM containing rich semantics. Then the formalization and the construction process of TSMM are discussed. Moreover, three types of reasoning rules based on TSMM are proposed. Lastly, experiments and the comparison with current text representation models show that the given model performs better than others.\nTitle:\nText Semantic Mining Model Based on the Algebra of Human Concept Learning\n\nAbstract:\nArtificial Intelligence development is stepping into a new era due to the recent exciting achievements from neural network and statistical machine learning research communities. Statistic neural-computing based machine learning has been deemed as one of promising roads towards realizing the ideal of Artificial Intelligence promoted since last century. Learning is the key in making progress. Statistic machine learning is to obtain a probability distribution or a function from a set of training samples according to a certain optimization target over the training cost based on a predefined model. While there are many significant improvements in image, sound and text recognition and analyzing using neural network based learning strategies, a new open question emerges, that is, what is the next? To ask this question is to mean that the neural network solution is not an ultimate solution, and there will be more challenges to meet in coming future. We discussed these aspects of deep neural network research work in this paper and focused on semantics in deep neural-network computing models. We try to browse how semantics or knowledge are to be involved in deep neural network models and how the semantics and knowledge will be a key factor towards making more intelligent machines. We argue that priori semantics and knowledge in modelling a neural network is important, which could be the key for researchers to design intelligent machine models to perform complex tasks.\nTitle:\nSemantics in Deep Neural-Network Computing.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.682, mean value for all the writers is 8.648, which makes it 0.882 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMulti-label learning has become a significant learning paradigm in the past few years due to its broad application scenarios and the ever-increasing number of techniques developed by researchers in this area. Among existing state-of-the-art works, generative statistical models are characterized by their good generalization ability and robustness on large number of labels through learning a low-dimensional label embedding. However, one issue of this branch of models is that the number of dimensions needs to be fixed in advance, which is difficult and inappropriate in many real-world settings. In this paper, we propose a Bayesian nonparametric model to resolve this issue. More specifically, we extend a Gamma-negative binomial process to three levels in order to capture the label-instance-feature structure. Furthermore, a mixing strategy for Gamma processes is designed to account for the multiple labels of an instance. The mixed process also leads to a difficulty in model inference, so an efficient Gibbs sampling inference algorithm is then developed to resolve this difficulty. Experiments on several real-world datasets show the performance of the proposed model on multi-label learning tasks, comparing with three state-of-the-art models from the literature.\nTitle:", "model_inf_time": 2.0}, {"id": "41487", "output": "Efficient Template Removal from Web Collections", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nTemplates are pieces of HTML code common to a set of web pages usually adopted by content providers to enhance the uniformity of layout and navigation of theirs Web sites. They are usually generated using authoring/publishing tools or by programs that build HTML pages to publish content from a database. In spite of their usefulness, the content of templates can negatively affect the quality of results produced by systems that automatically process information available in web sites, such as search engines, clustering and automatic categorization programs. Further, the information available in templates is redundant and thus processing and storing such information just once for a set of pages may save computational resources. In this paper, we present and evaluate methods for detecting templates considering a scenario where multiple templates can be found in a collection of Web pages. Most of previous work have studied template detection algorithms in a scenario where the collection has just a single template. The scenario with multiple templates is more realistic and, as it is discussed here, it raises important questions that may require extensions and adjustments in previously proposed template detection algorithms. We show how to apply and evaluate two template detection algorithms in this scenario, creating solutions for detecting multiple templates. The methods studied partitions the input collection into clusters that contain common HTML paths and share a high number of HTML nodes and then apply a single-template detection procedure over each cluster. We also propose a new algorithm for single template detection based on a restricted form of bottom-up tree-mapping that requires only small set of pages to correctly identify a template and which has a worst-case linear complexity. Our experimental results over a representative set of Web pages show that our approach is efficient and scalable while obtaining accurate results.\nTitle:\nOn Finding Templates on Web Collections\n\nAbstract:\nAn important requirement for emerging applications which aim to locate and integrate content distributed over the Web is to identify pages that are relevant for a given domain or task. In this paper, we address the problem of identifying pages that contain objects with a latent structure, i.e., the structure is implicitly represented in the page. We propose an algorithm which, given a set of instances of an object type, derives rules by automatically extracting statistically significant patterns present inside the objects. These rules can then be used to detect the presence of these objects in new, unseen pages. Our approach has several advantages when compared against learning-based text classifiers. Because it relies only on positive examples, constructing accurate object detectors is simpler than constructing learning classifiers, which require both positive and negative examples. Also, besides providing a classification decision for the presence of an object, the derived detectors are able to pinpoint the location of the object inside a Web page. This enables our algorithm to extract additional object fragments and apply online learning to automatically update the rules as new documents become available. An experimental evaluation, using a representative set of domains, indicates that our approach is effective. It is able to learn structural patterns and derive detectors that outperform state-of-art text classifiers and the online learning component leads to substantial improvements over the initial detectors.\nTitle:\nUsing latent-structure to detect objects on the web\n\nAbstract:\nIn this paper we describe a new approach to extract element labels from Web form interfaces. Having these labels is a requirement for several techniques that attempt to retrieve and integrate information that is hidden behind form interfaces, such as hidden Web crawlers and metasearchers. However, given the wide variation in form layout, even within a well-defined domain, automatically extracting these labels is a challenging problem. Whereas previous approaches to this problem have relied on heuristics and manually specified extraction rules, our technique makes use of a learning classifier ensemble to identify element-label mappings; and it applies a reconciliation step which leverages the classifier-derived mappings to boost extraction accuracy. We present a detailed experimental evaluation using over three thousand Web forms. Our results show that our approach is effective: it obtains significantly higher accuracy and is more robust to variability in form layout than previous label extraction techniques.\nTitle:\nLearning to extract form labels\n\nAbstract:\nIn this paper we address the problem of organizing hidden-Web databases. Given a heterogeneous set of Web forms that serve as entry points to hidden-Web databases, our goal is to cluster the forms according to the database domains to which they belong. We propose a new clustering approach that models Web forms as a set of hyperlinked objects and considers visible information in the form context-both within and in the neighborhood of forms-as the basis for similarity comparison. Since the clustering is performed over features that can be automatically extracted, the process is scalable. In addition, because it uses a rich set of metadata, our approach is able to handle a wide range of,forms, including content-rich forms that contain multiple attributes, as well as simple keyword-based search inter-faces. An experimental evaluation over real Web data shows that our strategy generates high-quality clusters-measured both in terms of entropy and F-measure. This indicates that our approach provides an effective and general solution to the problem of organizing hidden-Web databases.\nTitle:\nOrganizing Hidden-Web Databases By Clustering Visible Web Documents\n\nAbstract:\nWe describe a machine-learning-based approach for extracting attribute labels from Web form interfaces. Having these labels is a requirement for several techniques that attempt to retrieve and integrate data that reside in online databases and that are hidden behind form interfaces, including schema matching and clustering, and hidden-Web crawlers. Whereas previous approaches to this problem have relied on heuristics and manually specified extraction rules, our technique makes use of learning classifiers to identify form labels. Our preliminary experiments show this approach is promising and has high accuracy.\nTitle:\nAutomatically Extracting Form Labels\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 7.623, mean value for all the writers is 8.648, which makes it 0.875 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe widespread use of templates on the Web is considered harmful for two main reasons. Not only do they compromise the relevance judgment of many web IR and web mining methods such as clustering and classification, but they also negatively impact the performance and resource usage of tools that process web pages. In this paper we present a new method that efficiently and accurately removes templates found in collections of web pages. Our method works in two steps. First, the costly process of template detection is performed over a small set of sample pages. Then, the derived template is removed from the remaining pages in the collection. This leads to substantial performance gains when compared to previous approaches that combine template detection and removal. We show, through an experimental evaluation, that our approach is effective for identifying terms occurring in templates - obtaining F-measure values around 0.9, and that it also boosts the accuracy of web page clustering and classification methods.\nTitle:", "model_inf_time": 1.41}, {"id": "41488", "output": "Spatiotemporal Invariant Feature Detection via Video Volume Phase Congruency", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nGait recognition has already achieved satisfactory performance on small databases under ideal conditions. Most of the existing approaches represent gait pattern using a locomotion model or statistic model of human silhouette. However, it is still a challenging task to conduct human gait identification under variations of clothing and carrying condition in real scenes. In this paper, an adaptive part-based feature selection method is proposed to filter out interference feature blocks and a matching procedure is performed to identify the correct subject. Compared with the state-of-the-art methods on a large standard dataset, the proposed method shows an encouraging computational complexity reduction and performance improvement in identification rates.\nTitle:\nPart-based human gait identification under clothing and carrying condition variations\n\nAbstract:\nIn recent years, a great deal of efforts has been made to develop objective image quality assessment (IQA) that shows consistency with subjective quality evaluation. After structure similarity was found to be an important factor, IQA metrics such as SSIM showed a better performance than traditional ones. With the usage of phase congruency (PC) to obtain structure similarity measurement, a new metric named FSIM acquired even higher consistency. This paper is motivated to propose an improved structure similarity metric based on FSIM, where quaternion wavelet transform is exploited to extract the quaternion phase congruency map to represent the essential image structures. In addition, complex phase congruency map obtained as the co-product during quaternion wavelet transform, supplies complementary visual effects of detailed structure on image quality assessment. The proposed image quality assessment metric based on hybrid phase congruency (HPC) (IQA_HPC) is highlighted in emphasizing saliency distribution of different image structures to image quality assessment. As compared with the state-of-the art IQA methods, experimental results demonstrate that the new index IQA_HPC is able to gain a higher consistency with the subjective measurement.\nTitle:\nAn image quality assessment metric based on quaternion wavelet transform\n\nAbstract:\nSingle image super-resolution (SR) is a severely unconstrained task. While the self-example-based methods are able to reproduce sharp edges, they perform poorly for textures. For recovering the fine details, higher-level image segmentation and corresponding external texture database are employed in the example-based SR methods, but they involve too much human interaction. In this paper, we discuss the existing problems of example-based technique using scale space analysis. Accordingly, a robust pixel classification method is designed based on the phase congruency model in scale space, which can effectively divide images into edges, textures and flat regions. Then a super-resolution framework is proposed, which can adaptively emphasize the importance of high-frequency residuals in structural examples and scale invariant fractal property in textural regions. Experimental results show that our SR approach is able to present both sharp edges and vivid textures with few artifacts.\nTitle:\nSingle image super-resolution via phase congruency analysis\n\nAbstract:\nIn many image and computer vision applications, shadows interfere with fundamental tasks such as moving objects segmentation and tracking. In this paper, a novel method is proposed to detect the moving cast shadows in the scene. The normalized coefficients of orthogonal transform of image block are proved to be illumination invariant and are used to classify moving shadows and foreground objects. Five kinds of orthogonal transform: DCT, DFT, Haar Transform, SVD and Hadamard Transform, are utilized in our work to detect moving cast shadows. Experimental results show that the proposed method succeeds in detecting moving cast shadows within indoor and outdoor environments.\nTitle:\nDetection of moving cast shadows using image orthogonal transform\n\nAbstract:\nPhase map of the images captures the most fundamental cognitive features and thus is widely used in various digital image processing tasks. In this paper, we propose the Log Gabor Phase Similarity (LGPS), a novel full reference image quality assessment metrics based on measuring of similarities between phases in log Gabor transform domain. Phase can capture any changes in image details regardless of the fluctuation in contrast, and the similarity between phase maps provides a measure of the perceptual quality of images. An image is firstly decomposed by a filter bank consisting of a pair of log Gabor filters. The phase maps are then computed from the responses of each filter pair. We have developed a window-based similarity metric to evaluate the resemblance between phase maps so as to measure the quality of the image. Experimental results and comparative studies suggest that LGPS can be used to predict the perceived quality of images with different distortions.\nTitle:\nLgps: Phase Based Image Quality Assessment Metric\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.494, mean value for all the writers is 8.648, which makes it 0.722 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn applications of behavior recognition, the use of spatiotemporal invariant feature points can improve the robustness to noise, illumination and geometric distortions. In this paper, we develop a novel detection model of spatiotemporal invariant feature by generalizing the notion of image phase congruency to video volume phase congruency. The proposed model detects feature points by measuring the spatiotemporal phase congruency of Fourier series components along with their characteristic scale and principal orientation. Compared with other state-of-the-art methods, the key advantages of this interest point detector include the invariance to contrast variations and more precise feature location. Furthermore, an invariant feature descriptor is advanced based on the phase congruency map, resulting in enhanced discriminative power in classification tasks. Experimental results on KTH human motion dataset demonstrate the validity and effectiveness of the extracted invariant features in the human behavior recognition scheme.\nTitle:", "model_inf_time": 1.77}, {"id": "41489", "output": "Model Transformation for Migrating Legacy Automotive Models to AUTOSAR", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMany companies in the automotive industry have adopted MDD in their vehicle control software development. As a major automotive company, General Motors has been using a custom-built, domain-specific modeling language, implemented as an internal proprietary metamodel, to meet the modeling needs in its control software development. As AUTOSAR (AUTomotive Open System ARchitecture) is being developed as a standard to ease the process of integrating components provided by different suppliers and manufacturers, there is a growing demand to migrate these GM-specific, legacy models to AUTOSAR models. Given that AUTOSAR defines its own metamodel for various system artifacts in automotive software development, we explore using model transformations to address the challenges in migrating GM legacy models to their AUTOSAR equivalents. As a case study, we have built a model transformation using the MDWorkbench tool and the Atlas Transformation Language (ATL). This paper reports on the case study, makes observations based on our experience to assist in the development of similar types of transformations, and provides recommendations for further research.\nTitle:\nModel transformations for migrating legacy models: an industrial case study\n\nAbstract:\nThe notion of model transformation intent is proposed to capture the purpose of a transformation. In this paper, a framework for the description of model transformation intents is defined, which includes, for instance, a description of properties a model transformation has to satisfy to qualify as a suitable realization of an intent. Several common model transformation intents are identified, and the framework is used to describe six of them in detail. A case study from the automotive industry is used to demonstrate the usefulness of the proposed framework for identifying crucial properties of model transformations with different intents and to illustrate the wide variety of model transformation intents that an industrial model-driven software development process typically encompasses.\nTitle:\nModel Transformation Intents and Their Properties\n\nAbstract:\nModel transformation lies at the very core of model-driven engineering, and a large number of model transformation languages and tools have been proposed over the last few years. These tools can be used to develop, transform, merge, exchange, compare, and verify models and metamodels. In this paper, we present a comprehensive catalog of existing metamodel-based transformation tools and compare them using a qualitative framework. We begin by organizing the 60 tools we identified into a general classification based on the transformation approach used. We then compare these tools using a number of particular facets, where each facet belongs to one of six different categories and may contain several attributes. The results of the study are discussed in detail and made publicly available in a companion website with a capability to search for tools using the specified facets as search criteria. Our study provides a thorough picture of the state-of-the-art in model transformation techniques and tools. Our results are potentially beneficial to many stakeholders in the modeling community, including practitioners, researchers, and transformation tool developers.\nTitle:\nSurvey and classification of model transformation tools\n\nAbstract:\nModel Driven Development and Use Case Driven Development methodologies have inspired the proposal of a variety of software engineering approaches that synthesize state-based models from scenario-based models. However, little work has been done to comprehensively compare these different synthesis approaches. In this paper, we define a set of comparison criteria, and survey 21 different synthesis approaches presented in the literature based on the criteria. The differences and similarities are highlighted in the comparison results. We then discuss the challenges that current approaches may face and provide suggestions for future work for state-based model syntheses.\nTitle:\nA comparative survey of scenario-based to state-based model synthesis approaches\n\nAbstract:\nModel Driven Development (MDD) is a software engineering approach in which models constitute the basic units of software development. A key part of MDD is the notion of automated model transformation, in which models are stepwise refined into more detailed models, and eventually into code. The correctness of transformations is essential to the success of MDD, and while much research has concentrated on formal verification, testing remains the most efficient method of validation. Transformation testing is however different from testing code, and presents new challenges. In this paper, we survey the model transformation testing phases and the approaches proposed in the literature for each phase.\nTitle:\nModel transformation testing: the state of the art\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.58, mean value for all the writers is 8.648, which makes it 0.795 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMany companies in the automotive industry have adopted model-driven development in their vehicle software development. As a major automotive company, General Motors (GM) has been using a custom-built, domain-specific modeling language, implemented as an internal proprietary metamodel, to meet the modeling needs in its control software development. Since AUTomotive Open System ARchitecture (AUTOSAR) has been developed as a standard to ease the process of integrating components provided by different suppliers and manufacturers, there has been a growing demand to migrate these GM-specific, legacy models to AUTOSAR models. Given that AUTOSAR defines its own metamodel for various system artifacts in automotive software development, we explore applying model transformations to address the challenges in migrating GM-specific, legacy models to their AUTOSAR equivalents. As a case study, we have built and validated a model transformation using the MDWorkbench tool, the Atlas Transformation Language, and the Metamodel Coverage Checker tool. This paper reports on the case study, makes observations based on our experience to assist in the development of similar types of transformations, and provides recommendations for further research.\nTitle:", "model_inf_time": 1.65}, {"id": "41490", "output": "AXI-Based Environment for Post-Silicon Validation and Debugging", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWith a significant increase in the complexity of cores and their intercommunications, there is a need to review and enhance traditional debug methods for System on Chips (SoCs). As new SoCs tend to have many cores, the interactions among cores through functional interconnects such as bus or Network on Chips (NoCs) are becoming so complex. Therefore, debug techniques should address not only validation of the computational part of a design but such techniques have to monitor and validate the communication and synchronization among cores inside SoCs. In this paper, we consider NoC as a functional interconnection among cores and propose debug aware network interface (NI) which is compatible with AXI standard. The proposed interface enables provides a mechanism for cross-trigger debugging. Transactions issued by a processing element connected to the proposed debug aware NI are monitored by the proposed cross-trigger unit and trace data and trigger events will be extracted and routed to another processing element or Shared Debugging Unit (SDU). SDU combines debug traces from different processing elements. The major benefits of using our proposed architectures for debugging over traditional techniques are as follows: 1) the proposed debug aware NI can detect, mark and bypass severe faulty conditions such as deadlocks resulting from design errors or electrical faults in real time 2) there is no need for a large internal trace memory inside processing element because SDU can communicate to the external memory 3) debugging of applications which are running on multiple processors can facilitate by means of available features inside the proposed trigger mechanism.\nTitle:\nDebug Aware AXI-based Network Interface\n\nAbstract:\nPost-silicon debugging process is aimed at locating errors not detected during the process of pre-silicon verification. Although in the post-silicon validation engineers can exploit the high speed of hardware prototype to exercise huge amount of test vectors, low level of real-time observability and controllability of signals inside the prototype is a big issue. Various Design for Debug (DFD) techniques aim to improve the observability of signals and expedite the root cause analysis of errors. Typical practical DFD approaches are based on the Embedded Logic Analysis (ELA), using a trigger unit that can effectively control when to acquire the debug data. In this paper, we propose a hierarchical trigger generator that builds a trigger unit. Additionally, it provides resourceful and compact trace information for root cause analysis. Major advantages over traditional trigger units are: 1) by keeping the trace of interactions that leads to the failure, it facilitates the process of failure localization and root-cause analysis 2) it can be tuned for the specific location of a design to avoid the huge cost related to interfacing with trace signals 3) it can get parameterized to generate several units that can be placed inside the limited area in multiple debug rounds using a time-multiplex fashion.\nTitle:\nHierarchical Embedded Logic Analyzer for Accurate Root-Cause Analysis\n\nAbstract:\nIt has become indispensable to locate circuit defects and find the root-cause of errors as soon as the prototype of a system (first-silicon) gets ready. Various Design-for-Debug (DfD) solutions have been introduced as a means to increase the observability and controllability of internal signals, resulting to a speed-up in debugging process and a decrease in the time-to-market of new products. Assertion Based Verification (ABV) is one of the instrumental pre-silicon verification techniques. Once assertions are converted to hardware modules and incorporated into a debug infrastructure, the post-silicon debug can benefit from the additional observability provided by such assertions. In this paper, we first propose a new algorithm that generates clusters of assertion-checkers; in our proposed clustering algorithm, we resort to a graph partitioning algorithm to find the assertion-checkers that can be placed inside a cluster. The proposed method generates the clusters of assertion-checkers by means of exploring the logic-cones set of each assertion-checker. Moreover, coverage metrics for different configurations of clusters are defined. Then, we introduce several mechanisms through which the clusters of assertion-checkers can be incorporated into the DfD infrastructures. In our experiments, several case studies such as AXI bus, PCI bus protocol and a memory controller are considered; thereafter, the proposed debug infrastructure containing clusters of assertion-checkers is embedded into such case studies. It turns out that contrary to a non-clustering approach of placing assertion-checkers into a design the clustering algorithm along with the proposed method for incorporating assertion-checker clusters into a debug infrastructure lead to the better results in terms of the energy consumption and design coverage.\nTitle:\nAn infrastructure for debug using clusters of assertion-checkers.\n\nAbstract:\nThe High-Speed Serial Interface (HSSI) is a cornerstone of the modern communications. To achieve high data rates, sophisticated techniques such as equalization and pre-compensation have now become common in HSSIs. With the concurrent increasing of design complexity and decreasing of the timing budget, the post-silicon validation, debugging and testing of HSSIs are becoming critical. This paper presents a versatile scheme to accelerate the post-silicon validation. Using a novel jitter injection scheme and an FPGA-based Bit Error Rate Tester (BERT), we can validate and test HSSIs without the need of high-speed Automatic Test Equipment (ATE) instruments and Design-for-Test (DFT) features; this scheme also overcomes existing ATE instrument limitations. We can also utilize ATE to provide a more versatile scheme for HSSI validation, debugging and testing.\nTitle:\nA versatile scheme for the validation, testing and debugging of High Speed Serial Interfaces\n\nAbstract:\nDecrease in the Integrated Circuit (IC) feature sizes leads to the increase in the susceptibility to transient and permanent errors. The growing rate of such errors in ICs intensifies the need for a wide range of solutions addressing reliability at various levels of abstractions. Network on Chip (NoC) architecture has been introduced to address the increasing demand for communication bandwidth among processing cores. The structural redundancy inherited in NoC-based system can be leveraged to improve reliability and compensate for the effects of failures. In this paper, we propose a fault-tolerant NoC router NISHA, which stands for No-deadlock Interconnection of Subnets in Hierarchical Architectures. Armed with a new flow control mechanism, as well as an enhanced Virtual Channel (VC) regulator, the proposed router can mitigate the effects of both transient and permanent errors. A Dynamic/Static virtual channel allocation with respect to the local and global traffic is supported in NISHA; thereby, it maintains a deadlock-free state in the presence of routers or link failures in hierarchical topologies. Experimental results show an enhanced operation of NoC applications as well as the decrease in the average latency and energy consumption.\nTitle:\nNISHA: A fault-tolerant NoC router enabling deadlock-free Interconnection of Subnets in Hierarchical Architectures\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.151, mean value for all the writers is 8.648, which makes it 0.429 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWith a significant increase in the design complexity of cores and associated communication among them, post-silicon validation has become a demanding task in System on Chips (SoCs) design. To ensure that final products are fault-free and ready for market, the post-silicon validation goal is to catch bugs and pinpoint the root causes of errors that could escape from pre-silicon verification tools. Post-silicon validation involves running a hardware prototype in an environment that is similar to its final platform with its expected workload. As new SoCs tend to have many cores, the interactions among these cores are becoming so complex that post-silicon debug techniques should address not only validation of the functional aspects of a design but such techniques have to \"bulletproof\" the communication and synchronization among cores inside an SoC. In this paper, we propose an AXI based environment for post-silicon validation. The proposed environment involves Local Debugging Unit (LDU) and Shared Debugging Unit (SDU). LDU monitors trace of transactions issued by the hardware prototype and detect undesired conditions on bus. SDU combines debug traces from different LDUs. We embed the proposed SDU inside an AXI configurable interconnect. Major benefits of using our proposed debug platform over traditional techniques for silicon validation are as follows: 1) it detects and bypasses real time severe faulty conditions such as deadlocks resulting from design errors or electrical faults 2) there is no need for internal trace memory because SDU can communicate to the external memory through slave ports 3) it enables online monitoring of the trace buffer.\nTitle:", "model_inf_time": 1.9}, {"id": "41491", "output": "A fifth-order iterative method for nonlinear systems based on the Midpoint method", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWe derive new iterative methods with order of convergence four or higher, for solving nonlinear systems, by composing iteratively golden ratio methods with a modified Newton\u2019s method. We use different efficiency indices in order to compare the new methods with other ones and present several numerical tests which confirm the theoretical results.\nTitle:\nEfficient high-order methods based on golden ratio for nonlinear systems\n\nAbstract:\nIn this paper, we study a new family of iterative methods for solving nonlinear equations with sixth and seventh order convergence. The new methods are obtained by composing known methods of third and fourth order with Newton's method and using an adequate approximation for the last derivative, which provides high order of convergence and reduces the required number of functional evaluations per step. The new methods attain efficiency indices of 1.5651 and 1.6266, which makes them competitive. We introduce a new efficiency index involving the computational effort as well as the functional evaluations per iteration. We use this new index, in combination with the usual efficiency index, in order to compare the methods described in the paper with other known methods and present several numerical tests.\nTitle:\nA family of iterative methods with sixth and seventh order convergence for nonlinear equations\n\nAbstract:\nIn this work we introduce a technique for solving nonlinear systems that improves the order of convergence of any given iterative method which uses the Newton iteration as a predictor.\nTitle:\nIncreasing the convergence order of an iterative method for nonlinear systems.\n\nAbstract:\nSome variants of Newton's Method are developed in this work in order to solve systems of nonlinear equations, based in trapezoidal and midpoint rules of quadrature. We prove the quadratic convergence of one of these methods. Moreover, different numeric tests confirm theoretic results and allow us to compare these variants with Newton's classical method.\nTitle:\nVariants of Newton's method for functions of several variables\n\nAbstract:\nA family of multi-point iterative methods for solving nonlinear equations is described and the order of convergence of its elements is studied. The computational efficiency of some elements of this family, in terms of the number of function evaluations, is also provided. In addition, we present different numerical tests, which confirm or improve the theoretical results and allow us to compare some methods of this family.\nTitle:\nA class of multi-point iterative methods for nonlinear equations\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.7, mean value for all the writers is 8.648, which makes it 1.751 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe present a new iterative method of order of convergence 5, for solving nonlinear systems, by composing the Midpoint method with Newton's method and using an approximation for the Jacobian matrix in order to reduce the required number of functional evaluations per iteration. In addition, we compare the efficiency index of these methods with that of Newton's method and present several numerical tests, which confirm the theoretical results and allow us to compare these variants with Newton's method.\nTitle:", "model_inf_time": 1.51}, {"id": "41492", "output": "Fractal Dimension Analysis of Optimal Iterative Schemes for Nonlinear Equations", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper focuses on solving systems of nonlinear equations numerically. We propose an efficient iterative scheme including two steps and fourth order of convergence. The proposed method does not require the evaluation of second or higher order Frechet derivatives per iteration to proceed and reach fourth order of convergence. Finally, numerical results illustrate the efficiency of the method.\nTitle:\nOn a Novel Fourth-Order Algorithm for Solving Systems of Nonlinear Equations.\n\nAbstract:\nnew two-parametric family of derivative-free iterative methods for solving nonlinear equations is presented. First, a new biparametric family without memory of optimal order four is proposed. The improvement of the convergence rate of this family is obtained by using two self-accelerating parameters. These varying parameters are calculated in each iterative step employing only information from the current and the previous iteration. The corresponding R-order is 7 and the efficiency index 71/3 = 1.913. Numerical examples and comparison with some existing derivative-free optimal eighth-order schemes are included to confirm the theoretical results. In addition, the dynamical behavior of the designed method is analyzed and shows the stability of the scheme.\nTitle:\nAn efficient two-parametric family with memory for nonlinear equations\n\nAbstract:\nIn this work we introduce a technique for solving nonlinear systems that improves the order of convergence of any given iterative method which uses the Newton iteration as a predictor.\nTitle:\nIncreasing the convergence order of an iterative method for nonlinear systems.\n\nAbstract:\nIn this paper, a unified point of view that includes the most of one-point Newton-type iterative methods for solving nonlinear equations is introduced. A simple idea to design iterative methods with quadratic or cubic convergence is also described. This idea is extended to construct one-point iterative methods of order four. In addition, several numerical examples are given to illustrate and compare different known methods and some introduced by using this unifying idea.\nTitle:\nOne-point Newton-type iterative methods: A unified point of view.\n\nAbstract:\nA family of multi-point iterative methods for solving nonlinear equations is described and the order of convergence of its elements is studied. The computational efficiency of some elements of this family, in terms of the number of function evaluations, is also provided. In addition, we present different numerical tests, which confirm or improve the theoretical results and allow us to compare some methods of this family.\nTitle:\nA class of multi-point iterative methods for nonlinear equations\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.382, mean value for all the writers is 8.648, which makes it 1.479 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIn this paper, the dynamical behavior of different optimal iterative schemes for solving nonlinear equations with increasing order, is studied. The tendency of the complexity of the Julia set is analyzed and referred to the fractal dimension. In fact, this fractal dimension can be shown to be a powerful tool to compare iterative schemes that estimate the solution of a nonlinear equation. Based on the box-counting algorithm, several iterative derivative-free methods of different convergence orders are compared.\nTitle:", "model_inf_time": 1.32}, {"id": "41493", "output": "D-OBE: A Distributed Office By Example Language", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nDevelopers of a Wall Street financial application were able to exploit a relational DBMS to advantage for some data management tasks (the good). For others, the relational system was not helpful (the bad), or could be pressed into service only by means of major or minor contortions (the ugly). The authors identify database constructs that would have simplified developing the application (the ideal).\nTitle:\nUsing a relational system on Wall Street: the good, the bad, the ugly, and the ideal\n\nAbstract:\nBackground: Finding the subgraphs of a graph database that are isomorphic to a given query graph has practical applications in several fields, from cheminformatics to image understanding. Since subgraph isomorphism is a computationally hard problem, indexing techniques have been intensively exploited to speed up the process. Such systems filter out those graphs which cannot contain the query, and apply a subgraph isomorphism algorithm to each residual candidate graph. The applicability of such systems is limited to databases of small graphs, because their filtering power degrades on large graphs. Results: In this paper, SING (Subgraph search In Non-homogeneous Graphs), a novel indexing system able to cope with large graphs, is presented. The method uses the notion of feature, which can be a small subgraph, subtree or path. Each graph in the database is annotated with the set of all its features. The key point is to make use of feature locality information. This idea is used to both improve the filtering performance and speed up the subgraph isomorphism task. Conclusions: Extensive tests on chemical compounds, biological networks and synthetic graphs show that the proposed system outperforms the most popular systems in query time over databases of medium and large graphs. Other specific tests show that the proposed system is effective for single large graphs.\nTitle:\nSING: Subgraph search In Non-homogeneous Graphs.\n\nAbstract:\nFinancial trading strategies are based on queries over time-ordered data. The strategies value very recent data over older data, but require information about older data to avoid making poor decisions. One can imagine a streaming architecture that keeps a synopsis of older information available for many possible queries, but this may be too crude - and too expensive - an approximation of the query require- ments. Instead, we show several fundamental query types for which traders would prefer to issue an ad hoc query Q and then allow updates to change the answer to Q over time. To make the ad hoc portion fast, the architecture puts historical data into a well-structured form (organized by security and time) to support rapid querying whereas recent data is ordered by time alone. Periodically, some of the older recent data is moved to the historical data structures. The net effect is to allow queries to look at very recent and less recent data efficiently and only when needed. Several new research issues arise in this setting.\nTitle:\nThe Virtues and Challenges of Ad Hoc + Streams Querying in Finance\n\nAbstract:\nData reconciliation is the process of matching records across different databases. Data reconciliation requires \u201cjoining\u201d on fields that have traditionally been non-key fields. Generally, the operational databases are of sufficient quality for the purposes for which they were initially designed but since the data in the different databases do not have a canonical structure and may have errors, approximate matching algorithms are required.\nTitle:\nEfficient data reconciliation\n\nAbstract:\nConsider the class of distributed database systems consisting of a set of nodes connected by a high bandwidth network. Each node consists of a processor, a random access memory, and a slower but much larger memory such as a disk. There is no shared memory among the nodes. The data are horizontally partitioned often using a hash function. Such a description characterizes many parallel or distributed database systems that have recently been proposed, both commercial and academic. We study the optimization problem that arises when the query processor must repartition the relations and intermediate results participating in a multijoin query. Using estimates of the sizes of intermediate relations, we show (1) optimum solutions for closed chain queries; (2) the NP-completeness of the optimization problem for star, tree, and general graph queries; and (3) effective heuristics for these hard cases.Our general approach and many of our results extend to other attribute partitioning schemes, for example, sort-partitioning on attributes, and to partitioned object databases.\nTitle:\nOptimizing equijoin queries in distributed databases where relations are hash partitioned\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.505, mean value for all the writers is 8.648, which makes it 0.122 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe D-OBE (Distributed Office By Example) language for distributed office information systems is introduced. D-OBE is an extension of the OBE (Office By Example) and QBE (Query By Example) languages. A major problem was the design of a language simple enough for office workers and yet sufficiently powerful to handle the many facilities of a distributed office system. It is suggested that D-OBE achieves these goals by employing the proven user friendly QBE interface. Surprisingly few extensions to QBE were needed. The states of the servers are presented to the user as QBE tables that are manipulated by the familiar QBE operations. Another problem was to design D-OBE such that it would fit \u2018any\u2019 office. D-OBE is based on the observation that large offices divide their tasks among departments. Each department is made responsible for accomplishing its tasks, and is given control over the facilities to do it. A department will thus control its \u2018D-OBE cluster\u2019, which is a number of workstations connected to a database, mail, hardcopy and name server. Such a cluster may cooperate with other clusters. The use of logical ports help to reconfigure the network as the structure and goals of the office evolve. A naming server provides the \u2018yellow pages\u2019 of the network services. D-OBE identifies for each data object, workstation and server the person responsible for its proper handling.\nTitle:", "model_inf_time": 1.66}, {"id": "41494", "output": "Nonnegative Curds and Whey for Sparse Image Representation", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMultimedia content analysis and management are a promising and challenging theme. In this paper we develop a novel approach\n to image representation, which we call group sparse representation (GSR), for image classification and video retrieval. The\n basic idea is to represent a test image as a weighted combination of all the training images. In particular, we introduce\n two sets of weight coefficients, one for each training image and the other for each class. Moreover, we formulate our concern\n as a group nonnegative garrote model. The resulting representations are sparse, and they are appropriate for discriminant\n analysis. Experiments on Caltech101 and PASCAL VOC2008 image dataset and TRECVID2005 video corpus testify that our proposed\n approach is efficient and effective.\nTitle:\nGroup sparse representation for image categorization and semantic video retrieval.\n\nAbstract:\nNon-negative tensor factorization (NTF) has attracted great attention in the machine learning community. In this paper, we extend traditional non-negative tensor factorization into a supervised discriminative decomposition, referred as Supervised Non-negative Tensor Factorization with Maximum-Margin Constraint (SNTFM2). SNTFM2 formulates the optimal discriminative factorization of non-negative tensorial data as a coupled least-squares optimization problem via a maximum-margin method. As a result, SNTFM2 not only faithfully approximates the tensorial data by additive combinations of the basis, but also obtains a strong generalization power to discriminative analysis (in particular for classification in this paper). The experimental results show the superiority of our proposed model over state-of-the-art techniques on both toy and real world data sets. Copyright \u00a9 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nTitle:\nSupervised Nonnegative Tensor Factorization with Maximum-Margin Constraint.\n\nAbstract:\nWith the explosive growth of multimedia data in the web, multi-label image annotation has been attracted more and more attention. Although the amount of available data is large and growing, the number of labeled data is quite small. This paper proposes an approach to utilize both unlabeled data in target domain and labeled data in auxiliary domain to boost the performance of image annotation. Moreover, since different kinds of heterogeneous features in images have different intrinsic discriminative power for image understanding, group sparsity is introduced in our approach to effectively utilize those heterogeneous visual features with data of target and auxiliary domains. We call this approach semi-supervised cross-domain learning with group sparsity (S^2CLGS). The strength of the proposed S^2CLGS method for multi-label image annotation is to integrate semi-supervised discriminant analysis, cross-domain learning and sparse coding together. Experiments demonstrate the effectiveness of S^2CLGS in comparison with other image annotation algorithms.\nTitle:\nImage annotation by semi-supervised cross-domain learning with group sparsity\n\nAbstract:\nIn this paper we propose a robust learning-based face hallucination algorithm, which predicts a high-resolution face image from an input low-resolution image. It can be utilized for many computer vision tasks, such as face recognition and face tracking. With the help of a database of other high-resolution face images, we use a steerable pyramid to extract multi-orientation and multi-scale information of local low-level facial features both from the input low-resolution face image and other high-resolution ones, and use a pyramid-like parent structure and local best match approach to estimate the best prior; then, this prior is incorporated into a Bayesian maximum a posterior (MAP) framework, and finally the high-resolution version is optimized by a steepest decent algorithm. The experimental results show that we can enhance a 24x32 face image into a 96x128 one while the visual effect is relatively good.\nTitle:\nSteerable pyramid-based face hallucination\n\nAbstract:\nDifferent kinds of high-dimensional visual features can be extracted from a single image. Images can thus be treated as multiple view data when taking each type of extracted high-dimensional visual feature as a particular understanding of images. In this paper, we propose a framework of sparse unsupervised dimensionality reduction for multiple view data. The goal of our framework is to find a low-dimensional optimal consensus representation from multiple heterogeneous features by multiview learning. In this framework, we first learn low-dimensional patterns individually from each view, considering the specific statistical property of each view. We construct a low-dimensional optimal consensus representation from those learned patterns, the goal of which is to leverage the complementary nature of the multiple views. We formulate the construction of the low-dimensional consensus representation to approximate the matrix of patterns by means of a low-dimensional consensus base matrix and a loading matrix. To select the most discriminative features for the spectral embedding of multiple views, we propose to add an $\\\\ell_{1}$-norm into the loading matrix's columns and impose orthogonal constraints on the base matrix. We develop a new alternating algorithm, i.e., spectral sparse multiview embedding, to efficiently obtain the solution. Each row of the loading matrix encodes structured information corresponding to multiple patterns. In order to gain flexibility in sharing information across subsets of the views, we impose a novel structured sparsity-inducing norm penalty on the loading matrix's rows. This penalty makes the loading coefficients adaptively load shared information across subsets of the learned patterns. We call this method structured sparse multiview dimensionality reduction. Experiments on a toy benchmark image data set and two real-world Web image data sets demonstrate the effectiveness of the proposed algorithms.\nTitle:\nSparse Unsupervised Dimensionality Reduction for Multiple View Data\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.691, mean value for all the writers is 8.648, which makes it 0.037 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nIt has been of great interest to find sparse and/or nonnegative representations in computer vision literature. In this paper we propose a novel method to such a purpose and refer to it as nonnegative curds and whey (NNCW). The NNCW procedure consists of two stages. In the first stage we consider a set of sparse and nonnegative representations of a test image, each of which is a linear combination of the images within a certain class, by solving a set of regression-type nonnegative matrix factorization problems. In the second stage we incorporate these representations into a new sparse and nonnegative representation by using the group nonnegative garrote. This procedure is particularly appropriate for discriminant analysis owing to its supervised and nonnegativity nature in sparsity pursuing. Experiments on several benchmark face databases and Caltech 101 image dataset demonstrate the efficiency and effectiveness of our nonnegative curds and whey method.\nTitle:", "model_inf_time": 1.7}, {"id": "41495", "output": "Style-Preserving 2D Cartoon Synthesis via Non-Negative Style Factorization", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\n2D cartoon plays an important role in many areas, but it requires effective methods to relieve manual labors. In this paper, we propose a heterogeneous cartoon gesture recognition method with applications. Firstly, heterogeneous features with different dimensions are assigned to express cartoon and human-subject images according to their characteristics. Then for recognition, we simultaneously integrate shared structure learning (SSL) and graph-based transductive learning into a joint framework to learn reliable classifiers on heterogeneous features. Provided with the framework, the similarities between cartoon and human-subject gestures can be quantitatively evaluated in a cross-feature manner. Extensive experiments on self-defined datasets have demonstrated the effectiveness of our method. Finally, applications illustrate the usages in various aspects of 2D cartoon industry.\nTitle:\nRetrieval-based cartoon gesture recognition and applications via semi-supervised heterogeneous classifiers learning\n\nAbstract:\nIn this paper, we propose an approach to synthesize cartoons from the existing cartoon data by controlling the character's path which is defined by the cartoonists in a background image. First, detailed pre-experiments are conducted in which different cartoon features are extracted and compared. During the pre-experiments, three features extracted from edge, motion and color are demonstrated effectively for evaluating cartoon similarity according to the quantitative analysis. The three features are then fused and a Cartoon Frame Relationship Network is constructed. Based on the graph, we propose a Constrained Spreading Activation Algorithm to select candidate frames which are visually similar to the current frame to generate the next frame. The cartoons are synthesized by choosing the most appropriate frame from the candidates in accordance with the path designed by the cartoonists. When the new cartoons are applied into the background image, our approach coordinates the cartoon character's size according to the image's perspective as well. The experiment results demonstrate that the combination of the three proposed features are effective in similarity evaluation, and the candidates selected by Constrained Spreading Activation Algorithm, are more similar to the current frame compared with other algorithms. The results also show that our approach can synthesize visually smooth cartoons from the existing cartoon library.\nTitle:\nCartoon synthesis using constrained spreading activation network\n\nAbstract:\nIn this paper, we propose a novel approach, which reuses Traditional Chinese Cartoon to create new animations. In order to extract the cartoon character precisely, a segmentation method based on edge detection is implemented. Before reusing the data, a lower- dimensional space of the cartoon data is constructed by ISOmap. The character's gesture difference calculated by optical flow is combined with character's edge difference through a novel distance function, which is controlled by a weight parameter. The animation is created by reordering the existing data into a sequence, which is the shortest path between two designated data in the space. Our approach utilizes image processing, computer vision, and machine learning in cartoon creation and the experiment results demonstrate that the animation's quality can be effectively improved by the fusion of these techniques. Copyright \u00a9 2007 John Wiley & Sons, Ltd.\nTitle:\nAdaptive control in cartoon data reusing\n\nAbstract:\nCartoons play important roles in many areas, but it requires a lot of labor to produce new cartoon clips. In this paper, we propose a gesture recognition method for cartoon character images with two applications, namely content-based cartoon image retrieval and cartoon clip synthesis. We first define Edge Features (EF) and Motion Direction Features (MDF) for cartoon character images. The features are classified into two different groups, namely intra-features and inter-features. An Unsupervised Bi-Distance Metric Learning (UBDML) algorithm is proposed to recognize the gestures of cartoon character images. Different from the previous research efforts on distance metric learning, UBDML learns the optimal distance metric from the heterogeneous distance metrics derived from intra-features and inter-features. Content-based cartoon character image retrieval and cartoon clip synthesis can be carried out based on the distance metric learned by UBDML. Experiments show that the cartoon character image retrieval has a high precision and that the cartoon clip synthesis can be carried out efficiently.\nTitle:\nRetrieval based interactive cartoon synthesis via unsupervised bi-distance metric learning\n\nAbstract:\nIn this paper, we propose a new method to recognize gestures of cartoon images with two practical applications, i.e., content-based cartoon image retrieval and interactive cartoon clip synthesis. Upon analyzing the unique properties of four types of features including global color histogram, local color histogram (LCH), edge feature (EF), and motion direction feature (MDF), we propose to employ different features for different purposes and in various phases. We use EF to define a graph and then refine its local structure by LCH. Based on this graph, we adopt a transductive learning algorithm to construct local patches for each cartoon image. A spectral method is then proposed to optimize the local structure of each patch and then align these patches globally. MDF is fused with EF and LCH and a cartoon gesture space is constructed for cartoon image gesture recognition. We apply the proposed method to content-based cartoon image retrieval and interactive cartoon clip synthesis. The experiments demonstrate the effectiveness of our method.\nTitle:\nRecognizing Cartoon Image Gestures for Retrieval and Interactive Cartoon Clip Synthesis\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.908, mean value for all the writers is 8.648, which makes it 0.222 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nWe present a complete framework for synthesizing style-preserving 2D cartoons by learning from traditional Chinese cartoons. In contrast to reusing-based approaches which rely on rearranging or retrieving existing cartoon sequences, we aim to generate stylized cartoons with the idea of style factorization. Specifically, starting with 2D skeleton features of cartoon characters extracted by an improved rotoscoping system, we present a non-negative style factorization (NNSF) algorithm to obtain style basis and weights and simultaneously preserve class separability. Thus, factorized style basis can be combined with heterogeneous weights to re-synthesize style-preserving features, and then these features are used as the driving source in the character reshaping process via our proposed subkey-driving strategy. Extensive experiments and examples demonstrate the effectiveness of the proposed framework.\nTitle:", "model_inf_time": 1.89}, {"id": "41496", "output": "Multi-Round Conferencing for Diversity-Multiplexing Tradeoff in FDD and TDD Systems", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nMost communication systems use some form of feedback, often related to channel state information. The common models used in analyses either assume perfect channel state information at the receiver and/or noiseless state feedback links. However, in practical systems, neither is the channel estimate known perfectly at the receiver and nor is the feedback link perfect. In this paper, we study the achievable diversity multiplexing tradeoff using i.i.d. Gaussian codebooks, considering the errors in training the receiver and the errors in the feedback link for frequency division duplex (FDD) systems, where the forward and the feedback are independent multiple input multiple output (MIMO) channels. Our key result is that the maximum diversity order with one-bit of feedback information is identical to systems with more feedback bits. Thus, asymptotically in SNR, more than one bit of feedback does not improve the system performance at constant rates. Furthermore, the one-bit diversity-multiplexing performance is identical to the system which has perfect channel state information at the receiver along with noiseless feedback link. This achievability uses novel concepts of power controlled feedback and training, which naturally surface when we consider imperfect channel estimation and noisy feedback links. In the process of evaluating the proposed training and feedback protocols, we find an asymptotic expression for the joint probability of the SNR exponents of eigenvalues of the actual channel and the estimated channel which may be of independent interest.\nTitle:\nPower-controlled feedback and training for two-way MIMO channels\n\nAbstract:\nAbstract\u2014Two distinct models of feedback, suited for FDD (Frequency Division Duplex) and,TDD (Frequency Division Duplex) systems respectively, have been widely studied in the literature. In this paper, we compare these two models of feedback in terms of the diversity multiplexing,tradeoff for varying amount of channel state information at the terminals. We find that, when all imperfections are accounted for, the maximum achievable diversity order,in FDD systems,matches,the diversity order in TDD systems. TDD systems,achieve,better diversity order at higher multiplexing gains. In FDD systems, the maximum diversity order can be achieved,with just a single bit of feedback. Additional bits of feedback,(perfect or imperfect) do not affect the diversity order,if the receiver does not know,the channel state information.\nTitle:\nTwo Models for Noisy Feedback in MIMO Channels\n\nAbstract:\nIn this paper, we analyze the asymptotic performance of multiple antenna channels where the transmitter has either perfect or finite bit channel state information. Using the diversity- multiplexing tradeoff to characterize the system performance, we demonstrate that channel feed- back can fundamentally change the system behavior. Even one-bit of information can increase the diversity order of the system compared to the system with no transmitter information. In addition, as the amount of channel information at the transmitter increases, the diversity order for each multiplexing gain increases and goes to infinity for perfect transmitter information. The major reason for diversity order gain is a \"location-dependent\" temporal power control, which adapts the power control strategy based on the average channel conditions of the channel.\nTitle:\nOn the Asymptotic Performance of Multiple Antenna Channels with Fast Channel Feedback\n\nAbstract:\nTransmitter side information enables techniques such as beamforming, power control, and rate control in fading channels. It is commonly accepted in the literature that the addition of transmitter information (CSIT) to receiver information (CSIR) provides better performance than receiver information alone. In this work, we examine the performance of a symmetric, single-input, multiple-output (SIMO) channel in which CSIT is acquired through the use of training symbols, and we have a genie-aided receiver. We give a closed form expression for outage probability at high SNR while accounting for the resources consumed by training. We also analyze the diversity-multiplexing tradeoff and find that, though the diversity falls far below that of systems with perfect CSIT, it is still sufficiently superior to that achieved by CSIR-only systems to justify the cost of training. We show that, at zero multiplexing, transmitter training doubles the diversity order of a CSIR-only system and offers nonzero diversity at all achievable multiplexing gains.\nTitle:\nThe Case For Transmitter Training\n\nAbstract:\nWe consider a multiple antenna system with finite rate feedback, in which the quantized channel state information at the transmitter is used solely for temporal power control. We show that similar to systems without feedback, the tradeoff between diversity order and multiplexing gain exists. However, unlike the systems with feedback that apply both rate and power control, systems with only power control are unable of achieving non-zero diversity order at the maximum multiplexing gain. The analysis is based on asymptotic behavior of the distribution of order statistics of the eigenvalues of channel matrix, which is a key step in evaluating the diversity order.\nTitle:\nAchievable diversity and multiplexing in multiple antenna systems with quantized power control.\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 8.91, mean value for all the writers is 8.648, which makes it 0.224 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nMost communication systems use some form of feedback, often related to channel state information. In this paper, we study diversity multiplexing tradeoff for both frequency division duplex (FDD) and time division duplex (TDD) systems, when both receiver and transmitter knowledge about the channel is noisy and potentially mismatched. For FDD systems, we first extend the achievable tradeoff region for 1.5 rounds of message passing to get higher diversity compared to the best known scheme, in the regime of higher multiplexing gains. We then break the mold of all current channel state based protocols by using multiple rounds of conferencing to extract more bits about the actual channel. This iterative refinement of the channel increases the diversity order with every round of communication. The protocols are on-demand in nature, using high powers for training and feedback only when the channel is in poor states. The key result is that the diversity multiplexing tradeoff with perfect training and K levels of perfect feedback can be achieved, even when there are errors in training the receiver and errors in the feedback link, with a multiround protocol which has K rounds of training and K-1 rounds of binary feedback. The above result can be viewed as a generalization of Zheng and Tse, and Aggarwal and Sabharwal, where the result was shown to hold for K=1 and K=2 , respectively. For TDD systems, we also develop new achievable strategies with multiple rounds of communication between the transmitter and the receiver, which use the reciprocity of the forward and the feedback channel. The multiround TDD protocol achieves a diversity-multiplexing tradeoff which uniformly dominates its FDD counterparts, where no channel reciprocity is available.\nTitle:", "model_inf_time": 2.23}, {"id": "41497", "output": "Single-step Direct Message Transformation for Optimized H.245 Processing in 3G-324M", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper describes an object-oriented design and efficient implementation of 3G-324M protocol stack for real-time multimedia transmission. In particular, we discuss the implementations of 324M class hierarchical structure that includes classes H.245 (control) and H.223 (multiplexing) protocols. Our implementation is efficient and has been tested in a realistic 3G infrastructures in Hong Kong as well as in some China industries for optimizations of processing and transmission of real-time video, audio and data.\nTitle:\nObject-oriented design and implementations of 3g-324m protocol stack\n\nAbstract:\nIn this paper, we present the design and implementation details of an efficient and robust multimedia gateway which enables ubiquitous multimedia wireless communication. Primarily, we studied current technological trend and realized that in order to support various network technologies and to handle high call traffic conditions, a robust and efficient gateway is required for the universal adaptability and transcoding. To achieve the performance improvement of the gateway, we focus on efficiently implementing the H.245, which is the main call handling and signaling protocol. In our method, sets of H.245 messages are compiled, used and stored in a table so as to be reused for further incoming calls. After implementing this procedure, we have successfully tested our 3G-IP gateway that is robust enough for smoothly handling up to one million concurrent calls. It is also experimentally verified that our gateway provides the feature of invariant call setup time even in high traffic condition. As ubiquitous communications among the heterogeneous networks are in demanding today and our gateway will play the key role in this area.\nTitle:\nDesign of an efficient and robust multimedia gateway for pervasive communication.\n\nAbstract:\nIn order to support real-time video, audio and data communication among heterogeneous third generation (3G) handsets, 3G phones/terminals are required to support 3G-324M, the multimedia transmission protocol stack for 3G communication. This paper discusses some efficient approaches and experiences in the implementation of 3G-324M protocol stack. Specifically, we discuss: (1) event-driven approach for the overall information exchange; (2) single-step direct message transformation for the optimization of tree-structured message processing and (3) serialization of nested multiplex table entries in multiplexing/demultiplexing processing. Our implementation has been tested in a realistic heterogeneous 3G communication environment for transmission of real-time video, audio and data and its performance is satisfactory.\nTitle:\nEfficient implementation of 3G-324M protocol stack for multimedia communication\n\nAbstract:\nThis paper discusses an efficient design and implementation of control and multiplexing protocols H.245 and H.223, which is an important part of 3G324M protocol stack, for mobile wireless video conferencing. Our implementations aim to support the multipoint video conferencing with the capability of transmitting/receiving multiple video/audio streams simultaneously. Conference managements such as admission control, video/audio channel management are also discussed. As a result, the implementation improves efficiency and makes the conference more convenient to set up and to operate. Our prototype system is stable and its performance is satisfactory. Index Terms - 3G, 3G324M, H.245, H.223, Low bit Rate, Video-Conferencing\nTitle:\nEfficient 3G324M protocol Implementation for Low Bit Rate Multipoint Video Conferencing\n\nAbstract:\nThis paper discusses an efficient implementation of the multiplexing protocol H.223, which is an important part of 3G-324M protocol stack required for 3G mobile multimedia communications. Our implementation of the protocol aims to support the multi-point video conferencing with the capability of transmitting/receiving multiple video/audio streams simultaneously. Conference managements such as admission and audio channel scheduling are also discussed. As a result, the implementation improves efficiency and makes the conference more convenient to set up and operate. Our prototype system is stable and its performance is satisfactory.\nTitle:\nEfficient multiplexing protocol for low bit rate multi-point video conferencing\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.421, mean value for all the writers is 8.648, which makes it 0.66 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\n3G-324M is a multimedia transmission protocol designed for 3G communication environment. Meanwhile H.245 standard is a control protocol in 3G-324M and gives specific descriptions about terminal information messages in H.245 control channel as well as the procedures using them. The message syntax is defined using an external data representation standard called Abstract Syntax Notation One (ASN.1). For transmission, ASN.1 formatted data is transformed into bit-stream based on an ASN.1 encoding standard called Packed Encoding Rules (PER). In order to meet the requirement of high speed data transfer in 3G communication, it is important to design the procedure of message processing as simple as possible. In this paper, we propose Single-step Direct Message Transformation (SDMT)for the optimization of tree-structured message processing in H.245 module. By testing in realistic environments in some China industries, performance evaluation shows that code redundancies in terms of file size and code size are reduced significantly.\nTitle:", "model_inf_time": 2.29}, {"id": "41498", "output": "Minimizing Charging Vehicle Deployment in Wireless Rechargeable Sensor Networks", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nWireless energy transfer is a promising technology to prolong the lifetime of wireless sensor networks (WSNs), by employing charging vehicles to replenish energy to lifetime-critical sensors. Existing studies on sensor charging assumed that one or multiple charging vehicles being deployed. Such an assumption may have its limitation for a real sensor network. On one hand, it usually is insufficient to employ just one vehicle to charge many sensors in a large-scale sensor network due to the limited charging capacity of the vehicle or energy expirations of some sensors prior to the arrival of the charging vehicle. On the other hand, although the employment of multiple vehicles can significantly improve the charging capability, it is too costly in terms of the initial investment and maintenance costs on these vehicles. In this paper, we propose a novel charging model that a charging vehicle can carry multiple low-cost removable chargers and each charger is powered by a portable high-volume battery. When there are energy-critical sensors to be charged, the vehicle can carry the chargers to charge multiple sensors simultaneously, by placing one portable charger in the vicinity of one sensor. Under this novel charging model, we study the scheduling problem of the charging vehicle so that both the dead duration of sensors and the total travel distance of the mobile vehicle per tour are minimized. Since this problem is NP-hard, we instead propose a (3+\u03f5)-approximation algorithm if the residual lifetime of each sensor can be ignored; otherwise, we devise a novel heuristic algorithm, where \u03f5 is a given constant with 0 < \u03f5 \u2264 1. Finally, we evaluate the performance of the proposed algorithms through experimental simulations. Experimental results show that the performance of the proposed algorithms are very promising.\nTitle:\nImproving charging capacity for wireless sensor networks by deploying one mobile vehicle with multiple removable chargers.\n\nAbstract:\nCamera Sensor Networks (CSNs) has been deployed for many applications of video surveillance. In urban road or large buildings, camera sensors are deployed in public places for social security, and target tracking is a typical application of CSNs. Recently, development in electronic technology and image sensors makes it possible to deploy mobile cameras in CSNs. Due to the constraints of limited sensing range, it is necessary for mobile cameras to cooperate with each other to improve tracking quality and reduce moving distance. In this paper, a cooperative relay tracking problem is proposed and proved to be NP-hard. For single target tracking, we design a cooperative relay algorithm, which can shorten the total moving distance of cameras and ensure a continuous tracking for mobile target. Moreover, different from single target tracking problem, we design a genetic algorithm to obtain an available schedule for tracking multiple targets. The effectiveness of the proposed methods are validated by extensive simulation results, which shows that the energy consumed by cameras can be obviously reduced while guaranteeing continuous tracking.\nTitle:\nEnergy-efficient relay tracking with multiple mobile camera sensors.\n\nAbstract:\nIn WSNs (Wireless Sensor Networks), data storage and retrieval is a challenging problem because of the limited resource and the short communication radius of the sensor nodes. Most of the existed schemes choose one or more static sensor nodes or Sinks to act as the rendezvous nodes, which can be seen as the connectors between the data producers and the data consumers. However, those schemes cannot avoid both the \u201chot spot\u201d problem and the \u201cbottleneck\u201d problem, which refer to the much higher load balance of the sensor nodes around the rendezvous nodes. Moreover, most of the existing schemes never consider the dynamic nature of WSNs, which leads to the lack of adaptability. In this paper, we propose a novel dynamic-optimization-based framework named SRMSN using mobile Sinks to solve such a problem. SRMSN utilizes two heuristic methods, which are based on the virtual-grid-division technology and the diversity-factor-analysis technology, to determine the optimal target locations of the mobile Sinks in each time interval when each Sink node stay at a certain position and the optimal length of each of the time intervals adaptively, aiming at improving the adaptability and the efficiency of WSNs on data storage and retrieval. Simulation results show that SRMSN can reduce and balance the energy consumption greatly as well as decrease the average delay of data storage and retrieval in comparison with the state-of-the-art scheme on data storage and retrieval in WSNs.\nTitle:\nAchieve Adaptive Data Storage and Retrieval Using Mobile Sinks in Wireless Sensor Networks.\n\nAbstract:\nTracking mobile targets is one of the most important applications in wireless sensor networks (WSNs). Traditional tracking solutions are based on fixed sensor nodes and have two critical problems. First, in WSNs, the energy constraint is a main concern, but due to the mobility of targets, lots of sensor nodes in WSNs have to switch between active and sleep states frequently, which causes excessive energy consumption. Second, when there are holes in the deployment area, targets may fail to be detected while moving in the holes. To solve these problems, this paper exploits a few of mobile sensor nodes to continuously track mobile targets because the energy capacity of mobile nodes is less constrained. Based on a realistic detection model, a solution for scheduling mobile nodes to cooperate with ordinary fixed nodes is proposed. When targets move, mobile nodes move along with them for tracking. The results of extensive simulations show that mobile nodes help to track the target when holes appears in the coverage area and extend the effective monitoring time. Moreover, the proposed solution can effectively reduce the energy consumption of sensor nodes and prolong the lifetime of the networks.\nTitle:\nContinuous tracking for mobile targets with mobility nodes in WSNs\n\nAbstract:\nMobile sinks can be used to balance energy consumption for sensor nodes in Wireless Sensor Networks (WSNs). Mobile sinks are required to inform sensor nodes about their new location information whenever necessary. However, frequent location updates from mobile sinks can lead to both rapid energy consumption of sensor nodes and increased collisions in wireless transmissions. We propose a new solution with adaptive location updates for mobile sinks to resolve this problem. When a sink moves, it only needs to broadcast its location information within a local area other than among the entire network. Both theoretical analysis and simulation studies show that this solution consumes less energy in each sensor node and also decreases collisions in wireless transmissions, which can be used in large-scale WSNs.\nTitle:\nAdaptive location updates for mobile sinks in wireless sensor networks\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 10.265, mean value for all the writers is 8.648, which makes it 1.38 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe very limited sensor battery energy greatly hinders the large-scale, long-term deployments of wireless sensor networks. This paper studies the problem of scheduling the minimum charging vehicles to charge lifetime-critical sensors in a wireless rechargeable sensor network, by utilizing the breakthrough wireless charging technology. Existing studies still employ a number of charging vehicles to charge sensors. The purchase cost of a charging vehicle however is not inexpensive. To further reduce the number of employed charging vehicles, we propose a novel approximation algorithm, by exploring the combinatorial properties of the problem. The techniques exploited in this paper are essentially different from that in existing studies. Not only do we show that the approximation ratio of the proposed algorithm is much better than that of the state-of-the-art, but also extensive experimental results demonstrate that the number of scheduled charging vehicles by the proposed algorithm is at least 10% less than that by the existing algorithms and the total travel energy consumption of the charging vehicles is also smaller than that by the existing algorithms.\nTitle:", "model_inf_time": 1.7}, {"id": "41499", "output": "Attacks and Countermeasures Against SSL Protected Web Servers", "prompt": "You are a scholar that generates titles for abstracts. You will be provided a set of features to help you understand your writing style.\nFirst feature you will receive is similar abstract-title pairs from your past works:\n<SimilarPairs>\nAbstract:\nThis paper analyzes the problems within current anti-spoofing mechanisms and proposes a new SSL protected trust model. Then, this paper describes the attacks on SSL protected trust model. This paper also proposes the new Automatic Detecting Security Indicator (ADSI) scheme to defend against spoofing attacks on SSL protected Web servers. This paper describes the ADSI-based trust model. In a secure transaction, ADSI may randomly generate a picture and embed it into the current Web browser. This can be triggered by any security relevant events occurred on the browser, and then performs automatic checking on current active security status. When a mismatch of embedded images is detected, an alarm goes off to alert the users. Since an adversary is hard to replace or mimic the randomly generated picture, the Web-spoofing attack can not be mounted easily. In comparison with existing proposals, the proposed scheme has the following advantages: (1) weak security assumption and very low burden on the customer by automating the process of detection and recognition of the Web-spoofing for SSL-enabled communications, (2) little intrusive on the browser, and (3) easy implementation in trusted PC at Internet Cafe requiring neither Logo Certification Authority, nor the scheme of personalization.\nTitle:\nAttacks vs. Countermeasures of SSL Protected Trust Model\n\nAbstract:\nWith the development of smart terminals and mobile social networks, users can find potential friends who have similar interests by sharing personal attribute profile in mobile social networks (MSN). However, the personal attribute profile usually contains sensitive information, and if this information is captured by attackers, it may cause unexpected consequences. In this paper, we propose a privacy-preserving matching scheme which is based on both identity authentication and key agreement. The scheme relies on trusted third party which has powerful computation ability and can reduce the workload on intelligent terminal. Moreover, the scheme uses encryption and authentication techniques to guarantee that the attacker fails to get the real information of user's attribute profile, so the personal privacy can be protected during friend matching process. Security analysis shows that the proposed scheme can protect the user's privacy. The simulation result shows that the scheme is more efficient than existing works.\nTitle:\nPseudo Anonymous and Hidden Attribute Comparison Based on Quick Friend Matching in Mobile Social Networks\n\nAbstract:\nWith the popularity of Sensor-Cloud, its security issues get more attention from industry and academia. Especially, Sensor-Cloud underlying network is very vulnerable to internal attacks due to its limitations in computing, storage, and analysis. Most existing trust evaluation mechanisms are proposed to detect internal attack issues from the behavior level. However, there are some special internal attacks in the data level such as hidden data attacks, which are normal in the behavior level but generate malicious data to lead user to make wrong decisions. To detect this type of attacks, we design a fog-based detection system (FDS), which is based on the trust evaluation mechanism in the behavior level. In this paper, three types of scenes (the redundant data, the parameter curve characteristic, and the data validation) are defined, and three detection schemes are given. Some experiments are conducted, which manifest that FDS has certain advantages in detecting hidden data attacks.\nTitle:\nDetection Of Hidden Data Attacks Combined Fog Computing And Trust Evaluation Method In Sensor-Cloud System\n\nAbstract:\nAs more and more services and applications are emerging in the Internet, exposing user sensitive data in the Internet becomes more easily. The simplest way to protect the security of sensitive user data is to encrypt the data in advance, and then disclose the data decryption key only to those authorized users. However, the sensitive user data will be leaked while the decryption key is exposed to unauthorized users. In this paper, we propose a secure self-destructing scheme for electronic data (SSDD for short). We achieve this goal by first encrypting the data, and then distributing both the decryption key and a part of the cipher text into the distributed hash table (DHT) network. By security analysis, we show that our SSDD scheme can resist against not only the traditional cryptanalysis and the brute-force attacks, but also the attacks in the DHT network, such as the store sniffing attack, the lookup sniffing attack, and the standard DHT attacks.\nTitle:\nA Secure Self-Destructing Scheme for Electronic Data\n\nAbstract:\nAbstractThe wireless channels between tags and readers are not secure in Internet of Things. Before a reader processes packets from tags, the reader should detect the integrity of these packets. In existing research, these packets are detected by readers one by one. This process is time-consuming and impractical under some real-time scenarios. To decrease the detection time, we propose a fast radio-frequency identification batch detection protocol that is more suitable to detect whether the packets are modified by adversaries. The analysis and experimental results show that the proposed protocol has the following good properties: i it can detect the integrity of packets very fast; ii it has better performance in computational cost and storage cost of tags and servers; iii it can provide security protection in an efficient way. Copyright \u00a9 2013 John Wiley & Sons, Ltd.\nTitle:\nAn aggregated signature-based fast RFID batch detection protocol\n\n</SimilarPairs>\nNow you will receive features shedding light into how you use words and formulates sentence, compared to other writers:\n-The number of words the writer uses in a sentence on average is 9.797, mean value for all the writers is 8.648, which makes it 0.98 standard deviations away from the mean.\nUsing the features, generate the proper title. If you haven't received any features besides similar pairs, only make use of them.\nOnly output the title and nothing else.\nAbstract:\nThe anti-spoofing community has been intensively proposing new methods for defending against new web-spoofing techniques. In this paper, we analyze the problems within current anti-spoofing mechanisms, and propose a new SSL protected trust model. Then, we describe the attacks on SSL protected trusted communication. In this paper, we also propose the new Automatic Detecting Security Indicator scheme (ADSI) to defend against spoofing attacks on SSL protected web servers. In a secure transaction, ADSI will randomly choose a picture and embed it into the current web browser at a random place. This can be triggered by any security relevant event that has occurred on the browser, and then automatic checking will be performed on the current active security status. When a mismatch of embedded pictures is detected, an alarm goes off to alert the users. Since an adversary is hard to replace or mimic the randomly embedded picture, the web-spoofing attack cannot be mounted easily. In comparison with existing schemes, (1) the proposed scheme has the weakest security assumption, and places a very low burden on the user by automating the process of detection and recognition of web-spoofing for SSL-enabled trusted communication; (2) it has little intrusiveness on the browser; and (3) it can be implemented in a trusted PC at an Internet Cafe. Copyright (C) 2009 John Wiley & Sons, Ltd.\nTitle:", "model_inf_time": 1.58}]}